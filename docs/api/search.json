[{"title":"JS数组操作","slug":"JS数组操作","url":"/blog/posts/4cd9fba2c490/","content":"\n## 常用操作\n\n### 声明数组\n\n```js\nconst fruits = new Array('Apple', 'Banana');\nconsole.log(fruits.length);\n\n// 通过数组字面量创建一个有2个元素的'fruits'数组.\nconst fruits = ['Apple', 'Banana'];\nconsole.log(fruits.length);\n```\n\n### 数组去重\n\n```js\n// 方案一：Set + ...\nfunction noRepeat(arr) {\n  return [...new Set(arr)];\n}\nnoRepeat([1, 2, 3, 1, 2, 3]);\n\n// 方案二：Set + Array.from\nfunction noRepeat(arr) {\n  return Array.from(new Set(arr));\n}\nnoRepeat([1, 2, 3, 1, 2, 3]);\n\n// 方案三：双重遍历比对下标\nfunction noRepeat(arr) {\n  return arr.filter((v, idx) => idx == arr.lastIndexOf(v));\n}\nnoRepeat([1, 2, 3, 1, 2, 3]);\n\n// 方案四：单遍历 + Object特性\n// Object的特性是Key不会重复。\n// 这里使用values是因为可以保留类型，keys会变成字符串。\nfunction noRepeat(arr) {\n  return Object.values(\n    arr.reduce((s, n) => {\n      s[n] = n;\n      return s;\n    }, {})\n  );\n}\nnoRepeat([1, 2, 3, 1, 2, 3]);\nfunction uniqueArr(arr){\n    return [...new Set(arr)]\n}\n\n// 数组对象根据字段去重\n// 参数：arr:要去重的数组 key:根据去重的字段名\nexport const uniqueArrayObject = (arr = [], key = 'id') => {\n    if (arr.length === 0) return\n    let list = []\n    const map = {}\n    arr.forEach((item) => {\n        if (!map[item[key]]) {\n            map[item[key]] = item\n        }\n    })\n    list = Object.values(map)\n\n    return list\n}\n\n// 示例：\nconst responseList = [\n    { id: 1, name: '树哥' },\n    { id: 2, name: '黄老爷' },\n    { id: 3, name: '张麻子' },\n    { id: 1, name: '黄老爷' },\n    { id: 2, name: '张麻子' },\n    { id: 3, name: '树哥' },\n    { id: 1, name: '树哥' },\n    { id: 2, name: '黄老爷' },\n    { id: 3, name: '张麻子' },\n]\nuniqueArrayObject(responseList, 'id')\n// [{ id: 1, name: '树哥' },{ id: 2, name: '黄老爷' },{ id: 3, name: '张麻子' }]\n\n// 提取唯一值 我们可以使用Set对象和Spread运算符，创建一个剔除重复值的新数组。\nvar entries = [1, 2, 2, 3, 4, 5, 6, 6, 7, 7, 8, 4, 2, 1]\nvar unique_entries = [...new Set(entries)];\nconsole.log(unique_entries);// [1, 2, 3, 4, 5, 6, 7, 8]\n```\n\n### 返回两个数组中相同的元素\n\n```js\n// 方案一：filter + includes\nfunction intersection(arr1, arr2) {\n  return arr2.filter((v) => arr1.includes(v));\n}\nintersection([1, 2, 3], [3, 4, 5, 2]);\n\n// 方案二：同理变种用 hash\nfunction intersection(arr1, arr2) {\n  var set = new Set(arr2);\n  return arr1.filter((v) => set.has(v));\n}\nintersection([1, 2, 3], [3, 4, 5, 2]);\n```\n\n### 对比两个数组并且返回其中不同的元素\n\n```js\n// 方案一：filter + includes\n// 他原文有问题，以下方法的4,5没有返回\n\nfunction diffrence(arrA, arrB) {\n  return arrA.filter((v) => !arrB.includes(v));\n}\ndiffrence([1, 2, 3], [3, 4, 5, 2]);\n// 需要再操作一遍\nfunction diffrence(arrA, arrB) {\n  return arrA\n    .filter((v) => !arrB.includes(v))\n    .concat(arrB.filter((v) => !arrA.includes(v)));\n}\ndiffrence([1, 2, 3], [3, 4, 5, 2]);\n\n// 方案二：hash + 遍历\n// 算是方案1的变种吧，优化了includes的性能。\n```\n\n### 检查数组中某元素出现的次数\n\n```js\n// 方案一：reduce\nfunction countOccurrences(arr, value) {\n  return arr.reduce((a, v) => (v === value ? a + 1 : a + 0), 0);\n}\ncountOccurrences([1, 2, 3, 4, 5, 1, 2, 1, 2, 3], 1);\n\n// 方案二：filter\nfunction countOccurrences(arr, value) {\n  return arr.filter((v) => v === value).length;\n}\ncountOccurrences([1, 2, 3, 4, 5, 1, 2, 1, 2, 3], 1);\n```\n\n### 删除数组重复项\n\n```js\nconst numberArrays = [undefined,Infinity,\n  12,NaN,false,5,7,null,12,false,5,undefined,89,9,\n  null,Infinity,5, NaN];\nconst objArrays = [{ id: 1 }, { id: 4 }, { id: 1 }, { id: 5 }, { id: 4 }];\nconsole.log(\n  // prints [undefined, Infinity, 12, NaN, false, 5, 7, null, 89, 9]\n  Array.from(new Set(numberArrays)),\n  // prints [{id: 1}, {id: 4}, {id: 1}, {id: 5}, {id: 4}]\n  // nothing changes because even though the ids repeat in some objects\n  // the objects are different instances, different objects\n  Array.from(new Set(objArrays))\n);\nconst idSet = new Set();\nconsole.log(\n  // prints [{id: 1}, {id: 4}, {id: 5}] using id to track id uniqueness\n  objArrays.filter((obj) => {\n    const existingId = idSet.has(obj.id);\n    idSet.add(obj.id);\n    return !existingId;\n  })\n);\n```\n\n### 查找数组最大\n\n```js\n// 方案一：Math.max + ...\nfunction arrayMax(arr) {\n  return Math.max(...arr);\n}\narrayMax([-1, -4, 5, 2, 0]);\n\n// 方案二：Math.max + apply\nfunction arrayMax(arr) {\n  return Math.max.apply(Math, arr);\n}\narrayMax([-1, -4, 5, 2, 0]);\n\n// 方案三：Math.max + 遍历\nfunction arrayMax(arr) {\n  return arr.reduce((s, n) => Math.max(s, n));\n}\narrayMax([-1, -4, 5, 2, 0]);\n\n// 方案四：比较、条件运算法 + 遍历\nfunction arrayMax(arr) {\n  return arr.reduce((s, n) => (s > n ? s : n));\n}\narrayMax([-1, -4, 5, 2, 0]);\n\n// 方案五：排序\nfunction arrayMax(arr) {\n  return arr.sort((n, m) => m - n)[0];\n}\narrayMax([-1, -4, 5, 2, 0]);\n\n```\n### 查找数组最小\n\n```js\n// Math.max换成Math.min\n// s>n?s:n换成s<n?s:n\n// (n,m)=>m-n换成(n,m)=>n-m，或者直接取最后一个元素\n```\n\n### 返回以size为长度的数组分割的原数组\n\n```js\n// 方案一：Array.from + slice\nfunction chunk(arr, size = 1) {\n  return Array.from(\n    {\n      length: Math.ceil(arr.length / size),\n    },\n    (v, i) => arr.slice(i * size, i * size + size)\n  );\n}\nchunk([1, 2, 3, 4, 5, 6, 7, 8], 3);\n\n// 方案二：Array.from + splice\nfunction chunk(arr, size = 1) {\n  return Array.from(\n    {\n      length: Math.ceil(arr.length / size),\n    },\n    (v, i) => arr.splice(0, size)\n  );\n}\nchunk([1, 2, 3, 4, 5, 6, 7, 8], 3);\n\n// 方案三：遍历 + splice\nfunction chunk(arr, size = 1) {\n  var _returnArr = [];\n  while (arr.length) {\n    _returnArr.push(arr.splice(0, size));\n  }\n  return _returnArr;\n}\nchunk([1, 2, 3, 4, 5, 6, 7, 8], 3);\n```\n\n### 扁平化数组\n\n```js\n// 方案一：递归 + ...\nfunction flatten(arr, depth = -1) {\n  if (depth === -1) {\n    return [].concat(\n      ...arr.map((v) => (Array.isArray(v) ? this.flatten(v) : v))\n    );\n  }\n  if (depth === 1) {\n    return arr.reduce((a, v) => a.concat(v), []);\n  }\n  return arr.reduce(\n    (a, v) => a.concat(Array.isArray(v) ? this.flatten(v, depth - 1) : v),\n    []\n  );\n}\nflatten([1, [2, [3]]]);\n\n// 方案二：es6原生flat\nfunction flatten(arr, depth = Infinity) {\n  return arr.flat(depth);\n}\nflatten([1, [2, [3]]]);\n```\n\n### 从右删除n个元素\n```js\n// 方案一：slice\nfunction dropRight(arr, n = 0) {\n  return n < arr.length ? arr.slice(0, arr.length - n) : [];\n}\ndropRight([1, 2, 3, 4, 5], 2);\n\n// 方案二: splice\nfunction dropRight(arr, n = 0) {\n  return arr.splice(0, arr.length - n);\n}\ndropRight([1, 2, 3, 4, 5], 2);\n\n// 方案三: slice另一种\nfunction dropRight(arr, n = 0) {\n  return arr.slice(0, -n);\n}\ndropRight([1, 2, 3, 4, 5], 2);\n\n// 方案四: 修改length\nfunction dropRight(arr, n = 0) {\n  arr.length = Math.max(arr.length - n, 0);\n  return arr;\n}\ndropRight([1, 2, 3, 4, 5], 2);\n```\n\n### 截取第一个符合条件的元素及其以后的元素\n\n```js\n// 方案一：slice + 循环\nfunction dropElements(arr, fn) {\n  while (arr.length && !fn(arr[0])) arr = arr.slice(1);\n  return arr;\n}\ndropElements([1, 2, 3, 4, 5, 1, 2, 3], (v) => v == 2);\n\n// 方案二：findIndex + slice\nfunction dropElements(arr, fn) {\n  return arr.slice(Math.max(arr.findIndex(fn), 0));\n}\ndropElements([1, 2, 3, 4, 5, 1, 2, 3], (v) => v === 3);\n\n// 方案三：splice + 循环\nfunction dropElements(arr, fn) {\n  while (arr.length && !fn(arr[0])) arr.splice(0, 1);\n  return arr;\n}\ndropElements([1, 2, 3, 4, 5, 1, 2, 3], (v) => v == 2);\n```\n\n### 返回数组中下标间隔nth的元素\n\n```js\n// 方案一：filter\nfunction everyNth(arr, nth) {\n  return arr.filter((v, i) => i % nth === nth - 1);\n}\neveryNth([1, 2, 3, 4, 5, 6, 7, 8], 2);\n\n// 方案二：方案一修改判断条件\nfunction everyNth(arr, nth) {\n  return arr.filter((v, i) => (i + 1) % nth === 0);\n}\neveryNth([1, 2, 3, 4, 5, 6, 7, 8], 2);\n```\n\n### 返回数组中第n个元素（支持负数）\n\n```js\n// 方案一：slice\nfunction nthElement(arr, n = 0) {\n  return (n >= 0 ? arr.slice(n, n + 1) : arr.slice(n))[0];\n}\nnthElement([1, 2, 3, 4, 5], 0);\nnthElement([1, 2, 3, 4, 5], -1);\n\n// 方案二：三目运算符\nfunction nthElement(arr, n = 0) {\n  return n >= 0 ? arr[0] : arr[arr.length + n];\n}\nnthElement([1, 2, 3, 4, 5], 0);\nnthElement([1, 2, 3, 4, 5], -1);\n```\n\n### 返回数组头元素\n```js\n// 方案一：\nfunction head(arr) {\n  return arr[0];\n}\nhead([1, 2, 3, 4]);\n\n// 方案二：\nfunction head(arr) {\n  return arr.slice(0, 1)[0];\n}\nhead([1, 2, 3, 4]);\n```\n\n### 返回数组末尾元素\n\n```js\n// 方案一：\nfunction last(arr) {\n  return arr[arr.length - 1];\n}\n\n// 方案二：\nfunction last(arr) {\n  return arr.slice(-1)[0];\n}\nlast([1, 2, 3, 4, 5]);\n```\n\n### 数组乱排\n\n```js\n// 方案一：洗牌算法\nfunction shuffle(arr) {\n  let array = arr;\n  let index = array.length;\n\n  while (index) {\n    index -= 1;\n    let randomInedx = Math.floor(Math.random() * index);\n    let middleware = array[index];\n    array[index] = array[randomInedx];\n    array[randomInedx] = middleware;\n  }\n\n  return array;\n}\nshuffle([1, 2, 3, 4, 5]);\n\n/**\n * 方案二：sort + random\n */\nfunction shuffle(arr) {\n  return arr.sort((n, m) => Math.random() - 0.5);\n}\nshuffle([1, 2, 3, 4, 5]);\n```\n\n### 伪数组转换为数组\n```js\n// Array.from将伪数组变成数组，就是只要有length的属性就可以转成数组\nArray.from({ length: 2 });\nlet name = \"javascript\";\nconsole.log(name.length); // 10\nlet arr = Array.from(name);\nconsole.log(arr); // [ 'j', 'a', 'v', 'a', 's', 'c', 'r', 'i', 'p', 't' ]\n// prototype.slice\nArray.prototype.slice.call({ length: 2, 1: 1 });\n// prototype.splice\nArray.prototype.splice.call({ length: 2, 1: 1 }, 0);\n// Array.of()将一组值转换成数组，类似于声明数组\nlet arr = Array.of(10);\nlet arr2 = Array.of(\"hello\", \"world\");\nconsole.log(arr); // [ 10 ] \nconsole.log(arr2); // [ 'hello', 'world' ]\n```\n\n### 数组重排序\n\n```js\nconst shuffle = (arr) => arr.sort(() => Math.random() - 0.5)\nconst arr = [1, 2, 3, 4, 5]\nconsole.log(shuffle(arr))\n```\n\n### 数组随机打乱顺序\n\n通过0.5-Math.random()得到一个随机数，再通过两次sort排序打乱的更彻底,但是这个方法实际上并不够随机，如果是企业级运用，建议使用第二种洗牌算法\n\n```js\nshuffle(arr) {\n      return arr.sort(() => 0.5 - Math.random()). sort(() => 0.5 - Math.random());\n },\n\nfunction shuffle(arr) {\n  for (let i = arr.length - 1; i > 0; i--) {\n    const randomIndex = Math.floor(Math.random() * (i + 1))\n    ;[arr[i], arr[randomIndex]] = [arr[randomIndex], arr[i]]\n  }\n  return arr\n}\n```\n### 把数组最后一项移到第一项\n\n```js\nfunction (arr){\n    return arr.push(arr.shift());\n}\n```\n\n### 把数组的第一项放到最后一项\n\n```js\nfunction(arr){\n  return arr.unshift(arr.pop());\n}\n```\n\n### 各种数组克隆方法\n数组克隆的方法其实特别多了，看看有没有你没见过的！\n\n```js\nconst clone = (arr) => arr.slice(0);\nconst clone = (arr) => [...arr];\nconst clone = (arr) => Array.from(arr);\nconst clone = (arr) => arr.map((x) => x);\nconst clone = (arr) => JSON.parse(JSON.stringify(arr));\nconst clone = (arr) => arr.concat([]);\nconst clone = (arr) => structuredClone(arr);\n```\n\n### 交换数组值的位置\n```js\nconst array = [12, 24, 48];\nconst swap0ldway = (arr, i, j) => {\n  const arrayCopy = [...arr];\n  let temp = arayCopy[i];\n  arrayCopy[i] = arrayCopy[j];\n\n  arrayCopy[j] = temp;\n  return arrayCopy;\n};\n\nconst swapNewWay = (arr, i, j) => {\n  const arrayCopy = [...arr];\n  [arrayCopy[0], arrayCopy[2]] = [arrayCopy[2], arrayCopy[0]];\n  return arrayCopy;\n};\n\nconsole.log(swap0ldWay(array, 0, 2)); // outputs: [48, 24, 12]\nconsole.log(swapNewWay(array, 0, 2)); // outputs: [48, 24, 12]\n```\n\n### 随机排列数组中的元素\n```js\nvar my_list = [1, 2, 3, 4, 5, 6, 7, 8, 9];\nconsole.log(my_list.sort(function() {\n    return Math.random() - 0.5\n}));// [4, 8, 2, 9, 1, 3, 6, 5, 7]\n```\n### 展平多维数组\n```js\nvar entries = [1, [2, 5], [6, 7], 9];\nvar flat_entries = [].concat(...entries);// [1, 2, 5, 6, 7, 9]\n```\n\n### 使用length调整大小 / 清空数组\n```js\n// 如果我们要调整数组的大小：\nvar entries = [1, 2, 3, 4, 5, 6, 7];\nconsole.log(entries.length);// 7\nentries.length = 4;\nconsole.log(entries.length);// 4\nconsole.log(entries);// [1, 2, 3, 4]\n// 如果我们要清空数组：\nvar entries = [1, 2, 3, 4, 5, 6, 7];\nconsole.log(entries.length);// 7\nentries.length = 0;\nconsole.log(entries.length);// 0\nconsole.log(entries);// []\n```\n\n## 常用方法\n\n### 栈方法\n\n后进先出\n\n#### push()\n\n可以接收任意数量的参数，把它们逐个添加到数组末尾，并返回修改后数组的长度\n\n```js\nlet arr = [1, 2, 3];\narr.push(4);\nconsole.log(arr); // [ 1, 2, 3, 4 ]\nconsole.log(arr.length); // 4\n```\n\n#### pop()\n\n从数组末尾移除最后一项，减少数组的length值，然后返回移除的项\n\n```js\nlet arr = [1, 2, 3];\nlet delVal = arr.pop();\nconsole.log(arr); // [ 1, 2]\nconsole.log(delVal); // 3\n```\n\n### 队列方法\n\n先进先出\n\n#### shift()\n\n移除数组中的第一个项并返回该项，同时将数组长度减1\n\n```js\nlet arr = [1, 2, 3];\nlet delVal = arr.shift();\nconsole.log(delVal); // 1\nconsole.log(arr); // [ 2, 3 ]\nconsole.log(arr.length); // 2\n```\n\n#### unshift()\n\n在数组前端添加任意个项并返回新数组的长度\n\n```js\nlet arr = [1, 2, 3];\nlet arrLength = arr.unshift(0);\nconsole.log(arrLength); // 4\nconsole.log(arr); // [ 0, 1, 2, 3 ]\n```\n\n### 排序方法\n\n#### reverse()\n\n反转数组项的顺序\n\n```js\nlet arr = [1, 2, 3];\narr.reverse();\nconsole.log(arr); // [ 3, 2, 1 ]\n```\n\n#### sort()\n\n从小到大排序，但它的排序方法是根据数组转换字符串后来排序的\n\n```js\nlet arr = [1, 5, 10, 15];\nconsole.log(arr.sort()); // [ 1, 10, 15, 5 ] 原因：它们比较的是转换的字符串值\n// 从小到大排序\nconsole.log(arr.sort(compare)); // [ 1, 5, 10, 15 ]\nfunction compare(value1, value2) {\n  if (value1 < value2) {\n    return -1;\n  } else if (value1 > value2) {\n    return 1;\n  } else {\n    return 0;\n  }\n}\n```\n\n### 操作方法\n\n#### join()\n\nJavaScript数组中的join()方法是一个内置方法，通过连接数组的所有元素来创建并返回新字符串。join()方法将连接数组的项到字符串并返回该字符串。指定的分隔符用于分隔元素数组。默认分隔符是逗号(,)。\n\n```js\nconst elements = ['Fire', 'Air', 'Water'];\nconsole.log(elements.join());\n// expected output: \"Fire,Air,Water\"\nconsole.log(elements.join(''));\n// expected output: \"FireAirWater\"\nconsole.log(elements.join('-'));\n// expected output: \"Fire-Air-Water\"\n```\n\n#### concat()\n\n可以基于当前数组中的所有项创建一个新数组，不会影响原数组的值\n\n```js\nlet arr = [1, 2, 3];\nlet newArr = arr.concat([4, 5, 6], [7, 8, 9]);\nconsole.log(newArr); // [ 1, 2, 3, 4, 5, 6, 7, 8, 9 ]\nconsole.log(arr); // [1, 2, 3]\nconst array1 = ['a', 'b', 'c'];\nconst array2 = ['d', 'e', 'f'];\nconst array3 = array1.concat(array2);\n```\n\n#### slice()\n\n它能够基于当前数组中的一或多个项创建一个新数组\n\n- slice()方法可以接受一或两个参数，即要返回项的起始和结束位置\n- 在只有一个参数的情况下，slice()方法返回从该参数指定位置开始到当前数组末尾的所有项。\n- 如果有两个参数，该方法返回起始和结束位置之间的项——但不包括结束位置的项。\n- 注意，slice()方法不会影响原始数组\n\n```js\nlet arr = [1, 2, 3, 4];\nlet newArr = arr.slice(1, 2);\nconsole.log(newArr); // [ 2 ]\nlet newArr2 = arr.slice(1);\nconsole.log(newArr2); // [ 2, 3, 4 ]\n```\n\n#### splice()\n\n##### 删除\n\n可以删除任意数量的项，只需指定2个参数：要删除的第一项的位置和要删除的项数。例如，splice(0,2)会删除数组中的前两项。\n\n```js\nlet arr = [1, 2, 3, 4];\narr.splice(1, 2);\nconsole.log(arr); // [ 1, 4 ]\n```\n\n##### 插入\n\n可以向指定位置插入任意数量的项，只需提供3个参数：起始位置、0（要删除的项数）和要插入的项。如果要插入多个项，可以再传入第四、第五，以至任意多个项。例如，splice(2,0,“red”,“green”)会从当前数组的位置2开始插入字符串\"red\"和\"green\"。\n\n```js\nlet arr = [1, 2, 3, 4];\narr.splice(1, 0, \"java\", \"script\");\nconsole.log(arr); // [ 1, 'java', 'script', 2, 3, 4 ]\n```\n\n##### 替换\n\n可以向指定位置插入任意数量的项，且同时删除任意数量的项，只需指定3个参数：起始位置、要删除的项数和要插入的任意数量的项。插入的项数不必与删除的项数相等。例如，splice(2,1,“red”,“green”)会删除当前数组位置2的项，然后再从位置2开始插入字符串\"red\"和\"green\"。\n\n```js\nlet arr = [1, 2, 3, 4];\narr.splice(1, 1, \"java\", \"script\");\nconsole.log(arr); // [ 1, 'java', 'script', 3, 4 ]\n```\n\n#### arr.fill(target,start,end)\n\n使用给定的值，填充一个数组,ps:填充完后会改变原数组\n\n- target – 待填充的元素\n- start – 开始填充的位置-索引\n- end – 终止填充的位置-索引(不包括该位置)\n\n```js\nlet arr = [1, 2, 3, 4];\nlet arr2 = [5, 6, 7, 8];\n// 全部填充5\narr.fill(5);\nconsole.log(arr); // [ 5, 5, 5, 5 ]\n// 从索引为1到3填充9\narr2.fill(9, 1, 3);\nconsole.log(arr2); // [ 5, 9, 9, 8 ]\n```\n\n#### Array.isArray(arr)\n\n判断传入的值是否为数组\n\n```js\nlet arr = [];\nlet obj = {};\nconsole.log(Array.isArray(arr)); // true\nconsole.log(Array.isArray(obj)); // false\n```\n\n在当前数组内部，将制定位置的数组复制到其他位置，会覆盖原数组项，返回当前数组\n参数:\n\n- target --必选 索引从该位置开始替换数组项\n- start --可选 索引从该位置开始读取数组项，默认为0，如果为负值，则从右往左读。\n- end --可选 索引到该位置停止读取的数组项，默认是Array.length,如果是负值，表示倒数\n\n```js\nlet arr = [1, 2, 3, 4, 5, 6];\n\nconsole.log(arr.copyWithin(2, 0)); // [1, 2, 1, 2, 3, 4]\nconsole.log(arr.copyWithin(2, 0, 4)); // [ 1, 2, 1, 2, 1, 2 ]\n```\n\n### 位置方法\n\n#### at()\n\n```js\nconst array1 = [5, 12, 8, 130, 44];\nlet index = 2;\nconsole.log(Using an index of ${index} the item returned is ${array1.at(index)});\n// expected output: \"Using an index of 2 the item returned is 8\"\nindex = -2;\nconsole.log(Using an index of ${index} item returned is ${array1.at(index)});\n```\n\n#### indexOf()和lastIndexOf()\n\n这两个方法都接收两个参数：要查找的项和（可选的）表示查找起点位置的索引。其中，indexOf()方法从数组的开头（位置0）开始向后查找，lastIndexOf()方法则从数组的末尾开始向前查找。\n\n```js\nlet arr = [1, 2, 3, 2, 1];\n// 从0开始查询值为2的位置\nconsole.log(arr.indexOf(2)); // 1\n// 从索引为2开始查询值为2的位置\nconsole.log(arr.indexOf(2, 2)); // 3\n// 倒叙查询值为2的位置\nconsole.log(arr.lastIndexOf(2)); // 3\n// 倒叙查询值为2的位置\nconsole.log(arr.lastIndexOf(2, 2)); // 1\n```\n\n#### find()\n\n数组实例的find方法，用于找出第一个符合条件的数组成员。它的参数是一个回调函数，所有数组成员依次执行该回调函数，直到找出第一个返回值为true的成员，然后返回该成员。如果没有符合条件的成员，则返回undefined。\n\n```js\nlet arr = [1, 2, 3, 4, 5, 6];\nlet index = arr.find(val => val === 3);\nlet index2 = arr.find(val => val === 100);\nconsole.log(index); // 3\nconsole.log(index2); // undefined\nconst array1 = [5, 12, 8, 130, 44];\nconst found = array1.find(element => element > 10);\nconsole.log(found);\n```\n\n#### findIndex()\n\n和数组实例的findIndex方法的用法与find方法非常类似，返回第一个符合条件的数组成员的位置，如果所有成员都不符合条件，则返回-1。\n\n```js\nlet arr = [1, 2, 3, 4, 5, 6];\nlet index = arr.findIndex(val => val === 3);\nlet index2 = arr.findIndex(val => val === 100);\n\nconsole.log(index); // 2\nconsole.log(index2); // -1\n```\n\n#### includes()\n\n方法返回一个布尔值，表示某个数组是否包含给定的值，与字符串的includes方法类似。\n\n```js\nlet arr = [1, 2, 3, 4, 5, 6];\n\nconsole.log(arr.includes(3)); // true\nconsole.log(arr.includes(100)); // false\n```\n\n### 迭代方法\n\n#### every()\n\n对数组中的每一项运行给定函数，如果该函数对每一项都返回true，则返回true。\n\n```js\nlet arr = [1, 2, 3, 4, 5, 6];\n// 是否所有的值都大于3\nlet isTrue = arr.every(value => value > 3);\nconsole.log(isTrue); // false;\nconst isBelowThreshold = (currentValue) => currentValue < 40;\nconst array1 = [1, 30, 39, 29, 10, 13];\nconsole.log(array1.every(isBelowThreshold));\n```\n\n#### filter()\n\n对数组中的每一项运行给定函数，返回该函数会返回true的项组成的数组。\n\n```js\nlet arr = [1, 2, 3, 4, 5, 6];\n// 取数组中大于3的值重新组成新数组\nlet newArr = arr.filter(value => value > 3);\nconsole.log(newArr); // [ 4, 5, 6 ]\nconst words = ['spray', 'limit', 'elite', 'exuberant', 'destruction', 'present'];\nconst result = words.filter(word => word.length > 6);\nconsole.log(result);\n```\n\n#### forEach()\n\n对数组中的每一项运行给定函数。这个方法没有返回值。\n\n```js\nlet arr = [1, 2, 3, 4, 5, 6];\n// 迭代数组的每一项\narr.forEach((item, index) => {\n  console.log(item); // 1, 2, 3, 4, 5, 6\n})\narr.forEach(element => console.log(element));\n```\n\n#### map()\n\n对数组中的每一项运行给定函数，返回每次函数调用的结果组成的数组。\n\n```js\nlet arr = [1, 2, 3, 4, 5, 6];\n// 迭代数组每个值加上100返回新数组\nlet newArr = arr.map(val => val + 100);\nconsole.log(newArr); // [ 101, 102, 103, 104, 105, 106 ]\nlet modifiedArr = arr.map(function(element){\n    return element *3;\n});\n```\n\n#### some()\n\n对数组中的每一项运行给定函数，如果该函数对任一项返回true，则返回true。\n\n```js\nlet arr = [1, 2, 3, 4, 5, 6];\n// 迭代数组的每一项，只要有一项符合条件就返回true\nlet isTrue = arr.some(val => val >= 5);\nlet isTrue2 = arr.some(val => val > 6);\nconsole.log(isTrue); // true\nconsole.log(isTrue2); // false\nconst array = [1, 2, 3, 4, 5];\n// checks whether an element is even\nconst even = (element) => element % 2 === 0;\nconsole.log(array.some(even));\n// expected output: true\n```\n\n#### reduce()和reduceRight()\n\n这两个方法都会迭代数组的所有项，然后构建一个最终返回的值。其中，reduce()方法从数组的第一项开始，逐个遍历到最后。而reduceRight()则从数组的最后一项开始，向前遍历到第一项。\n\n```js\nlet arr = [1, 2, 3, 4];\n// 从左到右累加结果\nlet result = arr.reduce((val1, val2) => {\n  return val1 + val2;\n});\nconsole.log(result); // 10\n```\n> [10个超级实用的reduce使用技巧](https://mp.weixin.qq.com/s/6Ks2Y1gkR50xrI9odDHVGg)\n\n#### entries()，keys()和values()\n\nES6提供三个新的方法——entries()，keys()和values()——用于遍历数组。它们都返回一个遍历器对象，可以用for…of循环进行遍历，唯一的区别是keys()是对键名的遍历、values()是对键值的遍历，entries()是对键值对的遍历。\n\n```js\nlet arr = [1, 2, 3];\n// entries()是对键值对的遍历\nfor (let val of arr.entries()) {\n  console.log(val);\n  /**\n   [ 0, 1 ]\n   [ 1, 2 ]\n   [ 2, 3 ]\n   */\n}\n// keys()是对键名的遍历\nfor (let val of arr.keys()) {\n  console.log(val); // 0 1 2\n}\n// values()是对键值的遍历\nfor (let val of arr.values()) {\n  console.log(val); // 1 2 3\n}\n```","tags":["代码实战"],"categories":["JS"]},{"title":"编写JavaScript代码的小技巧","slug":"编写JavaScript代码的小技巧","url":"/blog/posts/b32de5bbeb68/","content":"\n### 生成随机颜色/随机数/Boolean值\n\n```js\n// 生成随机颜色\nconst generateRandomHexColor = () => Math.floor(Math.random() * 0xffffff).toString(16)\nconsole.log(generateRandomHexColor())\n\n// 获取随机颜色\n// 日常我们经常会需要获取一个随机颜色，通过随机数即可完成\nfunction getRandomColor(){\n    return `#${Math.floor(Math.random() * 0xffffff) .toString(16)}`;\n}\n\n// 带有范围的随机数生成器\nfunction randomNumber(max = 1, min = 0) {\n  if (min >= max) {\n    return max;\n  }\n  return Math.floor(Math.random() * (max - min) + min);\n}\n\n// 随机ID生成器\n// create unique id starting from current time in milliseconds\n// incrementing it by 1 everytime requested\nconst uniqueId = (() => {\n  const id = (function* () {\n    let mil = new Date().getTime();\n\n    while (true) yield (mil += 1);\n  })();\n\n  return () => id.next().value;\n})();\n// create unique incrementing id starting from provided value or zero\n// good for temporary things or things that id resets\nconst uniqueIncrementingId = ((lastId = 0) => {\n  const id = (function* () {\n    let numb = lastId;\n\n    while (true) yield (numb += 1);\n  })();\n\n  return (length = 12) => `${id.next().value}`.padStart(length, \"0\");\n})();\n// create unique id from letters and numbers\nconst uniqueAlphaNumericId = (() => {\n  const heyStack = \"0123456789abcdefghijklmnopqrstuvwxyz\";\n  const randomInt = () =>\n    Math.floor(Math.random() * Math.floor(heyStack.length));\n\n  return (length = 24) =>\n    Array.from({ length }, () => heyStack[randomInt()]).join(\"\");\n})();\n\n// 创建一个范围内的数字\nfunction range(maxOrStart, end = null, step = null) {\n  if (!end) {\n    return Array.from({ length: maxOrStart }, (_, i) => i);\n  }\n  if (end <= maxOrStart) {\n    return [];\n  }\n  if (step !== null) {\n    return Array.from(\n      { length: Math.ceil((end - maxOrStart) / step) },\n      (_, i) => i * step + maxOrStart\n    );\n  }\n  return Array.from(\n    { length: Math.ceil(end - maxOrStart) },\n    (_, i) => i + maxOrStart\n  );\n}\n\n// 随机获取一个Boolean值\n// 通过随机数获取，Math.random()的区间是0-0.99，用0.5在中间百分之五十的概率\nfunction randomBool() {\n    return 0.5 - Math.random()\n}\n\n// uuid\nexport const uuid = () => {\n    const temp_url = URL.createObjectURL(new Blob())\n    const uuid = temp_url.toString()\n    URL.revokeObjectURL(temp_url) //释放这个url\n    return uuid.substring(uuid.lastIndexOf('/') + 1)\n}\n// 示例：\nuuid() // a640be34-689f-4b98-be77-e3972f9bffdd\n\n/**\n * 生成随机数\n * @param {*} min \n * @param {*} max \n */\n\nfunction randomNum(min, max) {\n  switch (arguments.length) {\n    case 1:\n      return parseInt(Math.random() * min + 1, 10);\n    case 2:\n      return parseInt(Math.random() * (max - min + 1) + min, 10);\n    default:\n      return 0;\n  }\n}\nrandomNum(1,10)\n```\n\n### 其他操作\n\n```js\n// 复制到剪切板\nconst copyToClipboard = (text) => navigator.clipboard && navigator.clipboard.writeText && navigator.clipboard.writeText(text)\ncopyToClipboard(\"Hello World!\")\n\n// 将下标转为中文零一二三...\n// 日常可能有的列表我们需要将对应的012345转为中文的一、二、三、四、五...，在老的项目看到还有通过自己手动定义很多行这样的写法，于是写了一个这样的方法转换\nexport function transfromNumber(number){\n  const  INDEX_MAP = ['零'，'一'.....]\n  if(!number) return\n  if(number === 10) return INDEX_MAP[number]\n  return [...number.toString()].reduce( (pre, cur) => pre  + INDEX_MAP[cur] , '' )\n}\n\n// Boolean转换\n// 一些场景下我们会将boolean值定义为场景，但是在js中非空的字符串都会被认为是true\nfunction toBoolean(value, truthyValues = ['true']){\n  const normalizedValue = String(value).toLowerCase().trim();\n  return truthyValues.includes(normalizedValue);\n}\ntoBoolean('TRUE'); // true\ntoBoolean('FALSE'); // false\ntoBoolean('YES', ['yes']); // true\n\n// 计算两个坐标之间的距离\nfunction distance(p1, p2){\n    return Math.sqrt(Math.pow(p2.x - p1.x, 2) + Math.pow(p2.y - p1.y, 2));\n}\n\n// 获取列表最后一项\nfunction lastItem(list) {\n  if (Array.isArray(list)) {\n    return list.slice(-1)[0];\n  }\n  if (list instanceof Set) {\n    return Array.from(list).slice(-1)[0];\n  }\n  if (list instanceof Map) {\n    return Array.from(list.values()).slice(-1)[0];\n  }\n}\n\n// 格式化JSON字符串，stringify任何内容\nconst stringify = (() => {\n  const replacer = (key, val) => {\n    if (typeof val === \"symbol\") {\n      return val.toString();\n    }\n    if (val instanceof Set) {\n      return Array.from(val);\n    }\n    if (val instanceof Map) {\n      return Array.from(val.entries());\n    }\n    if (typeof val === \"function\") {\n      return val.toString();\n    }\n    return val;\n  };\n\n  return (obj, spaces = 0) => JSON.stringify(obj, replacer, spaces);\n})();\n\n// 轮询数据\nasync function poll(fn, validate, interval = 2500) {\n  const resolver = async (resolve, reject) => {\n    try {\n      // catch any error thrown by the \"fn\" function\n      const result = await fn(); // fn does not need to be asynchronous or return promise\n      // call validator to see if the data is at the state to stop the polling\n      const valid = validate(result);\n      if (valid === true) {\n        resolve(result);\n      } else if (valid === false) {\n        setTimeout(resolver, interval, resolve, reject);\n      } // if validator returns anything other than \"true\" or \"false\" it stops polling\n    } catch (e) {\n      reject(e);\n    }\n  };\n  return new Promise(resolver);\n}\n\n// 使用别名和默认值来销毁\nfunction demo1({ dt: data }) {\n  // rename \"dt\" to \"data\"\n  console.log(data); // prints {name: 'sample', id: 50}\n}\nfunction demo2({ dt: { name, id = 10 } }) {\n  // deep destruct \"dt\" and if no \"id\" use 10 as default\n  console.log(name, id); // prints 'sample', '10'\n}\ndemo1({\n  dt: { name: \"sample\", id: 50 },\n});\n\ndemo2({\n  dt: { name: \"sample\" },\n});\n\n// 循环任何内容\nfunction forEach(list, callback) {\n  const entries = Object.entries(list);\n  let i = 0;\n  const len = entries.length;\n\n  for (; i < len; i++) {\n    const res = callback(entries[i][1], entries[i][0], list);\n    if (res === true) break;\n  }\n}\n\n// 使函数参数为required\nfunction required(argName = \"param\") {\n  throw new Error(`\"${argName}\" is required`);\n}\nfunction iHaveRequiredOptions(arg1 = required(\"arg1\"), arg2 = 10) {\n  console.log(arg1, arg2);\n}\niHaveRequiredOptions(); // throws \"arg1\" is required\niHaveRequiredOptions(12); // prints 12, 10\niHaveRequiredOptions(12, 24); // prints 12, 24\niHaveRequiredOptions(undefined, 24); // throws \"arg1\" is required\n\n// Replace All\nvar example = \"potato potato\";\nconsole.log(example.replace(/pot/, \"tom\"));// \"tomato potato\"\nconsole.log(example.replace(/pot/g, \"tom\"));// \"tomato tomato\"\n\n// 防抖\nexport const debounce = (() => {\n    let timer = null\n    return (callback, wait = 800) => {\n        timer&&clearTimeout(timer)\n        timer = setTimeout(callback, wait)\n    }\n})()\n// 示例：如vue中使用\nmethods: {\n    loadList() {\n        debounce(() => {\n            console.log('加载数据')\n        }, 500)\n    }\n}\n\n// 节流\nexport const throttle = (() => {\n    let last = 0\n    return (callback, wait = 800) => {\n        let now = +new Date()\n        if (now - last > wait) {\n            callback()\n            last = now\n        }\n    }\n})()\n\n// 手机号脱敏\nexport const hideMobile = (mobile) => {\n  return mobile.replace(/^(\\d{3})\\d{4}(\\d{4})$/, \"$1****$2\")\n}\n// 大小写转换\n// 参数：str:待转换的字符串 type:1-全大写 2-全小写 3-首字母大写\nexport const turnCase = (str, type) => {\n    switch (type) {\n        case 1:\n            return str.toUpperCase()\n        case 2:\n            return str.toLowerCase()\n        case 3:\n            //return str[0].toUpperCase() + str.substr(1).toLowerCase() // substr 已不推荐使用\n            return str[0].toUpperCase() + str.substring(1).toLowerCase()\n        default:\n            return str\n    }\n}\n// 示例：\nturnCase('vue', 1) // VUE\nturnCase('REACT', 2) // react\nturnCase('vue', 3) // Vue\n\n// 金额格式化\n/*参数：\n{number} number：要格式化的数字\n{number} decimals：保留几位小数\n{string} dec_point：小数点符号\n{string} thousands_sep：千分位符号\n*/\nexport const moneyFormat = (number, decimals, dec_point, thousands_sep) => {\n    number = (number + '').replace(/[^0-9+-Ee.]/g, '')\n    const n = !isFinite(+number) ? 0 : +number\n    const prec = !isFinite(+decimals) ? 2 : Math.abs(decimals)\n    const sep = typeof thousands_sep === 'undefined' ? ',' : thousands_sep\n    const dec = typeof dec_point === 'undefined' ? '.' : dec_point\n    let s = ''\n    const toFixedFix = function(n, prec) {\n        const k = Math.pow(10, prec)\n        return '' + Math.ceil(n * k) / k\n    }\n    s = (prec ? toFixedFix(n, prec) : '' + Math.round(n)).split('.')\n    const re = /(-?\\d+)(\\d{3})/\n    while (re.test(s[0])) {\n        s[0] = s[0].replace(re, '$1' + sep + '$2')\n    }\n\n    if ((s[1] || '').length < prec) {\n        s[1] = s[1] || ''\n        s[1] += new Array(prec - s[1].length + 1).join('0')\n    }\n    return s.join(dec)\n}\n// 示例：\nmoneyFormat(10000000) // 10,000,000.00\nmoneyFormat(10000000, 3, '.', '-') // 10-000-000.000\n\n// 存储操作\nclass MyCache {\n    constructor(isLocal = true) {\n        this.storage = isLocal ? localStorage : sessionStorage\n    }\n    setItem(key, value) {\n        if (typeof (value) === 'object') value = JSON.stringify(value)\n        this.storage.setItem(key, value)\n    }\n    getItem(key) {\n        try {\n            return JSON.parse(this.storage.getItem(key))\n        } catch (err) {\n            return this.storage.getItem(key)\n        }\n    }\n    removeItem(key) {\n        this.storage.removeItem(key)\n    }\n    clear() {\n        this.storage.clear()\n    }\n    key(index) {\n        return this.storage.key(index)\n    }\n    length() {\n        return this.storage.length\n    }\n}\nconst localCache = new MyCache()\nconst sessionCache = new MyCache(false)\nexport { localCache, sessionCache }\n// 示例：\nlocalCache.getItem('user')\nsessionCache.setItem('name','树哥')\nsessionCache.getItem('token')\nlocalCache.clear()\n\n// 下载文件\n/*参数：\napi 接口\nparams 请求参数\nfileName 文件名\n*/\nconst downloadFile = (api, params, fileName, type = 'get') => {\n    axios({\n        method: type,\n        url: api,\n        responseType: 'blob', \n        params: params\n    }).then((res) => {\n        let str = res.headers['content-disposition']\n        if (!res || !str) {\n            return\n        }\n        let suffix = ''\n        // 截取文件名和文件类型\n        if (str.lastIndexOf('.')) {\n            fileName ? '' : fileName = decodeURI(str.substring(str.indexOf('=') + 1, str.lastIndexOf('.')))\n            suffix = str.substring(str.lastIndexOf('.'), str.length)\n        }\n        //  如果支持微软的文件下载方式(ie10+浏览器)\n        if (window.navigator.msSaveBlob) {\n            try {\n                const blobObject = new Blob([res.data]);\n                window.navigator.msSaveBlob(blobObject, fileName + suffix);\n            } catch (e) {\n                console.log(e);\n            }\n        } else {\n            //  其他浏览器\n            let url = window.URL.createObjectURL(res.data)\n            let link = document.createElement('a')\n            link.style.display = 'none'\n            link.href = url\n            link.setAttribute('download', fileName + suffix)\n            document.body.appendChild(link)\n            link.click()\n            document.body.removeChild(link)\n            window.URL.revokeObjectURL(link.href);\n        }\n    }).catch((err) => {\n        console.log(err.message);\n    })\n}\n\n// 使用：\ndownloadFile('/api/download', {id}, '文件名')\n```\n\n### 滚动到顶部/底部\n\n```js\n// 滚动到页面顶部\nexport const scrollToTop = () => {\n    const height = document.documentElement.scrollTop || document.body.scrollTop;\n    if (height > 0) {\n        window.requestAnimationFrame(scrollToTop);\n        window.scrollTo(0, height - height / 8);\n    }\n}\n// 滚动到元素位置\nexport const smoothScroll = element =>{\n    document.querySelector(element).scrollIntoView({\n        behavior: 'smooth'\n    });\n};\n// 示例：\nsmoothScroll('#target'); // 平滑滚动到ID为target 的元素\n\n// 滚动到顶部。将元素滚动到顶部最简单的方法是使用scrollIntoView。设置block为start可以滚动到顶部；设置behavior为smooth可以开启平滑滚动。\nconst scrollToTop = (element) => element.scrollIntoView({ behavior: \"smooth\", block: \"start\" });\n\n// 滚动到底部。与滚动到顶部一样，滚动到底部只需要设置block为end即可。\nconst scrollToBottom = (element) =>  element.scrollIntoView({ behavior: \"smooth\", block: \"end\" });\n\n// dom节点平滑滚动到可视区域，顶部，底部。原生的scrollTo方法没有动画，类似于锚点跳转，比较生硬，可以通过这个方法会自带平滑的过度效果\nfunction scrollTo(element) {\n    element.scrollIntoView({ behavior: \"smooth\", block: \"start\" }) // 顶部\n    element.scrollIntoView({ behavior: \"smooth\", block: \"end\" }) // 底部\n    element.scrollIntoView({ behavior: \"smooth\"}) // 可视区域\n}\n\n/**\n * 获取滚动条位置⬇\n */ \nfunction getScrollPosition(el = window) {\n  return {\n    x: el.pageXOffset !== undefined ? el.pageXOffset : el.scrollLeft,\n    y: el.pageYOffset !== undefined ? el.pageYOffset : el.scrollTop,\n  };\n}\n\n/**\n * 固定滚动条\n * 功能描述：一些业务场景，如弹框出现时，需要禁止页面滚动，这是兼容安卓和iOS禁止页面滚动的解决方案\n */\nlet scrollTop = 0;\n\nfunction preventScroll() {\n  // 存储当前滚动位置\n  scrollTop = window.scrollY;\n\n  // 将可滚动区域固定定位，可滚动区域高度为0后就不能滚动了\n  document.body.style[\"overflow-y\"] = \"hidden\";\n  document.body.style.position = \"fixed\";\n  document.body.style.width = \"100%\";\n  document.body.style.top = -scrollTop + \"px\";\n  // document.body.style['overscroll-behavior'] = 'none'\n}\n\nfunction recoverScroll() {\n  document.body.style[\"overflow-y\"] = \"auto\";\n  document.body.style.position = \"static\";\n  // document.querySelector('body').style['overscroll-behavior'] = 'none'\n\n  window.scrollTo(0, scrollTop);\n}\n\n/**\n * 滚动条回到顶部动画⬇\n */ \n// 方案一： c - c / 8\n// c没有定义\nfunction scrollToTop() {\n  const scrollTop =\n    document.documentElement.scrollTop || document.body.scrollTop;\n  if (scrollTop > 0) {\n    window.requestAnimationFrame(scrollToTop);\n    window.scrollTo(0, c - c / 8);\n  } else {\n    window.cancelAnimationFrame(scrollToTop);\n  }\n}\nscrollToTop();\n\n// 修正之后\nfunction scrollToTop() {\n  const scrollTop =\n    document.documentElement.scrollTop || document.body.scrollTop;\n  if (scrollTop > 0) {\n    window.requestAnimationFrame(scrollToTop);\n    window.scrollTo(0, scrollTop - scrollTop / 8);\n  } else {\n    window.cancelAnimationFrame(scrollToTop);\n  }\n}\nscrollToTop();\n\n// 模糊搜索\n/*\n * list 原数组\n * keyWord 查询的关键词\n * attribute 数组需要检索属性\n **/\nexport const fuzzyQuery = (list, keyWord, attribute = 'name') => {\n    const reg = new RegExp(keyWord)\n    const arr = []\n    for (let i = 0; i < list.length; i++) {\n        if (reg.test(list[i][attribute])) {\n            arr.push(list[i])\n        }\n    }\n    return arr\n}\n// 示例：\nconst list = [\n    { id: 1, name: '树哥' },\n    { id: 2, name: '黄老爷' },\n    { id: 3, name: '张麻子' },\n    { id: 4, name: '汤师爷' },\n    { id: 5, name: '胡万' },\n    { id: 6, name: '花姐' },\n    { id: 7, name: '小梅' }\n]\nfuzzyQuery(list, '树', 'name') // [{id: 1, name: '树哥'}]\n\n// 遍历树节点\nexport const foreachTree = (data, callback, childrenName = 'children') => {\n    for (let i = 0; i < data.length; i++) {\n        callback(data[i])\n        if (data[i][childrenName] && data[i][childrenName].length > 0) {\n            foreachTree(data[i][childrenName], callback, childrenName)\n        }\n    }\n}\n// 示例：假设我们要从树状结构数据中查找id为9的节点\nconst treeData = [{\n    id: 1,\n    label: '一级 1',\n    children: [{\n        id: 4,\n        label: '二级 1-1',\n        children: [{\n            id: 9,\n            label: '三级 1-1-1'\n        }, {\n            id: 10,\n            label: '三级 1-1-2'\n        }]\n    }]\n}, {\n    id: 2,\n    label: '一级 2',\n    children: [{\n        id: 5,\n        label: '二级 2-1'\n    }, {\n        id: 6,\n        label: '二级 2-2'\n    }]\n}, {\n    id: 3,\n    label: '一级 3',\n    children: [{\n        id: 7,\n        label: '二级 3-1'\n    }, {\n        id: 8,\n        label: '二级 3-2'\n    }]\n}],\nlet result = foreachTree(data, (item) => {\n          if (item.id === 9) {\n              result = item\n          }\n      })\nconsole.log('result', result)  // {id: 9,label: \"三级 1-1-1\"}   \n\n/**\n * 返回当前网页地址⬇\n */ \n// 方案一：location\nfunction currentURL() {\n  return window.location.href;\n}\ncurrentURL();\n\n// 方案二：a 标签\nfunction currentURL() {\n  var el = document.createElement(\"a\");\n  el.href = \"\";\n  return el.href;\n}\ncurrentURL();\n\n\n/**\n * 页面跳转，是否记录在history中⬇\n */ \n// 方案一：\nfunction redirect(url, asLink = true) {\n  asLink ? (window.location.href = url) : window.location.replace(url);\n}\n// 方案二：\nfunction redirect(url, asLink = true) {\n  asLink ? window.location.assign(url) : window.location.replace(url);\n}\n\n\n/**\n * 复制文本⬇\n */ \n// 方案一：\nfunction copy(str) {\n  const el = document.createElement(\"textarea\");\n  el.value = str;\n  el.setAttribute(\"readonly\", \"\");\n  el.style.position = \"absolute\";\n  el.style.left = \"-9999px\";\n  el.style.top = \"-9999px\";\n  document.body.appendChild(el);\n  const selected =\n    document.getSelection().rangeCount > 0\n      ? document.getSelection().getRangeAt(0)\n      : false;\n  el.select();\n  document.execCommand(\"copy\");\n  document.body.removeChild(el);\n  if (selected) {\n    document.getSelection().removeAllRanges();\n    document.getSelection().addRange(selected);\n  }\n}\n// 方案二：cliboard.js\n```\n\n### 判断操作\n```js\n/**\n * 判断当前位置是否为页面底部\n * 返回值为true/false\n */\nfunction bottomVisible() {\n  return (\n    document.documentElement.clientHeight + window.scrollY >=\n    (document.documentElement.scrollHeight ||\n      document.documentElement.clientHeight)\n  );\n}\n\n/**\n * 判断元素是否在可视范围内\n * partiallyVisible为是否为完全可见\n */\nfunction elementIsVisibleInViewport(el, partiallyVisible = false) {\n  const { top, left, bottom, right } = el.getBoundingClientRect();\n\n  return partiallyVisible\n    ? ((top > 0 && top < innerHeight) ||\n        (bottom > 0 && bottom < innerHeight)) &&\n        ((left > 0 && left < innerWidth) || (right > 0 && right < innerWidth))\n    : top >= 0 && left >= 0 && bottom <= innerHeight && right <= innerWidth;\n}\n\n// 判断一个参数是不是函数\n// 有时候我们的方法需要传入一个函数回调，但是需要检测其类型，我们可以通过Object的原型方法去检测，当然这个方法可以准确检测任何类型。\nfunction isFunction(v){\n   return ['[object Function]', '[object GeneratorFunction]', '[object AsyncFunction]', '[object Promise]'].includes(Object.prototype.toString.call(v));\n}\n\n// 判断是否是NodeJs环境\n// 前端的日常开发是离不开nodeJs的，通过判断全局环境来检测是否是nodeJs环境\nfunction isNode(){\n    return typeof process !== 'undefined' && process.versions != null && process.versions.node != null;\n}\n\n// 判断手机是Andoird还是IOS\n/** \n * 1: ios\n * 2: android\n * 3: 其它\n */\nexport const getOSType=() => {\n    let u = navigator.userAgent, app = navigator.appVersion;\n    let isAndroid = u.indexOf('Android') > -1 || u.indexOf('Linux') > -1;\n    let isIOS = !!u.match(/\\(i[^;]+;( U;)? CPU.+Mac OS X/);\n    if (isIOS) {\n        return 1;\n    }\n    if (isAndroid) {\n        return 2;\n    }\n    return 3;\n}\n```\n### 检查/检测操作\n```js\n// 检测元素是否在屏幕中\n// 最好的方法是使用IntersectionObserver。\nconst callback = (entries) => {\n  entries.forEach((entry) => {\n    if (entry.isIntersecting) {\n      // `entry.target` is the dom element\n      console.log(`${entry.target.id} is visible`);\n    }\n  });\n};\n\nconst options = {\n  threshold: 1.0,\n};\nconst observer = new IntersectionObserver(callback, options);\nconst btn = document.getElementById(\"btn\");\nconst bottomBtn = document.getElementById(\"bottom-btn\");\nobserver.observe(btn);\nobserver.observe(bottomBtn);\n\n// 检测设备\n// 使用navigator.userAgent来检测网站运行在哪种平台设备上。\nconst detectDeviceType = () =>\n  /Android|webOS|iPhone|iPad|iPod|BlackBerry|IEMobile|Opera Mini/i.test(\n    navigator.userAgent\n  ) ? \"Mobile\" : \"Desktop\";\nconsole.log(detectDeviceType());\n\n// 检测暗色主题\nconst isDarkMode = () => window.matchMedia && window.matchMedia(\"(prefers-color-scheme: dark)\").matches;\nconsole.log(isDarkMode())\n\n// 多个字符串检查\n// 通常，如果我们需要检查字符串是否等于多个值中的一个，往往很快会觉得疲惫不堪。幸运的是，JavaScript有一个内置的方法来帮助你解决这个问题\n// 普通写法\nconst isVowel = (letter) => {\n  if (\n    letter === \"a\" ||\n    letter === \"e\" ||\n    letter === \"i\" ||\n    letter === \"o\" ||\n    letter === \"u\"\n  ) {\n    return true;\n  }\n  return false;\n};\n// 简写方法\nconst isVowel = (letter) =>\n  [\"a\", \"e\", \"i\", \"o\", \"u\"].includes(letter);\n\n// 类型检查小工具\nconst isOfType = (() => {\n  // create a plain object with no prototype\n  const type = Object.create(null);\n  // check for null type\n  type.null = (x) => x === null;\n  // check for undefined type\n  type.undefined = (x) => x === undefined;\n  // check for nil type. Either null or undefined\n  type.nil = (x) => type.null(x) || type.undefined(x);\n  // check for strings and string literal type. e.g: 's', \"s\", `str`, new String()\n  type.string = (x) => !type.nil(x) && (typeof x === \"string\" || x instanceof String);\n  // check for number or number literal type. e.g: 12, 30.5, new Number()\n  type.number = (x) =>\n!type.nil(x) && // NaN & Infinity have typeof \"number\" and this excludes that\n    ((!isNaN(x) && isFinite(x) && typeof x === \"number\") ||\n      x instanceof Number);\n  // check for boolean or boolean literal type. e.g: true, false, new Boolean()\n  type.boolean = (x) =>\n    !type.nil(x) && (typeof x === \"boolean\" || x instanceof Boolean);\n  // check for array type\n  type.array = (x) => !type.nil(x) && Array.isArray(x);\n  // check for object or object literal type. e.g: {}, new Object(), Object.create(null)\n  type.object = (x) => ({}.toString.call(x) === \"[object Object]\");\n  // check for provided type instance\n  type.type = (x, X) => !type.nil(x) && x instanceof X;\n  // check for set type\n  type.set = (x) => type.type(x, Set);\n  // check for map type\n  type.map = (x) => type.type(x, Map);\n  // check for date type\n  type.date = (x) => type.type(x, Date);\n\n  return type;\n})();\n\n// 检查是否为空\nfunction isEmpty(x) {\n  if (Array.isArray(x) || typeof x === \"string\" || x instanceof String) {\n    return x.length === 0;\n  }\n  if (x instanceof Map || x instanceof Set) {\n    return x.size === 0;\n  }\n  if ({}.toString.call(x) === \"[object Object]\") {\n    return Object.keys(x).length === 0;\n  }\n  return false;\n}\n\n// 检测是否为空对象\n// 通过使用ES6的Reflect静态方法判断他的长度就可以判断是否是空数组了，也可以通过Object.keys()来判断\nfunction isEmpty(obj){\n    return  Reflect.ownKeys(obj).length === 0 && obj.constructor === Object;\n}\n\n// 检测两个dom节点是否覆盖重叠\n// 有些场景下我们需要判断dom是否发生碰撞了或者重叠了，我们可以通过getBoundingClientRect获取到dom的x1,y1,x2,y2坐标然后进行坐标比对即可判断出来\nfunction overlaps = (a, b) {\n   return (a.x1 < b.x2 && b.x1 < a.x2) || (a.y1 < b.y2 && b.y1 < a.y2);\n}\n\n/**\n * 检测设备类型⬇\n */ \n// 方案一： ua\nfunction detectDeviceType() {\n  return /Android|webOS|iPhone|iPad|iPod|BlackBerry|IEMobile|Opera Mini/i.test(\n    navigator.userAgent\n  )\n    ? \"Mobile\"\n    : \"Desktop\";\n}\ndetectDeviceType();\n\n// 方案二：事件属性\nfunction detectDeviceType() {\n  return \"ontouchstart\" in window || navigator.msMaxTouchPoints\n    ? \"Mobile\"\n    : \"Desktop\";\n}\ndetectDeviceType();\n\n// Falsey（假值）检查\n// 如果要检查变量是null、undefined、0、false、NaN还是空string，可以使用逻辑非(!)运算符一次检查所有变量，而无需编写多个条件。这使得检查变量是否包含有效数据变得相对容易多了。\n// 普通写法\nconst isFalsey = (value) => {\n  if (\n    value === null ||\n    value === undefined ||\n    value === 0 ||\n    value === false ||\n    value === NaN ||\n    value === \"\"\n  ) {\n    return true;\n  }\n  return false;\n};\n// 简写方法\nconst isFalsey = (value) => !value;\n```\n\n### 从URL中获取参数\n\n```js\n// JavaScript中有一个URL对象，通过它可以非常方便的获取URL中的参数。\nconst getParamByUrl = (key) => {\n  const url = new URL(location.href)\n  return url.searchParams.get(key)\n}\n\nexport const getSearchParams = () => {\n    const searchPar = new URLSearchParams(window.location.search)\n    const paramsObj = {}\n    for (const [key, value] of searchPar.entries()) {\n        paramsObj[key] = value\n    }\n    return paramsObj\n}\n// 示例：\n// 假设目前位于 https://****com/index?id=154513&age=18;\ngetSearchParams(); // {id: \"154513\", age: \"18\"}\n\n/**\n * 获取url中的参数⬇\n */ \n// 方案一：正则 + reduce\nfunction getURLParameters(url) {\n  return url.match(/([^?=&]+)(=([^&]*))/g).reduce((a, v) => (\n        (a[v.slice(0, v.indexOf(\"=\"))] = v.slice(v.indexOf(\"=\") + 1)), a\n      ),{}\n    );\n}\ngetURLParameters(location.href);\n\n// 方案二：split + reduce\nfunction getURLParameters(url) {\n  return url\n    .split(\"?\") //取？分割\n    .slice(1) //不要第一部分\n    .join() //拼接\n    .split(\"&\") //&分割\n    .map((v) => v.split(\"=\")) //=分割\n    .reduce((s, n) => {\n      s[n[0]] = n[1];\n      return s;\n    }, {});\n}\ngetURLParameters(location.href);\n// getURLParameters('')\n\n// 方案三: URLSearchParams\n```\n### 对象操作\n\n```js\n// 深拷贝对象\n// 深拷贝对象非常简单，先将对象转换为字符串，再转换成对象即可。\nconst deepCopy = obj => JSON.parse(JSON.stringify(obj))\n// 除了利用JSON的API，还有更新的深拷贝对象的structuredClone API，但并不是在所有的浏览器中都支持。\nstructuredClone(obj)\n\n// 深拷贝\nexport const clone = parent => {\n    // 判断类型\n    const isType = (obj, type) => {\n        if (typeof obj !== \"object\") return false;\n        const typeString = Object.prototype.toString.call(obj);\n        let flag;\n        switch (type) {\n            case \"Array\":\n                flag = typeString === \"[object Array]\";\n                break;\n            case \"Date\":\n                flag = typeString === \"[object Date]\";\n                break;\n            case \"RegExp\":\n                flag = typeString === \"[object RegExp]\";\n                break;\n            default:\n                flag = false;\n        }\n        return flag;\n    };\n    // 处理正则\n    const getRegExp = re => {\n        var flags = \"\";\n        if (re.global) flags += \"g\";\n        if (re.ignoreCase) flags += \"i\";\n        if (re.multiline) flags += \"m\";\n        return flags;\n    };\n    // 维护两个储存循环引用的数组\n    const parents = [];\n    const children = [];\n\n    const _clone = parent => {\n        if (parent === null) return null;\n        if (typeof parent !== \"object\") return parent;\n\n        let child, proto;\n\n        if (isType(parent, \"Array\")) {\n            // 对数组做特殊处理\n            child = [];\n        } else if (isType(parent, \"RegExp\")) {\n            // 对正则对象做特殊处理\n            child = new RegExp(parent.source, getRegExp(parent));\n            if (parent.lastIndex) child.lastIndex = parent.lastIndex;\n        } else if (isType(parent, \"Date\")) {\n            // 对Date对象做特殊处理\n            child = new Date(parent.getTime());\n        } else {\n            // 处理对象原型\n            proto = Object.getPrototypeOf(parent);\n            // 利用Object.create切断原型链\n            child = Object.create(proto);\n        }\n\n        // 处理循环引用\n        const index = parents.indexOf(parent);\n\n        if (index != -1) {\n            // 如果父数组存在本对象,说明之前已经被引用过,直接返回此对象\n            return children[index];\n        }\n        parents.push(parent);\n        children.push(child);\n\n        for (let i in parent) {\n            // 递归\n            child[i] = _clone(parent[i]);\n        }\n\n        return child;\n    };\n    return _clone(parent);\n};\n// 此方法存在一定局限性：一些特殊情况没有处理: 例如Buffer对象、Promise、Set、Map。如果确实想要完备的深拷贝，推荐使用lodash中的cloneDeep方法。\n\n// 条件对象键\nlet condition = true;\nconst man = {\n  someProperty: \"some value\",\n  // the parenthesis will execute the ternary that will\n  // result in the object with the property you want to insert\n  // or an empty object.then its content is spreaded in the wrapper object\n  ...(condition === true ? { newProperty: \"value\" } : {}),\n};\n\n// 使用变量作为对象键\nlet property = \"newValidProp\";\nconst man2 = {\n  someProperty: \"some value\",\n  // the \"square bracket\" notation is a valid way to acces object key\n  // like object[prop] but it is used inside to assign a property as well\n  // using the 'backtick' to first change it into a string\n\n  // but it is optional\n[\"${property}\"]: \"value\",\n};\n\n// 检查对象里的键\nconst sample = {\n  prop: \"value\",\n};\n// using the \"in\" keyword will still consider proptotype keys\n// which makes it unsafe and one of the issues with \"for...in\" loop\nconsole.log(\"prop\" in sample); // prints \"true\"\nconsole.log(\"toString\" in sample); // prints \"true\"\n// using the \"hasOwnProperty\" methods is safer\nconsole.log(sample.hasOwnProperty(\"prop\")); // prints \"true\"\nconsole.log(sample.hasOwnProperty(\"toString\")); // prints \"false\"\n\n// 深度克隆对象\nconst deepClone = (obj) => {\n  let clone = obj;\n  if (obj && typeof obj === \"object\") {\n    clone = new obj.constructor();\n\n    Object.getOwnPropertyNames(obj).forEach(\n      (prop) => (clone[prop] = deepClone(obj[prop]))\n    );\n  }\n  return clone;\n};\n\n// 深度冻结对象\nconst deepClone2 = (obj) => {\n  let clone = obj;\n  if (obj && typeof obj === \"object\") {\n    clone = new obj.constructor();\n\n    Object.getOwnPropertyNames(obj).forEach(\n      (prop) => (clone[prop] = deepClone(obj[prop]))\n    );\n  }\n  return clone;\n};\n```\n### 语法操作\n\n```js\n// 等待函数\n// JavaScript提供了setTimeout函数，但是它并不返回Promise对象，所以我们没办法使用async作用在这个函数上，但是我们可以封装等待函数。\nconst wait = (ms) => new Promise((resolve)=> setTimeout(resolve, ms))\nconst asyncFn = async () => {\n  await wait(1000)\n  console.log('等待异步函数执行结束')\n}\nasyncFn()\n\n// For-of和For-in循环。For-of和For-in循环是迭代array或object的好方法，因为无需手动跟踪object键的索引。\n//For-of\nconst arr = [1, 2, 3, 4, 5];\n// 普通写法\nfor (let i = 0; i < arr.length; i++) {\n  const element = arr[i];\n  // ...\n}\n// 简写方法\nfor (const element of arr) {\n  // ...\n}\n\n// For-in\nconst obj = {\n  a: 1,\n  b: 2,\n  c: 3,\n};\n// 普通写法\nconst keys = Object.keys(obj);\nfor (let i = 0; i < keys.length; i++) {\n  const key = keys[i];\n  const value = obj[key];\n  // ...\n}\n// 简写方法\nfor (const key in obj) {\n  const value = obj[key];\n  // ...\n}\n\n// 函数调用\n// 在三元运算符的帮助下，你还可以根据条件确定要调用哪个函数。\n// 注：函数的call signature必须相同，否则可能会遇到错误。\nfunction f1() {\n  // ...\n}\nfunction f2() {\n  // ...\n}\n// 普通写法\nif (condition) {\n  f1();\n} else {\n  f2();\n}\n// 简写方法\n(condition ? f1 : f2)();\n\n// Switch简写\n// 通常我们可以使用以键作为switch条件并将值作为返回值的对象来优化长switch语句。\nconst dayNumber = new Date().getDay();\n// 普通写法\nlet day;\nswitch (dayNumber) {\n  case 0:\n    day = \"Sunday\";\n    break;\n  case 1:\n    day = \"Monday\";\n    break;\n  case 2:\n    day = \"Tuesday\";\n    break;\n  case 3:\n    day = \"Wednesday\";\n    break;\n  case 4:\n    day = \"Thursday\";\n    break;\n  case 5:\n    day = \"Friday\";\n    break;\n  case 6:\n    day = \"Saturday\";\n}\n// 简写方法\nconst days = {\n  0: \"Sunday\",\n  1: \"Monday\",\n  2: \"Tuesday\",\n  3: \"Wednesday\",\n  4: \"Thursday\",\n  5: \"Friday\",\n  6: \"Saturday\",\n};\nconst day = days[dayNumber];\n\n// 回退值\n// ||运算符可以为变量设置回退值。\n// 普通写法\nlet name;\nif (user?.name) {\n  name = user.name;\n} else {\n  name = \"Anonymous\";\n}\n// 简写方法\nconst name = user?.name || \"Anonymous\";\n\n// 结构加赋值\nlet people = { name: null, age: null };\nlet result = { name: '张三',  age: 16 };\n({ name: people.name, age: people.age} = result);\nconsole.log(people) // {\"name\":\"张三\",\"age\":16}\n\n// 对基础数据类型进行解构\nconst {length : a} = '1234';\nconsole.log(a) // 4\n\n// 对数组解构快速拿到最后一项值\nconst arr = [1, 2, 3];\nconst { 0: first, length, [length - 1]: last } = arr;\nfirst; // 1\nlast; // 3\nlength; // 3\n\n// 创建模块或单例\nclass Service {\n  name = \"service\";\n}\nconst service = (function (S) {\n  // do something here like preparing data that you can use to initialize service\n  const service = new S();\n  return () => service;\n})(Service);\nconst element = (function (S) {\n  const element = document.createElement(\"DIV\");\n  // do something here to grab somethin on the dom\n  // or create elements with javasrcipt setting it all up\n  // than to return it\n  return () => element;\n})();\n\n// 短路条件\n// available && addToCart()代替\n// if (available) {\n//     addToCart();\n// }\n\n// 动态属性名称\nconst dynamic = 'flavour';\nvar item = {\n    name: 'Coke',\n    [dynamic]: 'Cherry'\n}\nconsole.log(item);// { name: \"Coke\", flavour: \"Cherry\" }\n```\n\n### 数值操作\n\n```js\n// 判断整数的不同方法\n/* 1.任何整数都会被1整除，即余数是0。利用这个规则来判断是否是整数。但是对字符串不准确 */\nfunction isInteger(obj) {\n return obj%1 === 0\n}\n/* 2. 添加一个是数字的判断 */\nfunction isInteger(obj) {\n return typeof obj === 'number' && obj%1 === 0\n}\n/* 3. 使用Math.round、Math.ceil、Math.floor判断整数取整后还是等于自己。利用这个特性来判断是否是整数*/\nfunction isInteger(obj) {\n return Math.floor(obj) === obj\n}\n/* 4. 通过parseInt判断 某些场景不准确 */\nfunction isInteger(obj) {\n return parseInt(obj, 10) === obj\n}\n/* 5. 通过位运算符*/\nfunction isInteger(obj) {\n return (obj | 0) === obj\n}\n\n// 参数求和\n// 之前看到有通过函数柯理化形式来求和的，通过reduce一行即可\nfunction sum(...args){\n    args.reduce((a, b) => a + b);\n}\n\n// ----数字Number-----\n/**\n * 数字千分位分割\n * @param {*} num \n */\nfunction commafy(num) {\n  return num.toString().indexOf(\".\") !== -1\n    ? num.toLocaleString()\n    : num.toString().replace(/(\\d)(?=(?:\\d{3})+$)/g, \"$1,\");\n}\ncommafy(1000)\n\n// 将数字转换为字符串\nvar converted_number = 5 + \"\";\nconsole.log(converted_number);// 5\nconsole.log(typeof converted_number);// string\n\n// 将字符串转换为数字 请注意这里的用法，因为它只适用于“字符串数字”。\nvar the_string = \"123\";\nconsole.log(+the_string);// 123\nthe_string = \"hello\";\nconsole.log(+the_string);// NaN\n```\n\n### 时间操作\n\n关于时间操作，没必要自己再写一大串代码了，强烈推荐使用day.js\nDay.js是一个仅2kb大小的轻量级JavaScript时间日期处理库，下载、解析和执行的JavaScript更少，为代码留下更多的时间。\n\n```js\n// 比较两个时间大小,通过调用getTime获取时间戳比较就可以了\nfunction compare(a, b){\n    return a.getTime() > b.getTime();\n}\n\n// 计算两个时间之间的月份差异\nfunction monthDiff(startDate, endDate){\n    return  Math.max(0, (endDate.getFullYear() - startDate.getFullYear()) * 12 - startDate.getMonth() + endDate.getMonth());\n}\n\n// 一步从时间中提取年月日时分秒.时间格式化轻松解决，一步获取到年月日时分秒毫秒，由于toISOString会丢失时区，导致时间差八小时，所以在格式化之前我们加上八个小时时间即可\nfunction extract(date){\n   const d = new Date(new Date(date).getTime() + 8*3600*1000);\n  return new Date(d).toISOString().split(/[^0-9]/).slice(0, -1);\n}\nconsole.log(extract(new Date())) // ['2022', '09', '19', '18', '06', '11', '187']\n\n// Date\n// ---------日期 Date------------\n\n/**\n * 获取当前时间戳⬇\n */ \n// 方案一：精确到秒\nconsole.log(Date.parse(new Date())) \n// 方案二：精确到毫秒\nconsole.log(Date.now()) \n// 方案三：精确到毫秒\nconsole.log(+new Date()) \n// 方案四：精确到毫秒\nconsole.log(new Date().getTime()) \n// 方案五：精确到毫秒\nconsole.log((new Date()).valueOf())\n\n/**\n * js字符串转时间戳⬇\n * mytime是待转换时间字符串，格式：'2018-9-12 9:11:23'\n * 为了兼容IOS，需先将字符串转换为'2018/9/11 9:11:23'\n */\nlet mytime = '2018-9-12 9:11:23';\nlet dateTmp = mytime.replace(/-/g, \"/\");\nconsole.log(new Date(dateTmp).getTime());\nconsole.log(Date.parse(dateTmp));\n\n/**\n * 时间戳转字符串\n */ \nvar dateFormat = function (timestamp) {\n  //先将时间戳转为Date对象，然后才能使用Date的方法\n  var time = new Date(timestamp);\n  var year = time.getFullYear(),\n    month = time.getMonth() + 1, //月份是从0开始的\n    day = time.getDate(),\n    hour = time.getHours(),\n    minute = time.getMinutes(),\n    second = time.getSeconds();\n  //add0()方法在后面定义\n  return (\n    year +\n    \"-\" +\n    this.add0(month) +\n    \"-\" +\n    this.add0(day) +\n    \"\" +\n    this.add0(hour) +\n    \":\" +\n    this.add0(minute) +\n    \":\" +\n    this.add0(second)\n  );\n};\nvar add0 = function (m) {\n  return m < 10 ? \"0\" + m : m;\n};\n/**\n * 格式化字符串\n */\nDate.prototype.format = function (fmt) { //author: meizz \n  var o = {\n      \"M+\": this.getMonth() + 1, //月份 \n      \"d+\": this.getDate(), //日 \n      \"h+\": this.getHours(), //小时 \n      \"m+\": this.getMinutes(), //分 \n      \"s+\": this.getSeconds(), //秒 \n      \"q+\": Math.floor((this.getMonth() + 3) / 3), //季度 \n      \"S\": this.getMilliseconds() //毫秒 \n  };\n  if (/(y+)/.test(fmt)) fmt = fmt.replace(RegExp.$1, (this.getFullYear() + \"\").substr(4 - RegExp.$1.length));\n  for (var k in o)\n  if (new RegExp(\"(\" + k + \")\").test(fmt)) fmt = fmt.replace(RegExp.$1, (RegExp.$1.length == 1) ? (o[k]) : ((\"00\" + o[k]).substr((\"\" + o[k]).length)));\n  return fmt;\n}\nconsole.log(new Date().format('yyyy-MM-dd hh:mm:ss'));\n/**\n * JavaScript Date对象(https://www.w3school.com.cn/js/jsref_obj_date.asp)\n */\n\n```\n\n### Promise\n\n```js\n// 顺序执行promise\nconst asyncSequentializer = (() => {\n  const toPromise = (x) => {\n    if (x instanceof Promise) {\n      // if promise just return it\n      return x;\n    }\n\n    if (typeof x === \"function\") {\n      // if function is not async this will turn its result into a promise\n      // if it is async this will await for the result\n      return (async () => await x())();\n    }\n    \n    return Promise.resolve(x);\n  };\n\n  return (list) => {\n    const results = [];\n\n    return (\n      list.reduce((lastPromise, currentPromise) => {\n          return lastPromise.then((res) => {\n            results.push(res); // collect the results\n            return toPromise(currentPromise);\n          });\n        }, toPromise(list.shift()))\n        // collect the final result and return the array of results as resolved promise\n        .then((res) => Promise.resolve([...results, res]))\n    );\n  };\n})();\n\n// 等待所有promise完成\nconst prom1 = Promise.reject(12);\nconst prom2 = Promise.resolve(24);\nconst prom3 = Promise.resolve(48);\nconst prom4 = Promise.resolve(\"error\");\n// completes when all promises resolve or at least one fail\n// if all resolve it will return an array of results in the same order of each promise\n// if fail it will return the error in catch\n\nPromise.all([prom1, prom2, prom3, prom4])\n  .then((res) => console.log(\"all\", res))\n  .catch((err) => console.log(\"all failed\", err));\n\n// completes with an array of objects with \"status\" and \"value\" or \"reason\" of each promise\n// status can be \"fullfilled\" or \"rejected\"\n// if fullfilled it will contain a \"value\" property\n\n// if failed it will contain a \"reasor property\nPromise.allSettled([prom1, prom2, prom3, prom4])\n  .then((res) => console.log(\"allSettled\", res))\n  .catch((err) => console.log(\"allSettled failed\", err));\n\n// completes with the first promise that resolves\n// fails if all promises fail\nPromise.any([prom1, prom2, prom3, prom4])\n  .then((res) => console.log(\"any\", res))\n  .catch((err) => console.log(\"any failed\", err));\n\n// completes with the first promise that either resolve or fail\n// whichever comes first\nPromise.race([prom1, prom2, prom3, prom4])\n  .then((res) => console.log(\"race\", res))\n  .catch((err) => console.log(\"race failed\", err));\n```\n\n### 全屏操作\n```js\n// 开启全屏\nexport const launchFullscreen = (element) => {\n    if (element.requestFullscreen) {\n        element.requestFullscreen()\n    } else if (element.mozRequestFullScreen) {\n        element.mozRequestFullScreen()\n    } else if (element.msRequestFullscreen) {\n        element.msRequestFullscreen()\n    } else if (element.webkitRequestFullscreen) {\n        element.webkitRequestFullScreen()\n    }\n}\n// 关闭全屏\nexport const exitFullscreen = () => {\n    if (document.exitFullscreen) {\n        document.exitFullscreen()\n    } else if (document.msExitFullscreen) {\n        document.msExitFullscreen()\n    } else if (document.mozCancelFullScreen) {\n        document.mozCancelFullScreen()\n    } else if (document.webkitExitFullscreen) {\n        document.webkitExitFullscreen()\n    }\n}\n/**\n * 进入全屏\n */\nfunction launchFullscreen(element) {\n    if (element.requestFullscreen) {\n        element.requestFullscreen();\n    } else if (element.mozRequestFullScreen) {\n        element.mozRequestFullScreen();\n    } else if (element.msRequestFullscreen) {\n        element.msRequestFullscreen();\n    } else if (element.webkitRequestFullscreen) {\n        element.webkitRequestFullScreen();\n    }\n}\n\nlaunchFullscreen(document.documentElement);\nlaunchFullscreen(document.getElementById(\"id\")); //某个元素进入全屏\n\n/**\n * 退出全屏\n */\nfunction exitFullscreen() {\n    if (document.exitFullscreen) {\n        document.exitFullscreen();\n    } else if (document.msExitFullscreen) {\n        document.msExitFullscreen();\n    } else if (document.mozCancelFullScreen) {\n        document.mozCancelFullScreen();\n    } else if (document.webkitExitFullscreen) {\n        document.webkitExitFullscreen();\n    }\n}\n\nexitFullscreen();\n\n/**\n * 全屏事件\n */\ndocument.addEventListener(\"fullscreenchange\", function (e) {\n    if (document.fullscreenElement) {\n        console.log(\"进入全屏\");\n    } else {\n        console.log(\"退出全屏\");\n    }\n});\n```\n\n### Cookie\n\n```js\n// -------cookie-------\n/**\n *\n * @param {*} key\n * @param {*} value\n * @param {*} expiredays 过期时间\n */\nfunction setCookie(key, value, expiredays) {\n  var exdate = new Date();\n  exdate.setDate(exdate.getDate() + expiredays);\n  document.cookie =\n    key +\n    \"=\" +\n    escape(value) +\n    (expiredays == null ? \"\" : \";expires=\" + exdate.toGMTString());\n}\n/**\n *\n * @param {*} name cookie key\n */\nfunction delCookie(name) {\n  var exp = new Date();\n  exp.setTime(exp.getTime() - 1);\n  var cval = getCookie(name);\n  if (cval != null) {\n    document.cookie = name + \"=\" + cval + \";expires=\" + exp.toGMTString();\n  }\n}\n/**\n *\n * @param {*} name cookie key\n */\nfunction getCookie(name) {\n  var arr,\n    reg = new RegExp(\"(^| )\" + name + \"=([^;]*)(;|$)\");\n  if ((arr = document.cookie.match(reg))) {\n    return arr[2];\n  } else {\n    return null;\n  }\n}\n// 清空\n// 有时候我们想清空，但是又无法获取到所有的cookie。\n// 这个时候我们可以了利用写满，然后再清空的办法。\n\n```\n\n### CSS操作\n```js\n// 获取元素css样式\nfunction getStyle(el, ruleName) {\n  return getComputedStyle(el, null).getPropertyValue(ruleName);\n}\n\n// 隐藏元素\n// 我们可以将元素的style.visibility设置为hidden，隐藏元素的可见性，但元素的空间仍然会被占用。如果设置元素的style.display为none，会将元素从渲染流中删除。\nconst hideElement = (el, removeFromFlow = false) => {\n  removeFromFlow ? (el.style.display = 'none')\n  : (el.style.visibility = 'hidden')\n}\n\n// 通过css检测系统的主题色从而全局修改样式\n// @media的属性prefers-color-scheme就可以知道当前的系统主题，当然使用前需要查查兼容性\n// ```css\n// @media (prefers-color-scheme: dark) { //... } \n// @media (prefers-color-scheme: light) { //... }\n// ```\n// javascript也可以轻松做到\n\nwindow.addEventListener('theme-mode', event =>{ \n    if(event.mode == 'dark'){}\n   if(event.mode == 'light'){} \n})\nwindow.matchMedia('(prefers-color-scheme: dark)') .addEventListener('change', event => { \n    if (event.matches) {} // dark mode\n})\n\n\n// -------浏览器对象 BOM-------\n/**\n * 告知浏览器支持的指定css属性情况\n * @param {String} key - css属性，是属性的名字，不需要加前缀\n * @returns {String} - 支持的属性情况\n */\nfunction validateCssKey(key) {\n  const jsKey = toCamelCase(key); // 有些css属性是连字符号形成\n  if (jsKey in document.documentElement.style) {\n    return key;\n  }\n  let validKey = \"\";\n  // 属性名为前缀在js中的形式，属性值是前缀在css中的形式\n  // 经尝试，Webkit也可是首字母小写webkit\n  const prefixMap = {\n    Webkit: \"-webkit-\",\n    Moz: \"-moz-\",\n    ms: \"-ms-\",\n    O: \"-o-\",\n  };\n  for (const jsPrefix in prefixMap) {\n    const styleKey = toCamelCase(`${jsPrefix}-${jsKey}`);\n    if (styleKey in document.documentElement.style) {\n      validKey = prefixMap[jsPrefix] + key;\n      break;\n    }\n  }\n  return validKey;\n}\n\n/**\n * 把有连字符号的字符串转化为驼峰命名法的字符串\n */\nfunction toCamelCase(value) {\n  return value.replace(/-(\\w)/g, (matched, letter) => {\n    return letter.toUpperCase();\n  });\n}\n\n// 检查浏览器是否支持某个css属性值（es6版）\n/**\n * 检查浏览器是否支持某个css属性值（es6版）\n * @param {String} key - 检查的属性值所属的css属性名\n * @param {String} value - 要检查的css属性值（不要带前缀）\n * @returns {String} - 返回浏览器支持的属性值\n */\nfunction valiateCssValue(key, value) {\n  const prefix = [\"-o-\", \"-ms-\", \"-moz-\", \"-webkit-\", \"\"];\n  const prefixValue = prefix.map((item) => {\n    return item + value;\n  });\n  const element = document.createElement(\"div\");\n  const eleStyle = element.style;\n  // 应用每个前缀的情况，且最后也要应用上没有前缀的情况，看最后浏览器起效的何种情况\n  // 这就是最好在prefix里的最后一个元素是''\n  prefixValue.forEach((item) => {\n    eleStyle[key] = item;\n  });\n  return eleStyle[key];\n}\n\n/**\n * 检查浏览器是否支持某个css属性值\n * @param {String} key - 检查的属性值所属的css属性名\n * @param {String} value - 要检查的css属性值（不要带前缀）\n * @returns {String} - 返回浏览器支持的属性值\n */\nfunction valiateCssValue(key, value) {\n  var prefix = [\"-o-\", \"-ms-\", \"-moz-\", \"-webkit-\", \"\"];\n  var prefixValue = [];\n  for (var i = 0; i < prefix.length; i++) {\n    prefixValue.push(prefix[i] + value);\n  }\n  var element = document.createElement(\"div\");\n  var eleStyle = element.style;\n  for (var j = 0; j < prefixValue.length; j++) {\n    eleStyle[key] = prefixValue[j];\n  }\n  return eleStyle[key];\n}\n\nfunction validCss(key, value) {\n  const validCss = validateCssKey(key);\n  if (validCss) {\n    return validCss;\n  }\n  return valiateCssValue(key, value);\n}\n```\n\n### HTML实用技巧\n```html\n<!-- 使用capture属性打开设备摄像头,user用于前置摄像头,environment用于后置摄像头-->\n<input type=\"file\" capture=\"user\" accept=\"image/*\">\n<!-- 网站自动刷新,实现每10秒刷新一次网站 -->\n<head>\n    <meta http-equiv=\"refresh\" content=\"10\">\n</head>\n<!-- 激活拼写检查,使用spellcheck属性并将其设置为true以激活拼写检查。使用lang属性指定待检查的语言。-->\n<input type=\"text\" spellcheck=\"true\" lang=\"en\">\n<!-- 指定要上传的文件类型,使用accept属性在input标签中指定允许用户上传的文件类型。-->\n<input type=\"file\" accept=\".jpeg,.png\">\n<!-- 阻止浏览器翻译,将translate属性设置为no会阻止浏览器翻译该内容 -->\n<p translate=\"no\">Brand name</p>\n<!-- 在input标签中输入多个项目,适用于文件和电子邮件。如果是电子邮件，则可以用逗号分隔 -->\n<input type=\"file\" multiple>\n<!-- 为视频创建海报（缩略图）使用poster属性，我们可以在视频加载时，或者在用户点击播放按钮之前，显示指定的缩略图。如果不指定图片，则默认使用视频的第一帧作为缩略图 -->\n<video poster=\"picture.png\"></video>\n<!-- 点击链接自动下载 -->\n<a href=\"image.png\" download>\n\n```","tags":["代码实战"],"categories":["JS"]},{"title":"CSS相关","slug":"CSS相关","url":"/blog/posts/4ab442c90ff9/","content":"\n\n## 居中\n\n```html\n<!-- div居中,需要设置宽度-->\n<div style=\"margin : 0 auto;width:80%\"></div>\n<!-- div里面的内容居中-->\n<div style=\"margin : 0 auto;width:80%;text-align:center\">\n    <button></button>\n</div>\n\n<!-- 两个div设置间距 -->\n<div style=\"margin:10px 0\"></div>\n\n<!--margin外间距是外边距，即盒子与盒子之间的距离，而padding是内边距，是盒子的边与盒子内部元素的距离。margin是用来隔开元素与元素的间距；padding是用来隔开元素与内容的间隔。margin是指从自身边框到另一个容器边框之间的距离，就是容器外距离。padding是指自身边框到自身内部另一个容器边框之间的距离，就是容器内距离。\n例如两个文本框的距离使用margin，文本框的边框和文本内容之间的距离使用padding-->\n<!-- 使用弹性容器 -->\n<div class=\"container\">\n    <div class=\"centered\">\n    \t<p>要居中的内容</p>\n    </div>\n</div>\n```\n```css\n.container {\n    display: flex; /* 设置为Flex容器 */\n    justify-content: center; /* 水平居中 */\n    align-items: center; /* 垂直居中 */\n    height: 100vh; /* 设置容器高度 */\n}\n.centered {\n  /* 可以给要居中的元素设置一些样式 */\n}\n```\n> [42种前端常用布局方案总结※](https://mp.weixin.qq.com/s/5ZSMlbjcvaMksx4zakhgzA)\n> [如何用一行CSS实现10种现代布局？](https://mp.weixin.qq.com/s/3yToJq5N8-8SXb0nPDL47g)\n\n## n个元素等比例在一行展示\n\n```html\n<div class=\"image-container\">\n    <img src=\"image1.jpg\" alt=\"Image 1\" />\n    <img src=\"image2.jpg\" alt=\"Image 2\" />\n    <img src=\"image3.jpg\" alt=\"Image 3\" />\n    <img src=\"image4.jpg\" alt=\"Image 4\" />\n</div>\n```\n```css\n.image-container {\n    display: flex; /* 使用 Flexbox 布局 */\n    flex-wrap: wrap; /* 等比例图片自动换行到下一行 */\n    justify-content: space-between; /* 图片之间间距相等 */\n}\nimg {\n    width: 25%; /* 将每个图片的宽度设置为 25% 使其等比例放缩 */\n    height: auto; /* 高度设置为 auto，使其自适应宽度 */\n    margin-bottom: 1rem; /* 使用 margin 来设置图片之间的间距 */\n    object-fit: cover; /* 自适应填充图片容器，保持图片比例 */\n}\n```\n> 扩展**display:flex**\n\n```css\ndiv{\n\tdisplay: flex\n\tflex-direction: row;/*属性决定主轴的方向（即项目的排列方向）*/\n    /*\n    - row（默认值）:主轴为水平方向，起点在左端。\n    - row-reverse:主轴为水平方向，起点在右端。\n    - column:主轴为垂直方向，起点在上沿。\n    - column-reverse:主轴为垂直方向，起点在下沿。\n    */\n    flex-wrap: nowrap;/*属性决定了如果一条轴线排不下,如何换行*/\n    /*\n    - nowrap（默认）:不换行。\n    - wrap:换行，第一行在上方。\n    - wrap-reverse:换行，第一行在下方。\n    */\n    justify-content: flex-start;/*属性定义了项目在主轴上的对齐方式。*/\n    /*\n    - flex-start（默认值）:左对齐\n    - flex-end:右对齐\n    - center:居中\n    - space-between:两端对齐，项目之间的间隔都相等。\n    - space-around:每个项目两侧的间隔相等。\n    */\n    align-items: stretch;/*属性定义项目在交叉轴上如何对齐。简单来讲，假如我们将flex-direction设置为row，即主轴为行。align-items可以决定元素在列上的布局*/\n    /*\n    - flex-start:交叉轴的起点对齐，一行根据上边对齐。\n    - flex-end:交叉轴的终点对齐，一行根据下边对齐。\n    - center:交叉轴的中点对齐。\n    - baseline:项目的第一行文字的基线对齐。\n    - stretch（默认值）:如果项目未设置高度或设为auto，将占满整个容器的高度。\n    */\n    align-content: stretch;/*属性定义了多根轴线的对齐方式。如果项目只有一根轴线，该属性不起作用。简单来讲，假如我们将flex-direction设置为row，即主轴为行。align-content决定了出现很多行时，这些行之间怎么对齐。其有一下几个属性：*/\n    /*\n    - flex-start:与交叉轴的起点对齐，跟作文一样，一行一行紧挨着。\n    - flex-end:与交叉轴的终点对齐，跟 flex-start类型，不过时从底部开始数。\n    - center:与交叉轴的中点对齐，从中间向下向上扩散。\n    - space-between:与交叉轴两端对齐，轴线之间的间隔平均分布。\n    - space-around:每根轴线两侧的间隔都相等。\n    - stretch（默认值）:轴线占满整个交叉轴。\n    */\n}\n\n```\n\n## 隐藏滚动条\n\n```css\n/**css隐藏滚动条**/\n.class::-webkit-scrollbar{ width:0 !important /*display:none;*/}\n::-webkit-scrollbar{width:0 !important /*display:none;*/}\n```\n\n## 渐变\n\n```css\n文字{\n    /*\n    linear-gradient是CSS中的一个渐变函数，用于在元素的背景中创建一个沿着一条直线方向的颜色渐变效果。linear-gradient函数的基本语法如下：background: linear-gradient(direction, color stop1, color stop2, ...);其中的参数解释如下：\t\n    - direction: 表示渐变的方向，可以是角度、关键字（top、bottom、left、right、to top left、to bottom right等）以及渐变轴线（由坐标(x1,y1)和(x2,y2)确定的一条直线，可使用两个坐标的百分比表示），也可以是任意组合;\n    - color stop: 表示渐变的颜色及其所在的位置，可以定义多个颜色值，用逗号分隔。例如，color stop1可以表示位于渐变的起始点的颜色停止点，而color stop2则对应着终点的颜色停止点。\n\n    CSS3渐变也支持透明度（transparent），可用于创建减弱变淡的效果。为了添加透明度，我们使用rgba()函数来定义颜色节点。rgba()函数中的最后一个参数可以是从0到1的值，它定义了颜色的透明度：0表示完全透明，1表示完全不透明。\n    */\n\tbackground-image:-webkit-linear-gradient(bottom,#708a41,#8585a5,#4b8e9a);\n    background-image: linear-gradient(top, hsla(0, 0%, 100%, .2) 1px, hsla(0, 0%, 100%, 0) 1px, hsla(0, 0%, 0%, .1) 100%);\n    /*\n     background-clip属性指定背景绘制区域。\n       - border-box默认值。背景绘制在边框方框内（剪切成边框方框）。\n       - padding-box背景绘制在衬距方框内（剪切成衬距方框）。\n       - content-box背景绘制在内容方框内（剪切成内容方框）。\n     */\n    -webkit-background-clip:text;\n    \n    /**文字中填充颜色transparent:透明色**/\n    -webkit-text-fill-color:transparent;\n    \n    /**text-stroke(文本边框)是text-stroke-width和text-stroke-color（边框填充颜色）两个属性的简写**/\n    -webkit-text-stroke:6px transparent;\n}\n背景图{\n    /*从下到上，从蓝色开始渐变、到高度40%位置是绿色渐变开始、最后以红色结束*/\n    background-image: linear-gradient(0deg, blue, green 40%, red);\n    /**创建一个从圆心开始，向四周渐变的径向渐变效果**/\n    background: radial-gradient(circle at center, #ffafbd, #ffc3a0);\n    /**创建一个从最外部向圆心渐变的径向渐变效果,farthest-corner关键字会将圆心设置在最远的角落，而不是默认的居中位置**/\n    background: radial-gradient(circle farthest-corner at center, #ffafbd, #ffc3a0);\n    /*\n    radial-gradient是CSS中的一个渐变函数，用于在元素的背景中创建一个从一个中心向周围辐射的颜色渐变效果。\n    radial-gradient函数的基本语法如下：background: radial-gradient(shape size at position, start-color, ..., last-color);其中的参数解释如下：\n    shape: 表示渐变形状，可以是circle(默认)或ellipse；\n    size: 表示渐变的大小\n      - closest-side表示最近侧的边缘，\n      - farthest-side表示最远侧的边缘，\n      - closest-corner表示最近角落，\n      - farthest-corner表示最远角落，\n      - contain表示至少需要占满整个容器，\n      - cover则表示覆盖整个容器；\n    at position: 表示渐变的中心位置，可以是长度(像素或百分比)或关键字(center、top、bottom、left、right等)\n    start-color和last-color: 表示渐变的起始颜色和结束颜色。可以定义多个颜色值，用逗号分隔。\n    */\n}\n\npage {\n    background: linear-gradient(-45deg, #ac7399, #a8a38e, #b6c24a);\n    background-size: 500% 500%;\n    animation: moiveAnimation 7s infinite;\n}\n\n@keyframes moiveAnimation {\n    0% {\n        background-position: 0% 50%\n    }\n\n    50% {\n        background-position: 100% 50%\n    }\n\n    100% {\n        background-position: 0% 50%\n    }\n}\n```\n> [超精美渐变色动态背景完整示例](https://blog.csdn.net/A757291228/article/details/124611342)\n\n## 文字自动换行\n\n```css\n/*文章内容自动换行*/\n#articleContent {\n    /*\n    - break-word:在长单词或URL地址内部进行换行。\n    - normal:只在允许的断字点换行（浏览器保持默认处理）\n    */\n    word-wrap: break-word;\n    \n    /*\n    - normal:使用浏览器默认的换行规则\n    - break-all:允许在单词内换行允许在单词内换行。\n    - keep-all:只能在半角空格或连字符处换行\n    */\n    word-break: break-all;\n    \n    /*\n     - normal:忽略多余的空白，只保留一个空白（默认）\n     - pre:保留空白(行为方式类似于html中的pre标签)\n     - nowrap:文本不会换行，会在在同一行上继续，直到遇到br标签为止\n     - pre-wrap:保留空白符序列，正常地进行换行\n     - pre-line:合并空白符序列，保留换行符\n     - inherit:从父元素继承white-space属性的值。\n     */\n    white-space: normal;\n}\n```\n> [前端正确处理“文字溢出”的思路](https://mp.weixin.qq.com/s/7-NxE7K6QPSPLHEKW8ME8g)\n\n## 动画@keyframes\n\n```css\n/**上下浮动动画**/\n@keyframes float {\n  /**0%是动画的开始时间，100%动画的结束时间。或者通过关键词\"from\"和\"to\"，等价于0%和100%。**/\t\n  100% {\n   /*\n    transform属性向元素应用2D或3D转换。该属性允许我们对元素进行旋转、缩放、移动或倾斜。应用多个属性使用空格如transform: rotate(45deg) scale(2) skew(10deg,5deg) translate(50px,90px);\n    - rotate(xxdeg)(2D),rotateX()(3D),rotateY()(3D),rotateZ(180deg)：以中心为基点，deg表示旋转的角度，为负数时表示逆时针旋转\n    - translate(x,y)，translateX(x)，translateY(y)：以中心为基点按照设定的x,y参数值,对元素进行进行平移。\n    - scale(x,y)，scaleX(X)，scaleY(Y)：缩放基数为1，如果其值大于1元素就放大，反之其值小于1为缩小。缩放后不影响文档流,不改变原有布局,元素还是会占用,和relative定位一样,或者可以考虑zoom属性\n    - skew(x,y)，skewX(x)，skewY(y)：以中心为基点，第一个参数是水平方向扭曲角度，第二个参数是垂直方向扭曲角度。\n    */\n   transform: translateY(20px);\n   \n   /**box-shadow属性可以设置一个或多个下拉阴影的框。**/\n   box-shadow: 0 40px 10px -18px hsla(0, 0%, 0%, .2), 0 40px 16px -12px hsla(0, 0%, 0%, .2)\n   transform-origin: right;/**(x,y)来改变元素基点**/\n  }\n}\n\n/**隐藏/显示动画**/\n@keyframes move{\n    0%{\n        transform: translate(100px, 0);\n        opacity: 0;\n    }\n    50%{\n        transform: translate(50px, 0);\n        opacity: 0.5;\n    }\n    100%{\n        transform: translate(0, 0);\n        opacity: 1;\n\t}\n}\n\n/**使用**/\ndiv{\n    box-shadow: 0 60px 12px -18px hsla(0, 0%, 0%, .1), 0 60px 20px -12px hsla(0, 0%, 0%, .1);\n    /*\n     animation: name duration timing-function delay iteration-count direction;\n     animation-name:规定需要绑定到选择器的keyframe名称。\n     animation-duration:规定完成动画所花费的时间，以秒或毫秒计\n     animation-timing-function:规定动画的速度曲线。\n       - linear:动画从头到尾的速度是相同的\n       - ease:默认。动画以低速开始，然后加快,在结束前变慢\n       - ease-in:动画以低速开始\n       - ease-out:动画以低速结束\n       - ease-in-out:动画以低速开始和结束\n       - cubic-bezier(n,n,n,n):在cubic-bezier函数中自己的值\n     animation-delay:规定在动画开始之前的延迟。\n     animation-iteration-count:规定动画应该播放的次数。默认为1次，可以填写数字\n     animation-direction:规定是否应该轮流反向播放动画。如果animation-direction值是“alternate”，则动画会在奇数次数（1、3、5等等）正常播放，而在偶数次数（2、4、6等等）向后播放。如果把动画设置为只播放一次，则该属性没有效果\n    */\n    animation: float 1s infinite ease-in-out alternate\n    /**animation: move .4s linear 1 normal**/\n}\ndiv:hover{\n    /*\n     animation-play-state属性规定动画正在运行还是暂停。只有两个属性可以设置：\n       - paused:规定动画已暂停\n       - running:规定动画正在播放\n     */\n\tanimation-play-state: paused\n     \n    /*\n     animation-fill-mode属性规定动画在播放之前或之后，其动画效果是否可见。(规定当动画不播放时（当动画完成时或当动画有一个延迟为开始播放时）要用到的元素样式)\n       - none表示等待期和完成期，元素样式都为初始状态样式，不受动画定义（@keyframes）的影响\n       - both表示等待期样式为第一帧样式，完成期保持最后一帧样式\n       - backwards表示等待期为第一帧样式，完成期跳转为初始样式\n       - forwards表示等待期保持初始样式，完成期间保持最后一帧样式\n     */\n     animation-fill-mode:none;\n}\n\n/**鼠标滑过翻转**/\nimg:hover {\n  animation: fadenum 2s;\n}\n@keyframes fadenum {\n   100%{ transform:rotate(360deg);}\n}\n\n```\n\n## transition\n\n```css\n/**鼠标滑动图标旋转**/\ndiv{\n    -webkit-transform: rotate(3600deg) !important;\n    -moz-transform: rotate(360deg) !important;\n    -o-transform: rotate(360deg) !important;\n    -ms-transform: rotate(360deg) !important;\n    transform: rotate(360deg) !important;\n    \n    /*\n     transition属性是一个简写属性，用于设置四个过渡属性.\n       - transition-property:规定设置过渡效果的CSS属性的名称。none没有属性会获得过渡效果。all所有属性都将获得过渡效果。property定义应用过渡效果的CSS属性名称列表，列表以逗号分隔。\n       - transition-duration:规定完成过渡效果需要多少秒或毫秒。\n       - transition-timing-function:规定速度效果的速度曲线。\n         -- inear规定以相同速度开始至结束的过渡效果（等于cubic-bezier(0,0,1,1)）\n         -- ease规定慢速开始，然后变快，然后慢速结束的过渡效果（cubic-bezier(0.25,0.1,0.25,1)）\n         -- ease-in规定以慢速开始的过渡效果（等于cubic-bezier(0.42,0,1,1)）\n         -- ease-out规定以慢速结束的过渡效果（等于cubic-bezier(0,0,0.58,1)）\n         -- ease-in-out规定以慢速开始和结束的过渡效果（等于cubic-bezier(0.42,0,0.58,1)）。\n         -- cubic-bezier(n,n,n,n)在cubic-bezier函数中定义自己的值。可能的值是0至1之间的数值。\n       - transition-delay:定义过渡效果何时开始。\n     */\n    -webkit-transition: all .7s;\n    -moz-transition: all .7s;\n    -o-transition: all .7s;\n    transition: all .7s;\n}\n\na{\n    /*光标样式*/\n    cursor: pointer;\n}\n```\n\n## 网站变灰\n\n```css\n/*在想要变灰的控件里加入此属性即可,如果想要网站整体变灰在html{}里面加入即可*/\n.gray {\n    -webkit-filter: grayscale(100%);\n    -webkit-filter:grayscale(1);\n    -moz-filter: grayscale(100%);\n    -ms-filter: grayscale(100%);\n    -o-filter: grayscale(100%);\n    filter: grayscale(100%);\n    filter: progid:DXImageTransform.Microsoft.BasicImage(grayscale=1);\n    filter:url(\"data:image/svg+xml;utf8,<svg xmlns=\\'http://www.w3.org/2000/svg\\'><filter id=\\'grayscale\\'><feColorMatrix type=\\'matrix\\' values=\\'0.3333 0.3333 0.3333 0 0 0.3333 0.3333 0.3333 0 0 0.3333 0.3333 0.3333 0 0 0 0 0 1 0\\'/></filter></svg>#grayscale\");\n}\n```\n\n\n## css伪类\n\n```css\n:link /*应用于未被访问过的链接*/\n:hover /*应用于鼠标悬停到的元素*/\n:active /*应用于被激活的元素*/\n:visited /*应用于被访问过的链接，与:link互斥。*/\n:focus /*应用于拥有键盘输入焦点的元素。*/\n\n:first-child /*选择某个元素的第一个子元素*/\n:last-child /*选择某个元素的最后一个子元素*/\n:nth-child(n) /*匹配属于其父元素的第n个子元素，不论元素的类型*/\n:nth-last-child() /*从这个元素的最后一个子元素开始算,选匹配属于其父元素的第n个子元素，不论元素的类型*/\n:nth-of-type() /*规定属于其父元素的第n个指定元素*/\n:nth-last-of-type() /*从元素的最后一个开始计算,规定属于其父元素的指定元素*/\n:first-of-type /*选择一个上级元素下的第一个同类子元素*/\n:last-of-type /*选择一个上级元素的最后一个同类子元素*/\n:only-child /*选择它的父元素的唯一一个子元素*/\n:only-of-type /*选择一个元素是它的上级元素的唯一一个相同类型的子元素*/\n:checked /*匹配被选中的input元素，这个input元素包括radio和checkbox。*/\n:empty /*选择的元素里面没有任何内容。*/\n:disabled /*匹配禁用的表单元素。*/\n:enabled /*匹配没有设置disabled属性的表单元素。*/\n:valid /*匹配条件验证正确的表单元素。*/\n:in-range /*选择具有指定范围内的值的<input>元素。*/\n:invalid input:invalid /*选择所有具有无效值的<input>元素。*/\n:optional input:optional /*选择不带\"required\"属性的<input>元素。*/\n:lang(language)\tp:lang(it) /*选择每个lang属性值以\"it\"开头的<p>元素。*/\n:focus /*选择获得焦点的<input>元素。*/\n:not(selector) :not(p) /*选择每个非<p>元素的元素。*/\n:out-of-range input:out-of-range /*选择值在指定范围之外的<input>元素。*/\n:read-only input:read-only /*选择指定了\"readonly\"属性的<input>元素。*/\n:read-write\tinput:read-write /*选择不带\"readonly\"属性的<input>元素。*/\n:required input:required /*选择指定了\"required\"属性的<input>元素。*/\n:root root /*选择元素的根元素。*/\n:target\t#news:target /*选择当前活动的#news元素（单击包含该锚名称的URL）*/\n```\n\n## css伪元素\n```css\n::after /*例: p::after\t在每个<p>元素之后插入内容。*/\n::before /*例: p::before 在每个<p>元素之前插入内容。*/\n::first-letter /*例: p::first-letter 选择每个<p>元素的首字母。*/\n::first-line /*例: p::first-line 选择每个<p>元素的首行。*/\n::selection /*例: p::selection 选择用户选择的元素部分。*/\n::marker /*例: li::marker {content:'>';} 把li前面的'•'变成'>'*/\n```\n> [CSS伪元素](https://www.w3school.com.cn/css/css_pseudo_elements.asp)\n\n## 21个超实用的CSS技巧\n\n### 文档布局\n仅用两行CSS，就可以创建响应式文档样式布局。\n\n```css\n.parent{\n  display: grid;\n  grid-template-columns: minmax(150px, 25%) 1fr;\n}\n```\n\n### 自定义光标\n\n```css\nhtml{\n  cursor:url('no.png'), auto;\n}\n```\n\n### 用图像填充文本\n\n```css\nh1{\n  background-image: url('images/flower.jpg');\n  background-clip: text;\n  color: transparent;\n  background-color: white;\n}\n```\n> 注意：使用此技术时，请始终指定background-color，因为如果由于某种原因图像未加载，可以将其用作回退值。\n\n### 为文本添加描边效果\n\n使用text-stroke属性可以使文本更清晰可见，因为会向文本添加描边笔触或轮廓。\n\n```css\n/* Apply a 5px wide crimson text stroke to h1 elements */\nh1 {\n  -webkit-text-stroke: 5px crimson;\n  text-stroke: 5px crimson;\n}\n```\n\n### :paused伪类\n\n使用:paused选择器在暂停状态下设置媒体元素的样式，与:paused类似的还有:playing。\n\n```css\n/* 目前只支持Safari浏览器 */\nvideo:paused {\n  opacity: 0.6;\n}\n```\n\n### 强调文本\n\n使用text-emphasis属性将强调标记应用于文本元素。你可以指定任意字符串（包括表情符号）作为值。\n\n```css\nh1 {\n  text-emphasis: \"⏰\";\n}\n```\n\n### 首字母下沉\n\n避免不必要的span，改用伪元素来设置内容的样式，同样，与:first-letter伪元素类似的还有:first-line伪元素。\n\n```css\nh1::first-letter{\n  font-size: 2rem;\n  color:#ff8A00;\n}\n```\n\n### 变量回退值\n\n```css\n:root {\n  --orange: orange;\n  --coral: coral;\n}\n\nh1 {\n  color: var(--black, crimson);\n}\n```\n\n### 更改书写模式\n\n你可以使用书写模式属性来指定文本在网站上的布局方式，垂直或水平布局。\n\n```css\n<h1>Cakes & Bakes</h1>\nh1 {\n  writing-mode: sideways-lr;\n}\n```\n\n### 彩虹动画\n\n为元素创建连续循环的颜色动画以吸引用户的注意力。\n\n```css\nbutton{\n  animation: rainbow-animation 200ms linear infinite;\n}\n\n@keyframes rainbow-animation {\n  to{\n    filter: hue-rotate(0deg);\n  }\n from{\n    filter: hue-rotate(360deg);\n  }\n}\n```\n\n### 悬停缩放\n\n```css\n/* 定义图片容器的高度和宽度，以及设置图元溢出时隐藏 */\n.img-container {\n  height: 250px; width: 250px; overflow: hidden;\n }\n\n/* 让图片填充整个容器 */\n\n.img-container img {\n  height: 100%; width: 100%; object-fit: cover; \n  transition: transform 200m ease-in;\n }\n\n img:hover{\n  transform: scale(1.2);\n }\n```\n\n### 属性选择器\n\n使用属性选择器根据属性选择HTML元素。\n\n```css\n<a href=\"\">HTML</a>\n<a>CSS</a>\n<a href=\"\">JavaScript</a>\n/* 为每个带href的a元素设置颜色 */\n\na[href] {\n  color: crimson;\n}\n```\n\n### 剪切元素\n\n使用clip-path属性创建有趣的视觉效果，例如将元素剪裁为自定义的三角形或六边形形状。\n\n```css\ndiv {\n  height: 150px;\n  width: 150px;\n  background-color: crimson;\n  clip-path: polygon(50% 0%, 0% 100%, 100% 100%);\n}\n```\n\n### 检测属性支持\n\n使用CSS @support rule直接在CSS中检测对CSS功能的支持。\n\n```css\n@supports (accent-color: #74992e) {\n/* 如果浏览器支持，则以下代码可以运行 */\n  blockquote {\n    color: crimson;\n  }\n\n}\n```\n\n### CSS嵌套\n\nCSS工作组一直在研究如何将嵌套添加到CSS中。通过嵌套，我们将能够编写更直观、更有条理、更高效的CSS。\n\n```css\n<header class=\"header\">\n  <p class=\"text\">Lorem ipsum, dolor</p>\n</header>\n/* 你可以在Safari浏览器中尝试使用CSS嵌套*/\n.header{\n  background-color: salmon;\n  .text{\n    font-size: 18px;\n  }\n}\n```\n\n### clamp函数\n\nclamp()函数可用于响应式和流畅的排版。\n\n```css\n/* Syntax: clamp(minimum, preferred, maximum) */\nh1{\n  font-size: clamp(2.25rem,6vw,4rem);\n}\n```\n\n### 设置可选字段的样式\n\n你可以使用:optional伪类设置表单字段的样式，例如输入框、下拉框和文本框，这些字段上没有必要属性。\n\n```css\n*:optional{\n  background-color: green;\n}\n```\n\n### 字间距属性\n\n使用word-spacing属性指定两个单词之间的空格长度。\n\n```css\np {\n  word-spacing: 1.245rem;\n}\n```\n\n### 创建渐变阴影\n\n创建渐变阴影以提供独特的用户体验。\n\n```css\n:root{\n  --gradient: linear-gradient(to bottom right, crimson, coral);\n}\n\ndiv {\n  height: 200px;\n  width: 200px;\n  background-image: var(--gradient);\n  border-radius: 1rem;\n  position: relative;\n}\n\ndiv::after {\n  content: \"\";\n  position: absolute;\n  inset: 0;\n  background-image: var(--gradient);\n  border-radius: inherit;\n  filter: blur(25px) brightness(1.5);\n  transform: translateY(15%) scale(0.95);\n  z-index: -1;\n}\n```\n\n### 更改标题位置\n\n使用caption-side属性将表格标题放在表格的指定一侧。\n\n### 创建文本列\n\n使用column属性为文本元素制作漂亮的列布局。\n\n```css\np{\n  column-count: 3;\n  column-gap: 4.45rem;\n  column-rule: 2px dotted crimson;\n}\n```\n\n## 细数那些惊艳一时的CSS属性\n\n### position: sticky\n\n标题在滚动的时候，会一直贴着最顶上。这种场景实际上很多：比如表格的标题栏、网站的导航栏、手机通讯录的人名首字母标题等等。\n\ncss部分\n```css\n.container {\n    background-color: oldlace;\n    height: 200px;\n    width: 140px;\n    overflow: auto;\n}\n.container div {\n    height: 20px;\n    background-color: aqua;\n    border: 1px solid;\n}\n.container .header {\n    position: sticky;\n    top: 0;\n    background-color: rgb(187, 153, 153);\n}\n```\nhtml部分\n```html\n<div class=\"container\">\n  <div class=\"header\">Header</div>\n  <div>1</div>\n  <div>2</div>\n  <div>3</div>\n</div>\n```\n\n### :empty选择器\n\n平时开发的时候数据都是通过请求接口获取的，也会存在接口没有数据的情况。这个时候正常的做法是给用户一个提示，让用户知道当前不是出bug了，而是确实没有数据。一般的做法是我们人为的判断当前数据返回列表的长度是否为0，如果为0则显示一个“暂无数据”给用户，反之则隐藏该提示。写过Vue的小伙伴是不是经常这么做：\n\n```html\n<div>\n    <template v-if=\"datas.length\">\n        <div v-for=\"data in datas\"></div>\n    </template>\n    <template v-else>\n        <div>暂无数据</div>\n    </template>\n</div>\n```\n但是有了:empty这个选择器后，你大可以把这个活交给CSS来干。\n\n```css\n.container {\n    height: 400px;\n    width: 600px;\n    background-color: antiquewhite;\n    display: flex;\n    justify-content: center;\n    align-items: center;\n}\n.container:empty::after {\n    content: \"暂无数据\";\n}\n```\n\n### gap\n\n日常开发中，都有用过padding和margin吧，margin一般用做边距，让两个元素隔开一点距离，但是对于一些场景下，我们很难通过计算得到一个除的尽的值，比如100px我要让3个元素等分，且每个元素隔开10px，这就很尴尬了。没关系！我们可以用gap属性，gap属性它适用于Grid布局、Flex布局以及多列布局，并不一定只是Grid布局中可以使用。比如我们要让每个元素之间隔开20px，那么使用gap我们可以这样：\n\n```css\ndiv{\n    display: flex | grid;\n    gap: 20px;\n}\n```\n\n### background-clip: text\n\n大家平时background-clip是不是都用来做一些裁切效果？你知道它还有个属性值是text吗？background-clip:text用来做带背景的文字效果，相信大家平时浏览一些网站的时候都会看到类似的实现，实际上通过CSS我们也能做到这种效果，可不要傻傻的以为都是用制图工具做的。\n\n### user-select\n\n网页和APP有个不同点是，网页上的字是可以通过光标选中的，而APP的不行。有的小伙伴可能会疑惑：那我网页上也用不着这个属性啊？非也非也，我们知道现在很多新的技术产生，可以在APP上嵌套webview或者是网页，比如Electron做的桌面端应用，大家没见过哪个桌面端应用是可以光标选中的吧？而user-select属性可以禁用光标选中，让网页看着和移动端一样。\n\n### :invalid伪类\n\n:invalid表示任意内容未通过验证的input或其他form元素。什么意思呢？举个例子。这是一个表单。\n\n```html\n<form>\n  <label for=\"url_input\">Enter a URL:</label>\n  <input type=\"url\" id=\"url_input\" />\n  <br />\n  <br />\n  <label for=\"email_input\">Enter an email address:</label>\n  <input type=\"email\" id=\"email_input\" required/>\n</form>\n```\n我们的需求是让input当值有效时，元素颜色为绿色，无效时为红色。\n\n```css\ninput:invalid {\n  background-color: #ffdddd;\n}\nform:invalid {\n  border: 5px solid #ffdddd;\n}\ninput:valid {\n  background-color: #ddffdd;\n}\nform:valid {\n  border: 5px solid #ddffdd;\n}\ninput:required {\n  border-color: #800000;\n  border-width: 3px;\n}\ninput:required:invalid {\n  border-color: #C00000;\n}\n```\n\n有了:invalid属性后，我们就可以不用JS也能实现校验提示的效果了。\n\n### :focus-within伪类\n\n:focus-within表示一个元素获得焦点，或该元素的后代元素获得焦点，就会匹配上。\n\nCSS\n```css\nform {\n    border: 1px solid;\n    width: 400px;\n    height: 300px;\n    display: flex;\n    justify-content: center;\n    align-items: center;\n}\nform:focus-within {\n    box-shadow: 0px 4px 4px rgba(0, 0, 0, 0.3);\n    background-color: beige;\n}\n```\n\nHTML\n```html\n<form>\n  <input type=\"text\" id=\"ipt\" placeholder=\"请输入...\" />\n</form>\n```\n可以根据子元素的状态来改变父元素的样式，方便的很。也能玩出不少花样来。\n\n### mix-blend-mode:difference\n\nmix-blend-mode:difference属性描述了元素的内容应该与元素的直系父元素的内容和元素的背景如何混合。其中，difference表示差值。\n\n```css\n.mode {\n    display: flex;\n    justify-content: center;\n    align-items: center;\n    mix-blend-mode:difference;\n}\n.dark {\n    position: relative;\n    left: 6px;\n    height: 24px;\n    width: 24px;\n    background-color: grey;\n    border-radius: 50%;\n}\n.light {\n    mix-blend-mode:difference;\n    position: relative;\n    left: -6px;\n    height: 16px;\n    width: 16px;\n    border-radius: 50%;\n    border: 4px solid grey;\n}\n```\n\n## 超酷的纯CSS Loading效果\n\n为保证运行正常，咱先规定下：\n\n```css\n* {\n  box-sizing: border-box;\n}\n```\n\n### 平滑加载\n\n```css\n<div class=\"progress-1\"></div>\n.progress-1 {\n    width:120px;\n    height:20px;\n    background: linear-gradient(#000 0 0) 0/0% no-repeat #ddd;\n    animation:p1 2s infinite linear;\n}\n@keyframes p1 {\n    100% {background-size:100%}\n}\n```\n\n1. linear-gradient(#000 0 0)你可以理解为linear-gradient(#000 0 100%)，如果还不熟悉，复制linear-gradient(#000 0 50%, #f00 50% 0)，替换原先的部分跑一下。觉得linear-gradient(#000 0 0)别扭的话，直接写#000即可。\n2. 0/0%是background-position: 0;/background-size: 0;的简写。\n\n### 按步加载\n\n```css\n<div class=\"progress-2\"></div>\n.progress-2 {\n    width:120px;\n    height:20px;\n    border-radius: 20px;\n    background: linear-gradient(orange 0 0) 0/0% no-repeat lightblue;\n    animation:p2 2s infinite steps(10);\n}\n@keyframes p2 {\n    100% {background-size:110%}\n}\n```\n\n1. steps(10)是step(10, end)的简写，指明刚开始没有，所以有**第2点**的处理\n2. 100% {background-size:110%}添加多一个step的百分比，上面的step是10，所以是`100% + (1/10)*100% = 110%`\n\n### 条纹加载\n\n```css\n<div class=\"progress-3\"></div>\n.progress-3 {\n    width:120px;\n    height:20px;\n    border-radius: 20px;\n    background: repeating-linear-gradient(135deg,#f03355 0 10px,#ffa516 0 20px) 0/0% no-repeat,repeating-linear-gradient(135deg,#ddd 0 10px,#eee 0 20px) 0/100%;\n    animation:p3 2s infinite;\n}\n@keyframes p3 {\n    100% {background-size:100%}\n}\n```\nrepeating-linear-gradient(135deg,#ddd 0 10px,#eee 0 20px) 0/100%;画出灰色的斑马线条纹，repeating-linear-gradient(135deg,#f03355 0 10px,#ffa516 0 20px) 0/0% no-repeat则是进度条加载的条纹。\n\n### 虚线加载\n\n```css\n<div class=\"progress-4\"></div>\n.progress-4 {\n    width:120px;\n    height:20px;\n    -webkit-mask:linear-gradient(90deg,#000 70%,#0000 0) 0/20%;\n    background:linear-gradient(#000 0 0) 0/0% no-repeat #ddd;\n    animation:p4 2s infinite steps(6);\n}\n@keyframes p4 {\n    100% {background-size:120%}\n}\n```\n-webkit-mask默认有值repeat，不然遮罩不会有五个。\n\n### 电池加载\n\n```css\n<div class=\"progress-5\"></div>\n.progress-5 {\n    width:80px;\n    height:40px;\n    border:2px solid #000;\n    padding:3px;\n    background: repeating-linear-gradient(90deg,#000 0 10px,#0000 0 16px) 0/0% no-repeat content-box content-box;\n    position: relative;\n    animation:p5 2s infinite steps(6);\n}\n.progress-5::before {\n    content:\"\";\n    position: absolute;\n    top: 50%;\n    left:100%;\n    transform: translateY(-50%);\n    width:10px;\n    height: 10px;\n    border: 2px solid #000;\n}\n@keyframes p5 {\n    100% {background-size:120%}\n}\n```\n原作者对.progress-5::before伪元素实现如下：\n\n```css\n.progress-5::before {\n    content:\"\";\n    position: absolute;\n    top:-2px;\n    bottom:-2px;\n    left:100%;\n    width:10px;\n    background: linear-gradient(#0000   calc(50% - 7px),#000 0 calc(50% - 5px),#0000 0 calc(50% + 5px),#000 0 calc(50% + 7px),#0000 0) left /100% 100%,linear-gradient(#000 calc(50% - 5px),#0000 0 calc(50% + 5px),#000 0) left /2px 100%,linear-gradient(#0000 calc(50% - 5px),#000 0 calc(50% + 5px),#0000 0) right/2px 100%;\n    background-repeat:no-repeat;\n}\n```\n\n> #0000是透明，同等transparent\n\n### 内嵌加载\n\n```css\n<div class=\"progress-6\"></div>\n.progress-6 {\n    width:120px;\n    height:22px;\n    border-radius: 20px;\n    color: #514b82;\n    border:2px solid;\n    position: relative;\n}\n.progress-6::before {\n    content:\"\";\n    position: absolute;\n    margin:2px;\n    inset:0 100% 0 0;\n    border-radius: inherit;\n    background: #514b82;\n    animation:p6 2s infinite;\n}\n@keyframes p6 {\n    100% {inset:0}\n}\n```\ninset:0 100% 0 0;右边内缩100%，所以在keyframes部分需要将inset设置为0。\n\n### 珠链加载\n\n```css\n<div class=\"progress-7\"></div>\n.progress-7 {\n    width:120px;\n    height:24px;\n    -webkit-mask:radial-gradient(circle closest-side,#000 94%,#0000) 0 0/25% 100%,linear-gradient(#000 0 0) center/calc(100% - 12px) calc(100% - 12px) no-repeat;\n    background:linear-gradient(#25b09b 0 0) 0/0% no-repeat #ddd;\n    animation:p7 2s infinite linear;\n}\n@keyframes p7 {\n    100% {background-size:100%}\n}\n```\n\n遮罩-webkit-mask中radial-gradient是将宽度四等份，每份以最小closest-side的边为直径画圆。\n\n### 斑马线加载\n\n```css\n<div class=\"progress-8\"></div>\n.progress-8 {\n    width:60px;\n    height:60px;\n    border-radius: 50%;\n    -webkit-mask:linear-gradient(0deg,#000 55%,#0000 0) bottom/100% 18.18%;\n    background: linear-gradient(#f03355 0 0) bottom/100% 0% no-repeat #ddd;\n    animation:p8 2s infinite steps(7);\n}\n@keyframes p8 {\n    100% {background-size:100% 115%}\n}\n```\n\n对linear-gradient描绘的角度做调整，再加上蒙版。\n\n### 水柱加载\n\n```css\n<div class=\"progress-9\"></div>\n.progress-9 {\n    --r1: 154%;\n    --r2: 68.5%;\n    width:60px;\n    height:60px;\n    border-radius: 50%;\n    background:\n        radial-gradient(var(--r1) var(--r2) at top ,#0000 79.5%,#269af2 80%) center left,\n        radial-gradient(var(--r1) var(--r2) at bottom,#269af2 79.5%,#0000 80%) center center,\n        radial-gradient(var(--r1) var(--r2) at top ,#0000 79.5%,#269af2 80%) center right,\n        #ccc;\n    background-size: 50.5% 220%;\n    background-position: -100% 0%,0% 0%,100% 0%;\n    background-repeat:no-repeat;\n    animation:p9 2s infinite linear;\n}\n@keyframes p9 {\n    33%  {background-position: 0% 33% ,100% 33% ,200% 33% }\n    66%  {background-position: -100% 66%,0% 66% ,100% 66% }\n    100% {background-position: 0% 100%,100% 100%,200% 100%}\n}\n```\nradial-gradient画出水平面的波动，就三个圆。var(--r1)直接调用定义好的属性值。\n\n### 信号加载\n\n```css\n<div class=\"progress-10\"></div>\n.progress-10 {\n    width:120px;\n    height:60px;\n    border-radius:200px 200px 0 0;\n    -webkit-mask:repeating-radial-gradient(farthest-side at bottom ,#0000 0,#000 1px 12%,#0000 calc(12% + 1px) 20%);\n    background: radial-gradient(farthest-side at bottom,#514b82 0 95%,#0000 0) bottom/0% 0% no-repeat #ddd;\n    animation:p10 2s infinite steps(6);\n}\n@keyframes p10 {\n    100% {background-size:120% 120%}\n}\n```\n\n用repeating-radial-gradient方法画出环状的蒙版遮罩。radial-gradient从底部向上圆形渐变填充。\n\n### 3d加载\n\nhtml部分\n```html\n<body>\n<div class=\"pl\">\n\t<div class=\"pl__dot\"></div>\n\t<div class=\"pl__dot\"></div>\n\t<div class=\"pl__dot\"></div>\n\t<div class=\"pl__dot\"></div>\n\t<div class=\"pl__dot\"></div>\n\t<div class=\"pl__dot\"></div>\n\t<div class=\"pl__dot\"></div>\n\t<div class=\"pl__dot\"></div>\n\t<div class=\"pl__dot\"></div>\n\t<div class=\"pl__dot\"></div>\n\t<div class=\"pl__dot\"></div>\n\t<div class=\"pl__dot\"></div>\n\t<div class=\"pl__text\">Loading…</div>\n</div>\n</body>\n```\ncss部分\n```css\n* {\n  border: 0;\n  box-sizing: border-box;\n  margin: 0;\n  padding: 0;\n}\n\n:root {\n  --bg: #454954;\n  --fg: #e3e4e8;\n  --fg-t: rgba(227, 228, 232, 0.5);\n  --primary1: #255ff4;\n  --primary2: #5583f6;\n  --trans-dur: 0.3s;\n  font-size: calc(16px + (20 - 16) * (100vw - 320px) / (1280 - 320));\n}\n\nbody {\n  background-color: var(--bg);\n  background-image: linear-gradient(135deg, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.2));\n  color: var(--fg);\n  font: 1em/1.5 \"Varela Round\", Helvetica, sans-serif;\n  height: 100vh;\n  min-height: 360px;\n  display: grid;\n  place-items: center;\n  transition: background-color var(--trans-dur), color var(--trans-dur);\n}\n\n.pl {\n  box-shadow: 2em 0 2em rgba(0, 0, 0, 0.2) inset, -2em 0 2em rgba(255, 255, 255, 0.1) inset;\n  display: flex;\n  justify-content: center;\n  align-items: center;\n  position: relative;\n  letter-spacing: 0.1em;\n  text-transform: uppercase;\n  transform: rotateX(30deg) rotateZ(45deg);\n  width: 15em;\n  height: 15em;\n}\n.pl, .pl__dot {\n  border-radius: 50%;\n}\n.pl__dot {\n  animation-name: shadow;\n  box-shadow: 0.1em 0.1em 0 0.1em black, 0.3em 0 0.3em rgba(0, 0, 0, 0.5);\n  top: calc(50% - 0.75em);\n  left: calc(50% - 0.75em);\n  width: 1.5em;\n  height: 1.5em;\n}\n.pl__dot, .pl__dot:before, .pl__dot:after {\n  animation-duration: 2s;\n  animation-iteration-count: infinite;\n  position: absolute;\n}\n.pl__dot:before, .pl__dot:after {\n  content: \"\";\n  display: block;\n  left: 0;\n  width: inherit;\n  transition: background-color var(--trans-dur);\n}\n.pl__dot:before {\n  animation-name: pushInOut1;\n  background-color: var(--bg);\n  border-radius: inherit;\n  box-shadow: 0.05em 0 0.1em rgba(255, 255, 255, 0.2) inset;\n  height: inherit;\n  z-index: 1;\n}\n.pl__dot:after {\n  animation-name: pushInOut2;\n  background-color: var(--primary1);\n  border-radius: 0.75em;\n  box-shadow: 0.1em 0.3em 0.2em rgba(255, 255, 255, 0.4) inset, 0 -0.4em 0.2em #2e3138 inset, 0 -1em 0.25em rgba(0, 0, 0, 0.3) inset;\n  bottom: 0;\n  clip-path: polygon(0 75%, 100% 75%, 100% 100%, 0 100%);\n  height: 3em;\n  transform: rotate(-45deg);\n  transform-origin: 50% 2.25em;\n}\n.pl__dot:nth-child(1) {\n  transform: rotate(0deg) translateX(5em) rotate(0deg);\n  z-index: 5;\n}\n.pl__dot:nth-child(1), .pl__dot:nth-child(1):before, .pl__dot:nth-child(1):after {\n  animation-delay: 0s;\n}\n.pl__dot:nth-child(2) {\n  transform: rotate(-30deg) translateX(5em) rotate(30deg);\n  z-index: 4;\n}\n.pl__dot:nth-child(2), .pl__dot:nth-child(2):before, .pl__dot:nth-child(2):after {\n  animation-delay: -0.1666666667s;\n}\n.pl__dot:nth-child(3) {\n  transform: rotate(-60deg) translateX(5em) rotate(60deg);\n  z-index: 3;\n}\n.pl__dot:nth-child(3), .pl__dot:nth-child(3):before, .pl__dot:nth-child(3):after {\n  animation-delay: -0.3333333333s;\n}\n.pl__dot:nth-child(4) {\n  transform: rotate(-90deg) translateX(5em) rotate(90deg);\n  z-index: 2;\n}\n.pl__dot:nth-child(4), .pl__dot:nth-child(4):before, .pl__dot:nth-child(4):after {\n  animation-delay: -0.5s;\n}\n.pl__dot:nth-child(5) {\n  transform: rotate(-120deg) translateX(5em) rotate(120deg);\n  z-index: 1;\n}\n.pl__dot:nth-child(5), .pl__dot:nth-child(5):before, .pl__dot:nth-child(5):after {\n  animation-delay: -0.6666666667s;\n}\n.pl__dot:nth-child(6) {\n  transform: rotate(-150deg) translateX(5em) rotate(150deg);\n  z-index: 1;\n}\n.pl__dot:nth-child(6), .pl__dot:nth-child(6):before, .pl__dot:nth-child(6):after {\n  animation-delay: -0.8333333333s;\n}\n.pl__dot:nth-child(7) {\n  transform: rotate(-180deg) translateX(5em) rotate(180deg);\n  z-index: 2;\n}\n.pl__dot:nth-child(7), .pl__dot:nth-child(7):before, .pl__dot:nth-child(7):after {\n  animation-delay: -1s;\n}\n.pl__dot:nth-child(8) {\n  transform: rotate(-210deg) translateX(5em) rotate(210deg);\n  z-index: 3;\n}\n.pl__dot:nth-child(8), .pl__dot:nth-child(8):before, .pl__dot:nth-child(8):after {\n  animation-delay: -1.1666666667s;\n}\n.pl__dot:nth-child(9) {\n  transform: rotate(-240deg) translateX(5em) rotate(240deg);\n  z-index: 4;\n}\n.pl__dot:nth-child(9), .pl__dot:nth-child(9):before, .pl__dot:nth-child(9):after {\n  animation-delay: -1.3333333333s;\n}\n.pl__dot:nth-child(10) {\n  transform: rotate(-270deg) translateX(5em) rotate(270deg);\n  z-index: 5;\n}\n.pl__dot:nth-child(10), .pl__dot:nth-child(10):before, .pl__dot:nth-child(10):after {\n  animation-delay: -1.5s;\n}\n.pl__dot:nth-child(11) {\n  transform: rotate(-300deg) translateX(5em) rotate(300deg);\n  z-index: 6;\n}\n.pl__dot:nth-child(11), .pl__dot:nth-child(11):before, .pl__dot:nth-child(11):after {\n  animation-delay: -1.6666666667s;\n}\n.pl__dot:nth-child(12) {\n  transform: rotate(-330deg) translateX(5em) rotate(330deg);\n  z-index: 6;\n}\n.pl__dot:nth-child(12), .pl__dot:nth-child(12):before, .pl__dot:nth-child(12):after {\n  animation-delay: -1.8333333333s;\n}\n.pl__text {\n  font-size: 0.75em;\n  max-width: 5rem;\n  position: relative;\n  text-shadow: 0 0 0.1em var(--fg-t);\n  transform: rotateZ(-45deg);\n}\n/* Animations */\n@keyframes shadow {\n  from {\n    animation-timing-function: ease-in;\n    box-shadow: 0.1em 0.1em 0 0.1em black, 0.3em 0 0.3em rgba(0, 0, 0, 0.3);\n  }\n  25% {\n    animation-timing-function: ease-out;\n    box-shadow: 0.1em 0.1em 0 0.1em black, 0.8em 0 0.8em rgba(0, 0, 0, 0.5);\n  }\n  50%, to {\n    box-shadow: 0.1em 0.1em 0 0.1em black, 0.3em 0 0.3em rgba(0, 0, 0, 0.3);\n  }\n}\n@keyframes pushInOut1 {\n  from {\n    animation-timing-function: ease-in;\n    background-color: var(--bg);\n    transform: translate(0, 0);\n  }\n  25% {\n    animation-timing-function: ease-out;\n    background-color: var(--primary2);\n    transform: translate(-71%, -71%);\n  }\n  50%, to {\n    background-color: var(--bg);\n    transform: translate(0, 0);\n  }\n}\n@keyframes pushInOut2 {\n  from {\n    animation-timing-function: ease-in;\n    background-color: var(--bg);\n    clip-path: polygon(0 75%, 100% 75%, 100% 100%, 0 100%);\n  }\n  25% {\n    animation-timing-function: ease-out;\n    background-color: var(--primary1);\n    clip-path: polygon(0 25%, 100% 25%, 100% 100%, 0 100%);\n  }\n  50%, to {\n    background-color: var(--bg);\n    clip-path: polygon(0 75%, 100% 75%, 100% 100%, 0 100%);\n  }\n}\n```\n\n### 文字旋转扭曲\n\n整体实现思路如下：\n1. 构建n个同心的div矩形。每个矩形的尺寸逐渐递减，每个矩形的旋转延迟时间也逐渐递减，实现旋转时的扭曲效果。并且每个div矩形使用伪类::after来显示文本内容。\n2. 设置滤镜filter。用来设置文本的模糊效果以及明暗对比度。\n3. 设置animation旋转动画。让矩形在指定的周期内做360度旋转。\n\nhtml部分\n\n```html\n<body>\n<div class=\"twist\" id=\"textArea\"></div>\n<script type=\"text/javascript\">\n    var text = \"CSS3_Rotate\";\n    var size = 860;\n    var delay = 2;\n    // 首先我们通过JS往容器中生成n个同心div矩形，这里我们生成42个。\n    var areaCount = 42;\n    var container = document.getElementById(\"textArea\");\n    for (var i = 0; i < areaCount; i++) {\n        addArea(\n            size - (size / areaCount) * i,\n            delay - (delay / areaCount) * i,\n            container\n        );\n    }\n    function addArea(areaSize, areaDelay, parentContainer) {\n        var area = document.createElement(\"div\");\n        area.setAttribute(\n            \"style\",\n            \"--size:\" + areaSize + \"px;\" + \"--delay:\" + areaDelay + \"s;\"\n        );\n        area.setAttribute(\n            \"title\",\n            text\n        );\n        parentContainer.appendChild(area);\n    }\n</script>\n</body>\n```\n\ncss部分\n\n```css\n*, *::before, *::after {\n  padding: 0;\n  margin: 0 auto;\n  box-sizing: border-box;\n}\n\nbody {\n  font-family: \"Lobster\", cursive;\n  background-color: #000;\n  color: #fff;\n  min-height: 100vh;\n  display: grid;\n  place-items: center;\n  overflow:hidden;\n}\n\n.twist {\n  position: relative;\n  color: aquamarine;\n  font-size: 78px;\n  /*blur(2px)表示元素外边框2px外使用高斯模糊效果,contrast(4)设置对比度，默认是1。可以使用百分比也可以使用小数表示*/\n  filter: blur(2px) contrast(4);\n}\n.twist > div {\n  position: absolute;\n  width: var(--size);\n  height: var(--size);\n  background-color: #000;\n  transform: translate(-50%, -50%);\n  border-radius: 50%;\n  overflow: hidden;\n  -webkit-animation: twist 4s var(--delay) infinite ease-in-out;\n          animation: twist 4s var(--delay) infinite ease-in-out;\n}\n.twist > div::after {\n  content: attr(title);\n  position: absolute;\n  left: 50%;\n  top: 50%;\n  transform: translate(-50%, -50%);\n}\n@-webkit-keyframes twist {\n  0% {\n    transform: translate(-50%, -50%) rotateZ(0deg);\n  }\n  50%, 100% {\n    transform: translate(-50%, -50%) rotateZ(360deg);\n  }\n}\n/*旋转和淡入淡出的动画效果，每个div元素延迟旋转的时间在之前创建同心矩形的时候已经设置了*/\n@keyframes twist {\n  0% {\n    transform: translate(-50%, -50%) rotateZ(0deg);\n  }\n  50%, 100% {\n    transform: translate(-50%, -50%) rotateZ(360deg);\n  }\n}\n```\n\n\n## 相关文章\n\n- [CSS@规则](http://c.biancheng.net/css3/at-rule.html)\n- [64个超级有用的CSS资源](https://mp.weixin.qq.com/s/xYUjsf4IKYORqOLNNnEHlA)\n- [HTML、CSS demo](https://www.html5tricks.com/page/26)\n- [10个值得收藏的CSS资源](https://mp.weixin.qq.com/s/Sn22fn_70QHWD753eFZHiQ)\n- [13个让你值得一试的CSS技巧](https://mp.weixin.qq.com/s/F22r72FW1CCYug_OtarZdQ)\n- [10个顶级的CSS动画库](https://mp.weixin.qq.com/s/fkocUOGricoH4Zad4U3Wjw)\n","tags":["代码实战"],"categories":["CSS"]},{"title":"分布式ID","slug":"分布式ID","url":"/blog/posts/50bd139ed1e1/","content":"\n\n## 分布式ID介绍\n\n### 什么是ID？\n\n日常开发中，我们需要对系统中的各种数据使用ID唯一表示，比如用户ID对应且仅对应一个人，商品ID对应且仅对应一件商品，订单ID对应且仅对应一个订单。我们现实生活中也有各种ID，比如身份证ID对应且仅对应一个人、地址ID对应且仅对应一个地址。简单来说，ID就是数据的唯一标识。\n\n### 什么是分布式ID？\n\n分布式ID是分布式系统下的ID。分布式ID不存在与现实生活中，属于计算机系统中的一个概念。简单举一个分库分表的例子。\n\n我司的一个项目，使用的是单机MySQL。但是，没想到的是，项目上线一个月之后，随着使用人数越来越多，整个系统的数据量将越来越大。单机MySQL已经没办法支撑了，需要进行分库分表（推荐Sharding-JDBC）。在分库之后，数据遍布在不同服务器上的数据库，数据库的自增主键已经没办法满足生成的主键唯一了。我们如何为不同的数据节点生成全局唯一主键呢？这个时候就需要生成分布式ID了。\n\n![img](https://oss.javaguide.cn/github/javaguide/system-design/distributed-system/id-after-the-sub-table-not-conflict.png)\n\n### 分布式ID需要满足哪些要求?\n\n![img](https://oss.javaguide.cn/github/javaguide/system-design/distributed-system/distributed-id-requirements.png)\n\n分布式ID作为分布式系统中必不可少的一环，很多地方都要用到分布式ID。一个最基本的分布式ID需要满足下面这些要求：\n\n- **全局唯一**：ID的全局唯一性肯定是首先要满足的！\n- **高性能**：分布式ID的生成速度要快，对本地资源消耗要小。\n- **高可用**：生成分布式ID的服务要保证可用性无限接近于100%。\n- **方便易用**：拿来即用，使用方便，快速接入！\n\n除了这些之外，一个比较好的分布式ID还应保证：\n\n- **安全**：ID中不包含敏感信息。\n- **有序递增**：如果要把ID存放在数据库的话，ID的有序性可以提升数据库写入速度。并且，很多时候，我们还很有可能会直接通过ID来进行排序。\n- **有具体的业务含义**：生成的ID如果能有具体的业务含义，可以让定位问题以及开发更透明化（通过ID就能确定是哪个业务）。\n- **独立部署**：也就是分布式系统单独有一个发号器服务，专门用来生成分布式ID。这样就生成ID的服务可以和业务相关的服务解耦。不过，这样同样带来了网络调用消耗增加的问题。总的来说，如果需要用到分布式ID的场景比较多的话，独立部署的发号器服务还是很有必要的。\n\n## 分布式ID常见解决方案\n\n### 数据库\n\n#### 数据库主键自增\n\n这种方式就比较简单直白了，就是通过关系型数据库的自增主键产生来唯一的ID。\n\n![数据库主键自增](https://oss.javaguide.cn/github/javaguide/system-design/distributed-system/the-primary-key-of-the-database-increases-automatically.png)\n\n以MySQL举例，我们通过下面的方式即可。\n\n**1.创建一个数据库表。**\n\n```sql\nCREATE TABLE sequence_id (\n  id bigint(20) unsigned NOT NULL AUTO_INCREMENT,\n  stub char(10) NOT NULL DEFAULT '',\n  PRIMARY KEY (id),\n  UNIQUE KEY stub (stub)\n) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;\n```\n\nstub字段无意义，只是为了占位，便于我们插入或者修改数据。并且，给stub字段创建了唯一索引，保证其唯一性。\n\n**2.通过replace into来插入数据。**\n\n```java\nBEGIN;\nREPLACE INTO sequence_id (stub) VALUES ('stub');\nSELECT LAST_INSERT_ID();\nCOMMIT;\n```\n\n插入数据这里，我们没有使用insert into而是使用replace into来插入数据，具体步骤是这样的：\n\n1. 第一步：尝试把数据插入到表中。\n\n2. 第二步：如果主键或唯一索引字段出现重复数据错误而插入失败时，先从表中删除含有重复关键字值的冲突行，然后再次尝试把数据插入到表中。\n\n这种方式的优缺点也比较明显：\n\n- **优点**：实现起来比较简单、ID有序递增、存储消耗空间小\n- **缺点**：支持的并发量不大、存在数据库单点问题（可以使用数据库集群解决，不过增加了复杂度）、ID没有具体业务含义、安全问题（比如根据订单ID的递增规律就能推算出每天的订单量，商业机密啊！）、每次获取ID都要访问一次数据库（增加了对数据库的压力，获取速度也慢）\n\n#### 数据库号段模式\n\n数据库主键自增这种模式，每次获取ID都要访问一次数据库，ID需求比较大的时候，肯定是不行的。如果我们可以批量获取，然后存在在内存里面，需要用到的时候，直接从内存里面拿就舒服了！这也就是我们说的基于数据库的号段模式来生成分布式ID。数据库的号段模式也是目前比较主流的一种分布式ID生成方式。像滴滴开源的[Tinyid](https://github.com/didi/tinyid/wiki/tinyid原理介绍)就是基于这种方式来做的。不过，TinyId使用了双号段缓存、增加多db支持等方式来进一步优化。以MySQL举例，我们通过下面的方式即可。\n\n**1.创建一个数据库表。**\n\n```sql\nCREATE TABLE sequence_id_generator (\n  id int(10) NOT NULL,\n  current_max_id bigint(20) NOT NULL COMMENT '当前最大id',\n  step int(10) NOT NULL COMMENT '号段的长度',\n  version int(20) NOT NULL COMMENT '版本号',\n  biz_type int(20) NOT NULL COMMENT '业务类型',\n  PRIMARY KEY (id)\n) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;\n```\n\ncurrent_max_id字段和step字段主要用于获取批量ID，获取的批量id为：current_max_id~current_max_id+step。\n\n![数据库号段模式](https://oss.javaguide.cn/github/javaguide/system-design/distributed-system/database-number-segment-mode.png)\n\nversion字段主要用于解决并发问题（乐观锁）,biz_type主要用于表示业务类型。\n\n**2.先插入一行数据。**\n\n```sql\nINSERT INTO sequence_id_generator (id, current_max_id, step, version, biz_type)\nVALUES(1, 0, 100, 0, 101);\n```\n\n**3.通过SELECT获取指定业务下的批量唯一ID**\n\n```sql\nSELECT current_max_id, step,version FROM sequence_id_generator where biz_type = 101\n```\n\n结果：\n\n```text\nid\tcurrent_max_id\tstep\tversion\tbiz_type\n1\t0\t100\t0\t101\n```\n\n**4.不够用的话，更新之后重新SELECT即可。**\n\n```sql\nUPDATE sequence_id_generator SET current_max_id = 0+100, version=version+1 WHERE version = 0  AND biz_type = 101\nSELECT current_max_id, step,version FROM sequence_id_generator where biz_type = 101\n```\n\n结果：\n\n```text\nid\tcurrent_max_id\tstep\tversion\tbiz_type\n1\t100\t100\t1\t101\n```\n\n相比于数据库主键自增的方式，数据库的号段模式对于数据库的访问次数更少，数据库压力更小。另外，为了避免单点问题，你可以从使用主从模式来提高可用性。\n\n**数据库号段模式的优缺点:**\n\n- **优点**：ID有序递增、存储消耗空间小\n- **缺点**：存在数据库单点问题（可以使用数据库集群解决，不过增加了复杂度）、ID没有具体业务含义、安全问题（比如根据订单ID的递增规律就能推算出每天的订单量，商业机密啊！）\n\n#### NoSQL\n\n![img](https://oss.javaguide.cn/github/javaguide/system-design/distributed-system/nosql-distributed-id.png)\n\n一般情况下，NoSQL方案使用Redis多一些。我们通过Redis的incr命令即可实现对id原子顺序递增。\n\n```bash\n127.0.0.1:6379> set sequence_id_biz_type 1\nOK\n127.0.0.1:6379> incr sequence_id_biz_type\n(integer) 2\n127.0.0.1:6379> get sequence_id_biz_type\n\"2\"\n```\n\n为了提高可用性和并发，我们可以使用Redis Cluster。Redis Cluster是Redis官方提供的Redis集群解决方案（3.0+版本）。除了Redis Cluster之外，你也可以使用开源的Redis集群方案[Codis](https://github.com/CodisLabs/codis)（大规模集群比如上百个节点的时候比较推荐）。\n\n除了高可用和并发之外，我们知道Redis基于内存，我们需要持久化数据，避免重启机器或者机器故障后数据丢失。Redis支持两种不同的持久化方式：**快照（snapshotting，RDB）**、**只追加文件（append-onlyfile,AOF）**。并且，Redis4.0开始支持**RDB和AOF的混合持久化**（默认关闭，可以通过配置项aof-use-rdb-preamble开启）。\n\n\n**Redis方案的优缺点：**\n\n- **优点**：性能不错并且生成的ID是有序递增的\n- **缺点**：和数据库主键自增方案的缺点类似\n\n除了Redis之外，**MongoDB ObjectId**经常也会被拿来当做分布式ID的解决方案。\n\n![img](https://oss.javaguide.cn/github/javaguide/system-design/distributed-system/mongodb9-objectId-distributed-id.png)\n\n**MongoDB ObjectId**一共需要12个字节存储：\n\n- 0~3：时间戳\n- 3~6：代表机器ID\n- 7~8：机器进程ID\n- 9~11：自增值\n\n**MongoDB方案的优缺点：**\n\n- **优点**：性能不错并且生成的ID是有序递增的\n- **缺点**：需要解决重复ID问题（当机器时间不对的情况下，可能导致会产生重复ID）、有安全性问题（ID生成有规律性）\n\n### 算法\n\n#### UUID\n\nUUID是Universally Unique Identifier（通用唯一标识符）的缩写。UUID包含32个16进制数字（8-4-4-4-12）。JDK就提供了现成的生成UUID的方法，一行代码就行了。\n\n```java\n//输出示例：cb4a9ede-fa5e-4585-b9bb-d60bce986eaa\nUUID.randomUUID()\n```\n\n[RFC 4122](https://tools.ietf.org/html/rfc4122)中关于UUID的示例是这样的：\n\n![img](https://oss.javaguide.cn/github/javaguide/system-design/distributed-system/rfc-4122-uuid.png)\n\n我们这里重点关注一下这个Version(版本)，不同的版本对应的UUID的生成规则是不同的。5种不同的Version(版本)值分别对应的含义（参考[维基百科对于UUID的介绍](https://zh.wikipedia.org/wiki/通用唯一识别码)）：\n\n- **版本1**:UUID是根据时间和节点ID（通常是MAC地址）生成；\n- **版本2**:UUID是根据标识符（通常是组或用户ID）、时间和节点ID生成；\n- **版本3、版本5**:版本5-确定性UUID通过散列（hashing）名字空间（namespace）标识符和名称生成；\n- **版本4**:UUID使用[随机性](https://zh.wikipedia.org/wiki/随机性)或[伪随机性](https://zh.wikipedia.org/wiki/伪随机性)生成。\n\n下面是Version1版本下生成的UUID的示例：\n\n![Version1版本下生成的UUID的示例](https://oss.javaguide.cn/github/javaguide/system-design/distributed-system/version1-uuid.png)\n\nJDK中通过UUID的randomUUID()方法生成的UUID的版本默认为4。\n\n```java\nUUID uuid = UUID.randomUUID();\nint version = uuid.version();//4\n```\n\n另外，Variant(变体)也有4种不同的值，这种值分别对应不同的含义。需要用到的时候，去看看维基百科对于UUID的Variant(变体)相关的介绍即可。\n\n从上面的介绍中可以看出，UUID可以保证唯一性，因为其生成规则包括MAC地址、时间戳、名字空间（Namespace）、随机或伪随机数、时序等元素，计算机基于这些规则生成的UUID是肯定不会重复的。虽然，UUID可以做到全局唯一性，但是，我们一般很少会使用它。比如使用UUID作为MySQL数据库主键的时候就非常不合适：\n1. **数据库主键要尽量越短越好，而UUID的消耗的存储空间比较大（32个字符串，128位）。**\n2. **UUID是无顺序的，InnoDB引擎下，数据库主键的无序性会严重影响数据库性能。**\n\n最后，我们再简单分析一下UUID的优缺点:\n\n- **优点**：生成速度比较快、简单易用\n- **缺点**：存储消耗空间大（32个字符串，128位）、不安全(基于MAC地址生成UUID的算法会造成MAC地址泄露)、无序（非自增）、没有具体业务含义、需要解决重复ID问题（当机器时间不对的情况下，可能导致会产生重复ID）\n\n#### Snowflake(雪花算法)\n\nSnowflake是Twitter开源的分布式ID生成算法。Snowflake由64bit的二进制数字组成，这64bit的二进制被分成了几部分，每一部分存储的数据都有特定的含义：\n\n- **第0位**：符号位（标识正负），始终为0，没有用，不用管。\n- **第1~41位**：一共41位，用来表示时间戳，单位是毫秒，可以支撑2^41毫秒（约69年）\n- **第42~52位**：一共10位，一般来说，前5位表示机房ID，后5位表示机器ID（实际项目中可以根据实际情况调整）。这样就可以区分不同集群/机房的节点。\n- **第53~64位**：一共12位，用来表示序列号。序列号为自增值，代表单台机器每毫秒能够产生的最大ID数(2^12=4096),也就是说单台机器每毫秒最多可以生成4096个唯一ID。\n\n![Snowflake示意图](https://oss.javaguide.cn/github/javaguide/system-design/distributed-system/snowflake-distributed-id-schematic-diagram.png)\n\n如果你想要使用Snowflake算法的话，一般不需要你自己再造轮子。有很多基于Snowflake算法的开源实现比如美团的Leaf、百度的UidGenerator，并且这些开源实现对原有的Snowflake算法进行了优化。另外，在实际项目中，我们一般也会对Snowflake算法进行改造，最常见的就是在Snowflake算法生成的ID中加入业务类型信息。\n\n我们再来看看Snowflake算法的优缺点：\n\n- **优点**：生成速度比较快、生成的ID有序递增、比较灵活（可以对Snowflake算法进行简单的改造比如加入业务ID）\n- **缺点**：需要解决重复ID问题（依赖时间，当机器时间不对的情况下，可能导致会产生重复ID）。\n\n##### 雪花算法生成分布式ID\n\n```java\npublic class SnowFlake {\n\n    /**\n     * 起始的时间戳\n     */\n    private final static long START_STMP = 1480166465631L;\n\n    /**\n     * 每一部分占用的位数\n     */\n    private final static long SEQUENCE_BIT = 12; // 序列号占用的位数\n    private final static long MACHINE_BIT = 5; // 机器标识占用的位数\n    private final static long DATACENTER_BIT = 5;// 数据中心占用的位数\n\n    /**\n     * 每一部分的最大值\n     */\n    private final static long MAX_DATACENTER_NUM = -1L ^ (-1L << DATACENTER_BIT);\n    private final static long MAX_MACHINE_NUM = -1L ^ (-1L << MACHINE_BIT);\n    private final static long MAX_SEQUENCE = -1L ^ (-1L << SEQUENCE_BIT);\n\n    /**\n     * 每一部分向左的位移\n     */\n    private final static long MACHINE_LEFT = SEQUENCE_BIT;\n    private final static long DATACENTER_LEFT = SEQUENCE_BIT + MACHINE_BIT;\n    private final static long TIMESTMP_LEFT = DATACENTER_LEFT + DATACENTER_BIT;\n\n    private long datacenterId; // 数据中心\n    private long machineId; // 机器标识\n    private long sequence = 0L; // 序列号\n    private long lastStmp = -1L;// 上一次时间戳\n\n    public SnowFlake(long datacenterId, long machineId) {\n        if (datacenterId > MAX_DATACENTER_NUM || datacenterId < 0) {\n            throw new IllegalArgumentException(\"datacenterId can't be greater than MAX_DATACENTER_NUM or less than 0\");\n        }\n        if (machineId > MAX_MACHINE_NUM || machineId < 0) {\n            throw new IllegalArgumentException(\"machineId can't be greater than MAX_MACHINE_NUM or less than 0\");\n        }\n        this.datacenterId = datacenterId;\n        this.machineId = machineId;\n    }\n\n    /**\n     * 产生下一个ID\n     *\n     * @return\n     */\n    public synchronized long nextId() {\n        long currStmp = getNewstmp();\n        if (currStmp < lastStmp) {\n            throw new RuntimeException(\"Clock moved backwards.  Refusing to generate id\");\n        }\n\n        if (currStmp == lastStmp) {\n            // 相同毫秒内，序列号自增\n            sequence = (sequence + 1) & MAX_SEQUENCE;\n            // 同一毫秒的序列数已经达到最大\n            if (sequence == 0L) {\n                currStmp = getNextMill();\n            }\n        } else {\n            // 不同毫秒内，序列号置为0\n            sequence = 0L;\n        }\n\n        lastStmp = currStmp;\n\n        return (currStmp - START_STMP) << TIMESTMP_LEFT // 时间戳部分\n                | datacenterId << DATACENTER_LEFT // 数据中心部分\n                | machineId << MACHINE_LEFT // 机器标识部分\n                | sequence; // 序列号部分\n    }\n\n    private long getNextMill() {\n        long mill = getNewstmp();\n        while (mill <= lastStmp) {\n            mill = getNewstmp();\n        }\n        return mill;\n    }\n\n    private long getNewstmp() {\n        return System.currentTimeMillis();\n    }\n\n    public static void main(String[] args) {\n        SnowFlake snowFlake = new SnowFlake(2, 3);\n\n        for (int i = 0; i < (1 << 12); i++) {\n            System.out.println(snowFlake.nextId());\n        }\n\n    }\n}\n\n/**\n * 饿汉式单例模式实现雪花算法\n **/\nclass SnowFlakeSingleE {\n\n    private static SnowFlakeSingleE snowFlake = new SnowFlakeSingleE();\n\n    private SnowFlakeSingleE() {}\n\n    public static SnowFlakeSingleE getInstance() {\n        return snowFlake;\n    }\n\n    // 序列号，同一毫秒内用此参数来控制并发\n    private long sequence = 0L;\n    // 上一次生成编号的时间串，格式：yyMMddHHmmssSSS\n    private String lastTime = \"\";\n\n    public synchronized String getNum() {\n        String nowTime = getTime(); // 获取当前时间串，格式：yyMMddHHmmssSSS\n        String machineId = \"01\"; // 机器编号，这里假装获取到的机器编号是2。实际项目中可从配置文件中读取\n        // 本次和上次不是同一毫秒，直接生成编号返回\n        if (!lastTime.equals(nowTime)) {\n            sequence = 0L; // 重置序列号，方便下次使用\n            lastTime = nowTime; // 更新时间串，方便下次使用\n            return new StringBuilder(nowTime).append(machineId).append(sequence).toString();\n        }\n        // 本次和上次在同一个毫秒内，需要用序列号控制并发\n        if (sequence < 99) { // 序列号没有达到最大值，直接生成编号返回\n            sequence = sequence + 1;\n            return new StringBuilder(nowTime).append(machineId).append(sequence).toString();\n        }\n        // 序列号达到最大值，需要等待下一毫秒的到来\n        while (lastTime.equals(nowTime)) {\n            nowTime = getTime();\n        }\n        sequence = 0L; // 重置序列号，方便下次使用\n        lastTime = nowTime; // 更新时间串，方便下次使用\n        return new StringBuilder(nowTime).append(machineId).append(sequence).toString();\n    }\n\n    private String getTime() {\n        return LocalDateTime.now().format(DateTimeFormatter.ofPattern(\"yyMMddHHmmssSSS\"));\n    }\n}\n\n\n/**\n * 懒汉式单例模式实现雪花算法\n **/\nclass SnowFlakeSingleL {\n\n    private static SnowFlakeSingleL snowFlake = null;\n\n    private SnowFlakeSingleL() {}\n\n    public static SnowFlakeSingleL getInstance() {\n        if (snowFlake == null) {\n            synchronized (SnowFlakeSingleL.class) {\n                if (snowFlake == null) {\n                    snowFlake = new SnowFlakeSingleL();\n                }\n                return snowFlake;\n            }\n        }\n        return snowFlake;\n    }\n\n    // 序列号，同一毫秒内用此参数来控制并发\n    private long sequence = 0L;\n    // 上一次生成编号的时间串，格式：yyMMddHHmmssSSS\n    private String lastTime = \"\";\n\n    public synchronized String getNum() {\n        String nowTime = getTime(); // 获取当前时间串，格式：yyMMddHHmmssSSS\n        String machineId = \"01\"; // 机器编号，这里假装获取到的机器编号是2。实际项目中可从配置文件中读取\n        // 本次和上次不是同一毫秒，直接生成编号返回\n        if (!lastTime.equals(nowTime)) {\n            sequence = 0L; // 重置序列号，方便下次使用\n            lastTime = nowTime; // 更新时间串，方便下次使用\n            return new StringBuilder(nowTime).append(machineId).append(sequence).toString();\n        }\n        // 本次和上次在同一个毫秒内，需要用序列号控制并发\n        if (sequence < 99) { // 序列号没有达到最大值，直接生成编号返回\n            sequence = sequence + 1;\n            return new StringBuilder(nowTime).append(machineId).append(sequence).toString();\n        }\n        // 序列号达到最大值，需要等待下一毫秒的到来\n        while (lastTime.equals(nowTime)) {\n            nowTime = getTime();\n        }\n        sequence = 0L; // 重置序列号，方便下次使用\n        lastTime = nowTime; // 更新时间串，方便下次使用\n        return new StringBuilder(nowTime).append(machineId).append(sequence).toString();\n    }\n\n    private String getTime() {\n        return LocalDateTime.now().format(DateTimeFormatter.ofPattern(\"yyMMddHHmmssSSS\"));\n    }\n}\n```\n\n\n### 开源框架\n\n#### UidGenerator(百度)\n\n[UidGenerator](https://github.com/baidu/uid-generator)是百度开源的一款基于Snowflake(雪花算法)的唯一ID生成器。不过，UidGenerator对Snowflake(雪花算法)进行了改进，生成的唯一ID组成如下。\n\n![img](https://oss.javaguide.cn/github/javaguide/system-design/distributed-system/uidgenerator-distributed-id-schematic-diagram.png)\n\n可以看出，和原始Snowflake(雪花算法)生成的唯一ID的组成不太一样。并且，上面这些参数我们都可以自定义。UidGenerator官方文档中的介绍如下：\n\n![img](https://oss.javaguide.cn/github/javaguide/system-design/distributed-system/uidgenerator-introduction-official-documents.png)\n\n自18年后，UidGenerator就基本没有再维护了，我这里也不过多介绍。想要进一步了解的朋友，可以看看[UidGenerator的官方介绍](https://github.com/baidu/uid-generator/blob/master/README.zh_cn.md)。\n\n#### Leaf(美团)\n\n[Leaf](https://github.com/Meituan-Dianping/Leaf)是美团开源的一个分布式ID解决方案。这个项目的名字Leaf（树叶）起源于德国哲学家、数学家莱布尼茨的一句话：“There are no two identical leaves in the world”（世界上没有两片相同的树叶）。这名字起得真心挺不错的，有点文艺青年那味了！\n\nLeaf提供了**号段模式**和**Snowflake**(雪花算法)这两种模式来生成分布式ID。并且它支持双号段，还解决了雪花ID系统时钟回拨问题。不过，时钟问题的解决需要弱依赖于Zookeeper。Leaf的诞生主要是为了解决美团各个业务线生成分布式ID的方法多种多样以及不可靠的问题。Leaf对原有的号段模式进行改进，比如它这里增加了双号段避免获取DB在获取号段的时候阻塞请求获取ID的线程。简单来说，就是我一个号段还没用完之前，我自己就主动提前去获取下一个号段（图片来自于美团官方文章：[《Leaf—美团点评分布式ID生成系统》openinnewwindow](https://tech.meituan.com/2017/04/21/mt-leaf.html)）。\n\n![img](https://oscimg.oschina.net/oscnet/up-5c152efed042a8fe7e13692e0339d577f5c.png)\n\n根据项目README介绍，在4C8GVM基础上，通过公司RPC方式调用，QPS压测结果近5w/s，TP9991ms。\n\n#### Tinyid(滴滴)\n\n[Tinyid](https://github.com/didi/tinyid)是滴滴开源的一款基于数据库号段模式的唯一ID生成器。数据库号段模式的原理我们在上面已经介绍过了。Tinyid有哪些亮点呢？为了搞清楚这个问题，我们先来看看基于数据库号段模式的简单架构方案。（图片来自于Tinyid的官方wiki:[《Tinyid原理介绍》](https://github.com/didi/tinyid/wiki/tinyid原理介绍)）\n\n![img](https://oscimg.oschina.net/oscnet/up-4afc0e45c0c86ba5ad645d023dce11e53c2.png)\n\n在这种架构模式下，我们通过HTTP请求向发号器服务申请唯一ID。负载均衡router会把我们的请求送往其中的一台tinyid-server。这种方案有什么问题呢？在我看来（Tinyid官方wiki也有介绍到），主要由下面这2个问题：\n\n- 获取新号段的情况下，程序获取唯一ID的速度比较慢。\n- 需要保证DB高可用，这个是比较麻烦且耗费资源的。\n\n除此之外，HTTP调用也存在网络开销。\n\nTinyid的原理比较简单，其架构如下图所示：\n\n![img](https://oscimg.oschina.net/oscnet/up-53f74cd615178046d6c04fe50513fee74ce.png)\n\n相比于基于数据库号段模式的简单架构方案，Tinyid方案主要做了下面这些优化：\n\n- **双号段缓存**：为了避免在获取新号段的情况下，程序获取唯一ID的速度比较慢。Tinyid中的号段在用到一定程度的时候，就会去异步加载下一个号段，保证内存中始终有可用号段。\n- **增加多db支持**：支持多个DB，并且，每个DB都能生成唯一ID，提高了可用性。\n- **增加tinyid-client**：纯本地操作，无HTTP请求消耗，性能和可用性都有很大提升。\n\nTinyid的优缺点这里就不分析了，结合数据库号段模式的优缺点和Tinyid的原理就能知道。\n\n## 总结\n\n通过这篇文章，我基本上已经把最常见的分布式ID生成方案都总结了一波。除了上面介绍的方式之外，像ZooKeeper这类中间件也可以帮助我们生成唯一ID。需要结合实际项目来选择最适合自己的方案。\n\n> [原文链接](https://javaguide.cn/distributed-system/distributed-id.html)\n> \n>\n> [深度介绍分布式系统原理与设计](https://mp.weixin.qq.com/s/-30WmwoYHg0oWZSWedE5MQ)\n> [一口气说出9种分布式ID生成方式](https://mp.weixin.qq.com/s/zXchd2SEjLkHftCe9-2_-Q)\n> [七种分布式全局ID生成策略，你更爱哪种](https://mp.weixin.qq.com/s/jGq7SvVggZ7gNqM2SZ320Q)\n> [一起学习下一线大厂的分布式唯一ID生成方案](https://mp.weixin.qq.com/s/dEkkSCbQzfhH3NuXsbbY0w)","categories":["Java"]},{"title":"Java类加载","slug":"Java类加载","url":"/blog/posts/c83e06a22e1f/","content":"\n## 类文件结构详解\n\n\n### 回顾一下字节码\n\n在Java中，JVM可以理解的代码就叫做字节码（即扩展名为.class的文件），它不面向任何特定的处理器，只面向虚拟机。Java语言通过字节码的方式，在一定程度上解决了传统解释型语言执行效率低的问题，同时又保留了解释型语言可移植的特点。所以Java程序运行时比较高效，而且，由于字节码并不针对一种特定的机器，因此，Java程序无须重新编译便可在多种不同操作系统的计算机上运行。\n\nClojure（Lisp语言的一种方言）、Groovy、Scala等语言都是运行在Java虚拟机之上。下图展示了不同的语言被不同的编译器编译成.class文件最终运行在Java虚拟机之上。.class文件的二进制格式可以使用[WinHex](https://www.x-ways.net/winhex/)查看。\n\n![java虚拟机](https://my-blog-to-use.oss-cn-beijing.aliyuncs.com/bg/desktop类文件结构概览.png)\n\n可以说.class文件是不同的语言在Java虚拟机之间的重要桥梁，同时也是支持Java跨平台很重要的一个原因。\n\n### Class文件结构总结\n\n根据Java虚拟机规范，Class文件通过ClassFile定义，有点类似C语言的结构体。ClassFile的结构如下：\n\n```java\nClassFile {\n    u4             magic; //Class文件的标志\n    u2             minor_version;//Class的小版本号\n    u2             major_version;//Class的大版本号\n    u2             constant_pool_count;//常量池的数量\n    cp_info        constant_pool[constant_pool_count-1];//常量池\n    u2             access_flags;//Class的访问标记\n    u2             this_class;//当前类\n    u2             super_class;//父类\n    u2             interfaces_count;//接口\n    u2             interfaces[interfaces_count];//一个类可以实现多个接口\n    u2             fields_count;//Class文件的字段属性\n    field_info     fields[fields_count];//一个类可以有多个字段\n    u2             methods_count;//Class文件的方法数量\n    method_info    methods[methods_count];//一个类可以有个多个方法\n    u2             attributes_count;//此类的属性表中的属性数\n    attribute_info attributes[attributes_count];//属性表集合\n}\n```\n\n通过分析ClassFile的内容，我们便可以知道class文件的组成。\n\n![ClassFile内容分析](https://oss.javaguide.cn/java-guide-blog/16d5ec47609818fc.jpeg)\n\n下面这张图是通过IDEA插件jclasslib查看的，你可以更直观看到Class文件结构。\n\n![img](https://oss.javaguide.cn/java-guide-blog/image-20210401170711475.png)\n\n使用jclasslib不光可以直观地查看某个类对应的字节码文件，还可以查看类的基本信息、常量池、接口、属性、函数等信息。下面详细介绍一下Class文件结构涉及到的一些组件。\n\n#### 魔数（MagicNumber）\n\n\n```java\nu4             magic; //Class文件的标志\n```\n\n每个Class文件的头4个字节称为魔数（MagicNumber）,它的唯一作用是**确定这个文件是否为一个能被虚拟机接收的Class文件**。\n\n程序设计者很多时候都喜欢用一些特殊的数字表示固定的文件类型或者其它特殊的含义。\n\n#### Class文件版本号（Minor&MajorVersion）\n\n\n```java\nu2             minor_version;//Class的小版本号\nu2             major_version;//Class的大版本号\n```\n\n紧接着魔数的四个字节存储的是Class文件的版本号：第5和第6位是次版本号，第7和第8位是主版本号。每当Java发布大版本（比如Java8，Java9）的时候，主版本号都会加1。你可以使用`javap -v`命令来快速查看Class文件的版本号信息。高版本的Java虚拟机可以执行低版本编译器生成的Class文件，但是低版本的Java虚拟机不能执行高版本编译器生成的Class文件。所以，我们在实际开发的时候要确保开发的的JDK版本和生产环境的JDK版本保持一致。\n\n#### 常量池（Constant Pool）\n\n```java\nu2             constant_pool_count;//常量池的数量\ncp_info        constant_pool[constant_pool_count-1];//常量池\n```\n\n紧接着主次版本号之后的是常量池，常量池的数量是`constant_pool_count-1`（常量池计数器是从1开始计数的，将第0项常量空出来是有特殊考虑的，索引值为0代表“不引用任何一个常量池项”）。\n\n常量池主要存放两大常量：字面量和符号引用。字面量比较接近于Java语言层面的的常量概念，如文本字符串、声明为final的常量值等。而符号引用则属于编译原理方面的概念，包括下面三类常量：\n\n- 类和接口的全限定名\n- 字段的名称和描述符\n- 方法的名称和描述符\n\n常量池中每一项常量都是一个表，这14种表有一个共同的特点：开始的第一位是一个u1类型的标志位-tag来标识常量的类型，代表当前这个常量属于哪种常量类型。\n\n|               类型               | 标志（tag） |          描述          |\n| :------------------------------: | :---------: | :--------------------: |\n|        CONSTANT_utf8_info        |      1      |   UTF-8 编码的字符串   |\n|      CONSTANT_Integer_info       |      3      |       整形字面量       |\n|       CONSTANT_Float_info        |      4      |      浮点型字面量      |\n|        CONSTANT_Long_info        |     ５      |      长整型字面量      |\n|       CONSTANT_Double_info       |     ６      |   双精度浮点型字面量   |\n|       CONSTANT_Class_info        |     ７      |   类或接口的符号引用   |\n|       CONSTANT_String_info       |     ８      |    字符串类型字面量    |\n|      CONSTANT_Fieldref_info      |     ９      |     字段的符号引用     |\n|     CONSTANT_Methodref_info      |     10      |   类中方法的符号引用   |\n| CONSTANT_InterfaceMethodref_info |     11      |  接口中方法的符号引用  |\n|    CONSTANT_NameAndType_info     |     12      |  字段或方法的符号引用  |\n|     CONSTANT_MothodType_info     |     16      |      标志方法类型      |\n|    CONSTANT_MethodHandle_info    |     15      |      表示方法句柄      |\n|   CONSTANT_InvokeDynamic_info    |     18      | 表示一个动态方法调用点 |\n\n.class文件可以通过javap -v class类名指令来看一下其常量池中的信息(javap -v class类名 -> temp.txt：将结果输出到temp.txt文件)。\n\n#### 访问标志(Access Flags)\n\n\n```java\nu2             access_flags;//Class的访问标记\n```\n\n在常量池结束之后，紧接着的两个字节代表访问标志，这个标志用于识别一些类或者接口层次的访问信息，包括：这个Class是类还是接口，是否为public或者abstract类型，如果是类的话是否声明为final等等。\n\n类访问和属性修饰符:\n\n![类访问和属性修饰符](https://my-blog-to-use.oss-cn-beijing.aliyuncs.com/2019-6/访问标志.png)\n\n我们定义了一个Employee类\n\n```java\npackage top.snailclimb.bean;\npublic class Employee {\n   ...\n}\n```\n\n通过`javap -v class类名`指令来看一下类的访问标志。\n\n![查看类的访问标志](https://my-blog-to-use.oss-cn-beijing.aliyuncs.com/2019-6/查看类的访问标志.png)\n\n#### 当前类（This Class）、父类（Super Class）、接口（Interfaces）索引集合\n\n```java\nu2             this_class;//当前类\nu2             super_class;//父类\nu2             interfaces_count;//接口\nu2             interfaces[interfaces_count];//一个类可以实现多个接口\n```\n\nJava类的继承关系由类索引、父类索引和接口索引集合三项确定。类索引、父类索引和接口索引集合按照顺序排在访问标志之后，类索引用于确定这个类的全限定名，父类索引用于确定这个类的父类的全限定名，由于Java语言的单继承，所以父类索引只有一个，除了`java.lang.Object`之外，所有的Java类都有父类，因此除了`java.lang.Object`外，所有Java类的父类索引都不为0。接口索引集合用来描述这个类实现了那些接口，这些被实现的接口将按`implements`(如果这个类本身是接口的话则是`extends`)后的接口顺序从左到右排列在接口索引集合中。\n\n#### 字段表集合（Fields）\n\n```java\nu2             fields_count;//Class文件的字段的个数\nfield_info     fields[fields_count];//一个类会可以有个字段\n```\n\n字段表（field info）用于描述接口或类中声明的变量。字段包括类级变量以及实例变量，但不包括在方法内部声明的局部变量。\n\n**field info(字段表)的结构:**\n\n![字段表的结构](https://my-blog-to-use.oss-cn-beijing.aliyuncs.com/2019-6/字段表的结构.png)\n\n- **access_flags**:字段的作用域（public,private,protected修饰符），是实例变量还是类变量（static修饰符）,可否被序列化（transient修饰符）,可变性（final）,可见性（volatile修饰符，是否强制从主内存读写）。\n- **name_index**:对常量池的引用，表示的字段的名称；\n- **descriptor_index**:对常量池的引用，表示字段和方法的描述符；\n- **attributes_count**:一个字段还会拥有一些额外的属性，attributes_count存放属性的个数；\n- **attributes[attributes_count]**:存放具体属性具体内容。\n\n上述这些信息中，各个修饰符都是布尔值，要么有某个修饰符，要么没有，很适合使用标志位来表示。而字段叫什么名字、字段被定义为什么数据类型这些都是无法固定的，只能引用常量池中常量来描述。\n\n**字段的access_flag的取值:**\n\n![字段的access_flag的取值](https://oss.javaguide.cn/JVM/image-20201031084342859.png)\n\n#### 方法表集合（Methods）\n\n```java\nu2             methods_count;//Class文件的方法的数量\nmethod_info    methods[methods_count];//一个类可以有个多个方法\n```\n\nmethods_count表示方法的数量，而method_info表示方法表。\n\nClass文件存储格式中对方法的描述与对字段的描述几乎采用了完全一致的方式。方法表的结构如同字段表一样，依次包括了访问标志、名称索引、描述符索引、属性表集合几项。\n\n**method_info(方法表的)结构:**\n\n![方法表的结构](https://my-blog-to-use.oss-cn-beijing.aliyuncs.com/2019-6/方法表的结构.png)\n\n**方法表的access_flag取值：**\n\n![方法表的access_flag取值](https://oss.javaguide.cn/JVM/image-20201031084248965.png)\n\n注意：因为volatile修饰符和transient修饰符不可以修饰方法，所以方法表的访问标志中没有这两个对应的标志，但是增加了synchronized、native、abstract等关键字修饰方法，所以也就多了这些关键字对应的标志。\n\n#### 属性表集合（Attributes）\n\n\n```java\nu2             attributes_count;//此类的属性表中的属性数\nattribute_info attributes[attributes_count];//属性表集合\n```\n\n在Class文件，字段表，方法表中都可以携带自己的属性表集合，以用于描述某些场景专有的信息。与Class文件中其它的数据项目要求的顺序、长度和内容不同，属性表集合的限制稍微宽松一些，不再要求各个属性表具有严格的顺序，并且只要不与已有的属性名重复，任何人实现的编译器都可以向属性表中写入自己定义的属性信息，Java虚拟机运行时会忽略掉它不认识的属性\n\n> [原文链接](https://javaguide.cn/java/jvm/class-file-structure.html)\n\n## 类加载过程详解\n\n\n### 类的生命周期\n\n类从被加载到虚拟机内存中开始到卸载出内存为止，它的整个生命周期可以简单概括为7个阶段：加载（Loading）、验证（Verification）、准备（Preparation）、解析（Resolution）、初始化（Initialization）、使用（Using）和卸载（Unloading）。其中，前三个阶段可以统称为连接（Linking）。这7个阶段的顺序如下图所示：\n\n![一个类的完整生命周期](https://oss.javaguide.cn/github/javaguide/java/jvm/lifecycle-of-a-class.png)\n\n### 类加载过程\n\nClass文件需要加载到虚拟机中之后才能运行和使用，那么虚拟机是如何加载这些Class文件呢？\n\n系统加载Class类型的文件主要三步：**加载->连接->初始化**。连接过程又可分为三步：**验证->准备->解析**。\n\n![类加载过程](https://oss.javaguide.cn/github/javaguide/java/jvm/class-loading-procedure.png)\n\n> 详见[Java Virtual Machine Specification - 5.3. Creation and Loading](https://docs.oracle.com/javase/specs/jvms/se8/html/jvms-5.html#jvms-5.3)。\n\n#### 加载\n\n类加载过程的第一步，主要完成下面3件事情：\n\n1. 通过全类名获取定义此类的二进制字节流。\n2. 将字节流所代表的静态存储结构转换为方法区的运行时数据结构。\n3. 在内存中生成一个代表该类的Class对象，作为方法区这些数据的访问入口。\n\n虚拟机规范上面这3点并不具体，因此是非常灵活的。比如：\"通过全类名获取定义此类的二进制字节流\"并没有指明具体从哪里获取（ZIP、JAR、EAR、WAR、网络、动态代理技术运行时动态生成、其他文件生成比如JSP...）、怎样获取。加载这一步主要是通过我们后面要讲到的**类加载器**完成的。类加载器有很多种，当我们想要加载一个类的时候，具体是哪个类加载器加载由**双亲委派模型**决定（不过，我们也能打破由双亲委派模型）。\n\n> 类加载器、双亲委派模型也是非常重要的知识点，这部分内容在[类加载器详解](https://javaguide.cn/java/jvm/classloader.html)这篇文章中有详细介绍到\n\n每个Java类都有一个引用指向加载它的ClassLoader。不过，数组类不是通过ClassLoader创建的，而是JVM在需要的时候自动创建的，数组类通过getClassLoader()方法获取ClassLoader的时候和该数组的元素类型的ClassLoader是一致的。一个非数组类的加载阶段（加载阶段获取类的二进制字节流的动作）是可控性最强的阶段，这一步我们可以去完成还可以自定义类加载器去控制字节流的获取方式（重写一个类加载器的loadClass()方法）。加载阶段与连接阶段的部分动作(如一部分字节码文件格式验证动作)是交叉进行的，加载阶段尚未结束，连接阶段可能就已经开始了。\n\n#### 验证\n\n**验证是连接阶段的第一步，这一阶段的目的是确保Class文件的字节流中包含的信息符合《Java虚拟机规范》的全部约束要求，保证这些信息被当作代码运行后不会危害虚拟机自身的安全。**\n\n验证阶段这一步在整个类加载过程中耗费的资源还是相对较多的，但很有必要，可以有效防止恶意代码的执行。任何时候，程序安全都是第一位。不过，验证阶段也不是必须要执行的阶段。如果程序运行的全部代码(包括自己编写的、第三方包中的、从外部加载的、动态生成的等所有代码)都已经被反复使用和验证过，在生产环境的实施阶段就可以考虑使用`-Xverify:none`参数来关闭大部分的类验证措施，以缩短虚拟机类加载的时间。验证阶段主要由四个检验阶段组成：\n\n1. 文件格式验证（Class文件格式检查）\n2. 元数据验证（字节码语义检查）\n3. 字节码验证（程序语义检查）\n4. 符号引用验证（类的正确性检查）\n\n![验证阶段示意图](https://oss.javaguide.cn/github/javaguide/java/jvm/class-loading-process-verification.png)\n\n文件格式验证这一阶段是基于该类的二进制字节流进行的，主要目的是保证输入的字节流能正确地解析并存储于方法区之内，格式上符合描述一个Java类型信息的要求。除了这一阶段之外，其余三个验证阶段都是基于方法区的存储结构上进行的，不会再直接读取、操作字节流了。\n\n> 方法区属于是JVM运行时数据区域的一块逻辑区域，是各个线程共享的内存区域。当虚拟机要使用一个类时，它需要读取并解析Class文件获取相关信息，再将信息存入到方法区。方法区会存储已被虚拟机加载的**类信息、字段信息、方法信息、常量、静态变量、即时编译器编译后的代码缓存等数据**。\n>\n> 关于方法区的详细介绍，推荐阅读[Java内存区域详解](https://javaguide.cn/java/jvm/memory-area.html)这篇文章。\n\n符号引用验证发生在类加载过程中的解析阶段，具体点说是JVM将符号引用转化为直接引用的时候（解析阶段会介绍符号引用和直接引用）。符号引用验证的主要目的是确保解析阶段能正常执行，如果无法通过符号引用验证，JVM会抛出异常，比如：\n\n- java.lang.IllegalAccessError：当类试图访问或修改它没有权限访问的字段，或调用它没有权限访问的方法时，抛出该异常。\n- java.lang.NoSuchFieldError：当类试图访问或修改一个指定的对象字段，而该对象不再包含该字段时，抛出该异常。\n- java.lang.NoSuchMethodError：当类试图访问一个指定的方法，而该方法不存在时，抛出该异常。\n- ......\n\n#### 准备\n\n**准备阶段是正式为类变量分配内存并设置类变量初始值的阶段**，这些内存都将在方法区中分配。对于该阶段有以下几点需要注意：\n\n1. 这时候进行内存分配的仅包括类变量（ClassVariables，即静态变量，被static关键字修饰的变量，只与类相关，因此被称为类变量），而不包括实例变量。实例变量会在对象实例化时随着对象一块分配在Java堆中。\n\n2. 从概念上讲，类变量所使用的内存都应当在**方法区**中进行分配。不过有一点需要注意的是：JDK7之前，HotSpot使用永久代来实现方法区的时候，实现是完全符合这种逻辑概念的。而在JDK7及之后，HotSpot已经把原本放在永久代的字符串常量池、静态变量等移动到堆中，这个时候类变量则会随着Class对象一起存放在Java堆中。\n\n   > 相关阅读：[《深入理解Java虚拟机（第3版）》勘误#75](https://github.com/fenixsoft/jvm_book/issues/75)\n\n3. 这里所设置的初始值\"通常情况\"下是数据类型默认的零值（如0、0L、null、false等），比如我们定义了publicstaticintvalue=111，那么value变量在准备阶段的初始值就是0而不是111（初始化阶段才会赋值）。特殊情况：比如给value变量加上了final关键字`public static final int value=111`，那么准备阶段value的值就被赋值为111。\n\n**基本数据类型的零值**：(图片来自《深入理解Java虚拟机》第3版7.33)\n\n![基本数据类型的零值](https://my-blog-to-use.oss-cn-beijing.aliyuncs.com/2019-6/基本数据类型的零值.png)\n\n#### 解析\n\n**解析阶段是虚拟机将常量池内的符号引用替换为直接引用的过程**。解析动作主要针对类或接口、字段、类方法、接口方法、方法类型、方法句柄和调用限定符7类符号引用进行。\n\n《深入理解Java虚拟机》7.34节第三版对符号引用和直接引用的解释如下：\n\n![符号引用和直接引用](https://oss.javaguide.cn/github/javaguide/java/jvm/symbol-reference-and-direct-reference.png)\n\n举个例子：在程序执行方法时，系统需要明确知道这个方法所在的位置。Java虚拟机为每个类都准备了一张方法表来存放类中所有的方法。当需要调用一个类的方法的时候，只要知道这个方法在方法表中的偏移量就可以直接调用该方法了。通过解析操作符号引用就可以直接转变为目标方法在类中方法表的位置，从而使得方法可以被调用。\n\n综上，解析阶段是虚拟机将常量池内的符号引用替换为直接引用的过程，也就是得到类或者字段、方法在内存中的指针或者偏移量。\n\n#### 初始化\n\n初始化阶段是执行初始化方法<clinit\\>()方法的过程，是类加载的最后一步，这一步JVM才开始真正执行类中定义的Java程序代码(字节码)。\n\n> 说明：<clinit\\>()方法是编译之后自动生成的。\n\n对于<clinit\\>()方法的调用，虚拟机会自己确保其在多线程环境中的安全性。因为<clinit\\>()方法是带锁线程安全，所以在多线程环境下进行类初始化的话可能会引起多个线程阻塞，并且这种阻塞很难被发现。\n\n对于初始化阶段，虚拟机严格规范了有且只有5种情况下，必须对类进行初始化(只有主动去使用类才会初始化类)：\n\n1. 当遇到`new`、`getstatic`、`putstatic`或`invokestatic`这4条直接码指令时，比如new一个类，读取一个静态字段(未被final修饰)、或调用一个类的静态方法时。\n\n- 当jvm执行`new`指令时会初始化类。即当程序创建一个类的实例对象。\n- 当jvm执行`getstatic`指令时会初始化类。即程序访问类的静态变量(不是静态常量，常量会被加载到运行时常量池)。\n- 当jvm执行`putstatic`指令时会初始化类。即程序给类的静态变量赋值。\n- 当jvm执行`invokestatic`指令时会初始化类。即程序调用类的静态方法。\n\n2. 使用java.lang.reflect包的方法对类进行反射调用时如Class.forname(\"...\"),newInstance()等等。如果类没初始化，需要触发其初始化。\n3. 初始化一个类，如果其父类还未初始化，则先触发该父类的初始化。\n4. 当虚拟机启动时，用户需要定义一个要执行的主类(包含main方法的那个类)，虚拟机会先初始化这个类。\n5. MethodHandle和VarHandle可以看作是轻量级的反射调用机制，而要想使用这2个调用，就必须先使用findStaticVarHandle来初始化要调用的类。\n6. 补充，来自[issue745](https://github.com/Snailclimb/JavaGuide/issues/745) 当一个接口中定义了JDK8新加入的默认方法（被default关键字修饰的接口方法）时，如果有这个接口的实现类发生了初始化，那该接口要在其之前被初始化。\n\n\n##### 类的引用（主动引用，一定会初始化）\n1. 创建类的实例（new操作、反射、cloning，反序列化）\n2. 访问某个类或接口的静态变量，或者对该静态变量赋值\n3. 调用类的静态方法\n4. 反射（Class.forName(“com.lyj.load”)）\n5. 初始化一个类的子类（会首先初始化子类的父类）\n6. JVM启动时标明的启动类，即文件名和类名相同的那个类\n\n##### 类的引用（被动引用）\n1. 当访问一个静态域时，只有真正声明这个域的类才会被初始化。例如：通过子类引用父类的静态变量，不会导致子类初始化。\n2. 通过数组定义类引用，不会触发此类的初始化。\n3. 引用常量不会触发此类的初始化（常量在编译阶段就存入调用类的常量池中了）\n\n##### 类加载的顺序\n1. 类初始化顺序：静态字段->静态代码块,按顺序自上而下全部执行->成员变量->非静态代码块->构造器\n2. 带有继承关系的，实例化子类时，加载顺序如下：\n父类的静态字段->父类静态代码块,按顺序自上而下全部执行->子类静态字段->子类静态代码块,按顺序自上而下全部执行->父类成员变量（非静态字段）->父类非静态代码块->父类构造器->子类成员变量->子类非静态代码块->子类构造器\n\n#### 总结\n\nJava在new一个对象的时候，会先查看对象所属的类有没有被加载到内存，如果没有的话，就会先通过类的全限定名（包名+类名）来加载。加载并初始化类完成后，再进行对象的创建工作。\n类加载的生命周期：加载(loading)-连接(Linking)-初始化(initialization)\n1. loading:使用类加载器对类进行加载（双亲委派机制）。使用双亲委托机制的好处是：能够有效确保一个类的全局唯一性，当程序中出现多个限定名相同的类时，类加载器在执行加载时，始终只会加载其中的某一个类。通过委派的方式，可以避免类的重复加载，当父加载器已经加载过某一个类时，子加载器就不会再重新加载这个类\n2. linking:连接分为验证-准备-解析三个阶段，验证的目的是确保Class文件的字节流中包含的信息符合《Java虚拟机规范》的全部约束要求。准备阶段是正式为类中定义的变量（即静态变量，被static修饰的变量）分配内存并设置类变量初始值的阶段，当类变量被final修饰时，在准备阶段就直接会被复制，不是使用初始值。解析阶段是Java虚拟机将常量池内的符号引用替换为直接引用的过程\n3. initialization:初始化，真正开始执行Java代码的阶段(比如给类属性赋真实的值)执行<clinit\\>方法的过程\n\n#### 相关文章\n\n- [在Java中new一个对象的流程是怎样的？](https://mp.weixin.qq.com/s/23nIQlguz0-jaEEyTwR16g)\n- [Java new一个对象时发生了什么？](https://mp.weixin.qq.com/s/X-LuMvirhSQkZahFHsRl8g)\n- [JVM的类初始化机制](https://mp.weixin.qq.com/s/i52wFjkksaAosUorPqT-1A)\n- [Java类加载器解析及常见类加载问题](https://mp.weixin.qq.com/s/4yPyhMc1MW2np0sjmuilog)\n- [深入理解JVM-类加载机制](https://mp.weixin.qq.com/s/VKu6U58F4vo6OCzKMF2zxA)\n- [自己手写一个热加载～](https://mp.weixin.qq.com/s/dZA3UJqfqK76zwJdhMa14g)\n- [六种方法创建Java对象](https://mp.weixin.qq.com/s/2ZDNDvJeB-WG5t3uWXvn4Q)\n- [JVM是如何加载Java类的？](https://mp.weixin.qq.com/s/1AXWlNXz6OG3OCyZiFU2yA)\n- [IDEA的debug调试为什么这么强？我挖出了背后的技术。](https://mp.weixin.qq.com/s/JIt5WCBtdHHORjeOmX5KaA)\n- [Class类文件的结构](https://mp.weixin.qq.com/s?__biz=Mzg2MDYzODI5Nw==&amp;mid=2247494349&amp;idx=1&amp;sn=3a297393ea4a38af4fd277f4fc83c284&amp;source=41#wechat_redirect)\n- [一把小刀，直插class文件的小心脏](https://mp.weixin.qq.com/s/mNDWN2P1mOBL8C5qLtXG8A)\n\n### 类卸载\n\n> 卸载这部分内容来自[issue#662](https://github.com/Snailclimb/JavaGuide/issues/662)由 **[guang19](https://github.com/guang19)** 补充完善。\n\n**卸载类即该类的Class对象被GC。**\n\n卸载类需要满足3个要求:\n\n1. 该类的所有的实例对象都已被GC，也就是说堆不存在该类的实例对象。\n2. 该类没有在其他任何地方被引用\n3. 该类的类加载器的实例已被GC\n\n所以，在JVM生命周期内，由jvm自带的类加载器加载的类是不会被卸载的。但是由我们自定义的类加载器加载的类是可能被卸载的。\n\n只要想通一点就好了，JDK自带的BootstrapClassLoader,ExtClassLoader,AppClassLoader负责加载JDK提供的类，所以它们(类加载器的实例)肯定不会被回收。而我们自定义的类加载器的实例是可以被回收的，所以使用我们自定义加载器加载的类是可以被卸载掉的\n\n> [原文链接](https://javaguide.cn/java/jvm/class-loading-process.html)\n\n## 类加载器详解\n\n### 回顾一下类加载过程\n\n开始介绍类加载器和双亲委派模型之前，简单回顾一下类加载过程。\n\n- 类加载过程：**加载->连接->初始化**。\n- 连接过程又可分为三步：**验证->准备->解析**。\n\n![类加载过程](https://oss.javaguide.cn/github/javaguide/java/jvm/class-loading-procedure.png)\n\n加载是类加载过程的第一步，主要完成下面3件事情：\n\n1. 通过全类名获取定义此类的二进制字节流\n2. 将字节流所代表的静态存储结构转换为方法区的运行时数据结构\n3. 在内存中生成一个代表该类的Class对象，作为方法区这些数据的访问入口\n\n### 类加载器\n\n#### 类加载器介绍\n\n类加载器从JDK1.0就出现了，最初只是为了满足Java Applet（已经被淘汰）的需要。后来，慢慢成为Java程序中的一个重要组成部分，赋予了Java类可以被动态加载到JVM中并执行的能力。\n\n根据官方API文档的介绍：\n\n> A class loader is an object that is responsible for loading classes. The class ClassLoader is an abstract class. Given the binary name of a class, a class loader should attempt to locate or generate data that constitutes a definition for the class. A typical strategy is to transform the name into a file name and then read a \"class file\" of that name from a file system.\n>\n> Every Class object contains a reference to the ClassLoader that defined it.\n>\n> Class objects for array classes are not created by class loaders, but are created automatically as required by the Java runtime. The class loader for an array class, as returned by Class.getClassLoader() is the same as the class loader for its element type; if the element type is a primitive type, then the array class has no class loader.\n\n翻译过来大概的意思是：\n\n> 类加载器是一个负责加载类的对象。ClassLoader是一个抽象类。给定类的二进制名称，类加载器应尝试定位或生成构成类定义的数据。典型的策略是将名称转换为文件名，然后从文件系统中读取该名称的“类文件”。\n>\n> 每个Java类都有一个引用指向加载它的ClassLoader。不过，数组类不是通过ClassLoader创建的，而是JVM在需要的时候自动创建的，数组类通过getClassLoader()方法获取ClassLoader的时候和该数组的元素类型的ClassLoader是一致的。\n\n从上面的介绍可以看出:\n\n- 类加载器是一个负责加载类的对象，用于实现类加载过程中的加载这一步。\n- 每个Java类都有一个引用指向加载它的ClassLoader。\n- 数组类不是通过ClassLoader创建的（数组类没有对应的二进制字节流），是由JVM直接生成的。\n\n\n```java\nclass Class<T> {\n  ...\n  private final ClassLoader classLoader;\n  @CallerSensitive\n  public ClassLoader getClassLoader() {\n     //...\n  }\n  ...\n}\n```\n\n简单来说，**类加载器的主要作用就是加载Java类的字节码（.class文件）到JVM中（在内存中生成一个代表该类的Class对象）**。字节码可以是Java源程序（.java文件）经过javac编译得来，也可以是通过工具动态生成或者通过网络下载得来。其实除了加载类之外，类加载器还可以加载Java应用所需的资源如文本、图像、配置文件、视频等等文件资源。本文只讨论其核心功能：加载类。\n\n#### 类加载器加载规则\n\nJVM启动的时候，并不会一次性加载所有的类，而是根据需要去动态加载。也就是说，大部分类在具体用到的时候才会去加载，这样对内存更加友好。对于已经加载的类会被放在ClassLoader中。在类加载的时候，系统会首先判断当前类是否被加载过。已经被加载的类会直接返回，否则才会尝试加载。也就是说，对于一个类加载器来说，相同二进制名称的类只会被加载一次。\n\n```java\npublic abstract class ClassLoader {\n  ...\n  private final ClassLoader parent;\n  // 由这个类加载器加载的类。\n  private final Vector<Class<?>> classes = new Vector<>();\n  // 由VM调用，用此类加载器记录每个已加载类。\n  void addClass(Class<?> c) {\n        classes.addElement(c);\n   }\n  ...\n}\n```\n\n#### 类加载器总结\n\nJVM中内置了三个重要的ClassLoader：\n\n1. **BootstrapClassLoader（启动类加载器）**：最顶层的加载类，由C++实现，通常表示为null，并且没有父级，主要用来加载JDK内部的核心类库（%JAVA_HOME%/lib目录下的rt.jar、resources.jar、charsets.jar等jar包和类）以及被-Xbootclasspath参数指定的路径下的所有类。\n2. **ExtensionClassLoader（扩展类加载器）**：主要负责加载%JRE_HOME%/lib/ext目录下的jar包和类以及被java.ext.dirs系统变量所指定的路径下的所有类。\n3. **AppClassLoader（应用程序类加载器）**：面向我们用户的加载器，负责加载当前应用classpath下的所有jar包和类。\n\n> 🌈拓展一下：\n>\n> - rt.jar：rt代表“RunTime”，rt.jar是Java基础类库，包含Javadoc里面看到的所有的类的类文件。也就是说，我们常用内置库java.xxx.\\*都在里面，比如java.util.\\*、java.io.\\*、java.nio.\\*、java.lang.\\*、java.sql.\\*、java.math.\\*。\n> - Java9引入了模块系统，并且略微更改了上述的类加载器。扩展类加载器被改名为平台类加载器（platformclassloader）。JavaSE中除了少数几个关键模块，比如说java.base是由启动类加载器加载之外，其他的模块均由平台类加载器所加载。\n\n除了这三种类加载器之外，用户还可以加入自定义的类加载器来进行拓展，以满足自己的特殊需求。就比如说，我们可以对Java类的字节码（.class文件）进行加密，加载时再利用自定义的类加载器对其解密。\n\n![类加载器层次关系图](https://oss.javaguide.cn/github/javaguide/java/jvm/class-loader-parents-delegation-model.png)\n\n除了BootstrapClassLoader是JVM自身的一部分之外，其他所有的类加载器都是在JVM外部实现的，并且全都继承自ClassLoader抽象类。这样做的好处是用户可以自定义类加载器，以便让应用程序自己决定如何去获取所需的类。每个ClassLoader可以通过getParent()获取其父ClassLoader，如果获取到ClassLoader为null的话，那么该类是通过BootstrapClassLoader加载的。\n\n\n```java\npublic abstract class ClassLoader {\n  ...\n  // 父加载器\n  private final ClassLoader parent;\n  @CallerSensitive\n  public final ClassLoader getParent() {\n     //...\n  }\n  ...\n}\n```\n\n**为什么获取到ClassLoader为null就是BootstrapClassLoader加载的呢**？这是因为BootstrapClassLoader由C++实现，由于这个C++实现的类加载器在Java中是没有与之对应的类的，所以拿到的结果是null。\n\n下面我们来看一个获取ClassLoader的小案例：\n\n```java\npublic class PrintClassLoaderTree {\n\n    public static void main(String[] args) {\n\n        ClassLoader classLoader = PrintClassLoaderTree.class.getClassLoader();\n\n        StringBuilder split = new StringBuilder(\"|--\");\n        boolean needContinue = true;\n        while (needContinue){\n            System.out.println(split.toString() + classLoader);\n            if(classLoader == null){\n                needContinue = false;\n            }else{\n                classLoader = classLoader.getParent();\n                split.insert(0, \"\\t\");\n            }\n        }\n    }\n\n}\n```\n\n输出结果(JDK 8)：\n\n```text\n|--sun.misc.Launcher$AppClassLoader@18b4aac2\n    |--sun.misc.Launcher$ExtClassLoader@53bd815b\n        |--null\n```\n\n从输出结果可以看出：\n\n- 我们编写的Java类PrintClassLoaderTree的ClassLoader是AppClassLoader；\n- AppClassLoader的父ClassLoader是ExtClassLoader；\n- ExtClassLoader的父ClassLoader是BootstrapClassLoader，因此输出结果为null。\n\n#### 自定义类加载器\n\n我们前面也说说了，除了BootstrapClassLoader其他类加载器均由Java实现且全部继承自java.lang.ClassLoader。如果我们要自定义自己的类加载器，很明显需要继承ClassLoader抽象类。ClassLoader类有两个关键的方法：\n\n- protectedClassloadClass(String name,boolean resolve)：加载指定二进制名称的类，实现了双亲委派机制。name为类的二进制名称，resove如果为true，在加载时调用`resolveClass(Class<?>c)`方法解析该类。\n- protectedClassfindClass(String name)：根据类的二进制名称来查找类，默认实现是空方法。\n\n官方API文档中写到：\n\n> Subclasses of `ClassLoader` are encouraged to override `findClass(String name)`, rather than this method.\n>\n> 建议`ClassLoader`的子类重写`findClass(String name)`方法而不是`loadClass(String name, boolean resolve)`方法。\n\n如果我们不想打破双亲委派模型，就重写ClassLoader类中的findClass()方法即可，无法被父类加载器加载的类最终会通过这个方法被加载。但是，如果想打破双亲委派模型则需要重写loadClass()方法。\n\n### 双亲委派模型\n\n#### 双亲委派模型介绍\n\n类加载器有很多种，当我们想要加载一个类的时候，具体是哪个类加载器加载呢？这就需要提到双亲委派模型了。\n\n根据官网介绍：\n\n> The ClassLoader class uses a delegation model to search for classes and resources. Each instance of ClassLoader has an associated parent class loader. When requested to find a class or resource, a ClassLoader instance will delegate the search for the class or resource to its parent class loader before attempting to find the class or resource itself. The virtual machine's built-in class loader, called the \"bootstrap class loader\", does not itself have a parent but may serve as the parent of a ClassLoader instance.\n\n翻译过来大概的意思是：\n\n> ClassLoader类使用委托模型来搜索类和资源。每个ClassLoader实例都有一个相关的父类加载器。需要查找类或资源时，ClassLoader实例会在试图亲自查找类或资源之前，将搜索类或资源的任务委托给其父类加载器。虚拟机中被称为\"bootstrapclassloader\"的内置类加载器本身没有父类加载器，但是可以作为ClassLoader实例的父类加载器。\n\n从上面的介绍可以看出：\n\n- ClassLoader类使用委托模型来搜索类和资源。\n- 双亲委派模型要求除了顶层的启动类加载器外，其余的类加载器都应有自己的父类加载器。\n- ClassLoader实例会在试图亲自查找类或资源之前，将搜索类或资源的任务委托给其父类加载器。\n\n下图展示的各种类加载器之间的层次关系被称为类加载器的**双亲委派模型（ParentsDelegationModel）**。\n\n![类加载器层次关系图](https://oss.javaguide.cn/github/javaguide/java/jvm/class-loader-parents-delegation-model.png)\n\n注意⚠️：双亲委派模型并不是一种强制性的约束，只是JDK官方推荐的一种方式。如果我们因为某些特殊需求想要打破双亲委派模型，也是可以的，后文会介绍具体的方法。\n\n其实这个双亲翻译的容易让别人误解，我们一般理解的双亲都是父母，这里的双亲更多地表达的是“父母这一辈”的人而已，并不是说真的有一个MotherClassLoader和一个FatherClassLoader。个人觉得翻译成单亲委派模型更好一些，不过，国内既然翻译成了双亲委派模型并流传了，按照这个来也没问题，不要被误解了就好。\n\n另外，类加载器之间的父子关系一般不是以继承的关系来实现的，而是通常使用组合关系来复用父加载器的代码。\n\n```java\npublic abstract class ClassLoader {\n  ...\n  // 组合\n  private final ClassLoader parent;\n  protected ClassLoader(ClassLoader parent) {\n       this(checkCreateClassLoader(), parent);\n  }\n  ...\n}\n```\n\n在面向对象编程中，有一条非常经典的设计原则：**组合优于继承，多用组合少用继承**。\n\n#### 双亲委派模型的执行流程\n\n双亲委派模型的实现代码非常简单，逻辑非常清晰，都集中在java.lang.ClassLoader的loadClass()中，相关代码如下所示。\n\n```java\nprotected Class<?> loadClass(String name, boolean resolve)\n    throws ClassNotFoundException\n{\n    synchronized (getClassLoadingLock(name)) {\n        //首先，检查该类是否已经加载过\n        Class c = findLoadedClass(name);\n        if (c == null) {\n            //如果c为null，则说明该类没有被加载过\n            long t0 = System.nanoTime();\n            try {\n                if (parent != null) {\n                    //当父类的加载器不为空，则通过父类的loadClass来加载该类\n                    c = parent.loadClass(name, false);\n                } else {\n                    //当父类的加载器为空，则调用启动类加载器来加载该类\n                    c = findBootstrapClassOrNull(name);\n                }\n            } catch (ClassNotFoundException e) {\n                //非空父类的类加载器无法找到相应的类，则抛出异常\n            }\n\n            if (c == null) {\n                //当父类加载器无法加载时，则调用findClass方法来加载该类\n                //用户可通过覆写该方法，来自定义类加载器\n                long t1 = System.nanoTime();\n                c = findClass(name);\n\n                //用于统计类加载器相关的信息\n                sun.misc.PerfCounter.getParentDelegationTime().addTime(t1 - t0);\n                sun.misc.PerfCounter.getFindClassTime().addElapsedTimeFrom(t1);\n                sun.misc.PerfCounter.getFindClasses().increment();\n            }\n        }\n        if (resolve) {\n            //对类进行link操作\n            resolveClass(c);\n        }\n        return c;\n    }\n}\n```\n\n每当一个类加载器接收到加载请求时，它会先将请求转发给父类加载器。在父类加载器没有找到所请求的类的情况下，该类加载器才会尝试去加载。结合上面的源码，简单总结一下双亲委派模型的执行流程：\n\n- 在类加载的时候，系统会首先判断当前类是否被加载过。已经被加载的类会直接返回，否则才会尝试加载（每个父类加载器都会走一遍这个流程）。\n- 类加载器在进行类加载的时候，它首先不会自己去尝试加载这个类，而是把这个请求委派给父类加载器去完成（调用父加载器loadClass()方法来加载类）。这样的话，所有的请求最终都会传送到顶层的启动类加载器BootstrapClassLoader中。\n- 只有当父加载器反馈自己无法完成这个加载请求（它的搜索范围中没有找到所需的类）时，子加载器才会尝试自己去加载（调用自己的findClass()方法来加载类）。\n\n🌈拓展一下：\n\n**JVM判定两个Java类是否相同的具体规则**：JVM不仅要看类的全名是否相同，还要看加载此类的类加载器是否一样。只有两者都相同的情况，才认为两个类是相同的。即使两个类来源于同一个Class文件，被同一个虚拟机加载，只要加载它们的类加载器不同，那这两个类就必定不相同。\n\n#### 双亲委派模型的好处\n\n双亲委派模型保证了Java程序的稳定运行，可以避免类的重复加载（JVM区分不同类的方式不仅仅根据类名，相同的类文件被不同的类加载器加载产生的是两个不同的类），也保证了Java的核心API不被篡改。如果没有使用双亲委派模型，而是每个类加载器加载自己的话就会出现一些问题，比如我们编写一个称为java.lang.Object类的话，那么程序运行的时候，系统就会出现两个不同的Object类。双亲委派模型可以保证加载的是JRE里的那个Object类，而不是你写的Object类。这是因为AppClassLoader在加载你的Object类时，会委托给ExtClassLoader去加载，而ExtClassLoader又会委托给BootstrapClassLoader，BootstrapClassLoader发现自己已经加载过了Object类，会直接返回，不会去加载你写的Object类。\n\n#### 打破双亲委派模型方法\n\n为了避免双亲委托机制，我们可以自己定义一个类加载器，然后重写loadClass()即可。\n\n> **🐛修正（参见：[issue871](https://github.com/Snailclimb/JavaGuide/issues/871)）**：自定义加载器的话，需要继承ClassLoader。如果我们不想打破双亲委派模型，就重写ClassLoader类中的findClass()方法即可，无法被父类加载器加载的类最终会通过这个方法被加载。但是，如果想打破双亲委派模型则需要重写loadClass()方法。\n\n为什么是重写loadClass()方法打破双亲委派模型呢？双亲委派模型的执行流程已经解释了：\n\n> 类加载器在进行类加载的时候，它首先不会自己去尝试加载这个类，而是把这个请求委派给父类加载器去完成（调用父加载器loadClass()方法来加载类）。\n\n我们比较熟悉的Tomcat服务器为了能够优先加载Web应用目录下的类，然后再加载其他目录下的类，就自定义了类加载器WebAppClassLoader来打破双亲委托机制。这也是Tomcat下Web应用之间的类实现隔离的具体原理。Tomcat的类加载器的层次结构如下：\n\n![Tomcat的类加载器的层次结构](https://oss.javaguide.cn/github/javaguide/java/jvm/tomcat-class-loader-parents-delegation-model.png)\n\n感兴趣的小伙伴可以自行研究一下Tomcat类加载器的层次结构，这有助于我们搞懂Tomcat隔离Web应用的原理，推荐资料是[《深入拆解Tomcat&Jetty》](http://gk.link/a/10Egr)。\n\n> [原文链接](https://javaguide.cn/java/jvm/classloader.html)\n> [我竟然被“双亲委派”给虐了](https://mp.weixin.qq.com/s/Q0MqcvbeI7gAcJH5ZaQWgA)\n> [Tomcat为什么要破坏Java双亲委派机制？](https://mp.weixin.qq.com/s/IGV4-y1cWWOUWOETZI2hsw)","categories":["Java"]},{"title":"Java中的Unsafe类","slug":"Java中的Unsafe类","url":"/blog/posts/e688a15ee155/","content":"\n> 本文整理完善自下面这两篇优秀的文章：\n>\n> - [Java魔法类：Unsafe应用解析-美团技术团队-2019](https://tech.meituan.com/2019/02/14/talk-about-java-magic-class-unsafe.html)\n> - [Java双刃剑之Unsafe类详解-码农参上-2021](https://xie.infoq.cn/article/8b6ed4195e475bfb32dacc5cb)\n\n阅读过JUC源码的同学，一定会发现很多并发工具类都调用了一个叫做Unsafe的类。那这个类主要是用来干什么的呢？有什么使用场景呢？\n\n## Unsafe介绍\n\nUnsafe是位于sun.misc包下的一个类，主要提供一些用于执行低级别、不安全操作的方法，如直接访问系统内存资源、自主管理内存资源等，这些方法在提升Java运行效率、增强Java语言底层资源操作能力方面起到了很大的作用。但由于Unsafe类使Java语言拥有了类似C语言指针一样操作内存空间的能力，这无疑也增加了程序发生相关指针问题的风险。在程序中过度、不正确使用Unsafe类会使得程序出错的概率变大，使得Java这种安全的语言变得不再“安全”，因此对Unsafe的使用一定要慎重。\n\n另外，Unsafe提供的这些功能的实现需要依赖本地方法（Native Method）。你可以将本地方法看作是Java中使用其他编程语言编写的方法。本地方法使用**native**关键字修饰，Java代码中只是声明方法头，具体的实现则交给本地代码。\n\n![img](https://oss.javaguide.cn/github/javaguide/java/basis/unsafe/image-20220717115231125.png)\n\n**为什么要使用本地方法呢？**\n\n1. 需要用到Java中不具备的依赖于操作系统的特性，Java在实现跨平台的同时要实现对底层的控制，需要借助其他语言发挥作用。\n2. 对于其他语言已经完成的一些现成功能，可以使用Java直接调用。\n3. 程序对时间敏感或对性能要求非常高时，有必要使用更加底层的语言，例如C/C++甚至是汇编。\n\n在JUC包的很多并发工具类在实现并发机制时，都调用了本地方法，通过它们打破了Java运行时的界限，能够接触到操作系统底层的某些功能。对于同一本地方法，不同的操作系统可能会通过不同的方式来实现，但是对于使用者来说是透明的，最终都会得到相同的结果。\n\n## Unsafe创建\n\nsun.misc.Unsafe部分源码如下：\n\n```java\npublic final class Unsafe {\n  // 单例对象\n  private static final Unsafe theUnsafe;\n  ......\n  private Unsafe() {\n  }\n  @CallerSensitive\n  public static Unsafe getUnsafe() {\n    Class var0 = Reflection.getCallerClass();\n    // 仅在引导类加载器BootstrapClassLoader加载时才合法\n    if(!VM.isSystemDomainLoader(var0.getClassLoader())) {\n      throw new SecurityException(\"Unsafe\");\n    } else {\n      return theUnsafe;\n    }\n  }\n}\n```\n\nUnsafe类为一单例实现，提供静态方法getUnsafe获取Unsafe实例。这个看上去貌似可以用来获取Unsafe实例。但是，当我们直接调用这个静态方法的时候，会抛出SecurityException异常：\n\n\n```bash\nException in thread \"main\" java.lang.SecurityException: Unsafe\n at sun.misc.Unsafe.getUnsafe(Unsafe.java:90)\n at com.cn.test.GetUnsafeTest.main(GetUnsafeTest.java:12)\n```\n\n**为什么public static方法无法被直接调用呢**？\n\n这是因为在getUnsafe方法中，会对调用者的classLoader进行检查，判断当前类是否由Bootstrap classLoader加载，如果不是的话那么就会抛出一个SecurityException异常。也就是说，只有启动类加载器加载的类才能够调用Unsafe类中的方法，来防止这些方法在不可信的代码中被调用。\n\n**为什么要对Unsafe类进行这么谨慎的使用限制呢**？\n\nUnsafe提供的功能过于底层（如直接访问系统内存资源、自主管理内存资源等），安全隐患也比较大，使用不当的话，很容易出现很严重的问题。\n\n**如若想使用Unsafe这个类的话，应该如何获取其实例呢**？\n\n这里介绍两个可行的方案。\n\n1、利用反射获得Unsafe类中已经实例化完成的单例对象theUnsafe。\n\n\n```java\nprivate static Unsafe reflectGetUnsafe() {\n    try {\n      Field field = Unsafe.class.getDeclaredField(\"theUnsafe\");\n      field.setAccessible(true);\n      return (Unsafe) field.get(null);\n    } catch (Exception e) {\n      log.error(e.getMessage(), e);\n      return null;\n    }\n}\n```\n\n2、从getUnsafe方法的使用限制条件出发，通过Java命令行命令`-Xbootclasspath/a`把调用Unsafe相关方法的类A所在jar包路径追加到默认的bootstrap路径中，使得A被引导类加载器加载，从而通过`Unsafe.getUnsafe`方法安全的获取Unsafe实例。\n\n```bash\n# 其中path为调用Unsafe相关方法的类所在jar包路径\njava -Xbootclasspath/a: ${path}\n```\n\n## Unsafe功能\n\n概括的来说，Unsafe类实现功能可以被分为下面8类：\n\n1. 内存操作\n2. 内存屏障\n3. 对象操作\n4. 数据操作\n5. CAS操作\n6. 线程调度\n7. Class操作\n8. 系统信息\n\n### 内存操作\n\n#### 介绍\n\n如果你是一个写过C或者C++的程序员，一定对内存操作不会陌生，而在Java中是不允许直接对内存进行操作的，对象内存的分配和回收都是由JVM自己实现的。但是在Unsafe中，提供的下列接口可以直接进行内存操作：\n\n```java\n//分配新的本地空间\npublic native long allocateMemory(long bytes);\n//重新调整内存空间的大小\npublic native long reallocateMemory(long address, long bytes);\n//将内存设置为指定值\npublic native void setMemory(Object o, long offset, long bytes, byte value);\n//内存拷贝\npublic native void copyMemory(Object srcBase, long srcOffset,Object destBase, long destOffset,long bytes);\n//清除内存\npublic native void freeMemory(long address);\n```\n\n使用下面的代码进行测试：\n\n```java\nprivate void memoryTest() {\n    int size = 4;\n    long addr = unsafe.allocateMemory(size);\n    long addr3 = unsafe.reallocateMemory(addr, size * 2);\n    System.out.println(\"addr: \"+addr);\n    System.out.println(\"addr3: \"+addr3);\n    try {\n        unsafe.setMemory(null,addr ,size,(byte)1);\n        for (int i = 0; i < 2; i++) {\n            unsafe.copyMemory(null,addr,null,addr3+size*i,4);\n        }\n        System.out.println(unsafe.getInt(addr));\n        System.out.println(unsafe.getLong(addr3));\n    }finally {\n        unsafe.freeMemory(addr);\n        unsafe.freeMemory(addr3);\n    }\n}\n```\n\n先看结果输出：\n\n```text\naddr: 2433733895744\naddr3: 2433733894944\n16843009\n72340172838076673\n```\n\n分析一下运行结果，首先使用allocateMemory方法申请4字节长度的内存空间，调用setMemory方法向每个字节写入内容为byte类型的1，当使用Unsafe调用getInt方法时，因为一个int型变量占4个字节，会一次性读取4个字节，组成一个int的值，对应的十进制结果为16843009。\n\n你可以通过下图理解这个过程：\n\n![img](https://oss.javaguide.cn/github/javaguide/java/basis/unsafe/image-20220717144344005.png)\n\n在代码中调用reallocateMemory方法重新分配了一块8字节长度的内存空间，通过比较addr和addr3可以看到和之前申请的内存地址是不同的。在代码中的第二个for循环里，调用copyMemory方法进行了两次内存的拷贝，每次拷贝内存地址addr开始的4个字节，分别拷贝到以addr3和addr3+4开始的内存空间上：\n\n![img](https://oss.javaguide.cn/github/javaguide/java/basis/unsafe/image-20220717144354582.png)\n\n拷贝完成后，使用getLong方法一次性读取8个字节，得到long类型的值为72340172838076673。\n\n需要注意，通过这种方式分配的内存属于堆外内存，是无法进行垃圾回收的，需要我们把这些内存当做一种资源去手动调用freeMemory方法进行释放，否则会产生内存泄漏。通用的操作内存方式是在try中执行对内存的操作，最终在finally块中进行内存的释放。\n\n**为什么要使用堆外内存？**\n\n- 对垃圾回收停顿的改善。由于堆外内存是直接受操作系统管理而不是JVM，所以当我们使用堆外内存时，即可保持较小的堆内内存规模。从而在GC时减少回收停顿对于应用的影响。\n- 提升程序I/O操作的性能。通常在I/O通信过程中，会存在堆内内存到堆外内存的数据拷贝操作，对于需要频繁进行内存间数据拷贝且生命周期较短的暂存数据，都建议存储到堆外内存。\n\n#### 典型应用\n\nDirectByteBuffer是Java用于实现堆外内存的一个重要类，通常用在通信过程中做缓冲池，如在Netty、MINA等NIO框架中应用广泛。DirectByteBuffer对于堆外内存的创建、使用、销毁等逻辑均由Unsafe提供的堆外内存API来实现。\n\n下面为DirectByteBuffer构造函数，创建DirectByteBuffer的时候，通过Unsafe.allocateMemory分配内存、Unsafe.setMemory进行内存初始化，而后构建Cleaner对象用于跟踪DirectByteBuffer对象的垃圾回收，以实现当DirectByteBuffer被垃圾回收时，分配的堆外内存一起被释放。\n\n```java\nDirectByteBuffer(int cap) {                   // package-private\n\n    super(-1, 0, cap, cap);\n    boolean pa = VM.isDirectMemoryPageAligned();\n    int ps = Bits.pageSize();\n    long size = Math.max(1L, (long)cap + (pa ? ps : 0));\n    Bits.reserveMemory(size, cap);\n\n    long base = 0;\n    try {\n        // 分配内存并返回基地址\n        base = unsafe.allocateMemory(size);\n    } catch (OutOfMemoryError x) {\n        Bits.unreserveMemory(size, cap);\n        throw x;\n    }\n    // 内存初始化\n    unsafe.setMemory(base, size, (byte) 0);\n    if (pa && (base % ps != 0)) {\n        // Round up to page boundary\n        address = base + ps - (base & (ps - 1));\n    } else {\n        address = base;\n    }\n    // 跟踪DirectByteBuffer对象的垃圾回收，以实现堆外内存释放\n    cleaner = Cleaner.create(this, new Deallocator(base, size, cap));\n    att = null;\n}\n```\n\n### 内存屏障\n\n#### 介绍\n\n在介绍内存屏障前，需要知道编译器和CPU会在保证程序输出结果一致的情况下，会对代码进行重排序，从指令优化角度提升性能。而指令重排序可能会带来一个不好的结果，导致CPU的高速缓存和内存中数据的不一致，而内存屏障（Memory Barrier）就是通过阻止屏障两边的指令重排序从而避免编译器和硬件的不正确优化情况。\n\n在硬件层面上，内存屏障是CPU为了防止代码进行重排序而提供的指令，不同的硬件平台上实现内存屏障的方法可能并不相同。在Java8中，引入了3个内存屏障的函数，它屏蔽了操作系统底层的差异，允许在代码中定义、并统一由JVM来生成内存屏障指令，来实现内存屏障的功能。Unsafe中提供了下面三个内存屏障相关方法：\n\n\n```java\n//内存屏障，禁止load操作重排序。屏障前的load操作不能被重排序到屏障后，屏障后的load操作不能被重排序到屏障前\npublic native void loadFence();\n//内存屏障，禁止store操作重排序。屏障前的store操作不能被重排序到屏障后，屏障后的store操作不能被重排序到屏障前\npublic native void storeFence();\n//内存屏障，禁止load、store操作重排序\npublic native void fullFence();\n```\n\n内存屏障可以看做对内存随机访问的操作中的一个同步点，使得此点之前的所有读写操作都执行后才可以开始执行此点之后的操作。以loadFence方法为例，它会禁止读操作重排序，保证在这个屏障之前的所有读操作都已经完成，并且将缓存数据设为无效，重新从主存中进行加载。\n\n看到这估计很多小伙伴们会想到volatile关键字了，如果在字段上添加了volatile关键字，就能够实现字段在多线程下的可见性。基于读内存屏障，我们也能实现相同的功能。下面定义一个线程方法，在线程中去修改flag标志位，注意这里的flag是没有被volatile修饰的：\n\n```java\n@Getter\nclass ChangeThread implements Runnable{\n    /**volatile**/ boolean flag=false;\n    @Override\n    public void run() {\n        try {\n            Thread.sleep(3000);\n        } catch (InterruptedException e) {\n            e.printStackTrace();\n        }\n        System.out.println(\"subThread change flag to:\" + flag);\n        flag = true;\n    }\n}\n```\n\n在主线程的while循环中，加入内存屏障，测试是否能够感知到flag的修改变化：\n\n```java\npublic static void main(String[] args){\n    ChangeThread changeThread = new ChangeThread();\n    new Thread(changeThread).start();\n    while (true) {\n        boolean flag = changeThread.isFlag();\n        unsafe.loadFence(); //加入读内存屏障\n        if (flag){\n            System.out.println(\"detected flag changed\");\n            break;\n        }\n    }\n    System.out.println(\"main thread end\");\n}\n```\n\n运行结果：\n\n```text\nsubThread change flag to:false\ndetected flag changed\nmain thread end\n```\n\n而如果删掉上面代码中的loadFence方法，那么主线程将无法感知到flag发生的变化，会一直在while中循环。可以用图来表示上面的过程：\n\n![img](https://oss.javaguide.cn/github/javaguide/java/basis/unsafe/image-20220717144703446.png)\n\n了解Java内存模型（JMM）的小伙伴们应该清楚，运行中的线程不是直接读取主内存中的变量的，只能操作自己工作内存中的变量，然后同步到主内存中，并且线程的工作内存是不能共享的。上面的图中的流程就是子线程借助于主内存，将修改后的结果同步给了主线程，进而修改主线程中的工作空间，跳出循环。\n\n#### 典型应用\n\n在Java8中引入了一种锁的新机制——**StampedLock**，它可以看成是读写锁的一个改进版本。StampedLock提供了一种乐观读锁的实现，这种乐观读锁类似于无锁的操作，完全不会阻塞写线程获取写锁，从而缓解读多写少时写线程“饥饿”现象。由于StampedLock提供的乐观读锁不阻塞写线程获取读锁，当线程共享变量从主内存load到线程工作内存时，会存在数据不一致问题。为了解决这个问题，StampedLock的validate方法会通过Unsafe的loadFence方法加入一个load内存屏障。\n\n```java\npublic boolean validate(long stamp) {\n   U.loadFence();\n   return (stamp & SBITS) == (state & SBITS);\n}\n```\n\n### 对象操作\n\n#### 介绍\n\n**对象属性**\n\n对象成员属性的内存偏移量获取，以及字段属性值的修改，在上面的例子中我们已经测试过了。除了前面的putInt、getInt方法外，Unsafe提供了全部8种基础数据类型以及Object的put和get方法，并且所有的put方法都可以越过访问权限，直接修改内存中的数据。阅读openJDK源码中的注释发现，基础数据类型和Object的读写稍有不同，基础数据类型是直接操作的属性值（value），而Object的操作则是基于引用值（referencevalue）。下面是Object的读写方法：\n\n```java\n//在对象的指定偏移地址获取一个对象引用\npublic native Object getObject(Object o, long offset);\n//在对象指定偏移地址写入一个对象引用\npublic native void putObject(Object o, long offset, Object x);\n```\n\n除了对象属性的普通读写外，Unsafe还提供了**volatile读写**和**有序写入**方法。volatile读写方法的覆盖范围与普通读写相同，包含了全部基础数据类型和Object类型，以int类型为例：\n\n```java\n//在对象的指定偏移地址处读取一个int值，支持volatile load语义\npublic native int getIntVolatile(Object o, long offset);\n//在对象指定偏移地址处写入一个int，支持volatile store语义\npublic native void putIntVolatile(Object o, long offset, int x);\n```\n\n相对于普通读写来说，volatile读写具有更高的成本，因为它需要保证可见性和有序性。在执行get操作时，会强制从主存中获取属性值，在使用put方法设置属性值时，会强制将值更新到主存中，从而保证这些变更对其他线程是可见的。有序写入的方法有以下三个：\n\n```java\npublic native void putOrderedObject(Object o, long offset, Object x);\npublic native void putOrderedInt(Object o, long offset, int x);\npublic native void putOrderedLong(Object o, long offset, long x);\n```\n\n有序写入的成本相对volatile较低，因为它只保证写入时的有序性，而不保证可见性，也就是一个线程写入的值不能保证其他线程立即可见。为了解决这里的差异性，需要对内存屏障的知识点再进一步进行补充，首先需要了解两个指令的概念：\n\n- Load：将主内存中的数据拷贝到处理器的缓存中\n- Store：将处理器缓存的数据刷新到主内存中\n\n顺序写入与volatile写入的差别在于，在顺序写时加入的内存屏障类型为StoreStore类型，而在volatile写入时加入的内存屏障是StoreLoad类型，如下图所示：\n\n![img](https://oss.javaguide.cn/github/javaguide/java/basis/unsafe/image-20220717144834132.png)\n\n在有序写入方法中，使用的是StoreStore屏障，该屏障确保Store1立刻刷新数据到内存，这一操作先于Store2以及后续的存储指令操作。而在volatile写入中，使用的是StoreLoad屏障，该屏障确保Store1立刻刷新数据到内存，这一操作先于Load2及后续的装载指令，并且，StoreLoad屏障会使该屏障之前的所有内存访问指令，包括存储指令和访问指令全部完成之后，才执行该屏障之后的内存访问指令。\n\n综上所述，在上面的三类写入方法中，在写入效率方面，按照put、putOrder、putVolatile的顺序效率逐渐降低。\n\n**对象实例化**\n\n使用Unsafe的allocateInstance方法，允许我们使用非常规的方式进行对象的实例化，首先定义一个实体类，并且在构造函数中对其成员变量进行赋值操作：\n\n```java\n@Data\npublic class A {\n    private int b;\n    public A(){\n        this.b =1;\n    }\n}\n```\n\n分别基于构造函数、反射以及Unsafe方法的不同方式创建对象进行比较：\n\n```java\npublic void objTest() throws Exception{\n    A a1=new A();\n    System.out.println(a1.getB());\n    A a2 = A.class.newInstance();\n    System.out.println(a2.getB());\n    A a3= (A) unsafe.allocateInstance(A.class);\n    System.out.println(a3.getB());\n}\n```\n\n打印结果分别为1、1、0，说明通过allocateInstance方法创建对象过程中，不会调用类的构造方法。使用这种方式创建对象时，只用到了Class对象，所以说如果想要跳过对象的初始化阶段或者跳过构造器的安全检查，就可以使用这种方法。在上面的例子中，如果将A类的构造函数改为private类型，将无法通过构造函数和反射创建对象，但allocateInstance方法仍然有效。\n\n#### 典型应用\n\n- **常规对象实例化方式**：我们通常所用到的创建对象的方式，从本质上来讲，都是通过new机制来实现对象的创建。但是，new机制有个特点就是当类只提供有参的构造函数且无显示声明无参构造函数时，则必须使用有参构造函数进行对象构造，而使用有参构造函数时，必须传递相应个数的参数才能完成对象实例化。\n- **非常规的实例化方式**：而Unsafe中提供allocateInstance方法，仅通过Class对象就可以创建此类的实例对象，而且不需要调用其构造函数、初始化代码、JVM安全检查等。它抑制修饰符检测，也就是即使构造器是private修饰的也能通过此方法实例化，只需提类对象即可创建相应的对象。由于这种特性，allocateInstance在java.lang.invoke、Objenesis（提供绕过类构造器的对象生成方式）、Gson（反序列化时用到）中都有相应的应用。\n\n### 数组操作\n\n#### 介绍\n\narrayBaseOffset与arrayIndexScale这两个方法配合起来使用，即可定位数组中每个元素在内存中的位置。\n\n```java\n//返回数组中第一个元素的偏移地址\npublic native int arrayBaseOffset(Class<?> arrayClass);\n//返回数组中一个元素占用的大小\npublic native int arrayIndexScale(Class<?> arrayClass);\n```\n\n#### 典型应用\n\n这两个与数据操作相关的方法，在java.util.concurrent.atomic包下的AtomicIntegerArray（可以实现对Integer数组中每个元素的原子性操作）中有典型的应用，如下图AtomicIntegerArray源码所示，通过Unsafe的arrayBaseOffset、arrayIndexScale分别获取数组首元素的偏移地址base及单个元素大小因子scale。后续相关原子性操作，均依赖于这两个值进行数组中元素的定位，如下图二所示的getAndAdd方法即通过checkedByteOffset方法获取某数组元素的偏移地址，而后通过CAS实现原子性操作。\n\n![img](https://oss.javaguide.cn/github/javaguide/java/basis/unsafe/image-20220717144927257.png)\n\n### CAS操作\n\n#### 介绍\n\n这部分主要为CAS相关操作的方法。\n\n```java\n/**\n  * CAS\n  * @param o         包含要修改field的对象\n  * @param offset    对象中某field的偏移量\n  * @param expected  期望值\n  * @param update    更新值\n  * @return          true|false\n  */\npublic final native boolean compareAndSwapObject(Object o, long offset,  Object expected, Object update);\n\npublic final native boolean compareAndSwapInt(Object o, long offset, int expected,int update);\n\npublic final native boolean compareAndSwapLong(Object o, long offset, long expected, long update);\n```\n\n**什么是CAS**\n\nCAS即比较并替换(CompareAndSwap)，是实现并发算法时常用到的一种技术。CAS操作包含三个操作数——内存位置、预期原值及新值。执行CAS操作的时候，将内存位置的值与预期原值比较，如果相匹配，那么处理器会自动将该位置值更新为新值，否则，处理器不做任何操作。我们都知道，CAS是一条CPU的原子指令（cmpxchg指令），不会造成所谓的数据不一致问题，Unsafe提供的CAS方法（如compareAndSwapXXX）底层实现即为CPU指令cmpxchg。\n\n#### 典型应用\n\n在JUC包的并发工具类中大量地使用了CAS操作，像在前面介绍synchronized和AQS的文章中也多次提到了CAS，其作为乐观锁在并发工具类中广泛发挥了作用。在Unsafe类中，提供了compareAndSwapObject、compareAndSwapInt、compareAndSwapLong方法来实现的对Object、int、long类型的CAS操作。以compareAndSwapInt方法为例：\n\n```java\npublic final native boolean compareAndSwapInt(Object o, long offset,int expected,int x);\n```\n\n参数中o为需要更新的对象，offset是对象o中整形字段的偏移量，如果这个字段的值与expected相同，则将字段的值设为x这个新值，并且此更新是不可被中断的，也就是一个原子操作。下面是一个使用compareAndSwapInt的例子：\n\n```java\nprivate volatile int a;\npublic static void main(String[] args){\n    CasTest casTest=new CasTest();\n    new Thread(()->{\n        for (int i = 1; i < 5; i++) {\n            casTest.increment(i);\n            System.out.print(casTest.a+\" \");\n        }\n    }).start();\n    new Thread(()->{\n        for (int i = 5 ; i <10 ; i++) {\n            casTest.increment(i);\n            System.out.print(casTest.a+\" \");\n        }\n    }).start();\n}\n\nprivate void increment(int x){\n    while (true){\n        try {\n            long fieldOffset = unsafe.objectFieldOffset(CasTest.class.getDeclaredField(\"a\"));\n            if (unsafe.compareAndSwapInt(this,fieldOffset,x-1,x))\n                break;\n        } catch (NoSuchFieldException e) {\n            e.printStackTrace();\n        }\n    }\n}\n```\n\n运行代码会依次输出：\n\n```text\n1 2 3 4 5 6 7 8 9\n```\n\n在上面的例子中，使用两个线程去修改int型属性a的值，并且只有在a的值等于传入的参数x减一时，才会将a的值变为x，也就是实现对a的加一的操作。流程如下所示：\n\n![img](https://oss.javaguide.cn/github/javaguide/java/basis/unsafe/image-20220717144939826.png)\n\n需要注意的是，在调用compareAndSwapInt方法后，会直接返回true或false的修改结果，因此需要我们在代码中手动添加自旋的逻辑。在AtomicInteger类的设计中，也是采用了将compareAndSwapInt的结果作为循环条件，直至修改成功才退出死循环的方式来实现的原子性的自增操作。\n\n### 线程调度\n\n#### 介绍\n\nUnsafe类中提供了park、unpark、monitorEnter、monitorExit、tryMonitorEnter方法进行线程调度。\n\n```java\n//取消阻塞线程\npublic native void unpark(Object thread);\n//阻塞线程\npublic native void park(boolean isAbsolute, long time);\n//获得对象锁（可重入锁）\n@Deprecated\npublic native void monitorEnter(Object o);\n//释放对象锁\n@Deprecated\npublic native void monitorExit(Object o);\n//尝试获取对象锁\n@Deprecated\npublic native boolean tryMonitorEnter(Object o);\n```\n\n方法park、unpark即可实现线程的挂起与恢复，将一个线程进行挂起是通过park方法实现的，调用park方法后，线程将一直阻塞直到超时或者中断等条件出现；unpark可以终止一个挂起的线程，使其恢复正常。\n\n此外，Unsafe源码中monitor相关的三个方法已经被标记为deprecated，不建议被使用：\n\n```java\n//获得对象锁\n@Deprecated\npublic native void monitorEnter(Object var1);\n//释放对象锁\n@Deprecated\npublic native void monitorExit(Object var1);\n//尝试获得对象锁\n@Deprecated\npublic native boolean tryMonitorEnter(Object var1);\n```\n\nmonitorEnter方法用于获得对象锁，monitorExit用于释放对象锁，如果对一个没有被monitorEnter加锁的对象执行此方法，会抛出IllegalMonitorStateException异常。tryMonitorEnter方法尝试获取对象锁，如果成功则返回true，反之返回false。\n\n#### 典型应用\n\nJava锁和同步器框架的核心类AbstractQueuedSynchronizer(AQS)，就是通过调用LockSupport.park()和LockSupport.unpark()实现线程的阻塞和唤醒的，而LockSupport的park、unpark方法实际是调用Unsafe的park、unpark方式实现的。\n\n```java\npublic static void park(Object blocker) {\n    Thread t = Thread.currentThread();\n    setBlocker(t, blocker);\n    UNSAFE.park(false, 0L);\n    setBlocker(t, null);\n}\npublic static void unpark(Thread thread) {\n    if (thread != null)\n        UNSAFE.unpark(thread);\n}\n\n```\nLockSupport的park方法调用了Unsafe的park方法来阻塞当前线程，此方法将线程阻塞后就不会继续往后执行，直到有其他线程调用unpark方法唤醒当前线程。下面的例子对Unsafe的这两个方法进行测试：\n\n```java\npublic static void main(String[] args) {\n    Thread mainThread = Thread.currentThread();\n    new Thread(()->{\n        try {\n            TimeUnit.SECONDS.sleep(5);\n            System.out.println(\"subThread try to unpark mainThread\");\n            unsafe.unpark(mainThread);\n        } catch (InterruptedException e) {\n            e.printStackTrace();\n        }\n    }).start();\n\n    System.out.println(\"park main mainThread\");\n    unsafe.park(false,0L);\n    System.out.println(\"unpark mainThread success\");\n}\n```\n\n程序输出为：\n\n```text\npark main mainThread\nsubThread try to unpark mainThread\nunpark mainThread success\n```\n\n程序运行的流程也比较容易看懂，子线程开始运行后先进行睡眠，确保主线程能够调用park方法阻塞自己，子线程在睡眠5秒后，调用unpark方法唤醒主线程，使主线程能继续向下执行。整个流程如下图所示：\n\n![img](https://oss.javaguide.cn/github/javaguide/java/basis/unsafe/image-20220717144950116.png)\n\n### Class操作\n\n#### 介绍\n\nUnsafe对Class的相关操作主要包括类加载和静态变量的操作方法。\n\n**静态属性读取相关的方法**\n\n```java\n//获取静态属性的偏移量\npublic native long staticFieldOffset(Field f);\n//获取静态属性的对象指针\npublic native Object staticFieldBase(Field f);\n//判断类是否需要实例化（用于获取类的静态属性前进行检测）\npublic native boolean shouldBeInitialized(Class<?> c);\n```\n\n创建一个包含静态属性的类，进行测试：\n\n```java\n@Data\npublic class User {\n    public static String name=\"Hydra\";\n    int age;\n}\nprivate void staticTest() throws Exception {\n    User user=new User();\n    System.out.println(unsafe.shouldBeInitialized(User.class));\n    Field sexField = User.class.getDeclaredField(\"name\");\n    long fieldOffset = unsafe.staticFieldOffset(sexField);\n    Object fieldBase = unsafe.staticFieldBase(sexField);\n    Object object = unsafe.getObject(fieldBase, fieldOffset);\n    System.out.println(object);\n}\n```\n\n运行结果：\n\n```text\nfalseHydra\n```\n\n在Unsafe的对象操作中，我们学习了通过objectFieldOffset方法获取对象属性偏移量并基于它对变量的值进行存取，但是它不适用于类中的静态属性，这时候就需要使用staticFieldOffset方法。在上面的代码中，只有在获取Field对象的过程中依赖到了Class，而获取静态变量的属性时不再依赖于Class。\n\n在上面的代码中首先创建一个User对象，这是因为如果一个类没有被实例化，那么它的静态属性也不会被初始化，最后获取的字段属性将是null。所以在获取静态属性前，需要调用shouldBeInitialized方法，判断在获取前是否需要初始化这个类。如果删除创建User对象的语句，运行结果会变为：\n\n```text\ntruenull\n```\n\n使用defineClass方法允许程序在运行时动态地创建一个类\n\n```java\npublic native Class<?> defineClass(String name, byte[] b, int off, int len, ClassLoader loader,ProtectionDomain protectionDomain);\n```\n\n在实际使用过程中，可以只传入字节数组、起始字节的下标以及读取的字节长度，默认情况下，类加载器（ClassLoader）和保护域（ProtectionDomain）来源于调用此方法的实例。下面的例子中实现了反编译生成后的class文件的功能：\n\n```java\nprivate static void defineTest() {\n    String fileName=\"F:\\\\workspace\\\\unsafe-test\\\\target\\\\classes\\\\com\\\\cn\\\\model\\\\User.class\";\n    File file = new File(fileName);\n    try(FileInputStream fis = new FileInputStream(file)) {\n        byte[] content=new byte[(int)file.length()];\n        fis.read(content);\n        Class clazz = unsafe.defineClass(null, content, 0, content.length, null, null);\n        Object o = clazz.newInstance();\n        Object age = clazz.getMethod(\"getAge\").invoke(o, null);\n        System.out.println(age);\n    } catch (Exception e) {\n        e.printStackTrace();\n    }\n}\n```\n\n在上面的代码中，首先读取了一个class文件并通过文件流将它转化为字节数组，之后使用defineClass方法动态的创建了一个类，并在后续完成了它的实例化工作，流程如下图所示，并且通过这种方式创建的类，会跳过JVM的所有安全检查。\n\n![img](https://oss.javaguide.cn/github/javaguide/java/basis/unsafe/image-20220717145000710.png)\n\n除了defineClass方法外，Unsafe还提供了一个defineAnonymousClass方法：\n\n```java\npublic native Class<?> defineAnonymousClass(Class<?> hostClass, byte[] data, Object[] cpPatches);\n```\n\n使用该方法可以用来动态的创建一个匿名类，在Lambda表达式中就是使用ASM动态生成字节码，然后利用该方法定义实现相应的函数式接口的匿名类。在JDK15发布的新特性中，在隐藏类（Hiddenclasses）一条中，指出将在未来的版本中弃用Unsafe的defineAnonymousClass方法。\n\n#### 典型应用\n\nLambda表达式实现需要依赖Unsafe的defineAnonymousClass方法定义实现相应的函数式接口的匿名类。\n\n### 系统信息\n\n#### 介绍\n\n这部分包含两个获取系统相关信息的方法。\n\n```java\n//返回系统指针的大小。返回值为4（32位系统）或8（64位系统）。\npublic native int addressSize();\n//内存页的大小，此值为2的幂次方。\npublic native int pageSize();\n```\n\n#### 典型应用\n\n这两个方法的应用场景比较少，在java.nio.Bits类中，在使用pageCount计算所需的内存页的数量时，调用了pageSize方法获取内存页的大小。另外，在使用copySwapMemory方法拷贝内存时，调用了addressSize方法，检测32位系统的情况。\n\n## 总结\n\n在本文中，我们首先介绍了Unsafe的基本概念、工作原理，并在此基础上，对它的API进行了说明与实践。相信大家通过这一过程，能够发现Unsafe在某些场景下，确实能够为我们提供编程中的便利。但是回到开头的话题，在使用这些便利时，确实存在着一些安全上的隐患，在我看来，一项技术具有不安全因素并不可怕，可怕的是它在使用过程中被滥用。尽管之前有传言说会在Java9中移除Unsafe类，不过它还是照样已经存活到了Java16。按照存在即合理的逻辑，只要使用得当，它还是能给我们带来不少的帮助，因此最后还是建议大家，在使用Unsafe的过程中一定要做到使用谨慎使用、避免滥用。\n\n> [原文链接](https://javaguide.cn/java/basis/unsafe.html)","categories":["Java"]},{"title":"代理","slug":"代理","url":"/blog/posts/5c9c6aec0510/","content":"\n## 代理模式\n\n代理模式是一种比较好理解的设计模式。简单来说就是我们使用代理对象来代替对真实对象(real object)的访问，这样就可以在不修改原目标对象的前提下，提供额外的功能操作，扩展目标对象的功能。**代理模式的主要作用是扩展目标对象的功能，比如说在目标对象的某个方法执行前后你可以增加一些自定义的操作**。代理模式有静态代理和动态代理两种实现方式，我们先来看一下静态代理模式的实现。\n\n### 静态代理\n\n静态代理中，我们对目标对象的每个方法的增强都是手动完成的，非常不灵活（比如接口一旦新增加方法，目标对象和代理对象都要进行修改）且麻烦(需要对每个目标类都单独写一个代理类)。实际应用场景非常非常少，日常开发几乎看不到使用静态代理的场景。上面我们是从实现和应用角度来说的静态代理，从JVM层面来说，静态代理在编译时就将接口、实现类、代理类这些都变成了一个个实际的class文件。静态代理实现步骤:\n\n1. 定义一个接口及其实现类；\n2. 创建一个代理类同样实现这个接口\n3. 将目标对象注入进代理类，然后在代理类的对应方法调用目标类中的对应方法。这样的话，我们就可以通过代理类屏蔽对目标对象的访问，并且可以在目标方法执行前后做一些自己想做的事情。\n\n下面通过代码展示！\n\n**1. 定义发送短信的接口**\n\n```java\npublic interface SmsService {\n    String send(String message);\n}\n```\n\n**2. 实现发送短信的接口**\n\n```java\npublic class SmsServiceImpl implements SmsService {\n    public String send(String message) {\n        System.out.println(\"send message:\" + message);\n        return message;\n    }\n}\n```\n\n**3. 创建代理类并同样实现发送短信的接口**\n\n```java\npublic class SmsProxy implements SmsService {\n\n    private final SmsService smsService;\n    \n    public SmsProxy(SmsService smsService) {\n        this.smsService = smsService;\n    }\n    @Override\n    public String send(String message) {\n        //调用方法之前，我们可以添加自己的操作\n        System.out.println(\"before method send()\");\n        smsService.send(message);\n        //调用方法之后，我们同样可以添加自己的操作\n        System.out.println(\"after method send()\");\n        return null;\n    }\n}\n```\n\n**4. 实际使用**\n\n```java\npublic class Main {\n    public static void main(String[] args) {\n        SmsService smsService = new SmsServiceImpl();\n        SmsProxy smsProxy = new SmsProxy(smsService);\n        smsProxy.send(\"java\");\n    }\n}\n```\n\n运行上述代码之后，控制台打印出：\n\n```bash\nbefore method send()\nsend message:java\nafter method send()\n```\n\n可以输出结果看出，我们已经增加了SmsServiceImpl的send()方法。\n\n### 动态代理\n\n相比于静态代理来说，动态代理更加灵活。我们不需要针对每个目标类都单独创建一个代理类，并且也不需要我们必须实现接口，我们可以直接代理实现类(CGLIB动态代理机制)。从JVM角度来说，动态代理是在运行时动态生成类字节码，并加载到JVM中的。说到动态代理，Spring AOP、RPC框架应该是两个不得不提的，它们的实现都依赖了动态代理。动态代理在我们日常开发中使用的相对较少，但是在框架中的几乎是必用的一门技术。学会了动态代理之后，对于我们理解和学习各种框架的原理也非常有帮助。就Java来说，动态代理的实现方式有很多种，比如**JDK动态代理**、**CGLIB动态代理**等等。\n\n> [guide-rpc-framework](https://github.com/Snailclimb/guide-rpc-framework)使用的是JDK动态代理，我们先来看看JDK动态代理的使用。另外，虽然[guide-rpc-framework](https://github.com/Snailclimb/guide-rpc-framework)没有用到**CGLIB动态代理**，我们这里还是简单介绍一下其使用以及和**JDK动态代理**的对比。\n\n#### JDK动态代理机制\n\n##### 介绍\n\n**在Java动态代理机制中InvocationHandler接口和Proxy类是核心**。Proxy类中使用频率最高的方法是：newProxyInstance()，这个方法主要用来生成一个代理对象。\n\n```java\npublic static Object newProxyInstance(ClassLoader loader, Class<?>[] interfaces, InvocationHandler h)\n    throws IllegalArgumentException{\n    ......\n}\n```\n\n这个方法一共有3个参数：\n\n1. **loader**:类加载器，用于加载代理对象。\n2. **interfaces**:被代理类实现的一些接口；\n3. **h**:实现了InvocationHandler接口的对象；\n\n要实现动态代理的话，还必须需要实现InvocationHandler来自定义处理逻辑。当我们的动态代理对象调用一个方法时，这个方法的调用就会被转发到实现InvocationHandler接口类的invoke方法来调用。\n\n```java\npublic interface InvocationHandler {\n    /**\n     * 当你使用代理对象调用方法的时候实际会调用到这个方法\n     */\n    public Object invoke(Object proxy, Method method, Object[] args) throws Throwable;\n}\n```\n\ninvoke()方法有下面三个参数：\n\n1. **proxy**:动态生成的代理类\n2. **method**:与代理类对象调用的方法相对应\n3. **args**:当前method方法的参数\n\n也就是说：**你通过Proxy类的newProxyInstance()创建的代理对象在调用方法的时候，实际会调用到实现InvocationHandler接口的类的invoke()方法**。你可以在invoke()方法中自定义处理逻辑，比如在方法执行前后做什么事情。\n\n##### JDK动态代理类使用步骤\n\n1. 定义一个接口及其实现类；\n2. 自定义InvocationHandler并重写invoke方法，在invoke方法中我们会调用原生方法（被代理类的方法）并自定义一些处理逻辑；\n3. 通过`Proxy.newProxyInstance(ClassLoaderloader,Class<?>[]interfaces,InvocationHandlerh)`方法创建代理对象；\n\n##### JDK动态代理原理(给一个接口实现匿名内部类)\n\n利用拦截器（必须实现InvocationHandler）加上反射机制生成一个代理接口的匿名类，在调用具体方法前调用InvokeHandler来处理。举个例子，假设有一个接口A，A有一个实现类B，现在要给B生成代理对象，那么实际上是给A接口自动生成了一个匿名实现类，并且在这个匿名实现类中调用到B中的方法。JDK代理是不需要第三方库支持，只需要JDK环境就可以进行代理，使用条件:\n\n1. 实现InvocationHandler\n2. 使用Proxy.newProxyInstance产生代理对象\n3. 被代理的对象必须要实现接口\n\n##### 代码示例\n\n**1. 定义发送短信的接口**\n\n```java\npublic interface SmsService {\n    String send(String message);\n}\n```\n\n**2. 实现发送短信的接口**\n\n```java\npublic class SmsServiceImpl implements SmsService {\n    public String send(String message) {\n        System.out.println(\"send message:\" + message);\n        return message;\n    }\n}\n```\n\n**3. 定义一个JDK动态代理类**\n\n```java\nimport java.lang.reflect.InvocationHandler;\nimport java.lang.reflect.InvocationTargetException;\nimport java.lang.reflect.Method;\n\npublic class DebugInvocationHandler implements InvocationHandler {\n    /**\n     * 代理类中的真实对象\n     */\n    private final Object target;\n\n    public DebugInvocationHandler(Object target) {\n        this.target = target;\n    }\n\n    public Object invoke(Object proxy, Method method, Object[] args) throws InvocationTargetException, IllegalAccessException {\n        //调用方法之前，我们可以添加自己的操作\n        System.out.println(\"before method \" + method.getName());\n        Object result = method.invoke(target, args);\n        //调用方法之后，我们同样可以添加自己的操作\n        System.out.println(\"after method \" + method.getName());\n        return result;\n    }\n}\n```\n\ninvoke()方法:当我们的动态代理对象调用原生方法的时候，最终实际上调用到的是invoke()方法，然后invoke()方法代替我们去调用了被代理对象的原生方法。\n\n**4. 获取代理对象的工厂类**\n\n```java\npublic class JdkProxyFactory {\n    public static Object getProxy(Object target) {\n        return Proxy.newProxyInstance(\n                target.getClass().getClassLoader(), // 目标类的类加载\n                target.getClass().getInterfaces(),  // 代理需要实现的接口，可指定多个\n                new DebugInvocationHandler(target)   // 代理对象对应的自定义InvocationHandler\n        );\n    }\n}\n\n```\ngetProxy()：主要通过Proxy.newProxyInstance()方法获取某个类的代理对象\n\n**5. 实际使用**\n\n```java\nSmsService smsService = (SmsService) JdkProxyFactory.getProxy(new SmsServiceImpl());\nsmsService.send(\"java\");\n```\n运行上述代码之后，控制台打印出：\n\n```text\nbefore method send\nsend message:java\nafter method send\n```\n\n#### CGLIB动态代理机制\n\n##### 介绍\n\n**JDK动态代理有一个最致命的问题是其只能代理实现了接口的类。为了解决这个问题，我们可以用CGLIB动态代理机制来避免**。[CGLIB](https://github.com/cglib/cglib)(*Code Generation Library*)是一个基于[ASM](http://www.baeldung.com/java-asm)的字节码生成库，它允许我们在运行时对字节码进行修改和动态生成。CGLIB通过继承方式实现代理。很多知名的开源框架都使用到了[CGLIB](https://github.com/cglib/cglib)，例如Spring中的AOP模块中：如果目标对象实现了接口，则默认采用JDK动态代理，否则采用CGLIB动态代理。**在CGLIB动态代理机制中MethodInterceptor接口和Enhancer类是核心**。你需要自定义MethodInterceptor并重写intercept方法，intercept用于拦截增强被代理类的方法。\n\n```java\npublic interface MethodInterceptor extends Callback{\n    // 拦截被代理类中的方法\n    public Object intercept(Object obj, java.lang.reflect.Method method, Object[] args,MethodProxy proxy) throws Throwable;\n}\n```\n1. **obj**:被代理的对象（需要增强的对象）\n2. **method**:被拦截的方法（需要增强的方法）\n3. **args**:方法入参\n4. **proxy**:用于调用原始方法\n\n你可以通过Enhancer类来动态获取被代理类，当代理类调用方法的时候，实际调用的是MethodInterceptor中的intercept方法。\n\n##### CGLIB动态代理类使用步骤\n\n1. 定义一个类；\n2. 自定义MethodInterceptor并重写intercept方法，intercept用于拦截增强被代理类的方法，和JDK动态代理中的invoke方法类似；\n3. 通过Enhancer类的create()创建代理类；\n\n##### CGLIB动态代理原理(生成子类)\n\n利用ASM框架，对代理对象类生成的class文件加载进来，通过修改其字节码生成子类来处理。举个例子，现在有一个类A，A没有接口，现在想给A生成一个代理对象，那么实际上是自动给A生成了一个子类，在这个子类中覆盖了A中的方法，所以要注意，A类以及它里边的方法不能是final类型的，否则无法生成代理。CGLib必须依赖于CGLib的类库，但是它需要类来实现任何接口代理的是指定的类生成一个子类，覆盖其中的方法，是一种继承但是针对接口编程的环境下推荐使用JDK的代理；\n\n如果被代理的对象有接口，则可以使用JDK动态代理，没有接口就可以使用CGLIB动态代理。\n\n##### 代码示例\n\n不同于JDK动态代理不需要额外的依赖。[CGLIB](https://github.com/cglib/cglib)(*Code Generation Library*)实际是属于一个开源项目，如果你要使用它的话，需要手动添加相关依赖。\n\n\n```xml\n<dependency>\n  <groupId>cglib</groupId>\n  <artifactId>cglib</artifactId>\n  <version>3.3.0</version>\n</dependency>\n```\n\n**1. 实现一个使用阿里云发送短信的类**\n\n```java\npackage github.javaguide.dynamicProxy.cglibDynamicProxy;\n\npublic class AliSmsService {\n    public String send(String message) {\n        System.out.println(\"send message:\" + message);\n        return message;\n    }\n}\n```\n\n**2. 自定义MethodInterceptor（方法拦截器）**\n\n```java\nimport net.sf.cglib.proxy.MethodInterceptor;\nimport net.sf.cglib.proxy.MethodProxy;\n\nimport java.lang.reflect.Method;\n\n/**\n * 自定义MethodInterceptor\n */\npublic class DebugMethodInterceptor implements MethodInterceptor {\n\n    /**\n     * @param o           被代理的对象（需要增强的对象）\n     * @param method      被拦截的方法（需要增强的方法）\n     * @param args        方法入参\n     * @param methodProxy 用于调用原始方法\n     */\n    @Override\n    public Object intercept(Object o, Method method, Object[] args, MethodProxy methodProxy) throws Throwable {\n        //调用方法之前，我们可以添加自己的操作\n        System.out.println(\"before method \" + method.getName());\n        Object object = methodProxy.invokeSuper(o, args);\n        //调用方法之后，我们同样可以添加自己的操作\n        System.out.println(\"after method \" + method.getName());\n        return object;\n    }\n\n}\n```\n\n**3. 获取代理类**\n\n```java\nimport net.sf.cglib.proxy.Enhancer;\n\npublic class CglibProxyFactory {\n    public static Object getProxy(Class<?> clazz) {\n        // 创建动态代理增强类\n        Enhancer enhancer = new Enhancer();\n        // 设置类加载器\n        enhancer.setClassLoader(clazz.getClassLoader());\n        // 设置被代理类\n        enhancer.setSuperclass(clazz);\n        // 设置方法拦截器\n        enhancer.setCallback(new DebugMethodInterceptor());\n        // 创建代理类\n        return enhancer.create();\n    }\n}\n```\n\n**4. 实际使用**\n\n```java\nAliSmsService aliSmsService = (AliSmsService) CglibProxyFactory.getProxy(AliSmsService.class);\naliSmsService.send(\"java\");\n```\n\n运行上述代码之后，控制台打印出：\n\n```bash\nbefore method send\nsend message:java\nafter method send\n```\n\n#### JDK动态代理和CGLIB动态代理对比\n\n1. JDK动态代理只能代理实现了接口的类或者直接代理接口，而CGLIB可以代理未实现任何接口的类。另外，CGLIB动态代理是通过生成一个被代理类的子类来拦截被代理类的方法调用，因此不能代理声明为final类型的类和方法。\n2. 就二者的效率来说，大部分情况都是JDK动态代理更优秀，随着JDK版本的升级，这个优势更加明显。\n\n### 静态代理和动态代理的对比\n\n1. **灵活性**：动态代理更加灵活，不需要必须实现接口，可以直接代理实现类，并且可以不需要针对每个目标类都创建一个代理类。另外，静态代理中，接口一旦新增加方法，目标对象和代理对象都要进行修改，这是非常麻烦的！\n2. **JVM层面**：静态代理在编译时就将接口、实现类、代理类这些都变成了一个个实际的class文件。而动态代理是在运行时动态生成类字节码，并加载到JVM中的。\n\n### 为什么要用代理模式？\n\n1. **中介隔离作用**\n   在某些情况下，一个客户类不想或者不能直接引用一个委托对象，而代理类对象可以在客户类和委托对象之间起到中介的作用，其特征是代理类和委托类实现相同的接口。\n2. **开闭原则，增加功能**\n   代理类除了是客户类和委托类的中介之外，我们还可以通过给代理类增加额外的功能来扩展委托类的功能，这样做我们只需要修改代理类而不需要再修改委托类，符合代码设计的开闭原则。代理类主要负责为委托类预处理消息、过滤消息、把消息转发给委托类，以及事后对返回结果的处理等。代理类本身并不真正实现服务，而是同过调用委托类的相关方法，来提供特定的服务。真正的业务功能还是由委托类来实现，但是可以在业务功能执行的前后加入一些公共的服务。例如我们想给项目加入缓存、日志这些功能，我们就可以使用代理类来完成，而没必要打开已经封装好的委托类。\n\n> [原文链接](https://javaguide.cn/java/basis/proxy.html)\n> [代理 demo](https://github.com/xmxe/demo/tree/master/study-demo/src/main/java/com/xmxe/designpattern/proxy)\n> [Spring AOP相关](https://github.com/xmxe/springboot/tree/3.x/springboot-aop)\n\n### 相关文章\n\n- [终于有人把java代理讲清楚了，万字详解！](https://mp.weixin.qq.com/s/C9Vpfcgl3NB_0rBpLh2yCA)\n- [设计模式---代理模式](https://www.cnblogs.com/daniels/p/8242592.html)\n- [一文读懂Java动态代理](https://mp.weixin.qq.com/s/1Jxxrbi8nk4pcD8I1ts5lQ)\n- [一文读懂Java动态代理，那些面试中你容易忽略的细节](https://mp.weixin.qq.com/s/FehKQzdgXfM2556v8TNNqg)\n- [Java动态代理的两种方式及其优缺点](https://mp.weixin.qq.com/s/kgB03P7Ocqj8EUTv55-zNw)\n- [面试官：为什么JDK动态代理只能代理接口？](https://mp.weixin.qq.com/s/9wJjsJRWsZsFSBTVWmA4Hg)\n","tags":["代码实战"],"categories":["Java"]},{"title":"序列化和反序列化","slug":"序列化和反序列化","url":"/blog/posts/6afc7d778a39/","content":"\n## 序列化和反序列化\n\n### 什么是序列化?什么是反序列化?\n\n如果我们需要持久化Java对象比如将Java对象保存在文件中，或者在网络传输Java对象，这些场景都需要用到序列化。简单来说：\n\n- **序列化**：将数据结构或对象转换成二进制字节流的过程\n- **反序列化**：将在序列化过程中所生成的二进制字节流转换成数据结构或者对象的过程\n\n对于Java这种面向对象编程语言来说，我们序列化的都是对象（Object）也就是实例化后的类(Class)，但是在C++这种半面向对象的语言中，struct(结构体)定义的是数据结构类型，而class对应的是对象类型。\n\n下面是序列化和反序列化常见应用场景：\n\n- 对象在进行网络传输（比如远程方法调用RPC的时候）之前需要先被序列化，接收到序列化的对象之后需要再进行反序列化；\n- 将对象存储到文件之前需要进行序列化，将对象从文件中读取出来需要进行反序列化；\n- 将对象存储到数据库（如Redis）之前需要用到序列化，将对象从缓存数据库中读取出来需要反序列化；\n- 将对象存储到内存之前需要进行序列化，从内存中读取出来之后需要进行反序列化。\n\n> 维基百科是如是介绍序列化的：\n> **序列化**（serialization）在计算机科学的数据处理中，是指将数据结构或对象状态转换成可取用格式（例如存成文件，存于缓冲，或经由网络中发送），以留待后续在相同或另一台计算机环境中，能恢复原先状态的过程。依照序列化格式重新获取字节的结果时，可以利用它来产生与原始对象相同语义的副本。对于许多对象，像是使用大量引用的复杂对象，这种序列化重建的过程并不容易。面向对象中的对象序列化，并不概括之前原始对象所关系的函数。这种过程也称为对象编组（marshalling）。从一系列字节提取数据结构的反向操作，是反序列化（也称为解编组、deserialization、unmarshalling）。\n\n综上：**序列化的主要目的是通过网络传输对象或者说是将对象存储到文件系统、数据库、内存中**。\n\n![img](https://oss.javaguide.cn/github/javaguide/a478c74d-2c48-40ae-9374-87aacf05188c.png)\n\n\n**序列化协议对应于TCP/IP4层模型的哪一层？**\n\n我们知道网络通信的双方必须要采用和遵守相同的协议。TCP/IP四层模型是下面这样的，序列化协议属于哪一层呢？\n\n1. 应用层\n2. 传输层\n3. 网络层\n4. 网络接口层\n\n![TCP/IP四层模型](https://oss.javaguide.cn/github/javaguide/cs-basics/network/tcp-ip-4-model.png)\n\n如上图所示，OSI七层协议模型中，表示层做的事情主要就是对应用层的用户数据进行处理转换为二进制流。反过来的话，就是将二进制流转换成应用层的用户数据。这不就对应的是序列化和反序列化么？OSI七层协议模型中的应用层、表示层和会话层对应的都是TCP/IP四层模型中的应用层，所以序列化协议属于TCP/IP协议应用层的一部分。\n\n### 如果有些字段不想进行序列化怎么办？(transient)\n\n对于不想进行序列化的变量，使用**transient**关键字修饰。\n\ntransient关键字的作用是：阻止实例中那些用此关键字修饰的的变量序列化；当对象被反序列化时，被transient修饰的变量值不会被持久化和恢复。关于transient还有几点注意：\n\n- transient只能修饰变量，不能修饰类和方法。\n- transient修饰的变量，在反序列化后变量值将会被置成类型的默认值。例如，如果是修饰int类型，那么反序列后结果就是0。\n- static变量因为不属于任何对象(Object)，所以无论有没有transient关键字修饰，均不会被序列化。\n\n## 常见序列化协议\n\nJDK自带的序列化方式一般不会用，因为序列化效率低并且存在安全问题。比较常用的序列化协议有Hessian、Kryo、Protobuf、ProtoStuff，这些都是基于二进制的序列化协议。像JSON和XML这种属于文本类序列化方式。虽然可读性比较好，但是性能较差，一般不会选择。\n\n\n### JDK自带的序列化方式\n\nJDK自带的序列化，只需实现java.io.Serializable接口即可。\n\n```java\n@AllArgsConstructor\n@NoArgsConstructor\n@Getter\n@Builder\n@ToString\npublic class RpcRequest implements Serializable {\n    private static final long serialVersionUID = 1905122041950251207L;\n    private String requestId;\n    private String interfaceName;\n    private String methodName;\n    private Object[] parameters;\n    private Class<?>[] paramTypes;\n    private RpcMessageTypeEnum rpcMessageTypeEnum;\n}\n```\n\n\n**serialVersionUID有什么作用？**\n\n序列化号serialVersionUID属于版本控制的作用。反序列化时，会检查serialVersionUID是否和当前类的serialVersionUID一致。如果serialVersionUID不一致则会抛出InvalidClassException异常。强烈推荐每个序列化类都手动指定其serialVersionUID，如果不手动指定，那么编译器会动态生成默认的serialVersionUID。\n\n**serialVersionUID不是被static变量修饰了吗？为什么还会被“序列化”？**\n\nstatic修饰的变量是静态变量，位于方法区，本身是不会被序列化的。static变量是属于类的而不是对象。你反序列之后，static变量的值就像是默认赋予给了对象一样，看着就像是static变量被序列化，实际只是假象罢了。\n\n\n**为什么不推荐使用JDK自带的序列化？**\n\n我们很少或者说几乎不会直接使用JDK自带的序列化方式，主要原因有下面这些原因：\n\n- **不支持跨语言调用**：如果调用的是其他语言开发的服务的时候就不支持了。\n\n- **性能差**：相比于其他序列化框架性能更低，主要原因是序列化之后的字节数组体积较大，导致传输成本加大。\n\n- **存在安全问题**：序列化和反序列化本身并不存在问题。但当输入的反序列化的数据可被用户控制，那么攻击者即可通过构造恶意输入，让反序列化产生非预期的对象，在此过程中执行构造的任意代码。\n  > 相关阅读：[应用安全:JAVA反序列化漏洞之殇-Cryin](https://cryin.github.io/blog/secure-development-java-deserialization-vulnerability/)、[Java反序列化安全漏洞怎么回事?-Monica](https://www.zhihu.com/question/37562657/answer/1916596031)。\n\n### Kryo\n\nKryo是一个高性能的序列化/反序列化工具，由于其变长存储特性并使用了字节码生成机制，拥有较高的运行速度和较小的字节码体积。另外，Kryo已经是一种非常成熟的序列化实现了，已经在Twitter、Groupon、Yahoo以及多个著名开源项目（如Hive、Storm）中广泛的使用。[guide-rpc-framework](https://github.com/Snailclimb/guide-rpc-framework)就是使用的kryo进行序列化，序列化和反序列化相关的代码如下：\n\n```java\n/**\n * Kryo serialization class, Kryo serialization efficiency is very high, but only compatible with Java language\n *\n * @author shuang.kou\n * @createTime 2020年05月13日 19:29:00\n */\n@Slf4j\npublic class KryoSerializer implements Serializer {\n\n    /**\n     * Because Kryo is not thread safe. So, use ThreadLocal to store Kryo objects\n     */\n    private final ThreadLocal<Kryo> kryoThreadLocal = ThreadLocal.withInitial(() -> {\n        Kryo kryo = new Kryo();\n        kryo.register(RpcResponse.class);\n        kryo.register(RpcRequest.class);\n        return kryo;\n    });\n\n    @Override\n    public byte[] serialize(Object obj) {\n        try (ByteArrayOutputStream byteArrayOutputStream = new ByteArrayOutputStream();\n             Output output = new Output(byteArrayOutputStream)) {\n            Kryo kryo = kryoThreadLocal.get();\n            // Object->byte:将对象序列化为byte数组\n            kryo.writeObject(output, obj);\n            kryoThreadLocal.remove();\n            return output.toBytes();\n        } catch (Exception e) {\n            throw new SerializeException(\"Serialization failed\");\n        }\n    }\n\n    @Override\n    public <T> T deserialize(byte[] bytes, Class<T> clazz) {\n        try (ByteArrayInputStream byteArrayInputStream = new ByteArrayInputStream(bytes);\n             Input input = new Input(byteArrayInputStream)) {\n            Kryo kryo = kryoThreadLocal.get();\n            // byte->Object:从byte数组中反序列化出对象\n            Object o = kryo.readObject(input, clazz);\n            kryoThreadLocal.remove();\n            return clazz.cast(o);\n        } catch (Exception e) {\n            throw new SerializeException(\"Deserialization failed\");\n        }\n    }\n\n}\n```\n\n> [Github地址](https://github.com/EsotericSoftware/kryo)。\n\n### Protobuf\n\nProtobuf出自于Google，性能还比较优秀，也支持多种语言，同时还是跨平台的。就是在使用中过于繁琐，因为你需要自己定义IDL文件和生成对应的序列化代码。这样虽然不灵活，但是，另一方面导致protobuf没有序列化漏洞的风险。\n\n> Protobuf包含序列化格式的定义、各种语言的库以及一个IDL编译器。正常情况下你需要定义proto文件，然后使用IDL编译器编译成你需要的语言\n\n一个简单的proto文件如下：\n\n```protobuf\n// protobuf的版本\nsyntax = \"proto3\";\n// SearchRequest会被编译成不同的编程语言的相应对象，比如Java中的class、Go中的struct\nmessage Person {\n  //string类型字段\n  string name = 1;\n  // int 类型字段\n  int32 age = 2;\n}\n```\n\n>  [Github地址](https://github.com/protocolbuffers/protobuf)\n\n\n### ProtoStuff\n\n由于Protobuf的易用性，它的哥哥Protostuff诞生了。protostuff基于Google protobuf，但是提供了更多的功能和更简易的用法。虽然更加易用，但是不代表ProtoStuff性能更差。\n\n> [Github地址](https://github.com/protostuff/protostuff)。\n\n### Hessian\n\nHessian是一个轻量级的，自定义描述的二进制RPC协议。Hessian是一个比较老的序列化实现了，并且同样也是跨语言的。\n\n![img](https://oss.javaguide.cn/github/javaguide/8613ec4c-bde5-47bf-897e-99e0f90b9fa3.png)\n\nDubbo2.x默认启用的序列化方式是Hessian2,但是，Dubbo对Hessian2进行了修改，不过大体结构还是差不多。\n\n### Fury\n\nFury是一个基于JIT动态编译和零拷贝的多语言序列化框架，支持Java/Python/Golang/JavaScript/C++等语言，提供全自动的对象多语言/跨语言序列化能力，和相比 JDK最高170倍的性能。\n\n> [GitHub地址](https://github.com/alipay/fury)\n> [官方网站](https://furyio.org)\n\n测试用例:\n\n```java\nimport io.fury.Fury;\nimport io.fury.Language;\nimport io.fury.ThreadSafeFury;\n\npublic class StudentDTO {\n    private Long id;\n    private String name;\n    private int age;\n    // ... 省略 getter setter\n\n    public static void main(String[] args) {\n        StudentDTO studentDTO = new StudentDTO();\n        studentDTO.setAge(18);\n        studentDTO.setName(\"Java极客技术\");\n        studentDTO.setId(1L);\n\n        Fury fury = Fury.builder().withLanguage(Language.JAVA).build();\n        fury.register(StudentDTO.class);\n        byte[] bytes = fury.serialize(studentDTO);\n        System.out.println(fury.deserialize(bytes));\n\n\n        ThreadSafeFury fury2 = Fury.builder().withLanguage(Language.JAVA)\n                // Allow to deserialize objects unknown types,\n                // more flexible but less secure.\n//                 .withSecureMode(false)\n                .buildThreadSafeFury();\n        fury2.getCurrentFury().register(StudentDTO.class);\n        byte[] bytes2 = fury2.serialize(studentDTO);\n        System.out.println(fury2.deserialize(bytes2));\n    }\n}\n```\n\n构造Fury的时候我们可以构造单线程Fury或者线程安全的Fuyr:\n\n创建单线程Fury\n\n```java\nFury fury = Fury.builder()\n  .withLanguage(Language.JAVA)\n  // 是否开启循环引用，如果并不需要的话可以关闭，提升性能\n  .withRefTracking(false)\n  // 是否压缩 Integer 或者 Long 类型的数字\n  // .withNumberCompressed(true)\n  // 类型前后一致性兼容\n  .withCompatibleMode(CompatibleMode.SCHEMA_CONSISTENT)\n  // .withCompatibleMode(CompatibleMode.COMPATIBLE)\n  // 是否开启多线程编译\n  .withAsyncCompilationEnabled(true)\n  .build();\nbyte[] bytes = fury.serialize(object);\nSystem.out.println(fury.deserialize(bytes));\n```\n\n创建线程安全的Fury\n\n```java\nThreadSafeFury fury = Fury.builder()\n  .withLanguage(Language.JAVA)\n  .withRefTracking(false)\n  .withCompatibleMode(CompatibleMode.SCHEMA_CONSISTENT)\n  .withAsyncCompilationEnabled(true)\n  .buildThreadSafeFury();\nbyte[] bytes = fury.serialize(object);\nSystem.out.println(fury.deserialize(bytes));\n```\n\n零拷贝用例\n\n> 零拷贝（Zero Copy）是一种优化技术，旨在减少数据在内存中的复制操作，提高数据传输和处理的效率。\n> 在传统的拷贝操作中，当数据从一个位置（例如磁盘、网络等）传输到另一个位置（例如应用程序的内存），通常需要将数据从源位置复制到中间缓冲区，然后再从中间缓冲区复制到目标位置。这种复制操作会占用时间、CPU和内存资源。而零拷贝技术通过避免这些不必要的数据复制，从而提高数据传输和处理的效率。具体而言，它允许数据在不进行额外复制的情况下直接从源位置传输到目标位置。\n> 在实现零拷贝时，通常会使用一些特定的技术和API，如操作系统提供的零拷贝接口、内核缓冲区、DMA（直接内存访问）等。这些技术可以减少或消除不必要的数据复制，从而提高系统的性能和吞吐量。\n> 零拷贝技术在处理大量数据的场景中特别有用，例如高性能网络传输、文件 I/O、数据库操作等。它可以减少不必要的CPU负担和内存消耗，并提高数据的传输速度和处理效率。\n\n```java\npublic class StudentDTO {\n    private Long id;\n    private String name;\n    private int age;\n\n    public static void main(String[] args) {\n        // 在main方法中创建一个Fury实例，该实例用于进行序列化和反序列化操作\n        Fury fury = Fury.builder()\n                .withLanguage(Language.JAVA)\n                .build();\n        // 创建一个包含不同类型对象的列表list\n        List<Object> list = Arrays.asList(\"str\", new byte[1000], new int[100], new double[100]);\n        // 创建一个存储BufferObject的集合bufferObjects，用于在序列化过程中收集需要单独处理的对象（即非基本类型对象）\n        Collection<BufferObject> bufferObjects = new ArrayList<>();\n        // 使用Fury的serialize方法对列表进行序列化，并传入一个lambda表达式来判断对象是否需要单独处理（即加入bufferObjects集合）\n        byte[] bytes = fury.serialize(list, e -> !bufferObjects.add(e));\n        // 使用Java8的流操作将bufferObjects集合转换为MemoryBuffer类型集合buffers\n        List<MemoryBuffer> buffers = bufferObjects.stream().map(BufferObject::toBuffer).collect(Collectors.toList());\n        // 使用Fury的deserialize方法对序列化的字节数组bytes进行反序列化，并传入buffers集合用于零拷贝操作\n        System.out.println(fury.deserialize(bytes, buffers));\n    }\n}\n\n```\n这段代码的主要目的是将一个包含不同类型对象的列表进行序列化和反序列化操作，其中序列化操作通过Fury库实现零拷贝。\n\n\n### 总结\n\nKryo是专门针对Java语言序列化方式并且性能非常好，如果你的应用是专门针对Java语言的话可以考虑使用，并且Dubbo官网的[一篇文章](https://dubbo.apache.org/zh/docs/v2.7/user/references/protocol/rest/)中提到说推荐使用Kryo作为生产环境的序列化方式。\n\n![img](https://my-blog-to-use.oss-cn-beijing.aliyuncs.com/2020-8/569e541a-22b2-4846-aa07-0ad479f07440.png)\n\n像Protobuf、ProtoStuff、hessian这类都是跨语言的序列化方式，如果有跨语言需求的话可以考虑使用。除了我上面介绍到的序列化方式的话，还有像Thrift，Avro这些。\n\n> [原文链接](https://javaguide.cn/java/basis/serialization.html)\n\n\n## 测试代码\n\n```java\npublic class SerializableTest {\n    public static void main(String[] args) throws Exception {\n        serializeFlyPig();\n        FlyPig flyPig = deserializeFlyPig();\n        System.out.println(flyPig.toString());\n    }\n    /**\n     * 序列化\n     */\n    private static void serializeFlyPig() throws Exception {\n        FlyPig flyPig = new FlyPig();\n        flyPig.setColor(\"black\");\n        flyPig.setName(\"riemann\");\n        flyPig.setName(\"audi\");\n        // ObjectOutputStream对象输出流，将flyPig对象存储到E盘的flyPig.txt文件中，完成对flyPig对象的序列化操作\n        ObjectOutputStream oos = new ObjectOutputStream(new FileOutputStream(new File(\"C:\\\\Users\\\\wangx\\\\Desktop\\\\flypig.txt\")));\n        oos.writeObject(flyPig);\n        System.out.println(\"FlyPig 对象序列化成功！\");\n        oos.close();\n    }\n\n    /**\n     * 反序列化\n     */\n    private static FlyPig deserializeFlyPig() throws Exception {\n        ObjectInputStream ois = new ObjectInputStream(new FileInputStream(new File(\"C:\\\\Users\\\\wangx\\\\Desktop\\\\flypig.txt\")));\n        FlyPig pig = (FlyPig) ois.readObject();\n        System.out.println(\"FlyPig 对象反序列化成功！\");\n        return pig;\n    }\n\n    /**\n     * map写文件\n     */\n    public void writeFileByMap(Map<String,Object> map){\n        File file = new File(\"\");\n        try{\n            StringBuffer stringBuffer = new StringBuffer();\n            FileWriter fileWriter = new FileWriter(file,true);\n            Set<Entry<String,Object>> set = map.entrySet();\n            Iterator<Entry<String,Object>> it = set.iterator();\n            while(it.hasNext()){\n                Map.Entry<String,Object> en = it.next();\n                if(en.getKey().equals(\"anObject\")){\n                    stringBuffer.append(en.getKey()+\":\"+en.getValue()).append(System.getProperty(\"line.separator\"));\n\n                }\n            }\n            fileWriter.write(stringBuffer.toString());\n            fileWriter.close();\n\n        }catch(Exception e){\n            e.printStackTrace();\n        }\n    }\n\n}\n\nclass FlyPig implements Serializable {\n\n    private static final long serialVersionUID = 1L;\n    \n    private static String AGE = \"269\";\n    private String name;\n    private String color;\n    transient private String car;\n\n    private String addTip;\n\n    public String getName() {\n        return name;\n    }\n\n    public void setName(String name) {\n        this.name = name;\n    }\n\n    public String getColor() {\n        return color;\n    }\n\n    public void setColor(String color) {\n        this.color = color;\n    }\n\n    public String getCar() {\n        return car;\n    }\n\n    public void setCar(String car) {\n        this.car = car;\n    }\n\n    public String getAddTip() {\n        return addTip;\n    }\n\n    public void setAddTip(String addTip) {\n        this.addTip = addTip;\n    }\n\n    @Override\n    public String toString() {\n        return \"FlyPig{\" + \"name='\" + name + '\\'' + \", color='\" + color + '\\'' + \", car='\" + car + '\\'' + \", AGE='\"\n                + AGE + '\\'' + '}';\n    }\n\n}\n\n```\n\n## 相关文章\n\n- [Java Serializable：明明就一个空的接口嘛](https://mp.weixin.qq.com/s/7ojBbuJ4For2VvgcpqIuVw)\n- [什么是序列化,怎么序列化,为什么序列化,反序列化会遇到什么问题，如何解决](https://mp.weixin.qq.com/s/iIqQeQNeDKimTT5nvye-ow)\n- [Java序列化和反序列化为什么要实现Serializable接口](https://mp.weixin.qq.com/s/RLzpPOlKv5omoqRv-5ckPQ)\n- [关于序列化/反序列化，我梭哈](https://mp.weixin.qq.com/s/uTrNn_C-wnKPQieUcQ9z5g)\n\n","tags":["代码实战"],"categories":["Java"]},{"title":"Spring中的Bean对象","slug":"Spring中的Bean对象","url":"/blog/posts/b435885d7cf1/","content":"\n## Spring IoC\n\n### 谈谈自己对于Spring IoC的了解\n\n\n**IoC(Inversion of Control,控制反转)** 是Spring中一个非常非常重要的概念，它不是什么技术，而是一种解耦的设计思想。IoC的主要目的是借助于“第三方”(Spring中的IoC容器)实现具有依赖关系的对象之间的解耦(IOC容器管理对象，你只管使用即可)，从而降低代码之间的耦合度。**IoC是一个原则，而不是一个模式，以下模式（但不限于）实现了IoC原则**。\n\n![ioc-patterns](https://my-blog-to-use.oss-cn-beijing.aliyuncs.com/2019-6/ioc-patterns.png)\n\n**Spring IoC容器就像是一个工厂一样，当我们需要创建一个对象的时候，只需要配置好配置文件/注解即可，完全不用考虑对象是如何被创建出来的**。IoC容器负责创建对象，将对象连接在一起，配置这些对象，并从创建中处理这些对象的整个生命周期，直到它们被完全销毁。在实际项目中一个Service类如果有几百甚至上千个类作为它的底层，我们需要实例化这个Service，你可能要每次都要搞清这个Service所有底层类的构造函数，这可能会把人逼疯。如果利用IOC的话，你只需要配置好，然后在需要的地方引用就行了，这大大增加了项目的可维护性且降低了开发难度。\n\n> 关于Spring IOC的理解，推荐看一下这个[知乎回答](https://www.zhihu.com/question/23277575/answer/169698662)，非常不错。\n\n**控制反转怎么理解呢**？举个例子：\"对象a依赖了对象b，当对象a需要使用对象b的时候必须自己去创建。但是当系统引入了IOC容器后，对象a和对象b之前就失去了直接的联系。这个时候，当对象a需要使用对象b的时候，我们可以指定IOC容器去创建一个对象b注入到对象a中\"。对象a获得依赖对象b的过程,由主动行为变为了被动行为，控制权反转，这就是控制反转名字的由来。**DI(Dependecy Inject,依赖注入)是实现控制反转的一种设计模式，依赖注入就是将实例变量传入到一个对象中去**。\n\n**IoC（InversionofControl:控制反转）**，是一种设计思想，而不是一个具体的技术实现。IoC的思想就是将原本在程序中手动创建对象的控制权，交由Spring框架来管理。不过，IoC并非Spring特有，在其他语言中也有应用。**为什么叫控制反转**?\n\n- **控制**：指的是对象创建（实例化、管理）的权力\n- **反转**：控制权交给外部环境（Spring框架、IoC容器）\n\n![img](https://guide-blog-images.oss-cn-shenzhen.aliyuncs.com/java-guide-blog/frc-365faceb5697f04f31399937c059c162.png)\n\n将对象之间的相互依赖关系交给IoC容器来管理，并由IoC容器完成对象的注入。这样可以很大程度上简化应用的开发，把应用从复杂的依赖关系中解放出来。IoC容器就像是一个工厂一样，当我们需要创建一个对象的时候，只需要配置好配置文件/注解即可，完全不用考虑对象是如何被创建出来的。在实际项目中一个Service类可能依赖了很多其他的类，假如我们需要实例化这个Service，你可能要每次都要搞清这个Service所有底层类的构造函数，这可能会把人逼疯。如果利用IoC的话，你只需要配置好，然后在需要的地方引用就行了，这大大增加了项目的可维护性且降低了开发难度。\n\n在Spring中，IoC容器是Spring用来实现IoC的载体，IoC容器实际上就是个Map（key，value），Map中存放的是各种对象。Spring时代我们一般通过XML文件来配置Bean，后来开发人员觉得XML文件来配置不太好，于是SpringBoot注解配置就慢慢开始流行起来。\n\n> 相关阅读：\n>\n> - [IoC源码阅读](https://javadoop.com/post/spring-ioc)\n> -  [面试被问了几百遍的IoC和AOP，还在傻傻搞不清楚？](https://mp.weixin.qq.com/s?__biz=Mzg2OTA0Njk0OA==&mid=2247486938&idx=1&sn=c99ef0233f39a5ffc1b98c81e02dfcd4&chksm=cea24211f9d5cb07fa901183ba4d96187820713a72387788408040822ffb2ed575d28e953ce7&token=1736772241&lang=zh_CN#rd)\n\n### 什么是Spring Bean？\n\n简单来说，Bean代指的就是那些被IoC容器所管理的对象。我们需要告诉IoC容器帮助我们管理哪些对象，这个是通过配置元数据来定义的。配置元数据可以是XML文件、注解或者Java配置类。\n\n```xml\n<!-- Constructor-arg with 'value' attribute -->\n<bean id=\"...\" class=\"...\">\n   <constructor-arg value=\"...\"/>\n</bean>\n```\n\n下图简单地展示了IoC容器如何使用配置元数据来管理对象。\n\n![img](https://guide-blog-images.oss-cn-shenzhen.aliyuncs.com/github/javaguide/system-design/framework/spring/062b422bd7ac4d53afd28fb74b2bc94d.png)\n\n**org.springframework.beans**和**org.springframework.context**这两个包是IoC实现的基础，如果想要研究IoC相关的源码的话，可以去看看\n\n### 将一个类声明为Bean的注解有哪些?\n\n- @Component：通用的注解，可标注任意类为Spring组件。如果一个Bean不知道属于哪个层，可以使用@Component注解标注。\n- @Repository:对应持久层即Dao层，主要用于数据库相关操作。\n- @Service:对应服务层，主要涉及一些复杂的逻辑，需要用到Dao层。\n- @Controller:对应SpringMVC控制层，主要用于接受用户请求并调用Service层返回数据给前端页面。\n\n### @Component和@Bean的区别是什么？\n\n- @Component注解作用于类，而@Bean注解作用于方法。\n- @Component通常是通过类路径扫描来自动侦测以及自动装配到Spring容器中（我们可以使用@ComponentScan注解定义要扫描的路径从中找出标识了需要装配的类自动装配到Spring的bean容器中）。@Bean注解通常是我们在标有该注解的方法中定义产生这个bean,@Bean告诉了Spring这是某个类的实例，当我需要用它的时候还给我。\n- @Bean注解比@Component注解的自定义性更强，而且很多地方我们只能通过@Bean注解来注册bean。比如当我们引用第三方库中的类需要装配到Spring容器时，则只能通过@Bean来实现。\n\n@Bean注解使用示例：\n\n```java\n@Configuration\npublic class AppConfig {\n    @Bean\n    public TransferService transferService() {\n        return new TransferServiceImpl();\n    }\n\n}\n```\n上面的代码相当于下面的xml配置\n\n```xml\n<beans>\n    <bean id=\"transferService\" class=\"com.acme.TransferServiceImpl\"/>\n</beans>\n```\n\n下面这个例子是通过@Component无法实现的。\n\n```java\n@Bean\npublic OneService getService(status) {\n    case (status)  {\n        when 1:\n                return new serviceImpl1();\n        when 2:\n                return new serviceImpl2();\n        when 3:\n                return new serviceImpl3();\n    }\n}\n```\n\n### 注入Bean的注解有哪些？\n\nSpring内置的@Autowired以及JDK内置的@Resource和@Inject都可以用于注入Bean。\n\n| Annotaion    | Package                            | Source       |\n| ------------ | ---------------------------------- | ------------ |\n| @Autowired | org.springframework.bean.factory | Spring 2.5+  |\n| @Resource  | javax.annotation                 | Java JSR-250 |\n| @Inject    | javax.inject                     | Java JSR-330 |\n\n@Autowired和@Resource使用的比较多一些。\n\n### @Autowired和@Resource的区别是什么？\n\nAutowired属于Spring内置的注解，默认的注入方式为byType（根据类型进行匹配），也就是说会优先根据接口类型去匹配并注入Bean（接口的实现类）。这会有什么问题呢？当一个接口存在多个实现类的话，byType这种方式就无法正确注入对象了,因为这个时候Spring会同时找到多个满足条件的选择，默认情况下它自己不知道选择哪一个。这种情况下，注入方式会变为byName（根据名称进行匹配），这个名称通常就是类名（首字母小写）。就比如说下面代码中的smsService就是我这里所说的名称，这样应该比较好理解了吧。\n\n\n```java\n// smsService就是我们上面所说的名称\n@Autowired\nprivate SmsService smsService;\n```\n\n举个例子，SmsService接口有两个实现类:SmsServiceImpl1和SmsServiceImpl2，且它们都已经被Spring容器所管理。\n\n\n```java\n// 报错，byName和byType都无法匹配到bean\n@Autowired\nprivate SmsService smsService;\n\n// 正确注入SmsServiceImpl1对象对应的bean\n@Autowired\nprivate SmsService smsServiceImpl1;\n\n// 正确注入SmsServiceImpl1对象对应的bean\n// smsServiceImpl1就是我们上面所说的名称\n@Autowired\n@Qualifier(value=\"smsServiceImpl1\")\nprivate SmsService smsService;\n```\n\n我们还是建议通过@Qualifier注解来显式指定名称而不是依赖变量的名称。\n\n@Resource属于JDK提供的注解，默认注入方式为byName。如果无法通过名称匹配到对应的Bean的话，注入方式会变为byType。@Resource有两个比较重要且日常开发常用的属性：name（名称）、type（类型）。\n\n```java\npublic @interface Resource {\n    String name() default \"\";\n    Class<?> type() default Object.class;\n}\n```\n\n如果仅指定name属性则注入方式为byName，如果仅指定type属性则注入方式为byType，如果同时指定name和type属性（不建议这么做）则注入方式为byType+byName。\n\n```java\n// 报错，byName和byType都无法匹配到bean\n@Resource\nprivate SmsService smsService;\n\n// 正确注入SmsServiceImpl1对象对应的bean\n@Resource\nprivate SmsService smsServiceImpl1;\n\n// 正确注入SmsServiceImpl1对象对应的bean（比较推荐这种方式）\n@Resource(name = \"smsServiceImpl1\")\nprivate SmsService smsService;\n```\n\n简单总结一下：\n\n- @Autowired是Spring提供的注解，@Resource是JDK提供的注解。\n- @Autowired默认的注入方式为byType（根据类型进行匹配），@Resource默认注入方式为byName（根据名称进行匹配）。\n- 当一个接口存在多个实现类的情况下，@Autowired和@Resource都需要通过名称才能正确匹配到对应的Bean。Autowired可以通过@Qualifier注解来显式指定名称，@Resource可以通过name属性来显式指定名称。\n\n**@Autowired和@Resource区别**😊\n\n- @Autowired与@Resource都可以用来装配bean,都可以写在字段上,或写在setter方法上。\n- @Autowired默认按类型装配（这个注解是属于spring的）,默认情况下必须要求依赖对象必须存在，如果要允许null值，可以设置它的required属性为false，如：@Autowired(required=false)，如果我们想使用名称装配可以结合@Qualifier注解进行使用，如下：\n\n```java\n@Autowired ()\n@Qualifier (\"baseDao\")\nprivate BaseDao baseDao;\n```\n\n- @Resource（这个注解属于J2EE的），默认按照名称进行装配，名称可以通过name属性进行指定，如果没有指定name属性，当注解写在字段上时，默认取字段名进行安装名称查找，如果注解写在setter方法上默认取属性名进行装配。当找不到与名称匹配的bean时才按照类型进行装配。但是需要注意的是，如果name属性一旦指定，就只会按照名称进行装配。\n- @Autowired只按照byType注入,由Spring提供，@Resource默认按byName自动注入，也提供按照byType注入\n\n> [Spring探索｜既生@Resource，何生@Autowired？](https://mp.weixin.qq.com/s/MZX97YKKmjuj7FxrjBQ1hg)\n> [@Autowired注解是如何实现的？](https://mp.weixin.qq.com/s/gRqZwUV791RtCI1xCoV3Qw)\n> [@Autowired到底是怎么把变量注入进来的？](https://mp.weixin.qq.com/s/Ecs4MTjFpCa6Rz75buTSNw)\n> [你所不知道的Spring中@Autowired那些实现细节](https://mp.weixin.qq.com/s/n_syhEFrXykI7ySRtahEmg)\n> [@Autowired的这些骚操作，你都知道吗？](https://mp.weixin.qq.com/s/2X5xv8I0b6TcXWVH-SC8Ug)\n\n**@Inject**\n\n1. @Inject是JSR330(Dependency Injection for Java)中的规范，需要导入javax.inject.Inject,实现注入。\n2. @Inject是根据类型进行自动装配的，如果需要按名称进行装配，则需要配合@Named\n3. @Inject可以作用在变量、setter方法、构造函数上。\n\n```java\nprivate Abc abc;\n@Inject\npublic void setAbc(@Named(\"beanName\") Abc abc){\n\tthis.abc = abc;\n}\n```\n\n> [@Autowired,@Resource,@Inject三个注解的区别](https://mp.weixin.qq.com/s/YLIsRBSiIjz3dCtSA9onDQ)\n\n1. @Autowired是Spring自带的，@Inject和@Resource都是JDK提供的，其中@Inject是JSR330规范实现的，@Resource是JSR250规范实现的，而Spring通过BeanPostProcessor来提供对JDK规范的支持。\n2. @Autowired、@Inject用法基本一样，不同之处为@Autowired有一个required属性，表示该注入是否是必须的，即如果为必须的，则如果找不到对应的bean，就无法注入，无法创建当前bean。\n3. @Autowired、@Inject是默认按照类型匹配的，@Resource是按照名称匹配的。如在spring-boot-data项目中自动生成的redisTemplate的bean，是需要通过byName来注入的。如果需要注入该默认的，则需要使用@Resource来注入，而不是@Autowired。\n4. 对于@Autowire和@Inject，如果同一类型存在多个bean实例，则需要指定注入的beanName。@Autowired和@Qualifier一起使用，@Inject和@Named一起使用。\n\n### Bean的作用域有哪些?\n\nSpring中Bean的作用域通常有下面几种：\n\n- **singleton**：IoC容器中只有唯一的bean实例。Spring中的bean默认都是单例的，是对单例设计模式的应用。\n- **prototype**：每次获取都会创建一个新的bean实例。也就是说，连续`getBean()`两次，得到的是不同的Bean实例。\n- **request（仅Web应用可用）**:每一次HTTP请求都会产生一个新的bean（请求bean），该bean仅在当前HTTPrequest内有效。\n- **session（仅Web应用可用）**:每一次来自新session的HTTP请求都会产生一个新的bean（会话bean），该bean仅在当前HTTPsession内有效。\n- **application/global-session（仅Web应用可用）**：每个Web应用在启动时创建一个Bean（应用Bean），该bean仅在当前应用启动时间内有效。\n- **websocket（仅Web应用可用）**：每一次WebSocket会话产生一个新的bean。\n\n**如何配置bean的作用域呢？**\n\nxml方式：\n\n```xml\n<bean id=\"...\" class=\"...\" scope=\"singleton\"></bean>\n```\n\n注解方式：\n\n```java\n@Bean\n@Scope(value = ConfigurableBeanFactory.SCOPE_PROTOTYPE)\npublic Person personPrototype() {\n    return new Person();\n}\n```\n\n### 单例Bean的线程安全问题了解吗？\n\n大部分时候我们并没有在项目中使用多线程，所以很少有人会关注这个问题。单例Bean存在线程问题，主要是因为当多个线程操作同一个对象的时候是存在资源竞争的。常见的有两种解决办法：\n\n1. 在Bean中尽量避免定义可变的成员变量。\n2. 在类中定义一个ThreadLocal成员变量，将需要的可变成员变量保存在ThreadLocal中（推荐的一种方式）。\n\n不过，大部分Bean实际都是无状态（没有实例变量）的（比如Dao、Service），这种情况下，Bean是线程安全的。\n\n\n## Spring Bean生命周期\n\n图示：\n![Spring Bean生命周期](https://images.xiaozhuanlan.com/photo/2019/24bc2bad3ce28144d60d9e0a2edf6c7f.jpg)\n\n与之比较类似的中文版本:\n\n![Spring Bean生命周期](https://images.xiaozhuanlan.com/photo/2019/b5d264565657a5395c2781081a7483e1.jpg)\n\n### 一、获取Bean\n\n![图片](https://mmbiz.qpic.cn/mmbiz_png/SJm51egHPPGPI5JCBzTotEAS720l5YpPYhVDaaP8cNKOLWfufL5rQXaMa7xPp4N8NAI2162lm2Rrwvl8sibVCjg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)\n\n#### 第一阶段获取Bean\n\n这里的流程图的入口在AbstractBeanFactory类的doGetBean方法，这里可以配合前面的getBean方法分析文章进行阅读。主要流程就是\n1. 先处理Bean的名称，因为如果以“&”开头的Bean名称表示获取的是对应的FactoryBean对象\n2. 从缓存中获取单例Bean，有则进一步判断这个Bean是不是在创建中，如果是的就等待创建完毕，否则直接返回这个Bean对象\n3. 如果不存在单例Bean缓存，则先进行循环依赖的解析\n4. 解析完毕之后先获取父类BeanFactory，获取到了则调用父类的getBean方法，不存在则先合并然后创建Bean\n\n### 二、创建Bean\n\n#### 2.1 创建Bean之前\n\n![图片](https://mmbiz.qpic.cn/mmbiz_png/SJm51egHPPGPI5JCBzTotEAS720l5YpPicgybuOPvUicWBAxrM1rT0PhJeZ1ftRibJGWGYM7P0f5XMga9QCrSlFFQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)\n\n**在真正创建Bean之前逻辑**\n这个流程图对应的代码在AbstractAutowireCapableBeanFactory类的createBean方法中。\n\n(1)这里会先获取RootBeanDefinition对象中的Class对象并确保已经关联了要创建的Bean的Class。\n(2)这里会检查3个条件：\n\n- Bean的属性中的beforeInstantiationResolved字段是否为true，默认是false。\n- Bean是原生的Bean。\n- Bean的hasInstantiationAwareBeanPostProcessors属性为true，这个属性在Spring准备刷新容器BeanPostProcessors的时候会设置，如果当前Bean实现了InstantiationAwareBeanPostProcessor则这个就会是true。\n\n当三个条件都存在的时候，就会调用实现的InstantiationAwareBeanPostProcessor接口的postProcessBeforeInstantiation方法，然后获取返回的Bean，如果返回的Bean不是null还会调用实现的BeanPostProcessor接口的postProcessAfterInitialization方法，这里用代码说明：\n\n```java\nprotected Object resolveBeforeInstantiation(String beanName, RootBeanDefinition mbd) {\n        Object bean = null;\n        //条件1\n        if (!Boolean.FALSE.equals(mbd.beforeInstantiationResolved)) {\n            //条件2跟条件3\n            if (!mbd.isSynthetic() && hasInstantiationAwareBeanPostProcessors()) {\n                Class<?> targetType = determineTargetType(beanName, mbd);\n                if (targetType != null) {\n                    //调用实现的postProcessBeforeInstantiation方法\n                    bean = applyBeanPostProcessorsBeforeInstantiation(targetType, beanName);\n                    if (bean != null) {\n                       //调用实现的postProcessAfterInitialization方法\n                        bean = applyBeanPostProcessorsAfterInitialization(bean, beanName);\n                    }\n                }\n            }\n            //不满足2或者3的时候就会设置为false\n            mbd.beforeInstantiationResolved = (bean != null);\n        }\n        return bean;\n    }\n```\n\n(3)如果上面3个条件其中一个不满足就不会调用实现的方法。默认这里都不会调用的这些BeanPostProcessors的实现方法。然后继续执行后面的doCreateBean方法。\n\n#### 2.2 真正的创建Bean，doCreateBean\n\n![](https://mmbiz.qpic.cn/mmbiz_png/SJm51egHPPGPI5JCBzTotEAS720l5YpPIhTibribNrjwS7O5fH8doMAibkvl5icWLeq16ibP52JcxspfB8nDtyMhKQA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)\n\n**doCreateBean方法逻辑**\n这个代码的实现还是在AbstractAutowireCapableBeanFactory方法中。流程是\n1. 先检查instanceWrapper变量是不是null，这里一般是null，除非当前正在创建的Bean在factoryBeanInstanceCache中存在这个是保存还没创建完成的FactoryBean的集合。\n2. 调用createBeanInstance方法实例化Bean，这个方法在后面会讲解\n3. 如果当前RootBeanDefinition对象还没有调用过实现了的MergedBeanDefinitionPostProcessor接口的方法，则会进行调用。\n4. 当满足以下三点\n（1）是单例Bean\n（2）尝试解析bean之间的循环引用\n（3）bean目前正在创建中\n则会进一步检查是否实现了SmartInstantiationAwareBeanPostProcessor接口如果实现了则调用是实现的getEarlyBeanReference方法\n5. 调用populateBean方法进行属性填充，这里后面会讲解\n6. 调用initializeBean方法对Bean进行初始化，这里后面会讲解\n\n##### 2.2.1 实例化Bean，createBeanInstance\n\n![图片](https://mmbiz.qpic.cn/mmbiz_png/SJm51egHPPGPI5JCBzTotEAS720l5YpP4WkTkskaiaq1XKqJAEKWhLeNicuTJSsicuK7licC9doicxAbdr01YF0taQg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)\n\n**实例化Bean**\n\n这里的逻辑稍微有一点复杂，这个流程图已经是简化过后的了。简要根据代码说明一下流程\n\n```java\n    protected BeanWrapper createBeanInstance(String beanName, RootBeanDefinition mbd, @Nullable Object[] args) {\n        //步骤1\n        Class<?> beanClass = resolveBeanClass(mbd, beanName);\n\n        if (beanClass != null && !Modifier.isPublic(beanClass.getModifiers()) && !mbd.isNonPublicAccessAllowed()) {\n            throw new BeanCreationException(mbd.getResourceDescription(), beanName,\n                    \"Bean class isn't public, and non-public access not allowed: \" + beanClass.getName());\n        }\n        //步骤2\n        Supplier<?> instanceSupplier = mbd.getInstanceSupplier();\n        if (instanceSupplier != null) {\n            return obtainFromSupplier(instanceSupplier, beanName);\n        }\n        //步骤3\n        if (mbd.getFactoryMethodName() != null) {\n            return instantiateUsingFactoryMethod(beanName, mbd, args);\n        }\n        boolean resolved = false;\n        boolean autowireNecessary = false;\n        if (args == null) {\n            synchronized (mbd.constructorArgumentLock) {\n                if (mbd.resolvedConstructorOrFactoryMethod != null) {\n                    resolved = true;\n                    autowireNecessary = mbd.constructorArgumentsResolved;\n                }\n            }\n        }\n        //步骤4.1\n        if (resolved) {\n            if (autowireNecessary) {\n                return autowireConstructor(beanName, mbd, null, null);\n            }\n            else {\n                return instantiateBean(beanName, mbd);\n            }\n        }\n\n          //步骤4.2\n        Constructor<?>[] ctors = determineConstructorsFromBeanPostProcessors(beanClass, beanName);\n        if (ctors != null || mbd.getResolvedAutowireMode() == AUTOWIRE_CONSTRUCTOR ||\n                mbd.hasConstructorArgumentValues() || !ObjectUtils.isEmpty(args)) {\n            return autowireConstructor(beanName, mbd, ctors, args);\n        }\n        //步骤5\n        ctors = mbd.getPreferredConstructors();\n        if (ctors != null) {\n            return autowireConstructor(beanName, mbd, ctors, null);\n        }\n        return instantiateBean(beanName, mbd);\n    }\n```\n\n1. 先检查Class是否已经关联了，并且对应的修饰符是否是public的\n2. 如果用户定义了Bean实例化的函数，则调用并返回\n3. 如果当前Bean实现了FactoryBean接口则调用对应的FactoryBean接口的getObject方法\n4. 根据getBean时候是否传入构造参数进行处理\n4.1如果没有传入构造参数，则检查是否存在已经缓存的无参构造器，有则使用构造器直接创建，没有就会调用instantiateBean方法先获取实例化的策略默认是CglibSubclassingInstantiationStrategy，然后实例化Bean。最后返回\n4.2如果传入了构造参数，则会先检查是否实现了SmartInstantiationAwareBeanPostProcessor接口，如果实现了会调用determineCandidateConstructors获取返回的候选构造器。\n4.3检查4个条件是否满足一个\n（1）构造器不为null，\n（2）从RootBeanDefinition中获取到的关联的注入方式是构造器注入（没有构造参数就是setter注入，有则是构造器注入）\n（3）含有构造参数\n（4）getBean方法传入构造参数不是空\n满足其中一个则会调用返回的候选构造器实例化Bean并返回，如果都不满足，则会根据构造参数选则合适的有参构造器然后实例化Bean并返回\n5. 如果上面都没有合适的构造器，则直接使用无参构造器创建并返回Bean。\n\n##### 2.2.2 填充Bean，populateBean\n\n![图片](https://mmbiz.qpic.cn/mmbiz_png/SJm51egHPPGPI5JCBzTotEAS720l5YpPRaNrWofKRqgPdvMFQn03uicb2NmqJCHcRzncyuoobJ7alPiaOpVPGR8g/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)\n\n**填充Bean**\n这里还是根据代码来说一下流程\n\n```java\n    protected void populateBean(String beanName, RootBeanDefinition mbd, @Nullable BeanWrapper bw) {\n        if (bw == null) {\n            if (mbd.hasPropertyValues()) {\n                throw new BeanCreationException(\n                        mbd.getResourceDescription(), beanName, \"Cannot apply property values to null instance\");\n            }\n            else {\n                // Skip property population phase for null instance.\n                return;\n            }\n        }\n        boolean continueWithPropertyPopulation = true;\n        //步骤1\n        if (!mbd.isSynthetic() && hasInstantiationAwareBeanPostProcessors()) {\n            for (BeanPostProcessor bp : getBeanPostProcessors()) {\n                if (bp instanceof InstantiationAwareBeanPostProcessor) {\n                    InstantiationAwareBeanPostProcessor ibp = (InstantiationAwareBeanPostProcessor) bp;\n                    if (!ibp.postProcessAfterInstantiation(bw.getWrappedInstance(), beanName)) {\n                        continueWithPropertyPopulation = false;\n                        break;\n                    }\n                }\n            }\n        }\n\n        if (!continueWithPropertyPopulation) {\n            return;\n        }\n        //步骤2--------------------\n        PropertyValues pvs = (mbd.hasPropertyValues() ? mbd.getPropertyValues() : null);\n\n        int resolvedAutowireMode = mbd.getResolvedAutowireMode();\n        if (resolvedAutowireMode == AUTOWIRE_BY_NAME || resolvedAutowireMode == AUTOWIRE_BY_TYPE) {\n            MutablePropertyValues newPvs = new MutablePropertyValues(pvs);\n            // Add property values based on autowire by name if applicable.\n            if (resolvedAutowireMode == AUTOWIRE_BY_NAME) {\n                autowireByName(beanName, mbd, bw, newPvs);\n            }\n            // Add property values based on autowire by type if applicable.\n            if (resolvedAutowireMode == AUTOWIRE_BY_TYPE) {\n                autowireByType(beanName, mbd, bw, newPvs);\n            }\n            pvs = newPvs;\n        }\n\n        boolean hasInstAwareBpps = hasInstantiationAwareBeanPostProcessors();\n        boolean needsDepCheck = (mbd.getDependencyCheck() != AbstractBeanDefinition.DEPENDENCY_CHECK_NONE);\n\n        PropertyDescriptor[] filteredPds = null;\n        //步骤3\n        if (hasInstAwareBpps) {\n            if (pvs == null) {\n                pvs = mbd.getPropertyValues();\n            }\n            for (BeanPostProcessor bp : getBeanPostProcessors()) {\n                if (bp instanceof InstantiationAwareBeanPostProcessor) {\n                    InstantiationAwareBeanPostProcessor ibp = (InstantiationAwareBeanPostProcessor) bp;\n                    PropertyValues pvsToUse = ibp.postProcessProperties(pvs, bw.getWrappedInstance(), beanName);\n                    if (pvsToUse == null) {\n                        if (filteredPds == null) {\n                            filteredPds = filterPropertyDescriptorsForDependencyCheck(bw, mbd.allowCaching);\n                        }\n                        pvsToUse = ibp.postProcessPropertyValues(pvs, filteredPds, bw.getWrappedInstance(), beanName);\n                        if (pvsToUse == null) {\n                            return;\n                        }\n                    }\n                    pvs = pvsToUse;\n                }\n            }\n        }\n        if (needsDepCheck) {\n            if (filteredPds == null) {\n                filteredPds = filterPropertyDescriptorsForDependencyCheck(bw, mbd.allowCaching);\n            }\n            checkDependencies(beanName, mbd, filteredPds, pvs);\n        }\n        //步骤4\n        if (pvs != null) {\n            applyPropertyValues(beanName, mbd, bw, pvs);\n        }\n    }\n```\n\n1. 检查当前Bean是否实现了InstantiationAwareBeanPostProcessor的postProcessAfterInstantiation方法则调用，并结束Bean的填充。\n2. 将按照类型跟按照名称注入的Bean分开，如果注入的Bean还没有实例化的这里会实例化，然后放到PropertyValues对象中。\n3. 如果实现了InstantiationAwareBeanPostProcessor类的postProcessProperties则调用这个方法并获取返回值，如果返回值是null，则有可能是实现了过期的postProcessPropertyValues方法，这里需要进一步调用postProcessPropertyValues方法\n4. 进行参数填充\n\n##### 2.2.3 初始化Bean，initializeBean\n\n![图片](https://mmbiz.qpic.cn/mmbiz_png/SJm51egHPPGPI5JCBzTotEAS720l5YpPZjJibwdUibfEibHoFzlWI6yFbIlaG2EvckACOCY5mneiaibpOZfZrtQICibw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)\n\n**初始化Bean**\n同时这里根据代码跟流程图来说明\n\n- 如果Bean实现了BeanNameAware,BeanClassLoaderAware,BeanFactoryAware则调用对应实现的方法。\n- Bean不为null并且bean不是合成的，如果实现了BeanPostProcessor的postProcessBeforeInitialization则会调用实现的postProcessBeforeInitialization方法。在ApplicationContextAwareProcessor类中实现了postProcessBeforeInitialization方法。而这个类会在Spring刷新容器准备beanFactory的时候会加进去，这里就会被调用，而调用里面会检查Bean是不是EnvironmentAware,EmbeddedValueResolverAware,ResourceLoaderAware,ApplicationEventPublisherAware,MessageSourceAware,ApplicationContextAware的实现类。这里就会调用对应的实现方法。代码如下\n\n```java\n    protected void prepareBeanFactory(ConfigurableListableBeanFactory beanFactory) {\n        .......\n        beanFactory.addBeanPostProcessor(new ApplicationContextAwareProcessor(this));\n        .......\n    public Object postProcessBeforeInitialization(Object bean, String beanName) throws BeansException {\n        if (!(bean instanceof EnvironmentAware || bean instanceof EmbeddedValueResolverAware ||\n                bean instanceof ResourceLoaderAware || bean instanceof ApplicationEventPublisherAware ||\n                bean instanceof MessageSourceAware || bean instanceof ApplicationContextAware)){\n            return bean;\n        }\n\n        AccessControlContext acc = null;\n\n        if (System.getSecurityManager() != null) {\n            acc = this.applicationContext.getBeanFactory().getAccessControlContext();\n        }\n\n        if (acc != null) {\n            AccessController.doPrivileged((PrivilegedAction<Object>) () -> {\n                invokeAwareInterfaces(bean);\n                return null;\n            }, acc);\n        }\n        else {\n            invokeAwareInterfaces(bean);\n        }\n\n        return bean;\n    }\n```\n\n- 实例化Bean然后，检查是否实现了InitializingBean的afterPropertiesSet方法，如果实现了就会调用\n- Bean不为null并且bean不是合成的，如果实现了BeanPostProcessor的postProcessBeforeInitialization则会调用实现的postProcessAfterInitialization方法。\n\n到此创建Bean的流程就没了，剩下的就是容器销毁的时候的了\n\n### 三、destory方法跟销毁Bean\n\nBean在创建完毕之后会检查用户是否指定了destroyMethodName以及是否实现了DestructionAwareBeanPostProcessor接口的requiresDestruction方法，如果指定了会记录下来保存在DisposableBeanAdapter对象中并保存在bean的disposableBeans属性中。代码在AbstractBeanFactory的registerDisposableBeanIfNecessary中\n\n```java\n    protected void registerDisposableBeanIfNecessary(String beanName, Object bean, RootBeanDefinition mbd) {\n          ......\n                registerDisposableBean(beanName,\n                        new DisposableBeanAdapter(bean, beanName, mbd, getBeanPostProcessors(), acc));\n            ......\n    }\npublic DisposableBeanAdapter(Object bean, String beanName, RootBeanDefinition beanDefinition,\n            List<BeanPostProcessor> postProcessors, @Nullable AccessControlContext acc) {\n          .......\n        String destroyMethodName = inferDestroyMethodIfNecessary(bean, beanDefinition);\n        if (destroyMethodName != null && !(this.invokeDisposableBean && \"destroy\".equals(destroyMethodName)) &&\n                !beanDefinition.isExternallyManagedDestroyMethod(destroyMethodName)) {\n            ......\n            this.destroyMethod = destroyMethod;\n        }\n        this.beanPostProcessors = filterPostProcessors(postProcessors, bean);\n    }\n```\n\n在销毁Bean的时候最后都会调用AbstractAutowireCapableBeanFactory的destroyBean方法。\n\n```java\n    public void destroyBean(Object existingBean) {\n        new DisposableBeanAdapter(existingBean, getBeanPostProcessors(), getAccessControlContext()).destroy();\n    }\n```\n\n这里是创建一个DisposableBeanAdapter对象，这个对象实现了Runnable接口，在实现的run方法中会调用实现的DisposableBean接口的destroy方法。并且在创建DisposableBeanAdapter对象的时候会根据传入的bean是否实现了DisposableBean接口来设置invokeDisposableBean变量，这个变量表实有没有实现DisposableBean接口\n\n```java\n    public DisposableBeanAdapter(Object bean, List<BeanPostProcessor> postProcessors,AccessControlContext acc) {\n        Assert.notNull(bean, \"Disposable bean must not be null\");\n        this.bean = bean;\n        this.beanName = bean.getClass().getName();\n        //根据传入的bean是否实现了`DisposableBean`接口来设置`invokeDisposableBean`变量\n        this.invokeDisposableBean = (this.bean instanceof DisposableBean);\n        this.nonPublicAccessAllowed = true;\n        this.acc = acc;\n        this.beanPostProcessors = filterPostProcessors(postProcessors, bean);\n    }\n\n    public void destroy() {\n        ......\n        //根据invokeDisposableBean决定是否调用destroy方法\n        if (this.invokeDisposableBean) {\n            if (logger.isTraceEnabled()) {\n                logger.trace(\"Invoking destroy() on bean with name '\" + this.beanName + \"'\");\n            }\n            try {\n                if (System.getSecurityManager() != null) {\n                    AccessController.doPrivileged((PrivilegedExceptionAction<Object>) () -> {\n                        ((DisposableBean) this.bean).destroy();\n                        return null;\n                    }, this.acc);\n                }\n                else {\n                    ((DisposableBean) this.bean).destroy();\n                }\n            }\n            catch (Throwable ex) {\n                String msg = \"Invocation of destroy method failed on bean with name '\" + this.beanName + \"'\";\n                if (logger.isDebugEnabled()) {\n                    logger.warn(msg, ex);\n                }\n                else {\n                    logger.warn(msg + \": \" + ex);\n                }\n            }\n        }\n    }\n```\n\n### 四、Bean的初始化和销毁的几种方式\n\n#### 初始化\n\n- 实现InitializingBean接口,覆盖其中的afterPropertiesSet()方法\n- 增加@PostConstruct注解\n- 自定义init方法(@Bean(initMethod = \"initMethod\"))\n执行的顺序依次是postConstruct,afterPropertiesSet,initMethod\n\n#### 销毁\n\n- 实现org.springframework.beans.factory.DisposableBean接口，覆盖destroy()方法\n- 自定义一个方法，在方法上面增加@PreDestroy注解\n- 在InitServiceImpl中增加一个自定义销毁方法，然后在配置类中增加Bean的destoryMethod\n执行的顺序依次是preDestroy,destroy,destroyMethod\n\n\n### 五、总结\n\n\n> [Bean的生命周期（五步、七步、十步法剖析）](https://blog.csdn.net/m0_61933976/article/details/128697003)\n> \n> **五步分析法**：\n> > 第一步：实例化Bean（调用无参数构造方法）。\n> > 第二步：Bean属性赋值（调用set方法）。\n> > 第三步：初始化Bean（会调用Bean的init方法。注意：这个init方法需要自己写）。\n> > 第四步：使用Bean。\n> > 第五步：销毁Bean（会调用Bean的destroy方法。注意：这个destroy方法需要自己写）。\n>\n> **七步分析法**：在以上的5步中，第3步是初始化Bean，如果你还想在初始化前和初始化后添加代码，可以加入“Bean后处理器”；需要编写一个类实现BeanPostproccessor接口，并重写里面的befor和after方法。\n>\n> > 第一步：实例化Bean。\n> > 第二步：Bean属性赋值。\n> > 第三步：执行“Bean后处理器”的before方法。\n> > 第四步：初始化Bean。\n> > 第五步：执行“Bean后处理器”的after方法。\n> > 第六步：使用Bean。\n> > 第七步：销毁Bean\n>\n> **十步分析法**：比七步添加的那三步在哪里？\n>\n> > （1）在“Bean后处理器”before方法之前干了什么事儿？检查Bean是否实现了Aware相关的接口，如果实现了接口则调用这些接口中的方法；调用这些方法的目的是为了给你传递一些数据，让你更加方便使用。\n> > （2）在“Bean后处理器”before方法之后干了什么事儿？检查Bean是否实现了InitializingBean接口，如果实现了，则调用接口中的方法。\n> > （3）使用Bean之后，或者说销毁Bean之前干了什么事儿？检查Bean是否实现了DisposableBean接口，如果实现了，则调用接口中的方法。总结：添加的这三个点位的特点，都是在检查你这个Bean是否实现了某些特定的接口，如果实现了这些接口，则Spring容器会调用这个接口中的方法！\n\n最后来一个大的流程\n\n**实例化前的准备阶段**\n![图片](https://mmbiz.qpic.cn/mmbiz_png/SJm51egHPPGPI5JCBzTotEAS720l5YpPQu4pzSyprviaBic07GicVGPvAUdAibkFqybnvOfgdzdw1M1iaMtm9qfBLDQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)\n\n\n**实例化前**\n![图片](https://mmbiz.qpic.cn/mmbiz_png/SJm51egHPPGPI5JCBzTotEAS720l5YpPic9W1CbpBia73nS2WJAGKRMdW9LtwbxG30IqbNT8ibvH5DfqcHO2IueBw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)\n\n\n**实例化后**\n![图片](https://mmbiz.qpic.cn/mmbiz_png/SJm51egHPPGPI5JCBzTotEAS720l5YpPp3ia71rKnC0FeypESdhAFYAqGicz9KP9LeBxaJHKmvMPUDIGrBdBkBiag/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)\n\n**初始化前**\n![图片](https://mmbiz.qpic.cn/mmbiz_png/SJm51egHPPGPI5JCBzTotEAS720l5YpPjfQ5qaic2Ro6hoqhCdoicgiabmkibR518z7vSpXxmibq91FH1XxgHvdet8Q/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)\n\n**初始化后&销毁**\n![图片](https://mmbiz.qpic.cn/mmbiz_png/SJm51egHPPGPI5JCBzTotEAS720l5YpPABU277ApFU3EVr8iaHxtFEVvsawgghYyJd7WlJQFwEkQvXoDW2sQEYQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)\n\n\n### 六、Bean的扩展接口\n\n**顺序一**\n\n- Bean容器找到配置文件中Spring Bean的定义。\n- Bean容器利用Java Reflection API创建一个Bean的实例。\n- 如果涉及到一些属性值利用set()方法设置一些属性值。\n- 如果Bean实现了BeanNameAware接口，调用setBeanName()方法，传入Bean的名字。\n- 如果Bean实现了BeanClassLoaderAware接口，调用setBeanClassLoader()方法，传入ClassLoader对象的实例。\n- 如果Bean实现了BeanFactoryAware接口，调用setBeanFactory()方法，传入BeanFactory对象的实例。\n- 与上面的类似，如果实现了其他\\*.Aware接口，就调用相应的方法。\n- 如果有和加载这个Bean的Spring容器相关的BeanPostProcessor对象，执行postProcessBeforeInitialization()方法\n- 如果Bean实现了InitializingBean接口，执行afterPropertiesSet()方法。\n- 如果Bean在配置文件中的定义包含init-method属性，执行指定的方法。\n- 如果有和加载这个Bean的Spring容器相关的BeanPostProcessor对象，执行postProcessAfterInitialization()方法\n- 当要销毁Bean的时候，如果Bean实现了DisposableBean接口，执行destroy()方法。\n- 当要销毁Bean的时候，如果Bean在配置文件中的定义包含destroy-method属性，执行指定的方法。\n\n**顺序二**\n\n- spring启动，加载类路径下配置文件，解析为BeanDefinition并装配到对应容器中\n- 查找并加载spring管理的bean，进行bean的实例化\n- Bean实例化后对Bean的引用和值进行属性注入\n- 若Bean实现接口BeanNameAware，则执行setBeanName()方法，获取bean的名字\n- 若Bean实现接口BeanFactoryAware，则执行setBeanFactory()方法，获取BeanFactory\n- 若Bean实现接口ApplicationContextAware，则执行setApplicationContext()方法，获取应用上下文\n- 若Bean实现BeanPostProcessor接口，则先执行postProcessBeforeInitialization()方法\n- 若Bean实现InitializingBean接口，则执行afterPropertiesSet()方法\n- 若Bean配置了init-method方法，则执行自定义方法\n- 若Bean实现BeanPostProcessor接口，则先执行postProcessAfterInitialization()方法\n- 如Bean实现了DisposableBean接口，则容器销毁时则执行destory()方法\n- 如果Bean配置了destory-method，则容器销毁时则执行自定义方法。\n\n**顺序三(自己测试的结果)**\n\n在将一个Bean对象配置在IOC容器中之后，这个Bean的生命周期就会交由IOC容器进行管理。一般担当管理者的角色是BeanFactory或ApplicationContext。在将一个bean对象配置在ioc容器中之后，这个bean的生命周期就会交由ioc容器进行管理。一般担当管理者的角色是BeanFactory和ApplicationContext。\n\n1. bean的创建\n在解析ioc容器时，根据解析容器的工厂，决定bean的初始化时间\nBeanFactory-getBean()方法调用时初始化bean\nApplicationContext-解析ioc容器时初始化bean\n2. setter注入\n根据bean子元素的配置实现bean之间的被动注入\n3. BeanNameAware\n如果bean实现了该接口，执行其setBeanName(String name)方法.参数name是bean在容器中的名称,即xml里面bean的id名称\n4. BeanFactoryAware\n如果实现了该接口，执行其setBeanFactory(BeanFactory factory)方法，参数是创建Bean的BeanFactory本身\n5. ApplicationContextAware\n如果这个Bean已经实现了该接口，会调用setApplicationContext(ApplicationContext)方法，传入Spring上下文（同样这个方式也可以实现步骤4的内容，但比4更好，因为ApplicationContext是BeanFactory的子接口，有更多的实现方法）\norg.springframework.beans.context.ApplicationContextAware.当需要从spring容器中获取bean时一般使用这种方式获取:\n```java\nApplicationContext appContext = new ClassPathXmlApplicationContext(\"applicationContext-common.xml\");\nAbcService abcService = (AbcService)appContext.getBean(\"abcService\");\n```\n但是这样就会存在一个问题：因为它会重新装载applicationContext-common.xml并实例化上下文bean，如果有些线程配置类也是在这个配置文件中，那么会造成做相同工作的的线程会被启两次。一次是web容器初始化时启动，另一次是上述代码显示的实例化了一次。当于重新初始化一遍！这样就产生了冗余,所以可以通过实现ApplicationContextAware接口获取bean,当一个类实现了ApplicationContextAware之后，这个类就可以方便获得ApplicationContext中的所有bean。换句话说，就是这个类可以直接获取spring配置文件中所有有引用到的bean对象.\n代码:\n\n```java\nprivate static ApplicationContext applicationContext;\n@Override\npublic void setApplicationContext(ApplicationContext arg0) throws BeansException {\n    applicationContext = arg0;\n}\n```\n注意：从ApplicationContextAware获取ApplicationContext上下文的情况，仅仅适用于当前运行的代码和已启动的Spring代码处于同一个Spring上下文，否则获取到的ApplicationContext是空的\n6. BeanPostProcessor(前置方法)\nioc容器中如果有bean实现了该接口，那所有的bean在初始化之前都会执行其实例的postProcessBeforeInitialization(Object bean, String beanName)前置方法，BeanPostProcessor经常被用作是Bean内容的更改,该方法最后返回bean\n7. @PostConstruct修饰的非静态方法\n8. InitializingBean\n如果实现了该接口，则允许一个bean在它的所有必须属性被BeanFactory设置后，来执行初始化的工作，会自动调用afterPropertiesSet()方法对Bean进行初始化，实现此接口的话正常情况下配置文件就不用指定init-method属性了。\n9. 如果Bean在Spring中配置了init-method属性，调用init-method属性指向的方法,此时完成bean的初始化\n10. BeanPostProcessor(后置方法)\nioc容器中如果有bean实现了接口，那所有的bean在初始化之后都会执行其实例的postProcessAfterInitialization(Object bean, String beanName)后置方法\n11. 实现SmartInitializingSingleton的接口后，当所有单例bean都初始化完成以后，Spring的IOC容器会回调该接口的afterSingletonsInstantiated()方法,主要应用场合就是在所有单例bean创建完成之后，可以在该回调中做一些事情。执行时机在ApplicationContextAware执行之后\n12. @PreDestroy修饰的方法\n13. ioc容器关闭时，如果bean实现了DisposableBean接口，则执行其destory()方法，在Bean生命周期结束前调用destory()方法做一些收尾工作,重写destroy()方法\n14. 如果这个Bean在Spring配置了destroy-method属性，执行destory-method属性指向的方法\n\n> [Spring Boot启动扩展点超详细总结，再也不怕面试官问了](https://mp.weixin.qq.com/s/l0O3C_UiO3CdfNE2V73qmA)\n\n\n![](/images/bean.png)\n\n![](https://img-blog.csdnimg.cn/500f240463544992ad05bab3408c56eb.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAS0vlsI_lk6U=,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center)\n\n**简单来说一个Bean的加载顺序：类构造方法 - postProcessBeforeInitialization前置方法 - @PostConstruct注解的方法 - InitializingBean的afterPropertiesSet()方法- XML中定义的bean init-method方法 - postProcessAfterInitialization后置方法**\n\n> [Spring Boot启动扩展点超详细总结，再也不怕面试官问了](https://mp.weixin.qq.com/s/l0O3C_UiO3CdfNE2V73qmA)\n\n#### BeanFactoryPostProcessor、BeanPostProcessor区别\n\nBeanFactoryPostProcessor：针对bean工厂，BeanFactory后置处理器，是对BeanDefinition对象进行修改，可以修改BeanDefinition对象中的属性。（BeanDefinition：存储bean标签的信息，用来生成bean实例）,BeanFactoryPostProcessor的实现类可以在当前BeanFactory初始化（spring容器加载bean定义文件）后，bean实例化之前修改bean的定义属性，达到影响之后实例化bean的效果。也就是说，Spring允许BeanFactoryPostProcessor在容器实例化任何其它bean之前读取配置元数据，并可以根据需要进行修改，例如可以把bean的scope从singleton改为prototype，也可以把property的值给修改掉。可以同时配置多个BeanFactoryPostProcessor，并通过设置’order’属性来控制各个BeanFactoryPostProcessor的执行次序.\nBeanPostProcessor：针对bean,Bean后置处理器，是对生成的Bean对象进行修改。BeanPostProcessor能在spring容器实例化bean之后，在执行bean的初始化方法前后，添加一些自己的处理逻辑。初始化方法包括以下两种：\n1. 实现InitializingBean接口的bean，对应方法为afterPropertiesSet\n2. xml定义中，通过init-method设置的方法,BeanPostProcessor是BeanFactoryPostProcessor之后执行的。\n\n> [BeanFactoryPostProcessor和BeanPostProcessor有什么区别？](https://mp.weixin.qq.com/s/ZjN1XPamDaYZmvFbyI1KTQ)\n\n#### BeanFactroy、ApplicationContext区别\n\n1. BeanFactroy采用的是延迟加载形式来注入Bean的，即只有在使用到某个Bean时(调用getBean())，才对该Bean进行加载实例化，这样，我们就不能发现一些存在的Spring的配置问题。而ApplicationContext则相反，它是在容器启动时，一次性创建了所有的Bean。这样，在容器启动时，我们就可以发现Spring中存在的配置错误。相对于基本的BeanFactory，ApplicationContext唯一的不足是占用内存空间。当应用程序配置Bean较多时，程序启动较慢。BeanFacotry延迟加载,如果Bean的某一个属性没有注入，BeanFacotry加载后，直至第一次使用调用getBean方法才会抛出异常；而ApplicationContext则在初始化自身是检验，这样有利于检查所依赖属性是否注入；所以通常情况下我们选择使用ApplicationContext。应用上下文则会在上下文启动后预载入所有的单实例Bean。通过预载入单实例bean,确保当你需要的时候，你就不用等待，因为它们已经创建好了。\n2. BeanFactory和ApplicationContext都支持BeanPostProcessor、BeanFactoryPostProcessor的使用，但两者之间的区别是：BeanFactory需要手动注册，而ApplicationContext则是自动注册。（Applicationcontext比beanFactory加入了一些更好使用的功能。而且beanFactory的许多功能需要通过编程实现而Applicationcontext可以通过配置实现。比如后处理bean，Applicationcontext直接配置在配置文件即可而beanFactory这要在代码中显示的写出来才可以被容器识别。）\n3. beanFactory主要是面对与spring框架的基础设施，面对spring自己。而Applicationcontex主要面对与spring使用的开发者。基本都会使用Applicationcontex并非beanFactory。\n\n> [Spring系列之beanFactory与ApplicationContext](https://mp.weixin.qq.com/s?__biz=Mzg2MDYzODI5Nw==&mid=2247493943&idx=1&sn=9eaa46ed730874fce003c66f76fe9c7f&source=41#wechat_redirect)\n> [简单把Spring容器分为了两大类！](https://mp.weixin.qq.com/s/aOOQiBmBmNy4ZjHtv1phdQ)\n\n#### BeanFactory和FactoryBean的区别\n\nBeanFactory是Spring容器的顶级接口，给具体的IOC容器的实现提供了规范。\nFactoryBean也是接口，为IOC容器中Bean的实现提供了更加灵活的方式，FactoryBean在IOC容器的基础上给Bean的实现加上了⼀个简单工厂模式和装饰模式,我们可以在getObject()方法中灵活配置。其实在Spring源码中有很多FactoryBean的实现类。\n区别：BeanFactory是个Factory，也就是IOC容器或对象工厂，FactoryBean是个Bean。在Spring中，所有的Bean都是由BeanFactory(也就是IOC容器)来进行管理的。但对FactoryBean而言，这个Bean不是简单的Bean，而是⼀个能生产或者修饰对象生成的工厂Bean,它的实现与设计模式中的工厂模式和修饰器模式类似。\n\n##### BeanFactory\n\nBeanFactory，以Factory结尾，表示它是⼀个工厂类(接口)，它负责生产和管理bean的⼀个工厂。在Spring中，BeanFactory是IOC容器的核心接口，它的职责包括：实例化、定位、配置应用程序中的对象及建立这些对象间的依赖。BeanFactory只是个接口，并不是IOC容器的具体实现，但是Spring容器给出了很多种实现，如DefaultListableBeanFactory、XmlBeanFactory、ApplicationContext等，其中XmlBeanFactory就是常用的⼀个，该实现将以XML方式描述组成应用的对象及对象间的依赖关系。XmlBeanFactory类将持有此XML配置元数据，并用它来构建⼀个完全可配置的系统或应用。都是附加了某种功能的实现。它为其他具体的IOC容器提供了最基本的规范，例如DefaultListableBeanFactory,XmlBeanFactory,ApplicationContext等具体的容器都是实现了BeanFactory，再在其基础之上附加了其他的功能。BeanFactory和ApplicationContext就是Spring框架的两个IOC容器，现在⼀般使用ApplicationnContext，其不但包含了BeanFactory的作用，同时还进行更多的扩展。BeanFacotry是Spring中比较原始的Factory。如XMLBeanFactory就是⼀种典型的BeanFactory。原始的BeanFactory无法⽀持Spring的许多插件，如AOP功能、Web应用等。ApplicationContext接口,它由BeanFactory接口派生而来，ApplicationContext包含BeanFactory的所有功能，通常建议比BeanFactory优先，ApplicationContext以⼀种更面向框架的方式工作以及对上下文进行分层和实现继承，ApplicationContext包还提供了以下的功能：MessageSource,提供国际化的消息访问\n资源访问，如URL和⽂件，事件传播，载入多个（有继承关系）上下文，使得每⼀个上下文都专注于⼀个特定的层次，比如应⽤的web层;\n\nBeanFactory提供的方法及其简单，仅提供了六种方法供客户调用：\n\n```java\n// 判断⼯⼚中是否包含给定名称的bean定义，若有则返回true\nboolean containsBean(String beanName)\n// 返回给定名称注册的bean实例。根据bean的配置情况，如果是singleton模式将返回⼀个共享实例，否则将返回⼀个新建的实例，如果没有找到指定bean,该⽅法可能会抛出异常\nObject getBean(String)\n// 返回以给定名称注册的bean实例，并转换为给定class类型\nObject getBean(String, Class)\n// 返回给定名称的bean的Class,如果没有找到指定的bean实例，则排除NoSuchBeanDefinitionException异常\nClass getType(String name)\n// 判断给定名称的bean定义是否为单例模式\nboolean isSingleton(String)\n// 返回给定bean名称的所有别名\nString[] getAliases(String name)\n```\n##### FactoryBean\n\n⼀般情况下，Spring通过反射机制利用<bean\\><bean\\>的class属性指定实现类实例化Bean，在某些情况下，实例化Bean过程比较复杂，如果按照传统的方式，则需要在<bean\\><bean\\>中提供大量的配置信息。配置⽅式的灵活性是受限的，这时采用编码的方式可能会得到⼀个简单的方案。Spring为此提供了⼀个org.springframework.bean.factory.FactoryBean的工厂类接口，用户可以通过实现该接⼝定制实例化Bean的逻辑。FactoryBean接口对于Spring框架来说占重要的地位，Spring自身就提供了70多个FactoryBean的实现。它们隐藏了实例化⼀些复杂Bean的细节，给上层应用带来了便利。从Spring3.0开始，FactoryBean开始⽀持泛型，即接口声明改为FactoryBean<T\\>的形式,以Bean结尾，表示它是⼀个Bean，不同于普通Bean的是：它是实现了FactoryBean<T\\>接口的Bean，根据该Bean的ID从BeanFactory中获取的实际上是FactoryBean的getObject()返回的对象，而不是FactoryBean本身，如果要获取FactoryBean对象，请在id前面加⼀个&符号来获取。例如自己实现⼀个FactoryBean，功能：用来代理⼀个对象，对该对象的所有方法做⼀个拦截，在调用前后都输出⼀行LOG，模仿ProxyFactoryBean的功能。FactoryBean是⼀个接口，当在IOC容器中的Bean实现了FactoryBean后，通过getBean(StringBeanName)获取到的Bean对象并不是FactoryBean的实现类对象，而是这个实现类中的getObject()方法返回的对象。要想获取FactoryBean的实现类，就要getBean(&BeanName)，在BeanName之前加上&。\n在该接口中还定义了以下3个⽅法：\n\n```java\n// 返回由FactoryBean创建的Bean实例，如果isSingleton()返回true，则该实例会放到Spring容器中单实例缓存池中；\nT getObject() throw Exception;\n// 返回由FactoryBean创建的Bean实例的作⽤域是singleton还是prototype；\nboolean isSingleton();\n// 返回FactoryBean创建的Bean类型。当配置⽂件中<bean>的class属性配置的实现类是FactoryBean时，通过getBean()⽅法返回的不是FactoryBean本身，⽽是FactoryBean#getObject()⽅法所返回的对象，相当于FactoryBean#getObject()代理了getBean()⽅法。\nClass<?> getObjectType();\n```\n**总结**\nBeanFactory是个Factory，也就是IOC容器或对象工厂，FactoryBean是个Bean。在Spring中，所有的Bean都是由BeanFactory(也就是IOC容器)来进行管理的。但对FactoryBean而言，这个Bean不是简单的Bean，而是⼀个能生产或者修饰对象生成的工厂Bean,它的实现与设计模式中的工厂模式和修饰器模式类似\n\n> [Spring中BeanFactory和FactoryBean有何区别？](https://mp.weixin.qq.com/s/r3rnVhU8vr58Cw__UWOVLA)\n> [FactoryBean和它的兄弟SmartFactoryBean！](https://mp.weixin.qq.com/s/zVtedq-kwlhqeQB7GZz6Qw)\n\n\n## Bean的调用\n\n```java\n// 1、使用BeanWrapper\nHelloWorld hw = new HelloWorld();\nBeanWrapper bw = new BeanWrapperImpl(hw);\nbw.setPropertyvalue(\"msg\",\"HelloWorld\");\nSystem.out.println(bw.getPropertyCalue(\"msg\"));\n// 2、使用BeanFactory\nInputStream is = new FileInputStream(\"config.xml\");\nXmlBeanFactory factory = new XmlBeanFactory(is);\nHelloWorld hw = (HelloWorld) factory.getBean(\"HelloWorld\");\nSystem.out.println(hw.getMsg());\n// 3、使用ApplicationContext\nApplicationContext actx = new FleSystemXmlApplicationContext(\"config.xml\");\nHelloWorld hw = (HelloWorld) actx.getBean(\"HelloWorld\");\nSystem.out.println(hw.getMsg());\n```\n> [Spring中获取Bean的八种方式](https://mp.weixin.qq.com/s/BW3khRkQwjBsXw7yJhCyXQ)\n\n\n## 相关文章\n\n- [11张流程图帮你搞定Spring Bean生命周期](https://mp.weixin.qq.com/s/I8tsf7cFXkHX1pUp7SPByw)\n- [面试官：说说Spring Bean的实例化过程？面试必问的！](https://mp.weixin.qq.com/s/5hAt9_KyyqHy7zzOjZ9LyQ)\n- [你知道Spring lazy-init懒加载的原理吗？](https://mp.weixin.qq.com/s/_je69-0J72X5YMCrS-92MQ)\n- [如何自己实现一个简单的Spring Bean容器](https://mp.weixin.qq.com/s/brlEwyKhwhSkljHLL1zmBA)\n- [实力总结四类Bean注入Spring的方式](https://mp.weixin.qq.com/s/AuTnuxIQDPFbuslDz9ffVg)\n- [最全的Spring依赖注入方式，你都会了吗？](https://mp.weixin.qq.com/s/TIDKofzCPz6qg2vj16JRMA)\n- [关于Spring注入方式的几道面试题，你能答上么](https://mp.weixin.qq.com/s?__biz=Mzg2MDYzODI5Nw==&amp;mid=2247494432&amp;idx=1&amp;sn=3acc7e7bf31c6d1f56ad830d6eb1ec41&amp;source=41#wechat_redirect)\n- [最全的Spring依赖注入方式，你都会了吗？](https://mp.weixin.qq.com/s/u1DcCsRrrHYFOVykwW4Dcg)\n- [Spring官方为什么建议构造器注入？](https://mp.weixin.qq.com/s/fVV6dYh0DQOoDiXwLR5miw)\n- [Bean放入Spring容器，你知道几种方式？](https://mp.weixin.qq.com/s/g9iRu1slTMx0dwYJiy2m7w)\n- [Spring注入Bean的7种方式，还有谁不会？？](https://mp.weixin.qq.com/s/i0Y-p7mda5FJCWCMJ8msdg)\n- [Spring注解@Bean和@Component的区别,你知道吗？](https://mp.weixin.qq.com/s/6CwABJAePAT6hzTmfk7Jjg)\n- [@Bean与@Component用在同一个类上，会怎么样？](https://mp.weixin.qq.com/s/lyH72PRAGcR2-aQvMZ1jPA)\n- [Bean异步初始化，让你的应用启动飞起来](https://mp.weixin.qq.com/s/aZCgJS3Uaj28UiKTtUFcmw)\n- [Spring中的父子容器是咋回事？](https://mp.weixin.qq.com/s/06Mmgnhhu98lQtQ8X13QBA)\n- [Spring容器原始Bean是如何创建的？](https://mp.weixin.qq.com/s/jB9Vzt-uAj6njg2ADVFmyw)","categories":["Spring"]},{"title":"阿里巴巴Java开发手册(泰山版)","slug":"阿里巴巴Java开发手册(泰山版)","url":"/blog/posts/d791cb4f9835/","content":"\n## 一、编程规约\n\n### (一) 命名风格\n\n1. **【强制】** 代码中的命名均不能以**下划线或美元符号**开始，也不能以**下划线或美元符号**结束。\n反例：\\_name/\\_\\_name/$name/name\\_/name$/name\\_\\_\n\n2. **【强制】** 所有编程相关的命名严禁使用拼音与英文混合的方式，更不允许直接使用中文的方式。\n说明：正确的英文拼写和语法可以让阅读者易于理解，避免歧义。注意，纯拼音命名方式更要避免采用。\n正例:ali/alibaba/taobao/cainiao/aliyun/youku/hangzhou等国际通用的名称，可视同英文。\n反例：DaZhePromotion[打折]/ getPingfenByName()[评分]/ int 某变量=3\n\n3. **【强制】** 类名使用UpperCamelCase风格，但以下情形例外：DO/BO/DTO/VO/AO/PO/UID等。\n正例：ForceCode/UserDO/HtmlDTO/XmlService/TcpUdpDeal/TaPromotion\n反例：forcecode/UserDo/HTMLDto/XMLService/TCPUDPDeal/TAPromotion\n\n4. **【强制】** 方法名、参数名、成员变量、局部变量都统一使用lowerCamelCase风格。\n正例：localValue/getHttpMessage()/inputUserId\n\n5. **【强制】** 常量命名全部大写，单词间用下划线隔开，力求语义表达完整清楚，不要嫌名字长。\n正例：MAX_STOCK_COUNT/CACHE_EXPIRED_TIME\n反例：MAX_COUNT/EXPIRED_TIME\n\n6. **【强制】** 抽象类命名使用Abstract或Base开头；异常类命名使用Exception结尾；测试类命名以它要测试的类的名称开始，以Test结尾。\n\n7. **【强制】** 类型与中括号紧挨相连来表示数组。\n正例：定义整形数组int[] arrayDemo;\n反例：在main参数中，使用String args[]来定义。\n\n8. **【强制】** POJO类中的任何布尔类型的变量，都不要加is前缀，否则部分框架解析会引起序列化错误。\n说明：在本文MySQL规约中的建表约定第一条，表达是与否的值采用is_xxx的命名方式，所以，需要在<resultMap\\>设置从is_xxx到xxx的映射关系。\n反例：定义为基本数据类型Boolean isDeleted的属性，它的方法也是isDeleted。框架在反向解析的时候，\"误以为\"对应的属性名称是deleted，导致属性获取不到，进而抛出异常。\n\n9. **【强制】** 包名统一使用小写，点分隔符之间有且仅有一个自然语义的英语单词。包名统一使用**单数**形式，但是类名如果有复数含义，类名可以使用复数形式。\n正例：应用工具类包名为com.alibaba.ei.kunlun.aap.util、类名为MessageUtils(此规则参考spring的框架结构)\n\n10. **【强制】** 避免在子父类的成员变量之间、或者不同代码块的局部变量之间采用完全相同的命名，使可读性降低。\n说明：子类、父类成员变量名相同，即使是public类型的变量也是能够通过编译，而局部变量在同一方法内的不同代码块中同名也是合法的，但是要避免使用。对于非setter/getter的参数名称也要避免与成员变量名称相同。\n反例：\n```java\npublic class ConfusingName {\n\tpublic int stock;\n\t// 非 setter/getter的参数名称，不允许与本类成员变量同名\n\tpublic void get(String alibaba) {\n\t\tif (condition) {\n\t\t\tfinal int money = 666;\n\t\t\t// ...\n\t\t}\n\t\tfor (int i = 0; i < 10; i++) {\n            //在同一方法体中，不允许与其它代码块中的money命名相同\n            final int money = 15978;\n            // ...\n\t\t}\n\t}\n}\n\nclass Son extends ConfusingName {\n    //不允许与父类的成员变量名称相同\n    public int stock;\n}\n```\n\n11. **【强制】** 杜绝完全不规范的缩写，避免望文不知义。\n反例：AbstractClass\"缩写\"命名成AbsClass;condition\"缩写\"命名成condi，此类随意缩写严重降低了代码的可阅读性。\n\n12. 【推荐】为了达到代码自解释的目标，任何自定义编程元素在命名时，使用尽量完整的单词组合来表达。\n正例：在JDK中对某个对象引用的volatile字段进行原子更新的类名为AtomicReferenceFieldUpdater。\n反例：常见的方法内变量为```int a;```的定义方式。\n\n13. 【推荐】在常量与变量的命名时，表示类型的名词放在词尾，以提升辨识度。\n正例：startTime/workQueue/nameList/TERMINATED_THREAD_COUNT\n反例：startedAt/QueueOfWork/listName/COUNT_TERMINATED_THREAD\n\n14. 【推荐】如果模块、接口、类、方法使用了设计模式，在命名时需体现出具体模式。\n说明：将设计模式体现在名字中，有利于阅读者快速理解架构设计理念。\n正例：\n```java\npublic class OrderFactory;\npublic class LoginProxy;\npublic class ResourceObserver;\n```\n\n15. 【推荐】接口类中的方法和属性不要加任何修饰符号（public也不要加），保持代码的简洁性，并加上有效的Javadoc注释。尽量不要在接口里定义变量，如果一定要定义变量，确定与接口方法相关，并且是整个应用的基础常量。\n正例：接口方法签名```void commit();```\n接口基础常量```String COMPANY = \"alibaba\";```\n反例：接口方法定义```public abstract void f();```\n说明：JDK8中接口允许有默认实现，那么这个default方法，是对所有实现类都有价值的默认实现。\n\n16. 接口和实现类的命名有两套规则：\n1)**【强制】** 对于Service和DAO类，基于SOA的理念，暴露出来的服务一定是接口，内部的实现类用Impl的后缀与接口区别。\n正例：CacheServiceImpl实现CacheService接口。\n2)【推荐】如果是形容能力的接口名称，取对应的形容词为接口名（通常是-able的形容词）。正例：AbstractTranslator实现Translatable接口。\n\n17. 【参考】枚举类名带上Enum后缀，枚举成员名称需要全大写，单词间用下划线隔开。\n说明：枚举其实就是特殊的常量类，且构造方法被默认强制是私有。\n正例：枚举名字为ProcessStatusEnum的成员名称：SUCCESS/UNKNOWN_REASON。\n\n18. 【参考】各层命名规约：\nA)Service/DAO层方法命名规约\n1)获取单个对象的方法用get做前缀。\n2)获取多个对象的方法用list做前缀，复数结尾，如：listObjects。\n3)获取统计值的方法用count做前缀。\n4)插入的方法用save/insert做前缀。\n5)删除的方法用remove/delete做前缀。\n6)修改的方法用update做前缀。\nB)领域模型命名规约\n1)数据对象：xxxDO，xxx即为数据表名。\n2)数据传输对象：xxxDTO，xxx为业务领域相关的名称。\n3)展示对象：xxxVO,xxx一般为网页名称。\n4)POJO是DO/DTO/BO/VO的统称，禁止命名成xxxPOJO。\n\n### (二) 常量定义\n\n1. **【强制】** 不允许任何魔法值(即未经预先定义的常量)直接出现在代码中。\n反例：\n```java\n//本例中同学A定义了缓存的key，然后缓存提取的同学B使用了Id#taobao来提取，少了下划线，导致故障。\nString key = \"Id#taobao_\" + tradeId\ncache.put(key, value);\n```\n\n2. **【强制】** 在long或者Long赋值时，数值后使用大写的L，不能是小写的l,小写容易跟数字1混淆，造成误解。\n说明：Long a = 2l;写的是数字的21，还是Long型的2。\n\n3. 【推荐】不要使用一个常量类维护所有常量，要按常量功能进行归类，分开维护。\n说明：大而全的常量类，杂乱无章，使用查找功能才能定位到修改的常量，不利于理解，也不利于维护。\n正例：缓存相关常量放在类CacheConsts下；系统配置相关常量放在类ConfigConsts下。\n\n4. 【推荐】常量的复用层次有五层：跨应用共享常量、应用内共享常量、子工程内共享常量、包内共享常量、类内共享常量。\n1)跨应用共享常量：放置在二方库中，通常是client.jar中的constant目录下。\n2)应用内共享常量：放置在一方库中，通常是子模块中的constant目录下。\n反例：易懂变量也要统一定义成应用内共享常量，两位工程师在两个类中分别定义了\"YES”的变量：\n类A中：public static final String YES = \"yes\";\n类B中：public static final String YES = \"y\";\nA.YES.equals(B.YES)，预期是true，但实际返回为false，导致线上问题。\n3)子工程内部共享常量：即在当前子工程的constant目录下。\n4)包内共享常量：即在当前包下单独的constant目录下。\n5)类内共享常量：直接在类内部private static final定义。\n\n5. 【推荐】如果变量值仅在一个固定范围内变化用enum类型来定义。\n说明：如果存在名称之外的延伸属性应使用enum类型，下面正例中的数字就是延伸信息，表示一年中的第几个季节。\n正例：\n```java\npublic enum SeasonEnum {\n\tSPRING(1), SUMMER(2), AUTUMN(3), WINTER ⑷；\n\tprivate int seq;\n\tSeasonEnum(int seq) {\n\t\tthis.seq = seq;\n\t}\n\tpublic int getSeqO {\n\t\treturn seq\n\t}\n}\n```\n\n### (三) 代码格式\n\n1. **【强制】** 如果是大括号内为空，则简洁地写成{}即可，大括号中间无需换行和空格；如果是非空代码块则：\n1)左大括号前不换行。\n2)左大括号后换行。\n3)右大括号前换行。\n4)右大括号后还有else等代码则不换行；表示终止的右大括号后必须换行。\n\n2. **【强制】** 左小括号和右边相邻字符之间不出现空格；右小括号和左边相邻字符之间也不出现空格；而左大括号前需要加空格。详见第5条下方正例提示。\n反例：if (空格a == b空格)\n\n3. **【强制】** if/for/while/switch/do等保留字与括号之间都必须加空格。\n\n4. **【强制】** 任何二目、三目运算符的左右两边都需要加一个空格。\n说明：包括赋值运算符=、逻辑运算符&&、加减乘除符号等。\n\n5. **【强制】** 采用4个空格缩进，禁止使用tab字符。\n说明：如果使用tab缩进，必须设置1个tab为4个空格。IDEA设置tab为4个空格时，请勿勾选Use tab character;而在eclipse中，必须勾选insert spaces for tabs\n正例：(涉及1-5点)\n```java\npublic static void main(String[] args) {\n    //缩进4个空格\n    String say = \"hello\";\n    //运算符的左右必须有一个空格\n    int flag = 0;\n    //关键词if与括号之间必须有一个空格，括号内的f与左括号，0与右括号不需要空格\n    if (flag == 0) {\n    \tSystem.out. println(say);\n    }\n    //左大括号前加空格且不换行；左大括号后换行\n    if (flag == 1) {\n    \tSystem.out. println(\"world\");\n    //右大括号前换行，右大括号后有else，不用换行\n    } else {\n    \tSystem.out.println(\"ok\");\n    //在右大括号后直接结束，则必须换行\n    }\n}\n```\n\n6. **【强制】** 注释的双斜线与注释内容之间有且仅有一个空格。\n正例：\n```java\n// 这是示例注释，请注意在双斜线之后有一个空格\nString commentstring = new StringO;\n```\n\n7. **【强制】** 在进行类型强制转换时，右括号与强制转换值之间不需要任何空格隔开。\n正例：\n```java\nlong first = 1000000000000L;\nint second = (int)first + 2;\n```\n\n8. **【强制】** 单行字符数限制不超过120个，超出需要换行，换行时遵循如下原则：\n1)第二行相对第一行缩进4个空格，从第三行开始，不再继续缩进，参考示例。\n2)运算符与下文一起换行。\n3)方法调用的点符号与下文一起换行。\n4)方法调用中的多个参数需要换行时，在逗号后进行。\n5)在括号前不要换行，见反例\n正例：\n```java\nStringBuilder sb = new StringBuilder;\n//超过120个字符的情况下，换行缩进4个空格，并且方法前的点号一起换行sb.append(\"zi\").append(\"xin\")...\n.append(\"huang\")...\n.append(\"huang\")...\n.append(\"huang\");\n```\n反例：\n```java\nStringBuilder sb = new StringBuilder;\n//超过120个字符的情况下，不要在括号前换行\nsb.append(\"you\").append(\"are\")...append\n(\"lucky\");\n//参数很多的方法调用可能超过120个字符，逗号后才是换行处\nmethod(args1, args2, args3, ...\n,argsX);\n```\n\n9. **【强制】** 方法参数在定义和传入时，多个参数逗号后边必须加空格。\n正例：下例中实参的**args1**，后边必须要有一个空格。\n```java\nmethod(args1, args2, args3);\n```\n\n10. **【强制】** IDE的text file encoding设置为UTF-8;IDE中文件的换行符使用Unix格式，不要使用Windows格式。\n\n11. 【推荐】单个方法的总行数不超过80行。\n说明：除注释之外的方法签名、左右大括号、方法内代码、空行、回车及任何不可见字符的总行数不超过80行。\n正例：代码逻辑分清红花和绿叶，个性和共性，绿叶逻辑单独出来成为额外方法，使主干代码更加清晰；共性逻辑抽取成为共性方法，便于复用和维护。\n\n12. 【推荐】没有必要增加若干空格来使变量的赋值等号与上一行对应位置的等号对齐。\n正例：\n```java\nint one = 1;\nlong two = 2L;\nfloat three = 3F;\nStringBuilder sb = new StringBuilder();\n```\n说明：增加sb这个变量，如果需要对齐，则给one、two、three都要增加几个空格，在变量比较多的情况下，是非常累赘的事情。\n\n13. 【推荐】不同逻辑、不同语义、不同业务的代码之间插入一个空行分隔开来以提升可读性。\n说明：任何情形，没有必要插入**多个空行**进行隔开。\n\n### (四) OOP规约\n\n1. **【强制】** 避免通过一个类的对象引用访问此类的静态变量或静态方法，无谓增加编译器解析成本，直接用**类名**来访问即可。\n\n2. **【强制】** 所有的覆写方法，必须加@Override注解。\n说明：getObject()与get0bject()的问题。一个是字母的O,一个是数字的0,加@Override可以准确判断是否覆盖成功。另外，如果在抽象类中对方法签名进行修改，其实现类会马上编译报错。\n\n3. **【强制】** 相同参数类型，相同业务含义，才可以使用Java的可变参数，避免使用Object。\n说明：可变参数必须放置在参数列表的最后。(提倡同学们尽量不用可变参数编程)\n正例：\n```java\npublic List<User> listUsers(String type, Long... ids) {...}\n```\n\n4. **【强制】** 外部正在调用或者二方库依赖的接口，不允许修改方法签名，避免对接口调用方产生影响。接口过时必须加@Deprecated注解，并清晰地说明采用的新接口或者新服务是什么。\n\n5. **【强制】** 不能使用过时的类或方法。\n说明：java.net.URLDecoder中的方法decode(String encodeStr)这个方法已经过时，应该使用双参数decode(String source, String encode)。接口提供方既然明确是过时接口，那么有义务同时提供新的接口；作为调用方来说，有义务去考证过时方法的新实现是什么。\n\n6. **【强制】** Object的equals方法容易抛空指针异常,应使用常量或确定有值的对象来调用equals。\n正例:\"test\".equals(object);\n反例:object.equals(\"test\");\n说明：推荐使用java.util.Objects#equals(JDK7引入的工具类)。\n\n7. **【强制】** 所有整型包装类对象之间**值的比较**，全部使用equals方法比较。\n说明：对于Integer var = ?在 **-128至127** 之间的赋值，Integer对象是在IntegerCache.cache产生，会复用已有对象，这个区间内的Integer值可以直接使用==进行判断，但是这个区间之外的所有数据，都会在堆上产生，并不会复用已有对象，这是一个大坑，推荐使用equals方法进行判断。\n\n8. **【强制】** 任何货币金额，均以最小货币单位且整型类型来进行存储。\n\n9. **【强制】** 浮点数之间的等值判断，基本数据类型不能用==来比较，包装数据类型不能用equals来判断。\n说明：浮点数采用\"尾数+阶码”的编码方式，类似于科学计数法的\"有效数字+指数”的表示方式。二进制无法精确表示大部分的十进制小数，具体原理参考《码出高效》\n反例：\n```java\nfloat a = 1.0f - 0.9f;\nfloat b = 0.9f - 0.8f;\nif (a == b) {\n\t//预期进入此代码快，执行其它业务逻辑\n\t//但事实上a==b的结果为false\n}\nFloat x = Float.valueOf(a);\nFloat y = Float.valueOf(b);\nif (x.equals(y)) {\n\t//预期进入此代码快，执行其它业务逻辑\n\t//但事实上equals的结果为false\n}\n```\n正例：\n(1)指定一个误差范围，两个浮点数的差值在此范围之内，则认为是相等的。\n```java\nfloat a = 1.0f - 0.9f;\nfloat b = 0.9f - 0.8f;\nfloat diff = 1e-6f;\nif (Math.abs(a - b) < diff) {\n\tSystem.out.println(\"true\");\n}\n```\n(2)使用BigDecimal来定义值，再进行浮点数的运算操作。\n```java\nBigDecimal a = new BigDecimal(\"1.0\");\nBigDecimal b = new BigDecimal(\"0.9\");\nBigDecimal c = new BigDecimal(\"0.8\");\nBigDecimal x = a.subtract(b);\nBigDecimal y = b.subtract(c);\nif (x.equals(y)) {\n\tSystem.out.println(\"true\");\n}\n```\n\n10. **【强制】** 定义数据对象DO类时，属性类型要与数据库字段类型相匹配。\n正例：数据库字段的bigint必须与类属性的Long类型相对应。\n反例：某个案例的数据库表id字段定义类型bigint unsigned，实际类对象属性为Integer，随着id越来越大，超过Integer的表示范围而溢出成为负数。\n\n11. **【强制】** 禁止使用构造方法BigDecimal(double)的方式把double值转化为BigDecimal对象\n说明：BigDecimal(double)存在精度损失风险，在精确计算或值比较的场景中可能会导致业务逻辑异常。如：BigDecimal g = new BigDecimal(0.1f);实际的存储值为:0.10000000149\n正例：优先推荐入参为String的构造方法，或使用BigDecimal的valueOf方法，此方法内部其实执行了Double的toString，而Double的toString按double的实际能表达的精度对尾数进行了截断。\n```java\nBigDecimal recommend1 = new BigDecimal(\"0.1\");\nBigDecimal recommend2 = BigDecimal.valueOf(0.1);\n```\n\n12. 关于基本数据类型与包装数据类型的使用标准如下：\n1)**【强制】** 所有的POJO类属性必须使用包装数据类型。\n2)**【强制】** RPC方法的返回值和参数必须使用包装数据类型。\n3)**【推荐】** 所有的局部变量使用基本数据类型。\n说明：POJO类属性没有初值是提醒使用者在需要使用时，必须自己显式地进行赋值，任何NPE问题，或者入库检查，都由使用者来保证。\n正例：数据库的查询结果可能是null，因为自动拆箱，用基本数据类型接收有NPE风险。\n反例：某业务的交易报表上显示成交总额涨跌情况，即正负x%，x为基本数据类型，调用的RPC服务，调用不成功时，返回的是默认值，页面显示为0%，这是不合理的，应该显示成中划线-。所以包装数据类型的null值，能够表示额外的信息，如：远程调用失败，异常退出。\n\n13. **【强制】** 定义DO/DTO/VO等POJO类时，不要设定任何属性**默认值**\n反例：POJO类的createTime默认值为new Date()，但是这个属性在数据提取时并没有置入具体值，在更新其它字段时又附带更新了此字段，导致创建时间被修改成当前时间。\n\n14. **【强制】** 序列化类新增属性时，请不要修改serialVersionUID字段，避免反序列失败；如果完全不兼容升级，避免反序列化混乱，那么请修改serialVersionUID值。\n说明：注意serialVersionUID不一致会抛出序列化运行时异常。\n\n15. **【强制】** 构造方法里面禁止加入任何业务逻辑，如果有初始化逻辑，请放在init方法中。\n\n16. **【强制】** POJO类必须写toString方法。使用IDE中的工具：source>generate toString时，如果继承了另一个POJO类，注意在前面加一下super.toString。\n说明：在方法执行抛出异常时，可以直接调用POJO的toString()方法打印其属性值，便于排查问题。\n\n17. **【强制】** 禁止在POJO类中，同时存在对应属性xxx的isXxx()和getXxx()方法。\n说明：框架在调用属性xxx的提取方法时，并不能确定哪个方法一定是被优先调用到，神坑之一。\n\n18. 【推荐】使用索引访问用String的split方法得到的数组时，需做最后一个分隔符后有无内容的检查，否则会有抛IndexOutOfBoundsException的风险。\n说明：\n```java\nString str = \"a,b,c„\";\nString[] ary = str.split(\",\");\n//预期大于3,结果是3\nSystem. out.println(ary.length);\n```\n19. 【推荐】当一个类有多个构造方法，或者多个同名方法，这些方法应该按顺序放置在一起，便于阅读，此条规则优先于下一条。\n\n20. 【推荐】类内方法定义的顺序依次是：公有方法或保护方法>私有方法>getter/setter方法。\n说明：公有方法是类的调用者和维护者最关心的方法，首屏展示最好；保护方法虽然只是子类关心，也可能是\"模板设计模式\"下的核心方法；而私有方法外部一般不需要特别关心，是一个黑盒实现；因为承载的信息价值较低，所有Service和DAO的getter/setter方法放在类体最后。\n\n21. 【推荐】setter方法中，参数名称与类成员变量名称一致，this.成员名=参数名。在getter/setter方法中，不要增加业务逻辑，增加排查问题的难度。\n反例：\n```java\npublic Integer getData 0 {\n\tif (condition) {\n\t\treturn this.data + 100;\n\t} else {\n\t\treturn this.data - 100;\n\t}\n}\n```\n\n22. 【推荐】循环体内，字符串的连接方式，使用StringBuilder的append方法进行扩展。\n说明：下例中反编译出的字节码文件显示每次循环都会new出一个StringBuilder对象然后进行append操作，最后通过toString方法返回String对象，造成内存资源浪费。\n反例：\n```java\nString str = \"start\";\nfor (int i = 0; i < 100; i++) {\n\tstr = str + \"hello\";\n}\n```\n\n23. 【推荐】final可以声明类、成员变量、方法、以及本地变量，下列情况使用final关键字：\n1)不允许被继承的类，如：String类。\n2)不允许修改引用的域对象，如：POJO类的域变量。\n3)不允许被覆写的方法，如：POJO类的setter方法。\n4)不允许运行过程中重新赋值的局部变量。\n5)避免上下文重复使用一个变量，使用final可以强制重新定义一个变量，方便更好地进行重构。\n\n24. 【推荐】慎用Object的clone方法来拷贝对象。\n说明：对象clone方法默认是浅拷贝，若想实现深拷贝需覆写clone方法实现域对象的深度遍历式拷贝。\n\n25. 【推荐】类成员与方法访问控制从严：\n1)如果不允许外部直接通过new来创建对象，那么构造方法必须是private。\n2)工具类不允许有public或default构造方法。\n3)类非static成员变量并且与子类共享，必须是protectedo\n4)类非static成员变量并且仅在本类使用，必须是private。\n5)类static成员变量如果仅在本类使用，必须是private。\n6)若是static成员变量，考虑是否为final。\n7)类成员方法只供类内部调用，必须是private,\n8)类成员方法只对继承类公开，那么限制为protected,\n说明：任何类、方法、参数、变量，严控访问范围。过于宽泛的访问范围，不利于模块解耦。思考：如果是一个private的方法，想删除就删除，可是一个public的service成员方法或成员变量，删除一下，不得手心冒点汗吗？变量像自己的小孩，尽量在自己的视线内，变量作用域太大，无限制的到处跑，那么你会担心的。\n\n### (五) 日期时间\n\n1. **【强制】** 日期格式化时，传入pattern中表示年份统一使用小写的y。\n说明：日期格式化时，yyyy表示当天所在的年，而大写的YYYY代表是week in which year(JDK7之后引入的概念)，意思是当天所在的周属于的年份，一周从周日开始，周六结束，只要本周跨年，返回的YYYY就是下一年。\n正例：表示日期和时间的格式如下所示：\n```java\nnew SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\")\n```\n\n2. **【强制】** 在日期格式中分清楚大写的M和小写的m,大写的H和小写的h分别指代的意义。\n说明：日期格式中的这两对字母表意如下：\n1)表示月份是大写的M；\n2)表示分钟则是小写的m；\n3)24小时制的是大写的H；\n4)12小时制的则是小写的h。\n\n3. **【强制】** 获取当前毫秒数：System.currentTimeMillis();而不是new Date().getTime()。\n说明：如果想获取更加精确的纳秒级时间值，使用System.nanoTime的方式。在JDK8中，针对统计时间等场景，推荐使用Instant类。\n\n4. **【强制】** 不允许在程序任何地方中使用：1)java.sql.Date 2)java.sql.Time 3)java.sql.Timestamp。\n说明：第1个不记录时间，getHours()抛出异常；第2个不记录日期，getYear()抛出异常；第3个在构造方法super((time\\/1000)*1000),fastTime和nanos分开存储秒和纳秒信息。\n反例：java.util.Date.after(Date)进行时间比较时，当入参是java.sql.Timestamp时，会触发JDK BUG(JDK9已修复)，可能导致比较时的意外结果。\n\n5. **【强制】** 不要在程序中写死一年为365天，避免在公历闰年时出现日期转换错误或程序逻辑错误。\n正例：\n```java\n//获取今年的天数\nint daysOfThisYear = LocalDate.now().lengthOfYear();\n//获取指定某年的天数\nLocalDate.of(2011, 1, 1).lengthOfYear();\n```\n反例：\n```java\n//第一种情况：在闰年366天时，出现数组越界异常\nint[] dayArray = new int[365];\n//第二种情况：一年有效期的会员制，今年1月26日注册，硬编码365返回的却是1月25日\nCalendar calendar = Calendar.getlnstance;\ncalendar.set(2020, 1, 26);\ncalendar.add(Calendar.DATE, 365);\n```\n\n6. 【推荐】避免公历闰年2月问题。闰年的2月份有29天，一年后的那一天不可能是2月29日。\n\n7. 【推荐】使用枚举值来指代月份。如果使用数字，注意Date，Calendar等日期相关类的月份month取值在0-11之间。\n说明：参考JDK原生注释，Month value is 0-based. e.g., 0 for January.\n正例：CalendarJANUARY，Calendar.FEBRUARY，Calendar.MARCH等来指代相应月份来进行传参或比较。\n\n### (六) 集合处理\n\n1. **【强制】** 关于hashCode和equals的处理，遵循如下规则：\n1)只要重写equals，就必须重写hashCode。\n2)因为Set存储的是不重复的对象，依据hashCode和equals进行判断，所以Set存储的对象必须重写这两个方法。\n3)如果自定义对象作为Map的键，那么必须覆写hashCode和equals。\n说明：String因为重写了hashCode和equals方法，所以我们可以愉快地使用String对象作为key来使用。\n\n2. **【强制】** 判断所有集合内部的元素是否为空，使用isEmpty()方法，而不是size()==0的方式。\n说明：前者的时间复杂度为O(1),而且可读性更好。\n正例：\n```java\nMap<String, Object> map = new HashMap<>();\nif(map.isEmpty()) {\n\tSystem.out.println(\"no element in this map.\");\n}\n```\n\n3. **【强制】** 在使用java.util.stream.Collectors类的toMap()方法转为Map集合时，一定要使用含有参数类型为BinaryOperator,参数名为mergeFunction的方法，否则当出现相同key值时会抛出IllegalStateException异常。\n说明：参数mergeFunction的作用是当出现key重复时，自定义对value的处理策略。\n正例：\n```java\nList<Pair<String, Double>> pairArrayList = new ArrayLists>(3);\npairArrayList.add(new Pair<>(\"version\", 6.19));\npairArrayList.add(new Pair<>(\"version\", 10.24));\npairArrayList.add(new Pair<>(\"version\", 13.14));\n\nMap<String, Double> map = pairArrayList.stream().collect(\n//生成的map集合中只有一个键值对：{version=13.14}\nCollectors.toMap(Pair::getKey, Pair::getValue, (v1, v2) -> v2));\n```\n反例：\n```java\nString[] departments = new String[]{\"iERP\", \"iERP\", \"EIBU\"};\n// 抛出IllegalStateException异常\nMap<Integer, String> map = Arrays.stream(departments) .collect(Collectors.toMap(String::hashCode, str -> str));\n```\n\n4. **【强制】** 在使用java.util.stream.Collectors类的toMap()方法转为Map集合时，一定要注意当value为null时会抛NPE异常。\n说明：在java.util.HashMap的merge方法里会进行如下的判断：\n```java\nif (value == null || remappingFunction == null)\n\tthrow new NullPointerException();\n```\n反例：\n```java\nList<Pair<String, Double>> pairArrayList = new ArrayList<>(2);\npairArrayList.add(new Pair<>(\"version1\", 4.22));\npairArrayList.add(new Pair<>(\"version2\", null));\nMap<String, Double> map = pairArrayList.stream().collect(\n// 抛出NullPointerException异常\nCollectors.toMap(Pair::getKey, Pair::getValue, (v1, v2) -> v2));\n```\n\n5. **【强制】** ArrayList的subList结果不可强转成ArrayList，否则会抛出ClassCastException异常：java.util.RandomAccessSubList cannot be cast to java.util.ArrayList。\n说明：subList返回的是ArrayList的内部类SubList，并不是ArrayList而是ArrayList的一个视图，对于SubList子列表的所有操作最终会反映到原列表上。\n\n6. **【强制】** 使用Map的方法keySet()/values()/entrySet()返回集合对象时，不可以对其进行添加元素操作，否则会抛出UnsupportedOperationException异常。\n\n7. **【强制】** Collections类返回的对象，如：emptyList()/singietonList()等都是immutable list，不可对其进行添加或者删除元素的操作。\n反例：如果查询无结果，返回Collections.emptyList()空集合对象，调用方一旦进行了添加元素的操作，就会触发UnsupportedOperationException异常。\n\n8. **【强制】** 在subList场景中，**高度注意**对父集合元素的增加或删除，均会导致子列表的遍历、增加、删除产生ConcurrentModificationException异常。\n\n9. **【强制】** 使用集合转数组的方法，必须使用集合的toArray(T[] array)，传入的是类型完全一致、长度为0的空数组。\n反例：直接使用toArray无参方法存在问题，此方法返回值只能是Object[]类，若强转其它类型数组将出现ClassCastException错误。\n正例：\n```java\nList<String> list = new ArrayList<>(2);\nlist.add(\"guan\");\nlist.add(\"bao\");\nString[] array = list.toArray(new String[0]);\n```\n说明：使用toArray带参方法，数组空间大小的length，\n1)**等于0**,动态创建与size相同的数组，性能最好。\n2)**大于0但小于size**，重新创建大小等于size的数组，增加GC负担。\n3)**等于size**，在高并发情况下，数组创建完成之后，size正在变大的情况下，负面影响与2相同。\n4)**大于size**，空间浪费，且在size处插入null值，存在NPE隐患。\n\n10. **【强制】** 在使用Collection接口任何实现类的addAll()方法时，都要对输入的集合参数进行NPE判断。\n说明：在ArrayList#addAII方法的第一行代码即Object[] a = c.toArray();其中c为输入集合参数，如果为null，则直接抛出异常。\n\n11. **【强制】** 使用工具类Arrays.asList()把数组转换成集合时，不能使用其修改集合相关的方法，它的add/remove/clear方法会抛出UnsupportedOperationException异常。\n说明：asList的返回对象是一个Arrays内部类，并没有实现集合的修改方法。Arrays.asList体现的是适配器模式，只是转换接口，后台的数据仍是数组。\n```java\nString[] str = new String[] { \"yang\", \"hao\" };\nList list = Arrays.asList(str);\n```\n第一种情况：list.add(\"yangguanba\")；运行时异常。\n第二种情况：str[0] = \"changed\";也会随之修改，反之亦然。\n\n12. **【强制】** 泛型通配符< ? extends T \\>来接收返回的数据，此写法的泛型集合不能使用add方法，而< ? super T \\>不能使用get方法，两者在接口调用赋值的场景中容易出错。\n说明：扩展说一下PECS(Producer Extends Consumer Super)原则：第一、频繁往外读取内容的，适合用< ? extends T \\>。第二、经常往里插入的，适合用< ? super T \\>\n\n13. **【强制】** 在无泛型限制定义的集合赋值给泛型限制的集合时，在使用集合元素时，需要进行instanceof判断，避免抛出ClassCastException异常。\n说明：毕竟泛型是在JDK5后才出现，考虑到向前兼容，编译器是允许非泛型集合与泛型集合互相赋值。\n反例：\n```java\nList<String> generics = null;\nList notGenerics = new ArrayList(10);\nnotGenerics.add(new Object());\nnotGenerics.add(new Integer(1));\ngenerics = notGenerics;\n// 此处抛出ClassCastException异常\nString string = generics.get(0);\n```\n\n14. **【强制】** 不要在foreach循环里进行元素的remove/add操作。remove元素请使用Iterator方式，如果并发操作，需要对Iterator对象加锁。\n正例：\n```java\nList<String> list = new ArrayList<>();\nlist.add(\"1\");\nlist.add(\"2\");\nIterator<String> iterator = list.iterator();\nwhile (iterator.hasNext()){\n\tString item = iterator.next();\n\tif (删除元素的条件){\n\t\titerator.remove();\n\t}\n}\n```\n反例：\n```java\nfor (String item : list) {\n    if (\"1\".equals(item)) {\n        list.remove(item);\n    }\n}\n```\n说明：以上代码的执行结果肯定会出乎大家的意料，那么试一下把\"1\"换成\"2\"，会是同样的结果吗？\n\n15. **【强制】** 在JDK7版本及以上，Comparator实现类要满足如下三个条件，不然Arrays.sort，Collections.sort会抛IllegalArgumentException异常。\n说明：三个条件如下\n1)x，y的比较结果和y，x的比较结果相反。\n2)x>y,y>z,则x>z。\n3)x=y，则x，z比较结果和y，z比较结果相同。\n反例：下例中没有处理相等的情况，交换两个对象判断结果并不互反，不符合第一个条件，在实际使用中可能会出现异常。\n```java\nnew Comparator<Student>() {\n    @Override\n    public int compare(Student o1, Student o2) {\n    \treturn o1.getld() > o2.getld()? 1 : -1;\n    }\n};\n```\n\n16. 【推荐】集合泛型定义时，在JDK7及以上，使用diamond语法或全省略。\n说明：菱形泛型，即diamond，直接使用<>来指代前边已经指定的类型。\n正例：\n```java\n// diamond方式,即<>\nHashMap<String, String> userCache = new HashMap<>(16);\n//全省略方式\nArrayList<User> users = new ArrayList(10);\n```\n\n17. 【推荐】集合初始化时，指定集合初始值大小。\n说明：HashMap使用HashMap(int initialcapacity)初始化，如果暂时无法确定集合大小，那么指定默认值(16)即可。\n正例：initialcapacity=(需要存储的元素个数/负载因子)+1。注意负载因子(即loader factor)默认为0.75，如果暂时无法确定初始值大小，请设置为16(即默认值)。\n反例：HashMap需要放置1024个元素，由于没有设置容量初始大小，随着元素不断增加，容量7次被迫扩大，resize需要重建hash表。当放置的集合元素个数达千万级别时，不断扩容会严重影响性能。\n\n18. 【推荐】使用entrySet遍历Map类集合KV，而不是keySet方式进行遍历。\n说明：keySet其实是遍历了2次，一次是转为Iterator对象，另一次是从hashMap中取出key所对应的value。而entrySet只是遍历了一次就把key和value都放到了entry中，效率更高。如果是JDK8，使用Map.forEach方法。\n正例：values()返回的是V值集合，是一个list集合对象；keySet()返回的是K值集合，是一个Set集合对象；entrySet()返回的是K-V值组合集合。\n\n19. 【推荐】高度注意Map类集合K/V能不能存储null值的情况，如下表格：\n\n| 集合类            | Key            | Value          | Super       | 说明                  |\n| ----------------- | -------------- | -------------- | ----------- | --------------------- |\n| Hashtable         | 不允许为null   | 不允许为null   | Dictionary  | 线呈安全              |\n| ConcurrentHashMap | 不允许为null   | 不允许为null   | AbstractMap | 锁分段技术(JDK8:CAS ) |\n| TreeMap           | 不允许为null   | **允许为null** | AbstractMap | 线呈不安全            |\n| HashM ap          | **允许为null** | **允许为null** | AbstractMap | 线呈不安全            |\n\n反例：由于HashMap的干扰，很多人认为ConcurrentHashMap是可以置入null值，而事实上，存储null值时会抛出NPE异常。\n\n20. 【参考】合理利用好集合的有序性(sort)和稳定性(order)，避免集合的无序性(unsort)和不稳定性(unorder)带来的负面影响。\n说明：有序性是指遍历的结果是按某种比较规则依次排列的。稳定性指集合每次遍历的元素次序是一定的。如：ArrayList是order/unsort;HashMap是unorder/unsort;TreeSet是order/sort。\n\n21. 【参考】利用Set元素唯一的特性，可以快速对一个集合进行去重操作，避免使用List的contains()进行遍历去重或者判断包含操作。\n\n### (七) 并发处理\n\n1. **【强制】** 获取单例对象需要保证线程安全，其中的方法也要保证线程安全。\n说明：资源驱动类、工具类、单例工厂类都需要注意。\n\n2. **【强制】** 创建线程或线程池时请指定有意义的线程名称，方便出错时回溯。\n正例：自定义线程工厂，并且根据外部特征进行分组，比如，来自同一机房的调用，把机房编号赋值给whatFeaturOfGroup\n```java\npublic class UserThreadFactory implements ThreadFactory {\n    private final String namePrefix;\n    private final AtomicInteger nextId = new Atomiclnteger(1);\n    //定义线程组名称，在jstack问题排查时，非常有帮助\n    UserThreadFactory(String whatFeaturOfGroup) {\n        namePrefix = \"From UserThreadFactory's \" + whatFeaturOfGroup + \"-Worker-\";\n    }\n\n    @Override\n    public Thread newThread(Runnable task) {\n        String name = namePrefix + nextld.getAndlncrement();\n        Thread thread = new Thread(null, task, name, 0, false);\n        System.out.println(thread.getName());\n        return thread;\n    }\n}\n```\n\n3. **【强制】** 线程资源必须通过线程池提供，不允许在应用中自行显式创建线程。\n说明：线程池的好处是减少在创建和销毁线程上所消耗的时间以及系统资源的开销，解决资源不足的问题。如果不使用线程池，有可能造成系统创建大量同类线程而导致消耗完内存或者\"过度切换”的问题。\n\n4. **【强制】** 线程池不允许使用Executors去创建，而是通过ThreadPoolExecutor的方式，这样的处理方式让写的同学更加明确线程池的运行规则，规避资源耗尽的风险。\n说明：Executors返回的线程池对象的弊端如下：\n1)FixedThreadPool和SingleThreadPool:\n允许的请求队列长度为lnteger.MAX_VALUE，可能会堆积大量的请求，从而导致OOM。\n2)CachedThreadPool:\n允许的创建线程数量为Integer.MAX_VALUE，可能会创建大量的线程，从而导致OOM。\n\n5. **【强制】** SimpleDateFormat是线程不安全的类，一般不要定义为static变量，如果定义为static，必须加锁，或者使用DateUtils工具类。\n正例：注意线程安全，使用DateUtils。亦推荐如下处理：\n```java\nprivate static final ThreadLocal<DateFormat> df = new ThreadLocal< DateFormat>() {\n    @Override\n    protected DateFormat initialValue(){\n        return new SimpleDateFormat(\"yyyy-MM-dd\");\n    }\n};\n```\n说明：如果是JDK8的应用，可以使用Instant代替Date,LocalDateTime代替Calendar,DateTimeFormatter代替SimpleDateFormat，官方给出的解释：simple beautiful strong immutable thread-safe。\n\n6. **【强制】** 必须回收自定义的ThreadLocal变量，尤其在线程池场景下，线程经常会被复用，如果不清理自定义的ThreadLocal变量，可能会影响后续业务逻辑和造成内存泄露等问题。尽量在代理中使用try-finally块进行回收。\n正例：\n```java\nobjectThreadLocal.set(userlnfo);\ntry {\n// ...\n} finally {\n\tobjectThreadLocal.remove();\n}\n```\n\n7. **【强制】** 高并发时，同步调用应该去考量锁的性能损耗。能用无锁数据结构，就不要用锁；能锁区块，就不要锁整个方法体；能用对象锁，就不要用类锁。\n说明：尽可能使加锁的代码块工作量尽可能的小，避免在锁代码块中调用RPC方法。\n\n8. **【强制】** 对多个资源、数据库表、对象同时加锁时，需要保持一致的加锁顺序，否则可能会造成死锁。\n说明：线程一需要对表A、B、C依次全部加锁后才可以进行更新操作，那么线程二的加锁顺序也必须是A、B、C，否则可能出现死锁。\n\n9. **【强制】** 在使用阻塞等待获取锁的方式中，必须在try代码块之外，并且在加锁方法与try代码块之间没有任何可能抛出异常的方法调用，避免加锁成功后，在finally中无法解锁。\n说明一：如果在lock方法与try代码块之间的方法调用抛出异常，那么无法解锁，造成其它线程无法成功获取锁。\n说明二：如果lock方法在try代码块之内，可能由于其它方法抛出异常，导致在finally代码块中，unlock对未加锁的对象解锁，它会调用AQS的tryRelease方法(取决于具体实现类)，抛出IllegalMonitorStateException异常。\n说明三：在Lock对象的lock方法实现中可能抛出unchecked异常，产生的后果与说明二相同。\n正例：\n```java\nLock lock = new XxxLock();\n// ...\nlock.lock();\ntry {\n\tdoSomething();\n\tdoOthers();\n} finally {\n\tlock.unlock();\n}\n```\n反例：\n```java\nLock lock = new XxxLock();\n// ...\ntry {\n    //如果此处抛出异常，则直接执行finally代码块\n    doSomething();\n    //无论加锁是否成功，finally代码块都会执行\n    lock.lock();\n\tdoOthers();\n} finally {\n\tlock.unlock();\n}\n```\n\n10. **【强制】** 在使用尝试机制来获取锁的方式中，进入业务代码块之前，必须先判断当前线程是否持有锁。锁的释放规则与锁的阻塞等待方式相同。\n说明：Lock对象的unlock方法在执行时，它会调用AQS的tryRelease方法(取决于具体实现类)，如果当前线程不持有锁，则抛出IllegalMonitorStateException异常。\n正例：\n```java\nLock lock = new XxxLock();\n// ...\nboolean isLocked = lock.tryLock();\nif (isLocked) {\n    try {\n        doSomething();\n        doOthers()\n    } finally {\n    \tlock.unlock();\n    }\n}\n```\n\n11. **【强制】** 并发修改同一记录时，避免更新丢失，需要加锁。要么在应用层加锁，要么在缓存加锁，要么在数据库层使用乐观锁，使用version作为更新依据。\n说明：如果每次访问冲突概率小于20%，推荐使用乐观锁，否则使用悲观锁。乐观锁的重试次数不得小于3次。\n\n12. **【强制】** 多线程并行处理定时任务时，Timer运行多个TimeTask时，只要其中之一没有捕获抛出的异常，其它任务便会自动终止运行，使用ScheduledExecutorService则没有这个问题。\n\n13. 【推荐】资金相关的金融敏感信息，使用悲观锁策略。\n说明：乐观锁在获得锁的同时已经完成了更新操作，校验逻辑容易出现漏洞，另外，乐观锁对冲突的解决策略有较复杂的要求，处理不当容易造成系统压力或数据异常，所以资金相关的金融敏感信息不建议使用乐观锁更新。\n正例：悲观锁遵循一锁二判三更新四释放的原则\n\n14. 【推荐】使用CountDownLatch进行异步转同步操作，每个线程退出前必须调用countDown方法，线程执行代码注意catch异常，确保countDown方法被执行到，避免主线程无法执行至await方法，直到超时才返回结果。\n说明：注意，子线程抛出异常堆栈，不能在主线程try-catch到。\n\n15. 【推荐】避免Random实例被多线程使用，虽然共享该实例是线程安全的，但会因竞争同一seed导致的性能下降。\n说明：Random实例包括java.util.Random的实例或者Math.random()的方式。\n正例：在JDK7之后，可以直接使用API ThreadLocalRandom，而在JDK7之前，需要编码保证每个线程持有一个单独的Random实例。\n\n16. 【推荐】通过双重检查锁(double-checked locking)(在并发场景下)实现延迟初始化的优化问题隐患(可参考The \"Double-Checked Locking is Broken\" Declaration)，推荐解决方案中较简单一种(适用于JDK5及以上版本)，将目标属性声明为volatile型(比如修改helper的属性声明为'private volatile Helper helper = null;')。\n反例：\n```java\npublic class LazylnitDemo {\n    private Helper helper = null;\n    public Helper getHelper() {\n        if (helper == null) {\n            synchronized (this) {\n                if (helper == null) { \n                    helper = new Helper();\n                }\n            }\n        }\n        return helper;\n    }\n// other methods and fields...\n}\n```\n\n17. 【参考】volatile解决多线程内存不可见问题。对于一写多读，是可以解决变量同步问题，但是如果多写，同样无法解决线程安全问题。\n说明：如果是count++操作，使用如下类实现：\n```java\nAtomiclnteger count = new Atomiclnteger();\ncount.addAndGet(l);\n```\n如果是JDK8，推荐使用LongAdder对象，比AtomicLong性能更好(减少乐观锁的重试次数)。\n\n18. 【参考】HashMap在容量不够进行resize时由于高并发可能出现死链，导致CPU飙升，在开发过程中注意规避此风险。\n\n19. 【参考】ThreadLocaI对象使用static修饰，ThreadLocal无法解决共享对象的更新问题。\n说明：这个变量是针对一个线程内所有操作共享的，所以设置为静态变量，所有此类实例共享此静态变量，也就是说在类第一次被使用时装载，只分配一块存储空间，所有此类的对象(只要是这个线程内定义的)都可以操控这个变量。\n\n### (八) 控制语句\n\n1. **【强制】** 在一个switch块内，每个case要么通过continue/break/return等来终止，要么注释说明程序将继续执行到哪一个case为止；在一个switch块内，都必须包含一个default语句并且放在最后，即使它什么代码也没有。\n说明：注意break是退出switch语句块，而return是退出方法体。\n\n2. **【强制】** 当switch括号内的变量类型为String并且此变量为外部参数时，必须先进行null判断。\n反例：如下的代码输出是什么？\n```java\npublic class Switchstring {\n    public static void main(String[] args) {\n    \tmethod(null);\n    }\n    public static void method(String param) {\n        switch (param) {\n        //肯定不是进入这里\n        case \"sth\":\n            System.out.println(\"it's sth\");\n            break;\n        //也不是进入这里\n        case \"null\":\n            System .out.println(\"it's null\");\n            break;\n        //也不是进入这里\n        default:\n        \tSystem.out.println(\"default\");\n        }\n    }\n}\n```\n\n3. **【强制】** 在if/else/for/while/do语句中必须使用大括号。\n说明：即使只有一行代码，禁止不采用大括号的编码方式：\n```java\nif (condition) statements;\n```\n\n4. **【强制】** 三目运算符condition?表达式1:表达式2中，高度注意表达式1和2在类型对齐时，可能抛出因自动拆箱导致的NPE异常。\n说明：以下两种场景会触发类型对齐的拆箱操作：\n1)表达式1或表达式2的值只要有一个是原始类型。\n2)表达式1或表达式2的值的类型不一致，会强制拆箱升级成表示范围更大的那个类型。\n反例：\n```java\nInteger a = 1;\nInteger b = 2;\nInteger c = null;\nBoolean flag = false;\n// a*b的结果是int类型，那么c会强制拆箱成int类型，抛出NPE异常\nInteger result=(flag ? a*b : c);\n```\n\n5. **【强制】** 在高并发场景中，避免使用”等于”判断作为中断或退出的条件。\n说明：如果并发控制没有处理好，容易产生等值判断被\"击穿\"的情况，使用大于或小于的区间判断条件来代替。\n反例：判断剩余奖品数量等于0时，终止发放奖品，但因为并发处理错误导致奖品数量瞬间变成了负数，这样的话，活动无法终止。\n\n6. 【推荐】当某个方法的代码行数超过10行时，return/throw等中断逻辑的右大括号后加一个空行。\n说明：这样做逻辑清晰，有利于代码阅读时重点关注。\n\n7. 【推荐】表达异常的分支时，少用if-else方式，这种方式可以改写成：\n```java\nif (condition) {\n\treturn obj;\n}\n//接着写else的业务逻辑代码；\n```\n说明：如果非使用if()...else if()...else...方式表达逻辑，避免后续代码维护困难，请勿超过3层。\n正例：超过3层的if-else的逻辑判断代码可以使用卫语句、策略模式、状态模式等来实现，其中卫语句示例如下:\n```java\npublic void findBoyfriend (Man man){\n    if (man.isUgly()) {\n        System.out.println(\"本姑娘是外貌协会的资深会员\");\n        return;\n    }\n    if (man.isPoor()){\n        System.out.println(\"贫贱夫妻百事哀\")；\n        return;\n    }\n    if (man.isBadTemper()){\n        System.out.println(\"银河有多远，你就给我滚多远\")\n        return;\n    }\n    System.out.println(\"可以先交往一段时间看看\")；\n\n}\n```\n\n8. 【推荐】除常用方法(如getXxx/isXxx)等外，不要在条件判断中执行其它复杂的语句，将复杂逻辑判断的结果赋值给一个有意义的布尔变量名，以提高可读性。\n说明：很多if语句内的逻辑表达式相当复杂，与、或、取反混合运算，甚至各种方法纵深调用，理解成本非常高。如果赋值一个非常好理解的布尔变量名字，则是件令人爽心悦目的事情。\n正例：\n```java\n//伪代码如下\nfinal boolean existed = (file.open(fileName, \"w\") != null) && (...) || (...);\nif (existed) {\n}\t...\n```\n反例：\n```java\npublic final void acquire (long arg){\n    if (!tryAcquire(arg) && acquireQueued(addWaiter(Node.EXCLUSIVE), arg)) {\n    \tselflnterrupt();\n    }\n```\n\n9. 【推荐】不要在其它表达式（尤其是条件表达式）中，插入赋值语句。\n说明：赋值点类似于人体的穴位，对于代码的理解至关重要，所以赋值语句需要清晰地单独成为一行。\n反例：\n```java\npublic Lock getLock(boolean fair) {\n    //算术表达式中出现赋值操作，容易忽略count值已经被改变\n    threshold = count = Integer MAX_VALUE| - 1;\n    //条件表达式中出现赋值操作，容易误认为是sync==fair\n    return (sync = fair) ? new FairSync() : new NonfairSync();\n}\n```\n\n10. 【推荐】循环体中的语句要考量性能，以下操作尽量移至循环体外处理，如定义对象、变量、获取数据库连接，进行不必要的try-catch操作（这个try-catch是否可以移至循环体外）。\n\n11. 【推荐】避免采用取反逻辑运算符。\n说明：取反逻辑不利于快速理解，并且取反逻辑写法必然存在对应的正向逻辑写法。\n正例：使用if (x < 628)来表达x小于628。\n反例：使用if (!(x >= 628))来表达x小于628。\n\n12. 【推荐】接口入参保护，这种场景常见的是用作批量操作的接口。\n反例：某业务系统，提供一个用户批量查询的接口，API文档上有说最多查多少个，但接口实现上没做任何保护，导致调用方传了一个1000的用户id数组过来后，查询信息后，内存爆了。\n\n13. 【参考】下列情形，需要进行参数校验：\n1)调用频次低的方法。\n2)执行时间开销很大的方法。此情形中，参数校验时间几乎可以忽略不计，但如果因为参数错误导致中间执行回退，或者错误，那得不偿失。\n3)需要极高稳定性和可用性的方法。\n4)对外提供的开放接口，不管是RPC/API/HTTP接口。\n5)敏感权限入口。\n\n14. 【参考】下列情形，不需要进行参数校验：\n1)极有可能被循环调用的方法。但在方法说明里必须注明外部参数检查。\n2)底层调用频度比较高的方法。毕竟是像纯净水过滤的最后一道，参数错误不太可能到底层才会暴露问题。一般DAO层与Service层都在同一个应用中，部署在同一台服务器中，所以DAO的参数校验，可以省略。\n3)被声明成private只会被自己代码所调用的方法，如果能瓣确定调用方法的代码传入参数已经做过检查或者肯定不会有问题，此时可以不校验参数。\n\n### (九) 注释规约\n\n1. **【强制】** 类、类属性、类方法的注释必须使用Javadoc规范，使用/\\*\\*内容\\*/格式，不得使用//xxx方式。\n说明：在IDE编辑窗口中，Javadoc方式会提示相关注释，生成Javadoc可以正确输出相应注释；在IDE中，工程调用方法时，不进入方法即可悬浮提示方法、参数、返回值的意义，提高阅读效率。\n\n2. **【强制】** 所有的抽象方法（包括接口中的方法）必须要用Javadoc注释、除了返回值、参数、异常说明外，还必须指出该方法做什么事情，实现什么功能。\n说明：对子类的实现要求，或者调用注意事项，请一并说明。\n\n3. **【强制】** 所有的类都必须添加创建者和创建日期。\n说明：在设置模板时，注意IDEA的@author为'${USER}',而eclipse的@author为'${user}',大小写有区别，而日期的设置统一为yyyy/MM/dd的格式。\n正例：\n```java\n/**\n  * @author yangguanbao\n  * @date 2016/10/31\n  */\n```\n\n4. **【强制】** 方法内部单行注释，在被注释语句上方另起一行，使用//注释。方法内部多行注释使用/* */注释，注意与代码对齐。\n\n5. **【强制】** 所有的枚举类型字段必须要有注释，说明每个数据项的用途。\n\n6. 【推荐】与其\"半吊子”英文来注释，不如用中文注释把问题说清楚。专有名词与关键字保持英文原文即可。\n反例：“TCP连接超时\"解释成\"传输控制协议连接超时\"，理解反而费脑筋。\n\n7. 【推荐】代码修改的同时，注释也要进行相应的修改，尤其是参数、返回值、异常、核心逻辑等的修改。\n说明：代码与注释更新不同步，就像路网与导航软件更新不同步一样，如果导航软件严重滞后，就失去了导航的意义。\n\n8. 【推荐】在类中删除未使用的任何字段和方法；在方法中删除未使用的任何参数声明与内部变量。\n\n9. 【参考】谨慎注释掉代码。在上方详细说明，而不是简单地注释掉。如果无用，则删除。\n说明：代码被注释掉有两种可能性：1)后续会恢复此段代码逻辑。2)永久不用。前者如果没有备注信息,难以知晓注释动机。后者建议直接删掉即可，假如需要查阅历史代码，登录代码仓库即可。\n\n10. 【参考】对于注释的要求：第一、能够准确反映设计思想和代码逻辑；第二、能够描述业务含义，使别的程序员能够迅速了解到代码背后的信息。完全没有注释的大段代码对于阅读者形同天书，注释是给自己看的，即使隔很长时间，也能清晰理解当时的思路；注释也是给继任者看的，使其能够快速接替自己的工作。\n\n11. 【参考】好的命名、代码结构是自解释的，注释力求精简准确、表达到位。避免出现注释的一个极端：过多过滥的注释，代码的逻辑一旦修改，修改注释是相当大的负担。\n反例：\n```java\n// put elephant into fridge\nput(elephant, fridge);\n```\n方法名put，加上两个有意义的变量名elephant和fridge，已经说明了这是在干什么，语义清晰的代码不需要额外的注释。\n\n12. 【参考】特殊注释标记，请注明标记人与标记时间。注意及时处理这些标记，通过标记扫描，经常清理此类标记。线上故障有时候就是来源于这些标记处的代码。\n1)待办事宜（**TODO**）:（标记人，标记时间，［预计处理时间］）\n表示需要实现，但目前还未实现的功能。这实际上是一个Javadoc的标签，目前的Javadoc还没有实现，但已经被广泛使用。只能应用于类，接口和方法（因为它是一个Javadoc标签）。\n2)错误，不能工作（**FIXME**）:（标记人，标记时间，［预计处理时间］）\n在注释中用FIXME标记某代码是错误的，而且不能工作，需要及时纠正的情况。\n\n### (十) 其它\n\n1. **【强制】** 在使用正则表达式时，利用好其预编译功能，可以有效加快正则匹配速度。\n说明：不要在方法体内定义：Pattern pattern = Pattern.compile(\"规则\");\n\n2. **【强制】** 避免用Apache Beanutils进行属性的copy。\n说明：Apache BeanUtils性能较差，可以使用其他方案比如Spring BeanUtils, Cglib BeanCopier，注意均是浅拷贝。\n\n3. **【强制】** velocity调用POJO类的属性时，直接使用属性名取值即可，模板引擎会自动按规范调用POJO的getXxx()，如果是boolean基本数据类型变量（boolean命名不需要加is前缀），会自动调用isXxx()方法。\n说明：注意如果是Boolean包装类对象，优先调用getXxx()的方法。\n\n4. **【强制】** 后台输送给页面的变量必须加$!{var}中间的感叹号。\n说明：如果var等于null或者不存在，那么${var}会直接显示在页面上。\n\n5. **【强制】** 注意Math.random()这个方法返回是double类型，注意取值的范围0<=x<1（能够取到**零**值，注意除零异常），如果想获取整数类型的随机数，不要将x放大10的若干倍然后取整，直接使用Random对象的**nextInt**或者**nextLong**方法。\n\n6. 【推荐】不要在视图模板中加入任何复杂的逻辑。\n说明：根据MVC理论，视图的职责是展示，不要抢模型和控制器的活。\n\n7. 【推荐】任何数据结构的构造或初始化，都应指定大小，避免数据结构无限增长吃光内存。\n\n8. 【推荐】及时清理不再使用的代码段或配置信息。\n说明：对于垃圾代码或过时配置，坚决清理干净，避免程序过度臃肿，代码冗余。\n正例：对于暂时被注释掉，后续可能恢复使用的代码片断，在注释代码上方，统一规定使用三个斜杠(///)来说明注释掉代码的理由。如：\n```java\npublic static void hello(){\n    ///业务方通知活动暂停\n    // Business business = new Business();\n    // business.active();\n    System. out.println(\"it's finished\");\n}\n\n```\n\n## 二、异常日志\n\n### (一) 错误码\n\n1. **【强制】** 错误码的制定原则：快速溯源、简单易记、沟通标准化。\n说明：错误码想得过于完美和复杂，就像康熙字典中的生僻字一样，用词似乎精准，但是字典不容易随身携带并且简单易懂。\n正例：错误码回答的问题是谁的错？错在哪？1)错误码必须能够快速知晓错误来源，可快速判断是谁的问题。2)错误码易于记忆和比对（代码中容易equals）。3)错误码能够脱离文档和系统平台达到线下轻量化地自由沟通的目的。\n\n2. **【强制】** 错误码不体现版本号和错误等级信息。\n说明：错误码以不断追加的方式进行兼容。错误等级由日志和错误码本身的释义来决定。\n\n3. **【强制】** 全部正常，但不得不填充错误码时返回五个零：00000。\n\n4. **【强制】** 错误码为字符串类型，共5位，分成两个部分：错误产生来源+四位数字编号。\n说明：错误产生来源分为A/B/C，A表示错误来源于用户，比如参数错误，用户安装版本过低，用户支付超时等问题；B表示错误来源于当前系统，往往是业务逻辑出错，或程序健壮性差等问题；C表示错误来源于第三方服务，比如CDN服务出错，消息投递超时等问题；四位数字编号从0001到9999，大类之间的步长间距预留100\n\n5. **【强制】** 编号不与公司业务架构，更不与组织架构挂钩，一切与平台先到先申请的原则进行，审批生效，编号即被永久固定。\n\n6. **【强制】** 错误码使用者避免随意定义新的错误码。\n说明：尽可能在原有错误码附表中找到语义相同或者相近的错误码在代码中使用即可。\n\n7. **【强制】** 错误码不能直接输出给用户作为提示信息使用。\n说明：堆栈（stack_trace）、错误信息（error_message）、错误码（error_code）、提示信息（user_tip）是一个有效关联并互相转义的和谐整体，但是请勿互相越俎代庖。\n\n8. 【推荐】错误码之外的业务独特信息由error_message来承载，而不是让错误码本身涵盖过多具体业务属性。\n\n9. 【推荐】在获取第三方服务错误码时，向上抛出允许本系统转义，由C转为B，并且在错误信息上带上原有的第三方错误码。\n\n10. 【参考】错误码分为一级宏观错误码、二级宏观错误码、三级宏观错误码。\n说明：在无法更加具体确定的错误场景中，可以直接使用一级宏观错误码，分别是：A0001（用户端错误）、B0001（系统执行出错）、C0001（调用第三方服务出错）。\n正例：调用第三方服务出错是一级，中间件错误是二级，消息月艮务出错是三级。\n\n11. 【参考】错误码的后三位编号与HTTP状态码没有任何关系。\n\n12. 【参考】错误码尽量有利于不同文化背景的开发者进行交流与代码协作。\n说明：英文单词形式的错误码不利于非英语母语国家（如阿拉伯语、希伯来语、俄罗斯语等）之间的开发者互相协作。\n\n13. 【参考】错误码即人性，感性认知+口口相传，使用纯数字来进行错误码编排不利于感性记忆和分类。\n说明：数字是一个整体，每位数字的地位和含义是相同的。\n反例：一个五位数字12345，第1位是错误等级，第2位是错误来源，345是编号，人的大脑不会主动地分辨每位数字的不同含义。\n\n### (二) 异常处理\n\n1. **【强制】** Java类库中定义的可以通过预检查方式规避的RuntimeException异常不应该通过catch的方式来处理，比如：NullPointerException，IndexOutOfBoundsException等等。\n说明：无法通过预检查的异常除外，比如，在解析字符串形式的数字时，可能存在数字格式错误，不得不通过catch NumberFormatException来实现。\n正例:```if (obj != null) {...}```\n反例:```try { obj.method();} catch(NullPointerException e) {...}```\n\n2. **【强制】** 异常不要用来做流程控制，条件控制。\n说明：异常设计的初衷是解决程序运行中的各种意外情况，且异常的处理效率比条件判断方式要低很多。\n\n3. **【强制】** catch时请分清稳定代码和非稳定代码，稳定代码指的是无论如何不会出错的代码。对于非稳定代码的catch尽可能进行区分异常类型，再做对应的异常处理。\n说明：对大段代码进行try-catch，使程序无法根据不同的异常做出正确的应激反应，也不利于定位问题，这是一种不负责任的表现。\n正例：用户注册的场景中，如果用户输入非法字符，或用户名称已存在，或用户输入密码过于简单，在程序上作出分门别类的判断，并提示给用户。\n\n4. **【强制】** 捕获异常是为了处理它，不要捕获了却什么都不处理而抛弃之，如果不想处理它，请将该异常抛给它的调用者。最外层的业务使用者，必须处理异常，将其转化为用户可以理解的内容。\n\n5. **【强制】** 事务场景中，抛出异常被catch后，如果需要回滚，一定要注意手动回滚事务。\n\n6. **【强制】** finally块必须对资源对象、流对象进行关闭，有异常也要做try-catch。\n说明：如果JDK7及以上，可以使用try-with-resources方式。\n\n7. **【强制】** 不要在finally块中使用return。\n说明：try块中的return语句执行成功后，并不马上返回，而是继续执行finally块中的语句，如果此处存在return语句，则在此直接返回，无情丢弃掉try块中的返回点。\n反例：\n```java\nprivate int x = 0;\npublic int checkReturn(){\n    try {\n        // x等于1，此处不返回\n        return ++x;\n    } finally {\n        //返回的结果是2\n        return ++x;\n    }\n}\n```\n\n8. **【强制】** 捕获异常与抛异常，必须是完全匹配，或者捕获异常是抛异常的父类。\n说明：如果预期对方抛的是绣球，实际接到的是铅球，就会产生意外情况。\n\n9. **【强制】** 在调用RPC、二方包、或动态生成类的相关方法时，捕捉异常必须使用Throwable类来进行拦截。\n说明：通过反射机制来调用方法，如果找不到方法，抛出NoSuchMethodException。什么情况会抛出NoSuchMethodError呢？二方包在类冲突时,仲裁机制可能导致引入非预期的版本使类的方法签名不匹配，或者在字节码修改框架（比如：ASM）动态创建或修改类时，修改了相应的方法签名。这些情况，即使代码编译期是正确的，但在代码运行期时，会抛出NoSuchMethodError。\n\n10. 【推荐】方法的返回值可以为null，不强制返回空集合，或者空对象等，必须添加注释充分说明什么情况下会返回null值。\n说明：本手册明确防止NPE是调用者的责任。即使被调用方法返回空集合或者空对象，对调用者来说，也并非高枕无忧，必须考虑到远程调用失败、序列化失败、运行时异常等场景返回null的情况。\n\n11. 【推荐】防止NPE，是程序员的基本修养，注意NPE产生的场景：\n1)返回类型为基本数据类型，return包装数据类型的对象时，自动拆箱有可能产生NPE。\n反例：public int f() { return Integer对象}，如果为null，自动解箱抛NPE。\n2)数据库的查询结果可能为null。\n3)集合里的元素即使isNotEmpty，取出的数据元素也可能为null。\n4)远程调用返回对象时，一律要求进行空指针判断，防止NPE。\n5)对于Session中获取的数据，建议进行NPE检查，避免空指针。\n6)级联调用obj.getA().getB().getC();一连串调用，易产生NPE。\n正例：使用JDK8的Optional类来防止NPE问题。\n\n12. 【推荐】定义时区分unchecked/checked异常，避免直接抛出new RuntimeException(),更不允许抛出Exception或者Throwable，应使用有业务含义的自定义异常。推荐业界已定义过的自定义异常，如：DAOException/ServiceException等。\n\n13. 【参考】对于公司外的http/api开放接口必须使用\"错误码”；而应用内部推荐异常抛出；跨应用间RPC调用优先考虑使用Result方式，封装isSuccess()方法、\"错误码”、\"错误简短信息”；而应用内部推荐异常抛出。\n说明：关于RPC方法返回方式使用Result方式的理由：\n1)使用抛异常返回方式，调用方如果没有捕获到就会产生运行时错误。\n2)如果不加栈信息，只是new自定义异常，加入自己的理解的error message，对于调用端解决问题的帮助不会太多。如果加了栈信息，在频繁调用出错的情况下，数据序列化和传输的性能损耗也是问题。\n\n14. 【参考】避免出现重复的代码(Don't Repeat Yourself)，即DRY原则。\n说明：随意复制和粘贴代码，必然会导致代码的重复，在以后需要修改时，需要修改所有的副本，容易遗漏。必要时抽取共性方法，或者抽象公共类，甚至是组件化。\n正例：一个类中有多个public方法，都需要进行数行相同的参数校验操作，这个时候请抽取：\n```java\nprivate boolean checkParam(DTO dto) {...}\n```\n\n### (三) 日志规约\n\n1. **【强制】** 应用中不可直接使用日志系统(Log4js Logback)中的API，而应依赖使用日志框架(SLF4J、JCL--Jakarta Commons Logging)中的API，使用门面模式的日志框架，有利于维护和各个类的日志处理方式统一。\n说明：日志框架(SLF4J、JCL--Jakarta Commons Logging)的使用方式(推荐使用SLF4J)\n使用SLF4J:\n```java\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\nprivate static final Logger logger = LoggerFactory.getLogger(Test.class);\n```\n使用JCL:\n```java\nimport org.apache.commons.logging.Log;\nimport org.apache.commons.logging.LogFactory;\nprivate static final Log log = LogFactory.getLog(Test.class);\n```\n\n2. **【强制】** 所有日志文件至少保存15天，因为有些异常具备以\"周”为频次发生的特点。对于当天日志，以\"应用名.log”来保存，保存在/home/admin/应用名/logs/</font\\>目录下，过往日志格式为:{logname}.log.{保存日期}，日期格式：yyyy-MM-dd\n说明：以mppserver应用为例，日志保存在/home/admin/mppserver/logs/mppserver.log，历史日志名称为mppserver.log.2016-08-01\n\n3. **【强制】** 应用中的扩展日志(如打点、临时监控、访问日志等)命名方式：appName_logType_logName.log。logType:日志类型，如stats/monitor/access等；logName:日志描述。这种命名的好处：通过文件名就可知道日志文件属于什么应用，什么类型，什么目的，也有利于归类查找。\n说明：推荐对日志进行分类，如将错误日志和业务日志分开存放，便于开发人员查看，也便于通过日志对系统进行及时监控。\n正例：mppserver应用中单独监控时区转换异常，如：mppserver_monitor_timeZoneConvert.log\n\n4. **【强制】** 在日志输出时，字符串变量之间的拼接使用占位符的方式。\n说明：因为String字符串的拼接会使用StringBuilder的append()方式，有一定的性能损耗。使用占位符仅是替换动作，可以有效提升性能。\n正例:logger.debug(\"Processing trade with id: {} and symbol: {}\", id, symbol);\n\n5. **【强制】** 对于trace/debug/info级别的日志输出，必须进行日志级别的开关判断。\n说明：虽然在debug(参数)的方法体内第一行代码isDisabled(Level.DEBUG」NT)为真时(Slf4j的常见实现Log4j和Logback)，就直接return，但是参数可能会进行字符串拼接运算。此外，如果debug(getName())这种参数内有getName()方法调用，无谓浪费方法调用的开销。\n正例：\n```java\n//如果判断为真，那么可以输出trace和debug级别的日志\nif (logger.isDebugEnabled()){\n\tlogger.debug(\"Current ID is: {} and name is: {}\", id, getName。)；\n}\n```\n\n6. **【强制】** 避免重复打印日志，浪费磁盘空间，务必在log4j.xml中设置additivity=false。\n正例：< logger name=\"com.taobao.dubbo.config\" additivity=\"false\" >\n\n7. **【强制】** 生产环境禁止直接使用System.out或System.err输出日志或使用e.printStackTrace()打印异常堆栈。\n说明：标准日志输出与标准错误输出文件每次Jboss重启时才滚动，如果大量输出送往这两个文件，容易造成文件大小超过操作系统大小限制。\n\n8. **【强制】** 异常信息应该包括两类信息：案发现场信息和异常堆栈信息。如果不处理，那么通过关键字throws往上抛出。\n正例:logger.error(各各类参数或者对象toStringO + \"\\_\" + e.getMessage(), e);\n\n9. **【强制】** 日志打印时禁止直接用JSON工具将对象转换成String。\n说明：如果对象里某些get方法被重写，存在抛出异常的情况，则可能会因为打印日志而影响正常业务流程的执行。\n正例：打印日志时仅打印出业务相关属性值或者调用其对象的toString()方法。\n\n10. 【推荐】谨慎地记录日志。生产环境禁止输出debug日志；有选择地输出info日志；如果使用warn来记录刚上线时的业务行为信息，一定要注意日志输出量的问题，避免把服务器磁盘撑爆，并记得及时删除这些观察日志。\n说明：大量地输出无效日志，不利于系统性能提升，也不利于快速定位错误点。记录日志时请思考：这些日志真的有人看吗？看到这条日志你能做什么？能不能给问题排查带来好处？\n\n11. 【推荐】可以使用warn日志级别来记录用户输入参数错误的情况，避免用户投诉时，无所适从。如非必要，请不要在此场景打出error级别，避免频繁报警。\n说明：注意日志输出的级别，error级别只记录系统逻辑出错、异常或者重要的错误信息。\n\n12. 【推荐】尽量用英文来描述日志错误信息，如果日志中的错误信息用英文描述不清楚的话使用中文描述即可，否则容易产生歧义。\n说明：国际化团队或海外吝隧的服务器由于字符集问题，使用全英文来注释和描述日志错误信息。\n\n## 三、单元测试\n\n1. **【强制】** 好的单元测试必须遵守AIR原则。\n说明：单元测试在线上运行时，感觉像空气（AIR）一样并不存在，但在测试质量的保障上，却是非常关键的。好的单元测试宏观上来说，具有自动化、独立性、可重复执行的特点。\n- **A** : Automatic（自动化）\n- **I** : Independent（独立性）\n- **R** : Repeatable（可重复）\n\n2. **【强制】** 单元测试应该是全自动执行的，并且非交互式的。测试用例通常是被定期执行的，执行过程必须完全自动化才有意义。输出结果需要人工检查的测试不是一个好的单元测试。单元测试中不准使用System.out来进行人肉验证，必须使用assert来验证。\n\n3. **【强制】** 保持单元测试的独立性。为了保证单元测试稳定可靠且便于维护，单元测试用例之间决不能互相调用，也不能依赖执行的先后次序。\n反例：method2需要依赖method1的执行，将执行结果作为method2的输入。\n\n4. **【强制】** 单元测试是可以重复执行的，不能受到外界环境的影响。\n说明：单元测试通常会被放到持续集成中，每次有代码check in时单元测试都会被执行。如果单测对外部环境（网络、月服务、中间件等）有依赖，容易导致持续集成机制的不可用。\n正例：为了不受外界环境影响，要求设计代码时就把SUT的依赖改成注入，在测试时用spring这样的DI框架注入一个本地（内存）实现或者Mock实现。\n\n5. **【强制】** 对于单元测试，要保证测试粒度足够小，有助于精确定位问题。单测粒度至多是类级U,—般是方法级别。\n说明：只有测试粒度小才能在出错时尽快定位到出错位置。单测不负责检查跨类或者跨系统的交互逻辑，那是集成测试的领域。\n\n6. **【强制】** 核心业务、核心应用、核心模块的增量代码确保单元测试通过。\n说明：新增代码及时补充单元测试，如果新增代码影响了原有单元测试，请及时修正。\n\n7. **【强制】** 单元测试代码必须写在如下工程目录：src/test/java，不允许写在业务代码目录下。说明：源码编译时会跳过此目录，而单元测试框架默认是扫描此目录。\n\n8. 【推荐】单元测试的基本目标：语句覆盖率达到70%；核心模块的语句覆盖率和分支覆盖率都要达到100%\n说明：在工程规约的应用分层中提到的DAO层，Manager层，可重用度高的Service，都应该进行单元测试。\n\n9. 【推荐】编写单元测试代码遵守BCDE原则，以保证被测试模块的交付质量。\n- **B** : Border，边界值测试，包括循环边界、特殊取值、特殊时间点、数据顺序等。\n- **C** : Correct，正确的输入，并得到预期的结果。\n- **D** : Design，与设计文档相结合，来编写单元测试。\n- **E** : Error，强制错误信息输入（如：非法数据、异常流程、业务允许外等），并得到预期的结果。\n\n10. 【推荐】对于数据库相关的查询，更新，删除等操作，不能假设数据库里的数据是存在的，或者直接操作数据库把数据插入进去，请使用程序插入或者导入数据的方式来准备数据。\n反例：删除某一行数据的单元测试，在数据库中，先直接手动增加一行作为删除目标，但是这一行新增数据并不符合业务插入规则，导致测试结果异常。\n\n11. 【推荐】和数据库相关的单元测试，可以设定自动回滚机制，不给数据库造成脏数据。或者对单元测试产生的数据有明确的前后缀标识。\n正例：在阿里巴巴企业智能事业部的内部单元测试中，使用**ENTERPRISE_INTELLIGENCE_UNIT_TEST**的前缀来标识单元测试相关代码。\n\n12. 【推荐】对于不可测的代码在适当的时机做必要的重构，使代码变得可测，避免为了达到测试要求而书写不规范测试代码。\n\n13. 【推荐】在设计评审阶段，开发人员需要和测试人员一起确定单元测试范围，单元测试最好覆盖所有测试用例（UC）。\n\n14. 【推荐】单元测试作为一种质量保障手段，在项目提测前完成单元测试，不建议项目发布后补充单元测试用例。\n\n15. 【参考】为了更方便地进行单元测试，业务代码应避免以下情况：\n- 构造方法中做的事情过多。\n- 存在过多的全局变量和静态方法。\n- 存在过多的外部依赖。\n- 存在过多的条件语句。\n说明：多层条件语句建议使用卫语句、策略模式、状态模式等方式重构。\n\n16. 【参考】不要对单元测试存在如下误解：\n- 那是测试同学干的事情。本文是开发手册，凡是本文内容都是与开发同学强相关的。\n- 单元测试代码是多余的。系统的整体功能与各单元部件的测试正常与否是强相关的。\n- 单元测试代码不需要维护。一年半载后，那么单元测试几乎处于废弃状态。\n- 单元测试与线上故障没有辩证关系。好的单元测试能够最大限度地规避线上故障。\n\n## 四、安全规约\n\n1. **【强制】** 隶属于用户个人的页面或者功能必须进行权限控制校验。\n说明：防止没有做水平权限校验就可随意访问、修改、删除别人的数据，比如查看他人的私信内容。\n\n2. **【强制】** 用户敏感数据禁止直接展示，必须对展示数据进行脱敏。\n说明：中国大陆个人手机号码显示为:137****0969，隐藏中间4位，防止隐私泄露。\n\n3. **【强制】** 用户输入的SQL参数严格使用参数绑定或者METADATA字段值限定，防止SQL注入,禁止字符串拼接SQL访问数据库。\n反例：某系统签名大量被恶意修改，即是因为对于危险字符#--没有进行转义，导致数据库更新时，where后边的信息被注释掉，对全库进行更新。\n\n4. **【强制】** 用户请求传入的任何参数必须做有效性验证。\n说明：忽略参数校验可能导致：\n- page size过大导致内存溢出\n- 恶意order by导致数据库慢查询\n- 缓存击穿\n- SSRF\n- 任意重定向\n- SQL注入，Shell注入，反序列化注入\n- 正则输入源串拒绝服务ReDoS\nJava代码用正则来验证客户端的输入，有些正则写法验证普通用户输入没有问题，但是如果攻击人员使用的是特殊构造的字符串来验证，有可能导致死循环的结果。\n\n5. **【强制】** 禁止向HTML页面输出未经安全过滤或未正确转义的用户数据。\n\n6. **【强制】** 表单、AJAX提交必须执行CSRF安全验证。\n说明：CSRF(Cross-site request forgery)跨站请求伪造是一类常见编程漏洞。对于存在CSRF漏洞的应用/网站，攻击者可以事先构造好URL,只要受害者用户一访问，后台便在用户不知情的情况下对数据库中用户参数进行相应修改。\n\n7. **【强制】** URL外部重定向传入的目标地址必须执行白名单过滤。\n\n8. **【强制】** 在使用平台资源，譬如短信、邮件、电话、下单、支付，必须实现正确的防重放的机制，如数量限制、疲劳度控制、验证码校验，避免被滥刷而导致资损。\n说明：如注册时发送验证码到手机，如果没有限制次数和频率，那么可以利用此功能骚扰到其它用户，并造成短信平台资源浪费。\n\n9. 【推荐】发贴、评论、发送即时消息等用户生成内容的场景必须实现防刷、文本内容违禁词过滤等风控策略。\n\n## 五、MySQL数据库\n\n### (一) 建表规约\n\n1. **【强制】** 表达是与否概念的字段，必须使用is_xxx的方式命名，数据类型是unsigned tinyint（1表示是，0表示否）。\n说明：任何字段如果为非负数，必须是unsigned,\n注意：POJO类中的任何布尔类型的变量，都不要加is前缀，所以，需要在<resultMap\\>设置从is_xxx到Xxx的映射关系。数据库表示是与否的值，使用tinyint类型，坚持is_xxx的命名方式是为了明确其取值含义与取值范围。\n正例：表达逻辑删除的字段名is_deleted，1表示删除，0表示未删除。\n\n2. **【强制】** 表名、字段名必须使用小写字母或数字，禁止出现数字开头，禁止两个下划线中间只出现数字。数据库字段名的修改代价很大，因为无法进行预发布，所以字段名称需要慎重考虑。\n说明：MySQL在Windows下不区分大小写，但在Linux下默认是区分大小写。因此，数据库名、表名、字段名，都不允许出现任何大写字母，避免节外生枝。\n正例：aliyun_admin，rdc_config，level3_name\n反例：AliyunAdmin，rdcConfig，level_3_name\n\n3. **【强制】** 表名不使用复数名词。\n说明：表名应该仅仅表示表里面的实体内容，不应该表示实体数量，对应于DO类名也是单数形式，符合表达习惯。\n\n4. **【强制】** 禁用保留字，如desc、range、match、delayed等，请参考MySQL官方保留字。\n\n5. **【强制】** 主键索引名为pk-字段名；唯一索引名为uk\\_字段名;普通索引名则为idx\\_字段名。说明：pk\\_即primary key;uk\\_即unique key;idx\\_即index的简称。\n\n6. **【强制】** 小数类型为decimal，禁止使用float和double。\n说明：在存储的时候，float和double都存在精度损失的问题，很可能在比较值的时候，得到不正确的结果。如果存储的数据范围超过decimal的范围，建议将数据拆成整数和小数并分开存储。\n\n7. **【强制】** 如果存储的字符串长度几乎相等，使用char定长字符串类型。\n\n8. **【强制】** varchar是可变长字符串，不预先分配存储空间，长度不要超过5000，如果存储长度大于此值，定义字段类型为text，独立出来一张表，用主键来对应，避免影响其它字段索引效率。\n\n9. **【强制】** 表必备三字段：id,gmt_create,gmt_modified。\n说明：其中id必为主键，类型为bigint unsigned、单表时自增、步长为1。gmt_create,gmt_modified的类型均为datetime类型，前者现在时表示主动式创建，后者过去分词表示被动式更新。\n\n10. 【推荐】表的命名最好是遵循\"业务名称一表的作用\"。\n正例：alipay_task/force_project/trade_config\n\n11. 【推荐】库名与应用名称尽量一致。\n\n12. 【推荐】如果修改字段含义或对字段表示的状态追加时，需要及时更新字段注释。\n\n13. 【推荐】字段允许适当冗余，以提高查询性能，但必须考虑数据一致。冗余字段应遵循：\n1)不是频繁修改的字段。\n2)不是唯一索引的字段。\n3)不是varchar超长字段，更不能是text字段。\n正例：各业务线经常冗余存储商品名称，避免查询时需要调用IC月服务获取。\n\n14. 【推荐】单表行数超过500万行或者单表容量超过2GB，才推荐进行分库分表。\n说明：如果预计三年后的数据量根本达不到这个级别，请不要在创建表时就分库分表。\n\n15. 【参考】合适的字符存储长度，不但节约数据库表空间、节约索引存储，更重要的是提升检索速度。\n正例：无符号值可以避免误存负数,且扩大了表示范围。\n\n| 对象     | 年龄区间  | 类型              | 字节 | 表示范围                  |\n| -------- | --------- | ----------------- | ---- | ------------------------- |\n| 人       | 150岁之内 | tinyint unsigned  | 1    | 无符号值：0到255          |\n| 龟       | 数百岁    | smallint unsigned | 2    | 无符号值：0到65535        |\n| 恐龙化石 | 数千万年  | int unsigned      | 4    | 无符号值：0到约43亿       |\n| 太阳     | 约50亿年  | bigint unsigned   | 8    | 无符号值：0至约10的19次方 |\n\n \n\n### (二) 索引规约\n\n1. **【强制】** 业务上具有唯一特性的字段，即使是组合字段，也必须建成唯一索引。\n说明：不要以为唯一索引影响了insert速度，这个速度损耗可以忽略，但提高查找速度是明显的；另外，即使在应用层做了非常完善的校验控制，只要没有唯一索引，根据墨菲定律，必然有脏数据产生。\n\n2. **【强制】** 超过三个表禁止join。需要join的字段，数据类型保持绝对一致；多表关联查询时，保证被关联的字段需要有索引。\n说明：即使双表join也要注意表索引、SQL性能。\n\n3. **【强制】** 在varchar字段上建立索引时，必须指定索引长度，没必要对全字段建立索引，根据实际文本区分度决定索引长度。\n说明：索引的长度与区分度是一对矛盾体，一般对字符串类型数据，长度为20的索引，区分度会高达90%以上，可以使用count(distinct left(列名，索引长度))/count(\\*)的区分度来确定。\n\n4. **【强制】** 页面搜索严禁左模糊或者全模糊，如果需要请走搜索引擎来解决。\n说明：索引文件具有B-Tree的最左前缀匹配特性，如果左边的值未确定，那么无法使用此索引。\n\n5. 【推荐】如果有order by的场景，请注意利用索引的有序性。order by最后的字段是组合索引的一部分，并且放在索引组合顺序的最后，避免出现file_sort的情况，影响查询性能。 \n正例：where a=? and b=? order by c;索引： a_b_c\n反例：索引如果存在范围查询，那么索引有序性无法利用，如：WHERE a>10 ORDER BY b;索引a_b无法排序。\n\n6. 【推荐】利用覆盖索引来进行查询操作，避免回表。\n说明：如果一本书需要知道第11章是什么标题，会翻开第11章对应的那一页吗？目录浏览一下就好，这个目录就是起到覆盖索引的作用。\n正例：能够建立索引的种类分为主键索引、唯一索引、普通索引三种，而覆盖索引只是一种查询的一种效果，用explain的结果，extra列会出现:using index。\n\n7. 【推荐】利用延迟关联或者子查询优化超多分页场景。\n说明：MySQL并不是跳过offset行，而是取offset+N行，然后返回放弃前offset行，返回N行，那当offset特别大的时候，效率就非常的低下，要么控制返回的总页数，要么对超过特定阈值的页数进行SQL改写。\n正例：先快速定位需要获取的id段，然后再关联：\n```sql\nSELECT a.* FROM 表 1 a, (select id from 表 1 where 条件 LIMIT 100000,20 ) b where a.id=b.id\n```\n\n8. 【推荐】SQL性能优化的目标：至少要达到range级别，要求是ref级别，如果可以是consts最好。\n说明：\n1)consts单表中最多只有一个匹配行(主键或者唯一索引)，在优化阶段即可读取到数据。\n2)ref指的是使用普通的索引(normal index )。\n3)range对索引进行范围检索。\n反例：explain表的结果，type=index，索引物理文件全扫描，速度非常慢，这个index级别比较range还低，与全表扫描是小巫见大巫。\n\n9. 【推荐】建组合索引的时候，区分度最高的在最左边。\n正例：如果where a=? and b=?，a列的几乎接近于唯一值，那么只需要单建idx_a索引即可。\n说明：存在非等号和等号混合判断条件时，在建索引时，请把等号条件的列前置。如：where c>? and d=? 那么即使c的区分度更高，也必须把d放在索引的最前列，即建立组合索引idx_d_c。\n\n10. 【推荐】防止因字段类型不同造成的隐式转换，导致索引失效。\n\n11. 【参考】创建索引时避免有如下极端误解：\n1)索引宁滥勿缺。认为一个查询就需要建一个索引。\n2)吝啬索引的创建。认为索引会消耗空间、严重拖慢记录的更新以及行的新增速度。\n3)抵制惟一索引。认为惟一索引一律需要在应用层通过\"先查后插\"方式解决。\n\n### (三) SQL语句\n\n1. **【强制】** 不要使用count(列名)或count(常量)来替代count(\\*)，count(\\*)是SQL92定义的标准统计行数的语法，跟数据库无关，跟NULL和非NULL无关。\n说明：count(\\*)会统计值为NULL的行，而count(列名)不会统计此列为NULL值的行。\n\n2. **【强制】** count(distinct col)计算该列除NULL之外的不重复行数注意count(distinct col1, col2)如果其中一列全为NULL，那么即使另一列有不同的值，也返回为0。\n\n3. **【强制】** 当某一列的值全是NULL时，count(col)的返回结果为0，但sum(col)的返回结果为 NULL，因此使用sum()时需注意NPE问题。\n正例：可以使用如下方式来避免sum的NPE问题：SELECT IFNULL(SUM(column), 0) FROM table;\n\n4. **【强制】** 使用**ISNULL()** 来判断是否为NULL值。\n说明：NULL与任何值的直接比较都为NULL\n1)**NULL<>NULL**的返回结果是NULL，而不是false\n2)**NULL=NULL**的返回结果是NULL，而不是true\n3)**NULL<>1**的返回结果是NULL，而不是true\n反例：在SQL语句中，如果在null前换行，影响可读性。select * from table where column1 is null and column3 is not null;而'ISNULL(co山mn)'是一个整体，简洁易懂。从性能数据上分析，'ISNULL(column)' 执行效率更快一些。\n\n5. **【强制】** 代码中写分页查询逻辑时，若count为0应直接返回，避免执行后面的分页语句。\n\n6. **【强制】** 不得使用外键与级联，一切外键概念必须在应用层解决。\n说明：(概念解释)学生表中的student_id是主键，那么成绩表中的student_id则为外键。如果更新学生表中的student_id，同时触发成绩表中的student_id更新，即为级联更新。外键与级联更新适用于单机低并发，不适合分布式、高并发集群；级联更新是强阻塞，存在数据库更新风暴的风险；外键影响数据库的插入速度。\n\n7. **【强制】** 禁止使用存储过程，存储过程难以调试和扩展，更没有移植性。\n\n8. **【强制】** 数据订正(特别是删除或修改记录操作)时，要先select，避免出现误删除，确认无误才能执行更新语句。\n\n9. **【强制】** 对于数据库中表记录的查询和变更，只要涉及多个表,都需要在列名前加表的别名（或表名）进行限定。\n说明：对多表进行查询记录、更新记录、删除记录时，如果对操作列没有限定表的别名（或表名），并且操作列在多个表中存在时，就会抛异常。\n正例:\n```sql\nselect t1 .name from table_first as t1 , table_second as t2 where t1 .id=t2.id;\n```\n反例：在某业务中，由于多表关联查询语句没有加表的别名（或表名）的限制，正常运行两年后，最近在某个表中增加一个同名字段，在预发布环境做数据库变更后，线上查询语句出现出1052异常：Column 'name' in field list is ambiguous。\n\n10. 【推荐】SQL语句中表的别名前加as,并且以tl、t2、t3、...的顺序依次命名。\n说明：1)别名可以是表的简称，或者是根据表出现的顺序，以t1、t2、t3的方式命名。2)别名前加as使别名更容易识别。\n正例：\n```sql\nselect t1 .name from table_first as t1, table_second as t2 where t1.id=t2.id;\n```\n\n11. 【推荐】in操作能避免则避免，若实在避免不了，需要仔细评估in后边的集合元素数量，控制在1000个之内。\n\n12. 【参考】因国际化需要，所有的字符存储与表示，均采用utf8字符集，那么字符计数方法需要注意。\n说明：\n```sql\nSELECT LENGTH(\"轻松工作\");--返回为12\nSELECT CHARACTER_LENGTH(\"轻松工作\");--返回为4\n```\n如果需要存储表情，那么选择utf8mb4来进行存储，注意它与utf8编码的区别。\n\n13. 【参考】TRUNCATE TABLE比DELETE速度快，且使用的系统和事务日志资源少，但TRUNCATE无事务且不触发trigger，有可能造成事故，故不建议在开发代码中使用此语句。\n说明：TRUNCATE TABLE在功能上与不带WHERE子句的DELETE语句相同。\n\n### (四) ORM映射\n\n1. **【强制】** 在表查询中，一律不要使用\\*作为查询的字段列表，需要哪些字段必须明确写明。\n说明：1)增加查询分析器解析成本。2)增减字段容易与resultMap配置不一致。3)无用字段增加网络消耗，尤其是text类型的字段。\n\n2. **【强制】** POJO类的布尔属性不能加is，而数据库字段必须加is\\_，要求在resultMap中进行字段与属性之间的映射。\n说明：参见定义POJO类以及数据库字段定义规定，在sql.xml增加映射，是必须的。\n\n3. **【强制】** 不要用resultclass当返回参数，即使所有类属性名与数据库字段一一对应，也需要定义<resultMap\\>;反过来，每一个表也必然有一个<resultMap\\>与之对应。\n说明：配置映射关系，使字段与DO类解耦，方便维护。\n\n4. **【强制】** sql.xml配置参数使用：#{}，#param#不要使用${}此种方式容易出现SQL注入。\n\n5. **【强制】** iBATIS自带的queryForList(String statementName,int start,int size)不推荐使用。\n说明：其实现方式是在数据库取到statementName对应的SQL语句的所有记录，再通过subList取start,size的子集合。\n正例：\n```java\nMap<String, Object> map = new HashMap<>();\nmap.put(\"start\", start); map.put(\"size\",size);\n```\n\n6. **【强制】** 不允许直接拿HashMap与Hashtable作为查询结果集的输出。\n反例：某同学为避免写一个<resultMap\\>，直接使用HashTable来接收数据库返回结果，结果出现日常是把bigint转成Long值，而线上由于数据库版本不一样，解析成BigInteger，导致线上问题。\n\n7. **【强制】** 更新数据表记录时，必须同时更新记录对应的gmt_modified字段值为当前时间。\n\n8. 【推荐】不要写一个大而全的数据更新接口。传入为POJO类，不管是不是自己的目标更新字段，都进行update table set c1 =value1,c2=value2,c3=value3;这是不对的。执行SQL时，不要更新无改动的字段，一是易出错；二是效率低；三是增加binlog存储。\n\n9. 【参考】@Transactional事务不要滥用。事务会影响数据库的qps，另外使用事务的地方需要考虑各方面的回滚方案，包括缓存回滚、搜索引擎回滚、消息补偿、统计修正等。\n\n10. 【参考】<isEqual\\>中的compareValue是与属性值对比的常量，一般是数字，表示相等时带上此条件；<isNotEmpty\\>表示不为空且不为null时执行；<isNotNull\\>表示不为null值时执行。\n\n## 六、工程结构\n\n### (一) 应用分层\n\n1. 【推荐】图中默认上层依赖于下层，箭头关系表示可直接依赖，如：开放接口层可以依赖于\nWeb层，也可以直接依赖于Service层，依此类推：\n- 开放接口层：可直接封装Service方法暴露成RPC接口；通过Web封装成http接口；网关控制层等。\n- 终端显示层：各个端的模板渲染并执行显示的层。当前主要是velocity渲染，JS渲染，JSP渲染，移动端展示等。\n- Web层：主要是对访问控制进行转发，各类基本参数校验，或者不复用的业务简单处理等。\n- Service层：相对具体的业务逻辑月服务层。\n- Manager层：通用业务处理层，它有如下特征：\n1)对第三方平台封装的层，预处理返回结果及转化异常信息。\n2)对Service层通用能力的下沉，如缓存方案、中间件通用处理。\n3)与DAO层交互，对多个DAO的组合复用。\n- DAO层：数据访问层，与底层MySQL、Oracle、Hbase、OB等进行数据交互。\n- 外部接口或第三方平台：包括其它部门RPC开放接口，基础平台，其它公司的HTTP接口。\n\n2. 【参考】（分层异常处理规约）在DAO层，产生的异常类型有很多，无法用细粒度的异常进行catch，使用catch(Exception e)方式，并throw new DAOException(e)，不需要打印日志，因为日志在Manager/Service层一定需要捕获并打印到日志文件中去如果同台服务器再打日志，浪费性能和存储。在Service层出现异常时，必须记录出错日志到磁盘，尽可能带上参数信息,相当于保护案发现场。Manager层与Service同机部署，日志方式与DAO层处理一致，如果是单独部署，则采用与Service—致的处理方式。Web层绝不应该继续往上抛异常，因为已经处于顶层，如果意识到这个异常将导致页面无法正常渲染，那么就应该直接跳转到友好错误页面，尽量加上友好的错误提示信息。开放接口层要将异常处理成错误码和错误信息方式返回。\n\n3. 【参考】分层领域模型规约：\n- DO(Data Object):此对象与数据库表结构一一对应，通过DAO层向上传输数据源对象。\n- DTO(Data Transfer Object):数据传输对象，Service或Manager向外传输的对象。\n- BO(Business Object)：业务对象，可以由Service层输出的封装业务逻辑的对象。\n- Query :数据查询对象，各层接收上层的查询请求。注意超过2个参数的查询封装，禁止使用Map类来传输。\n- VO(View Object):显示层对象，通常是Web向模板渲染引擎层传输的对象。\n\n### (二) 二方库依赖\n\n1. **【强制】** 定义GAV遵从以下规则：\n1)**G**roupID格式：com.{公司/BU}.业务线[.子业务线]，最多4级。\n说明：{公司/BU}例如：alibaba/taobao/tmall/aliexpress等BU一级；子业务线可选。\n正例:com.taobao.jstorm或com.alibaba.dubbo.register\n2)**A**rtifactID格式：产品线名-模块名。语义不重复不遗漏，先到中央仓库去查证一下。\n正例：dubbo-client/fastjson-api/jstorm-tool\n3)**V**ersion:详细规定参考下方。\n\n2. **【强制】** 二方库版本号命名方式：主版本号.次版本号.修订号\n1)主版本号:产品方向改变，或者大规模API不兼容，或者架构不兼容升级。\n2)次版本号：保持相对兼容性，增加主要功能特性，影响范围极小的API不兼容修改。\n3)修订号:保持完全兼容性，修复BUG、新增次要功能特性等。\n说明：注意起始版本号必须为：**1.0.0**，而不是0.0.1。\n反例：仓库内某二方库版本号从1.0.0.0开始，一直默默\"升级\"成1.0.0.64，完全失去版本的语义信息。\n\n3. **【强制】** 线上应用不要依赖SNAPSHOT版本(安全包除外)；正式发布的类库必须先去中央仓库进行查证，使RELEASE版本号有延续性，且版本号不允许覆盖升级。\n说明：不依赖SNAPSHOT版本是保证应用发布的幂等性。另外，也可以加快编译时的打包构建。\n\n4. **【强制】** 二方库的新增或升级，保持除功能点之外的其它jar包仲裁结果不变。如果有改变，必须明确评估和验证。\n说明：在升级时，进行dependency:resolve前后信息比对，如果仲裁结果完全不一致，那么通过dependency:tree命令，找出差异点，进行<exclude\\>排除jar包。\n\n5. **【强制】** 二方库里可以定义枚举类型，参数可以使用枚举类型，但是接口返回值不允许使用枚举类型或者包含枚举类型的POJO对象。\n\n6. **【强制】** 依赖于一个二方库群时，必须定义一个统一的版本变量，避免版本号不一致。\n说明：依赖springframework-core,-context,-beans，它们都是同一个版本，可以定义一个变量来保存版本：${spring.version}，定义依赖的时候，引用该版本。\n\n7. **【强制】** 禁止在子项目的pom依赖中出现相同的GroupId，相同的ArtifactId，但是不同的Version。\n说明：在本地调试时会使用各子项目指定的版本号，但是合并成一个war，只能有一个版本号出现在最后的lib目录中。曾经出现过线下调试是正确的，发布到线上却出故障的先例。\n\n8. 【推荐】底层基础技术框架、核心数据管理平台、或近硬件端系统谨慎引入第三方实现。\n\n9. 【推荐】所有pom文件中的依赖声明放在<dependencies\\>语句块中，所有版本仲裁放在<dependencyManagement\\>语句块中。\n说明：<dependencyManagement\\>里只是声明版本，并不实现引入，因此子项目需要显式的声明依赖，version和scope都B读取自父pom。而<dependencies\\>所有声明在主pom的<dependencies\\>里的依赖都会自动引入，并默认被所有的子项目继承。\n\n10. 【推荐】二方库不要有配置项，最低限度不要再增加配置项。\n\n11. 【推荐】不要使用不稳定的工具包或者Utils类。\n说明：不稳定指的是提供方无法做到向下兼容，在编译阶段正常，但在运行时产生异常，因此，尽量使用业界稳定的二方工具包。\n\n12. 【参考】为避免应用二方库的依赖冲突问题，二方库发布者应当遵循以下原则：\n1)**精简可控原则**。移除一切不必要的API和依赖，只包含Service API、必要的领域模型对象、Utils类、常量、枚举等。如果依赖其它二方库，尽量是provided引入，让二方库使用者去依赖具体版本号；无log具体实现，只依赖日志框架。\n2)**稳定可追溯原则**。每个版本的变化应该被记录，二方库由谁维护，源码在哪里，都需要能方便查到。除非用户主动升级版本，否则公共二方库的行为不应该发生变化。\n\n### (三) 服务器\n\n1. 【推荐】高并发服务器建议调小TCP协议的time_wait超时时间。\n说明：操作系统默认240秒后，才会关闭处于time_wait状态的连接，在高并发访问下，服务器端会因为处于time_wait的连接数太多，可能无法建立新的连接，所以需要在服务器上调小此等待值。\n正例：在linux服务器上请通过变更/etc/sysctl.conf文件去修改该缺省值（秒）：net.ipv4.tcp_fi n_ti meout = 30\n\n2. 【推荐】调大服务器所支持的最大文件句柄数（File Descriptor，简写为fd）。\n说明：主流操作系统的设计是将TCP/UDP连接采用与文件一样的方式去管理，即一个连接对应于一个fd。主流的linux服务器默认所支持最大fd数量为1024当并发连接数很大时很容易因为fd不足而出现“open too many files”错误，导致新的连接无法建立。建议将linux服务器所支持的最大句柄数调高数倍（与服务器的内存数量相关）。\n\n3. 【推荐】给JVM环境参数设置-XX:+HeapDumpOnOutOfMemoryError参数，让JVM碰到OOM场景时输出dump信息。\n说明：OOM的发生是有概率的，甚至相隔数月才出现一例，出错时的堆内信息对解决问题非常有帮助。\n\n4. 【推荐】在线上生产环境，JVM的Xms和Xmx设置一样大小的内存容量，避免在GC后调整堆大小带来的压力。\n\n5. 【参考】服务器内部重定向必须使用forward；外部重定向地址必须使用URL Broker生成，否则因线上采用HTTPS协议而导致浏览器提示\"不安全“。此外，还会带来URL维护不一致的问题。\n\n\n## 七、设计规约\n\n1. **【强制】**  **存储方案**和**底层数据结构**的设计获得评审一致通过，并沉淀成为文档。\n说明：有缺陷的底层数据结构容易导致系统风险上升，可扩展性下降，重构成本也会因历史数据迁移和系统平滑过渡而陡然增加，所以，存储方案和数据结构需要认真地进行设计和评审，生产环境提交执行后，需要进行double check。\n正例：评审内容包括存储介质选型、表结构设计能否满足技术方案、存取性能和存储空间能否满足业务发展、表或字段之间的辩证关系、字段名称、字段类型、索引等；数据结构变更（如在原有表中新增字段）也需要进行评审通过后上线。\n\n2. **【强制】** 在需求分析阶段，如果与系统交互的User超过**一类**并且相关的User Case超过**5个**，使用用例图来表达更加清晰的结构化需求。\n\n3. **【强制】** 如果某个业务对象的状态超过**3个**，使用状态图来表达并且明确状态变化的各个触发条件。\n说明：状态图的核心是对象状态，首先明确对象有多少种状态，然后明确两两状态之间是否存在直接转换关系，再明确触发状态转换的条件是什么。\n正例：淘宝订单状态有已下单、待付款、已付款、待发货、已发货、已收货等。比如已下单与已收货这两种状态之间是不可能有直接转换关系的。\n\n4. **【强制】** 如果系统中某个功能的调用链路上的涉及对象超过**3个**，使用时序图来表达并且明确各调用环节的输入与输出。\n说明：时序图反映了一系列对象间的交互与协作关系，清晰立体地反映系统的调用纵深链路。\n\n5. **【强制】** 如果系统中模型类超过**5个**，并且存在复杂的依赖关系，使用类图来表达并且明确类之间的关系。\n说明：类图像建筑领域的施工图，如果搭平房，可能不需要，但如果建造蚂蚁Z空间大楼，肯定需要详细的施工图。\n\n6. **【强制】** 如果系统中超过**2个**对象之间存在协作关系，并且需要表示复杂的处理流程，使用活动图来表示。\n说明：活动图是流程图的扩展，增加了能够体现协作关系的对象泳道，支持表示并发等。\n\n7. 【推荐】系统架构设计时明确以下目标：\n- 确定系统边界。确定系统在技术层面上的做与不做。\n- 确定系统内模块之间的关系。确定模块之间的依赖关系及模块的宏观输入与输出。\n- 确定指导后续设计与演化的原则。使后续的子系统或模块设计在一个既定的框架内和技术方向上继续演化。\n- 确定非功能性需求。非功能性需求是指安全性、可用性、可扩展性等。\n\n8. 【推荐】需求分析与系统设计在考虑主干功能的同时，需要充分评估异常流程与业务边界。\n反例：用户在淘宝付款过程中，银行扣款成功，发送给用户扣款成功短信，但是支付宝入款时由于断网演练产生异常，淘宝订单页面依然显示未付款，导致用户投诉。\n\n9. 【推荐】类在设计与实现时要符合单一原则。\n说明：单一原则最易理解却是最难实现的一条规则，随着系统演进，很多时候，忘记了类设计的初衷。\n\n10. 【推荐】谨慎使用继承的方式来进行扩展，优先使用聚合/组合的方式来实现。\n说明：不得已使用继承的话，必须符合里氏代换原则，此原则说父类能够出现的地方子类一定能够出现，比如，\"把钱交出来\"，钱的子类美元、欧元、人民币等都可以出现。\n\n11. 【推荐】系统设计阶段，根据依赖倒置原则，尽量依赖抽象类与接口，有利于扩展与维护。\n说明：低层次模块依赖于高层次模块的抽象，方便系统间的解耦。\n\n12. 【推荐】系统设计阶段，注意对扩展开放，对修改闭合。\n说明：极端情况下，交付的代码是不可修改的，同一业务域内的需求变化，通过模块或类的扩展来实现。\n\n13. 【推荐】系统设计阶段，共性业务或公共行为抽取出来公共模块、公共配置、公共类、公共方法等，在系统中不出现重复代码的情况。\n说明：随着代码的重复次数不断增加，维护成本指数级上升。\n\n14. 【推荐】避免如下误解：敏捷开发=讲故事+编码+发布。\n说明：敏捷开发是快速交付迭代可用的系统，省略多余的设计方案，摒弃传统的审批流程，但核心关键点上的必要设计和文档沉淀是需要的。\n反例：某团队为了业务快速发展，敏捷成了产品经理催进度的借口，系统中均是勉强能运行但像面条一样的代码，可维护性和可扩展性极差，一年之后，不得不进行大规模重构，得不偿失。\n\n15. 【参考】设计文档的作用是明确需求、理顺逻辑、后期维护，次要目的用于指导编码。\n说明：避免为了设计而设计，系统设计文档有助于后期的系统维护和重构，所以设计结果需要进行分类归档保存。\n\n16. 【参考】可扩展性的本质是找到系统的变化点，并隔离变化点。\n说明：世间众多设计模式其实就是一种设计模式即隔离变化点的模式。\n正例：极致扩展性的标志，就是需求的新增，不会在原有代码交付物上进行任何形式的修改。\n\n17. 【参考】设计的本质就是识别和表达系统难点。\n说明：识别和表达完全是两回事，很多人错误地认为识别到系统难点在哪里，表达只是自然而然的事情，但是大家在设计评审中经常出现语焉不详，甚至是词不达意的情况。准确地表达系统难点需要具备如下能力：表达规则和表达工具的熟练性。抽象思维和总结能力的局限性。基础知识体系的完备性。深入浅出的生动表达力。\n\n18. 【参考】代码即文档的观点是错误的，清晰的代码只是文档的某个片断，而不是全部。\n说明：代码的深度调用，模块层面上的依赖关系网，业务场景逻辑，非功能性需求等问题是需要相应的文档来完整地呈现的。\n\n19. 【参考】在做无障碍产品设计时，需要考虑到：\n- 所有可交互的控件元素必须能被tab键聚焦，并且焦点）1顺序需符合自然操作逻辑。\n- 用于登陆校验和请求拦截的验证码均需提供图形验证以外的其它方式。\n- 自定义的控件类型需明确交互方式。\n正例：用户登陆场景中，输入框的按钮都需要考虑tab键聚焦，符合自然逻辑的操作顺序如下，\"输入用户名，输入密码，输入验证码，点击登录\"，其中验证码实现语音验证方式。如果有自定义标签实现的控件设置控件类型可使用role属性。\n","categories":["Java"]},{"title":"Spring Security介绍","slug":"Spring Security介绍","url":"/blog/posts/56b099421a97/","content":"\n\n> [github地址](https://github.com/xmxe/spring-security)\n\n### HttpSecurity常用方法\n\n| [HttpSecurity常用方法](https://blog.csdn.net/qq_52006948/article/details/122729236)                | 说明                                                         |\n| ------------------- | ------------------------------------------------------------ |\n| openidLogin()       | 用于基于OpenId的验证                                       |\n| headers()           | 将安全标头添加到响应                                         |\n| cors()              | 配置跨域资源共享（CORS）                                   |\n| sessionManagement()| 允许配置会话管理                                             |\n| portMapper()        | 允许配置一个PortMapper(HttpSecurity#(getSharedObject(class)))，其他提供SecurityConfigurer的对象使用PortMapper从HTTP重定向到HTTPS或者从HTTPS重定向到HTTP。默认情况下，Spring Security使用一个PortMapperImpl映射HTTP端口8080到HTTPS端口8443，HTTP端口80到HTTPS端口443 |\n| jee()               | 配置基于容器的预认证。在这种情况下，认证由Servlet容器管理   |\n| x509()              | 配置基于x509的认证                                           |\n| rememberMe          | 允许配置“记住我”的验证                                       |\n| authorizeRequests() | 允许基于使用HttpServletRequest限制访问                       |\n| requestCache()      | 允许配置请求缓存                                             |\n| exceptionHandling() | 允许配置错误处理                                             |\n| securityContext()   | 在HttpServletRequests之间的SecurityContextHolder上设置SecurityContext的管理。当使用WebSecurityConfigurerAdapter时，这将自动应用|\n| servletApi()        | 将HttpServletRequest方法与在其上找到的值集成到SecurityContext中。当使用WebSecurityConfigurerAdapter时，这将自动应用|\n| csrf()              | 添加CSRF支持，使用WebSecurityConfigurerAdapter时，默认启用|\n| logout()            | 添加退出登录支持。当使用WebSecurityConfigurerAdapter时，这将自动应用。默认情况是，访问URL”/logout”，使HTTP Session无效来清除用户，清除已配置的任何#rememberMe()身份验证，清除SecurityContextHolder，然后重定向到”/login?success” |\n| anonymous()         | 允许配置匿名用户的表示方法。当与WebSecurityConfigurerAdapter结合使用时，这将自动应用。默认情况下，匿名用户将使用org.springframework.security.authentication.AnonymousAuthenticationToken表示，并包含角色“ROLE_ANONYMOUS” |\n| formLogin()         | 指定支持基于表单的身份验证。如果未指定FormLoginConfigurer#loginPage(String)，则将生成默认登录页面 |\n| oauth2Login()       | 根据外部OAuth 2.0或OpenID Connect1.0提供程序配置身份验证    |\n| requiresChannel()   | 配置通道安全。为了使该配置有用，必须提供至少一个到所需信道的映射 |\n| httpBasic()         | 配置Http Basic验证                                         |\n| addFilterAt()       | 在指定的Filter类的位置添加过滤器                             |\n\n---\n\n### Spring Security认证流程\n\n1. Spring Security支持多种用户认证的方式，最常用的是基于用户名和密码的用户认证方式，其认证流程如下图所示：\n\n    ![](images/rzlc.png)\n\n2. “记住我”功能的认证流程如下图所示：\n\n    ![](images/rememberme.png)\n\n3. Spring Security的用户认证流程是由一系列的过滤器链来实现的，默认的关于用户认证的过滤器链大致如下图所示：\n\n    ![](images/filterchain.png)\n\n\n| SpringSecurity采用的是责任链的设计模式， | 它有一条很长的过滤器链。                                     |\n| ----------------------------------------- | ------------------------------------------------------------ |\n| WebAsyncManagerIntegrationFilter          | 将Security上下文与Spring Web中用于处理异步请求映射的WebAsyncManager进行集成。 |\n| SecurityContextPersistenceFilter          | 在每次请求处理之前将该请求相关的安全上下文信息加载到SecurityContextHolder中，然后在该次请求处理完成之后，将SecurityContextHolder中关于这次请求的信息存储到一个“仓储”中，然后将SecurityContextHolder中的信息清除，例如在Session中维护一个用户的安全信息就是这个过滤器处理的。 |\n| HeaderWriterFilter                        | 用于将头信息加入响应中                                       |\n| CsrfFilter                                | 用于处理跨站请求伪造                                         |\n| LogoutFilter                              | 用于处理退出登录                                             |\n| UsernamePasswordAuthenticationFilter      | 用于处理基于表单的登录请求，从表单中获取用户名和密码。默认情况下处理来自/login的请求。从表单中获取用户名和密码时，默认使用的表单name值为username和password，这两个值可以通过设置这个过滤器的usernameParameter和passwordParameter两个参数的值进行修改 |\n| DefaultLoginPageGeneratingFilter          | 如果没有配置登录页面，那系统初始化时就会配置这个过滤器，并且用于在需要进行登录时生成一个登录表单页面。 |\n| BasicAuthenticationFilter                 | 检测和处理http basic认证                                   |\n| RequestCacheAwareFilter                   | 用来处理请求的缓存                                           |\n| SecurityContextHolderAwareRequestFilter   | 主要是包装请求对象request                                   |\n| AnonymousAuthenticationFilter             | 检测SecurityContextHolder中是否存在Authentication对象，如果不存在为其提供一个匿名Authentication |\n| SessionManagementFilter                   | 管理session的过滤器                                        |\n| ExceptionTranslationFilter                | 处理AccessDeniedException和AuthenticationException异常。  |\n| FilterSecurityInterceptor                 | 可以看做过滤器链的出口                                       |\n| RememberMeAuthenticationFilter            | 当用户没有登录而直接访问资源时,从cookie里找出用户的信息,如果Spring Security能够识别出用户提供的remember me cookie,用户将不必填写用户名和密码,而是直接登录进入系统，该过滤器默认不开启。|\n\n> [spring security的认证和授权流程](https://blog.csdn.net/u011066470/article/details/119086893)\n> [一文带你搞懂Spring Security认证授权流程](https://zhuanlan.zhihu.com/p/502290821)\n\n---\n\n### Spring Security登录流程\n\n1. 用户在页面输入账户密码并提交。\n2. UsernamePasswordAuthenticationFilter拦截认证请求并获取用户输入的账号和密码，\n然后创建一个未认证的Authentication对象并交给AuthenticationManager进行认证。\n3. AuthenticationManager调用相应的AuthenticationProvider对象对未认证的Authentication对象进行认证。\n4. AuthenticationProvider从未认证的Authentication对象中获取用户输入的账号，并调用UserDetailsService对象查询该账号的正确信息，然后检查用户输入的账号信息与正确的账号信息是否相同--(UserDetailsService查询用户所输入的账号所对应的正确的密码和角色等信息并封装成UserDetails对象，然后返回给AuthenticationProvider进行认证)，如果不相同则认证失败并返回，如果相同认证成功并创建一个已认证的Authentication对象。\n调用链\nAuthenticationManager.authenticate()-->ProviderManager.authenticate()-->DaoAuthenticationProvider(AbstractUserDetailsAuthenticationProvider).authenticate()处理。\n在最后的authenticate()⽅法中，调⽤了UserDetailsService.loadUserByUsername()并进⾏了密码校验，校验成功就构造⼀个认证过的UsernamePasswordAuthenticationToken对象放⼊SecurityContext。\n5. SecurityContext将已认证的Authentication对象保存在Spring Security上下文环境中表示用户已认证。\n\n\n---\n\n### 相关文章\n\n> Spring Security系列～\n> - [文章案例地址](https://github.com/lenve/spring-security-samples)\n> - [Spring Security系列文章](https://mp.weixin.qq.com/s/2sVZxZDXP_dq-YgS86u4DQ)\n> - [离线 PDF 地址](https://pan.baidu.com/s/1PQPSWZONgo3_4TYOSjV8Vw?pwd=6p63)提取码: 6p63\n> - [Spring Security5.x教程合集(江南一点雨)](http://www.javaboy.org/springsecurity/)\n\n\n- [官方文档](https://docs.spring.io/spring-security/reference/)\n- [SpringBoot安全认证Security](https://zhuanlan.zhihu.com/p/67519928)\n- [单点登陆注解@EnableOAuth2Sso](https://mp.weixin.qq.com/s?__biz=MzI1NDY0MTkzNQ==&mid=2247488278&idx=1&sn=b21345a1daa86dd48ea89cdb9138def8&scene=21#wechat_redirect)\n- [Spring Security中的权限注解很神奇吗?(@PreAuthorize)](https://mp.weixin.qq.com/s/TaPlws-ZLTDUnffuiw-r1Q)\n- [想要控制好权限，这八个注解你必须知道](https://mp.weixin.qq.com/s/1NlWRwiBs8dl3Lu40haz5Q)\n- [进入SpringBoot2.7，有一个重要的类过期了](https://mp.weixin.qq.com/s/WFbcvzqIM2muK3Ha4B0a3w)\n- [新版SpringSecurity如何自定义JSON登录](https://mp.weixin.qq.com/s/KKdhQNrNfALzcHxyMFsqAw)\n- [Spring Security中，想在权限中使用通配符，怎么做？](https://mp.weixin.qq.com/s/2Ci_Xg8wTrRcEnjgt18CDw)\n- [如何在项目中自定义权限表达式](https://mp.weixin.qq.com/s/NTyYPGOSF9NobwtHas97sA)\n- [权限想要细化到按钮，怎么做？](https://mp.weixin.qq.com/s/g_8UprUi6cX70q4kTv4W9g)\n- [Spring Security用户认证和权限控制（默认实现）](https://blog.csdn.net/weixin_44516305/article/details/87860966)\n- [Spring Security用户认证和权限控制（自定义实现）](https://blog.csdn.net/weixin_44516305/article/details/88868791)\n- [Spring Security动态权限实现方案！](https://mp.weixin.qq.com/s/Bau8poOA4fMh3DNb9GaR1A)\n- [Spring Security最佳实践，看了必懂！※](https://mp.weixin.qq.com/s/rJqXpL7Zy9q_TU5B_E7nSw)\n- [认证和授权：前后端分离状态下使用Spring Security实现安全访问控制](https://mp.weixin.qq.com/s/kB2SViBlcKwl2cV91FbidQ)\n- [Spring Security中的RememberMe登录](https://mp.weixin.qq.com/s/ZWwlheacryW5Zmo-Xw269g)\n- [一文带你搞懂Spring Security](https://mp.weixin.qq.com/s/3L-RTpvB9Q248_g7fBtDVA)\n- [盘点Spring Security框架中的八大经典设计模式](https://mp.weixin.qq.com/s/xZrsy7bp8xN3EJblz7TLHw)\n- [Spring Security6全新写法，大变样！](https://mp.weixin.qq.com/s/RXNt1M3KND4aJbr0ZhHing)\n- [最新版Spring Security，该如何实现动态权限管理？](https://mp.weixin.qq.com/s/DrwmgouVzWvJPpnvWJuxhQ)","categories":["Spring"]},{"title":"Redis详解","slug":"Redis详解","url":"/blog/posts/a31926113ddc/","content":"\n## 3种常用的缓存读写策略详解\n\n### Cache Aside Pattern（旁路缓存模式）\n\nCache Aside Pattern是我们平时使用比较多的一个缓存读写模式，比较适合读请求比较多的场景。Cache Aside Pattern中服务端需要同时维系db和cache，并且是以db的结果为准。下面我们来看一下这个策略模式下的缓存读写步骤。\n\n**写**：\n\n- 先更新db\n- 然后直接删除cache。\n\n简单画了一张图帮助大家理解写的步骤。\n\n![img](https://oss.javaguide.cn/github/javaguide/database/redis/cache-aside-write.png)\n\n**读**：\n\n- 从cache中读取数据，读取到就直接返回\n- cache中读取不到的话，就从db中读取数据返回\n- 再把数据放到cache中。\n\n简单画了一张图帮助大家理解读的步骤。\n\n![img](https://oss.javaguide.cn/github/javaguide/database/redis/cache-aside-read.png)\n\n你仅仅了解了上面这些内容的话是远远不够的，我们还要搞懂其中的原理。比如说面试官很可能会追问：**在写数据的过程中，可以先删除cache，后更新db么？答案：那肯定是不行的！因为这样可能会造成数据库（db）和缓存（Cache）数据不一致的问题**。\n\n举例：请求1先写数据A，请求2随后读数据A的话，就很有可能产生数据不一致性的问题。这个过程可以简单描述为：\n\n> 请求1先把cache中的A数据删除->请求2从db中读取数据->请求1再把db中的A数据更新\n\n当你这样回答之后，面试官可能会紧接着就追问：**在写数据的过程中，先更新db，后删除cache就没有问题了么？答案：理论上来说还是可能会出现数据不一致性的问题，不过概率非常小，因为缓存的写入速度是比数据库的写入速度快很多。**\n\n举例：请求1先读数据A，请求2随后写数据A，并且数据A在请求1请求之前不在缓存中的话，也有可能产生数据不一致性的问题。这个过程可以简单描述为：\n\n> 请求1从db读数据A->请求2更新db中的数据A（此时缓存中无数据A，故不用执行删除缓存操作）->请求1将数据A写入cache\n\n现在我们再来分析一下Cache Aside Pattern的缺陷。\n\n**缺陷1：首次请求数据一定不在cache的问题**\n\n解决办法：可以将热点数据可以提前放入cache中。\n\n**缺陷2：写操作比较频繁的话导致cache中的数据会被频繁被删除，这样会影响缓存命中率。**\n\n解决办法：\n\n- 数据库和缓存数据强一致场景：更新db的时候同样更新cache，不过我们需要加一个锁/分布式锁来保证更新cache的时候不存在线程安全问题。\n- 可以短暂地允许数据库和缓存数据不一致的场景：更新db的时候同样更新cache，但是给缓存加一个比较短的过期时间，这样的话就可以保证即使数据不一致的话影响也比较小。\n\n### Read/Write Through Pattern（读写穿透）\n\nRead/Write Through Pattern中服务端把cache视为主要数据存储，从中读取数据并将数据写入其中。cache服务负责将此数据读取和写入db，从而减轻了应用程序的职责。这种缓存读写策略小伙伴们应该也发现了在平时在开发过程中非常少见。抛去性能方面的影响，大概率是因为我们经常使用的分布式缓存Redis并没有提供cache将数据写入db的功能。\n\n**写（Write Through）**：\n\n- 先查cache，cache中不存在，直接更新db。\n- cache中存在，则先更新cache，然后cache服务自己更新db（**同步更新cache和db**）。\n\n简单画了一张图帮助大家理解写的步骤。\n\n![img](https://oss.javaguide.cn/github/javaguide/database/redis/write-through.png)\n\n**读（Read Through）**：\n\n- 从cache中读取数据，读取到就直接返回。\n- 读取不到的话，先从db加载，写入到cache后返回响应。\n\n简单画了一张图帮助大家理解读的步骤。\n\n![img](https://oss.javaguide.cn/github/javaguide/database/redis/read-through.png)\n\nRead-Through Pattern实际只是在Cache-Aside Pattern之上进行了封装。在Cache-Aside Pattern下，发生读请求的时候，如果cache中不存在对应的数据，是由客户端自己负责把数据写入cache，而Read Through Pattern则是cache服务自己来写入缓存的，这对客户端是透明的。和Cache Aside Pattern一样，Read-Through Pattern也有首次请求数据一定不再cache的问题，对于热点数据可以提前放入缓存中。\n\n### Write Behind Pattern（异步缓存写入）\n\nWrite Behind Pattern和Read/Write Through Pattern很相似，两者都是由cache服务来负责cache和db的读写。但是，两个又有很大的不同：**Read/Write Through是同步更新cache和db，而Write Behind则是只更新缓存，不直接更新db，而是改为异步批量的方式来更新db**。很明显，这种方式对数据一致性带来了更大的挑战，比如cache数据可能还没异步更新db的话，cache服务可能就就挂掉了。这种策略在我们平时开发过程中也非常非常少见，但是不代表它的应用场景少，比如消息队列中消息的异步写入磁盘、MySQL的Innodb Buffer Pool机制都用到了这种策略。Write Behind Pattern下db的写性能非常高，非常适合一些数据经常变化又对数据一致性要求没那么高的场景，比如浏览量、点赞量\n\n## Redis5种基本数据结构详解\n\nRedis共有5种基本数据结构：String（字符串）、List（列表）、Set（集合）、Hash（散列）、Zset（有序集合）。这5种数据结构是直接提供给用户使用的，是数据的保存形式，其底层实现主要依赖这8种数据结构：简单动态字符串（SDS）、LinkedList（双向链表）、HashTable（哈希表）、SkipList（跳跃表）、Intset（整数集合）、ZipList（压缩列表）、QuickList（快速列表）。Redis基本数据结构的底层数据结构实现如下：\n\n| String | List                         | Hash                | Set             | Zset              |\n| :----- | :--------------------------- | :------------------ | :-------------- | :---------------- |\n| SDS    | LinkedList/ZipList/QuickList | Hash Table、ZipList | ZipList、Intset | ZipList、SkipList |\n\nRedis3.2之前，List底层实现是LinkedList或者ZipList。Redis3.2之后，引入了LinkedList和ZipList的结合QuickList，List的底层实现变为QuickList。你可以在Redis官网上找到Redis数据结构非常详细的介绍：\n\n> [Redis Data Structures](https://redis.com/redis-enterprise/data-structures/)\n> [Redis Data types tutorial](https://redis.io/docs/manual/data-types/data-types-tutorial/)\n\n### String（字符串）\n\n#### 介绍\n\nString是Redis中最简单同时也是最常用的一个数据结构。String是一种二进制安全的数据结构，可以用来存储任何类型的数据比如字符串、整数、浮点数、图片（图片的base64编码或者解码或者图片的路径）、序列化后的对象。\n\n![img](https://oss.javaguide.cn/github/javaguide/database/redis/image-20220719124403897.png)\n\n虽然Redis是用C语言写的，但是Redis并没有使用C的字符串表示，而是自己构建了一种**简单动态字符串（Simple Dynamic String，SDS**）。相比于C的原生字符串，Redis的SDS不光可以保存文本数据还可以保存二进制数据，并且获取字符串长度复杂度为O(1)（C字符串为O(N)）,除此之外，Redis的SDS API是安全的，不会造成缓冲区溢出。\n\n#### 常用命令\n\n| 命令                           | 介绍                             |\n| ------------------------------ | -------------------------------- |\n| SET key value                  | 设置指定key的值                |\n| SETNX key value                | 只有在key不存在时设置key的值 |\n| GET key                        | 获取指定key的值                |\n| MSET key1 value1 key2 value2 … | 设置一个或多个指定key的值      |\n| MGET key1 key2 ...             | 获取一个或多个指定key的值      |\n| STRLEN key                     | 返回key所储存的字符串值的长度  |\n| INCR key                       | 将key中储存的数字值增一        |\n| DECR key                       | 将key中储存的数字值减一        |\n| EXISTS key                     | 判断指定key是否存在            |\n| DEL key（通用）                | 删除指定的key                   |\n| EXPIRE key seconds（通用）     | 给指定key设置过期时间          |\n\n> 更多Redis String命令以及详细使用指南，请查看Redis官网对应的[介绍](https://redis.io/commands/?group=string)。\n\n**基本操作**：\n\n\n```bash\n> SET key value\nOK\n> GET key\n\"value\"\n> EXISTS key\n(integer) 1\n> STRLEN key\n(integer) 5\n> DEL key\n(integer) 1\n> GET key\n(nil)\n```\n\n**批量设置**：\n\n\n```bash\n> MSET key1 value1 key2 value2\nOK\n> MGET key1 key2 # 批量获取多个key对应的value\n1) \"value1\"\n2) \"value2\"\n```\n\n**计数器（字符串的内容为整数的时候可以使用）：**\n\n```bash\n> SET number 1\nOK\n> INCR number #将key中储存的数字值增一\n(integer) 2\n> GET number\n\"2\"\n> DECR number # 将key中储存的数字值减一\n(integer) 1\n> GET number\n\"1\"\n```\n\n**设置过期时间（默认为永不过期）**：\n\n```bash\n> EXPIRE key 60\n(integer) 1\n> SETNX key 60 value # 设置值并设置过期时间\nOK\n> TTL key\n(integer) 56\n```\n\n#### 应用场景\n\n**需要存储常规数据的场景**\n\n- 举例：缓存session、token、图片地址、序列化后的对象(相比较于Hash存储更节省内存)。\n- 相关命令：SET、GET。\n\n**需要计数的场景**\n\n- 举例：用户单位时间的请求数（简单限流可以用到）、页面单位时间的访问数。\n- 相关命令：SET、GET、INCR、DECR。\n\n**分布式锁**\n\n利用SETNX key value命令可以实现一个最简易的分布式锁（存在一些缺陷，通常不建议这样实现分布式锁）。\n\n### List（列表）\n\n#### 介绍\n\nRedis中的List其实就是链表数据结构的实现。\n\n> 我在[线性数据结构:数组、链表、栈、队列](https://javaguide.cn/cs-basics/data-structure/linear-data-structure.html)这篇文章中详细介绍了链表这种数据结构，我这里就不多做介绍了。\n\n许多高级编程语言都内置了链表的实现比如Java中的LinkedList，但是C语言并没有实现链表，所以Redis实现了自己的链表数据结构。Redis的List的实现为一个**双向链表**，即可以支持反向查找和遍历，更方便操作，不过带来了部分额外的内存开销。\n\n![img](https://oss.javaguide.cn/github/javaguide/database/redis/image-20220719124413287.png)\n\n#### 常用命令\n\n| 命令                        | 介绍                                       |\n| --------------------------- | ------------------------------------------ |\n| RPUSH key value1 value2 ... | 在指定列表的尾部（右边）添加一个或多个元素 |\n| LPUSH key value1 value2 ... | 在指定列表的头部（左边）添加一个或多个元素 |\n| LSET key index value        | 将指定列表索引index位置的值设置为 value  |\n| LPOP key                    | 移除并获取指定列表的第一个元素(最左边)     |\n| RPOP key                    | 移除并获取指定列表的最后一个元素(最右边)   |\n| LLEN key                    | 获取列表元素数量                           |\n| LRANGE key start end        | 获取列表start和end之间的元素          |\n\n> 更多Redis List命令以及详细使用指南，请查看Redis官网对应的[介绍](https://redis.io/commands/?group=list)。\n\n**通过RPUSH/LPOP或者LPUSH/RPOP实现队列**：\n\n```bash\n> RPUSH myList value1\n(integer) 1\n> RPUSH myList value2 value3\n(integer) 3\n> LPOP myList\n\"value1\"\n> LRANGE myList 0 1\n1) \"value2\"\n2) \"value3\"\n> LRANGE myList 0 -1\n1) \"value2\"\n2) \"value3\"\n```\n\n**通过RPUSH/RPOP或者LPUSH/LPOP实现栈**：\n\n```bash\n> RPUSH myList2 value1 value2 value3\n(integer) 3\n> RPOP myList2 # 将list的头部(最右边)元素取出\n\"value3\"\n```\n\n我专门画了一个图方便大家理解RPUSH,LPOP,lpush,RPOP命令：\n\n![img](https://oss.javaguide.cn/github/javaguide/database/redis/redis-list.png)\n\n**通过LRANGE查看对应下标范围的列表元素**：\n\n\n\n```bash\n> RPUSH myList value1 value2 value3\n(integer) 3\n> LRANGE myList 0 1\n1) \"value1\"\n2) \"value2\"\n> LRANGE myList 0 -1\n1) \"value1\"\n2) \"value2\"\n3) \"value3\"\n```\n\n通过LRANGE命令，你可以基于List实现分页查询，性能非常高！\n\n**通过LLEN查看链表长度**：\n\n```bash\n> LLEN myList\n(integer) 3\n```\n\n#### 应用场景\n\n**信息流展示**\n\n- 举例：最新文章、最新动态。\n- 相关命令：LPUSH、LRANGE。\n\n**消息队列**\n\nRedis List数据结构可以用来做消息队列，只是功能过于简单且存在很多缺陷，不建议这样做。\n\n相对来说，Redis5.0新增加的一个数据结构Stream更适合做消息队列一些，只是功能依然非常简陋。和专业的消息队列相比，还是有很多欠缺的地方比如消息丢失和堆积问题不好解决。\n\n### Hash（哈希）\n\n#### 介绍\n\nRedis中的Hash是一个String类型的field-value（键值对）的映射表，特别适合用于存储对象，后续操作的时候，你可以直接修改这个对象中的某些字段的值。Hash类似于JDK1.8前的HashMap，内部实现也差不多(数组+链表)。不过，Redis的Hash做了更多优化。\n\n![img](https://oss.javaguide.cn/github/javaguide/database/redis/image-20220719124421703.png)\n\n#### 常用命令\n\n| 命令                                      | 介绍                                                     |\n| ----------------------------------------- | -------------------------------------------------------- |\n| HSET key field value                      | 设置指定哈希表中指定字段的值                             |\n| HSETNX key field value                    | 只有指定字段不存在时设置指定字段的值                     |\n| HMSET key field1 value1 field2 value2 ... | 同时将一个或多个field-value(域-值)对设置到指定哈希表中 |\n| HGET key field                            | 获取指定哈希表中指定字段的值                             |\n| HMGET key field1 field2 ...               | 获取指定哈希表中一个或者多个指定字段的值                 |\n| HGETALL key                               | 获取指定哈希表中所有的键值对                             |\n| HEXISTS key field                         | 查看指定哈希表中指定的字段是否存在                       |\n| HDEL key field1 field2 ...                | 删除一个或多个哈希表字段                                 |\n| HLEN key                                  | 获取指定哈希表中字段的数量                               |\n| HINCRBY key field increment               | 对指定哈希中的指定字段做运算操作（正数为加，负数为减）   |\n\n> 更多Redis Hash命令以及详细使用指南，请查看Redis官网对应的[介绍](https://redis.io/commands/?group=hash)。\n\n**模拟对象数据存储**：\n\n```bash\n> HMSET userInfoKey name \"guide\" description \"dev\" age 24\nOK\n> HEXISTS userInfoKey name # 查看key对应的value中指定的字段是否存在。\n(integer) 1\n> HGET userInfoKey name # 获取存储在哈希表中指定字段的值。\n\"guide\"\n> HGET userInfoKey age\n\"24\"\n> HGETALL userInfoKey # 获取在哈希表中指定key的所有字段和值\n1) \"name\"\n2) \"guide\"\n3) \"description\"\n4) \"dev\"\n5) \"age\"\n6) \"24\"\n> HSET userInfoKey name \"GuideGeGe\"\n> HGET userInfoKey name\n\"GuideGeGe\"\n> HINCRBY userInfoKey age 2\n(integer) 26\n```\n\n#### 应用场景\n\n**对象数据存储场景**\n\n- 举例：用户信息、商品信息、文章信息、购物车信息。\n- 相关命令：HSET（设置单个字段的值）、HMSET（设置多个字段的值）、HGET（获取单个字段的值）、HMGET（获取多个字段的值）。\n\n### Set（集合）\n\n#### 介绍\n\nRedis中的Set类型是一种无序集合，集合中的元素没有先后顺序但都唯一，有点类似于Java中的HashSet。当你需要存储一个列表数据，又不希望出现重复数据时，Set是一个很好的选择，并且Set提供了判断某个元素是否在一个Set集合内的重要接口，这个也是List所不能提供的。你可以基于Set轻易实现交集、并集、差集的操作，比如你可以将一个用户所有的关注人存在一个集合中，将其所有粉丝存在一个集合。这样的话，Set可以非常方便的实现如共同关注、共同粉丝、共同喜好等功能。这个过程也就是求交集的过程。\n\n![img](https://oss.javaguide.cn/github/javaguide/database/redis/image-20220719124430264.png)\n\n#### 常用命令\n\n| 命令                                  | 介绍                                      |\n| ------------------------------------- | ----------------------------------------- |\n| SADD key member1 member2 ...          | 向指定集合添加一个或多个元素              |\n| SMEMBERS key                          | 获取指定集合中的所有元素                  |\n| SCARD key                             | 获取指定集合的元素数量                    |\n| SISMEMBER key member                  | 判断指定元素是否在指定集合中              |\n| SINTER key1 key2 ...                  | 获取给定所有集合的交集                    |\n| SINTERSTORE destination key1 key2 ... | 将给定所有集合的交集存储在destination中 |\n| SUNION key1 key2 ...                  | 获取给定所有集合的并集                    |\n| SUNIONSTORE destination key1 key2 ... | 将给定所有集合的并集存储在destination中 |\n| SDIFF key1 key2 ...                   | 获取给定所有集合的差集                    |\n| SDIFFSTORE destination key1 key2 ...  | 将给定所有集合的差集存储在destination中 |\n| SPOP key count                        | 随机移除并获取指定集合中一个或多个元素    |\n| SRANDMEMBER key count                 | 随机获取指定集合中指定数量的元素          |\n\n> 更多Redis Set命令以及详细使用指南，请查看Redis官网对应的[介绍](https://redis.io/commands/?group=set)。\n\n**基本操作**：\n\n```bash\n> SADD mySet value1 value2\n(integer) 2\n> SADD mySet value1 # 不允许有重复元素，因此添加失败\n(integer) 0\n> SMEMBERS mySet\n1) \"value1\"\n2) \"value2\"\n> SCARD mySet\n(integer) 2\n> SISMEMBER mySet value1\n(integer) 1\n> SADD mySet2 value2 value3\n(integer) 2\n```\n\n- `mySet`:`value1`、`value2`。\n- `mySet2`：`value2`、`value3`。\n\n**求交集**：\n\n```bash\n> SINTERSTORE mySet3 mySet mySet2\n(integer) 1\n> SMEMBERS mySet3\n1) \"value2\"\n```\n\n**求并集**：\n\n```bash\n> SUNION mySet mySet2\n1) \"value3\"\n2) \"value2\"\n3) \"value1\"\n```\n\n**求差集**：\n\n```bash\n> SDIFF mySet mySet2 # 差集是由所有属于mySet但不属于A的元素组成的集合\n1) \"value1\"\n```\n\n#### 应用场景\n\n**需要存放的数据不能重复的场景**\n\n- 举例：网站UV统计（数据量巨大的场景还是HyperLogLog更适合一些）、文章点赞、动态点赞等场景。\n- 相关命令：SCARD（获取集合数量）。\n\n![img](https://oss.javaguide.cn/github/javaguide/database/redis/image-20220719073733851.png)\n\n**需要获取多个数据源交集、并集和差集的场景**\n\n- 举例：共同好友(交集)、共同粉丝(交集)、共同关注(交集)、好友推荐（差集）、音乐推荐（差集）、订阅号推荐（差集+交集）等场景。\n- 相关命令：SINTER（交集）、SINTERSTORE（交集）、SUNION（并集）、SUNIONSTORE（并集）、SDIFF（差集）、SDIFFSTORE（差集）。\n\n![img](https://oss.javaguide.cn/github/javaguide/database/redis/image-20220719074543513.png)\n\n**需要随机获取数据源中的元素的场景**\n\n- 举例：抽奖系统、随机。\n- 相关命令：SPOP（随机获取集合中的元素并移除，适合不允许重复中奖的场景）、SRANDMEMBER（随机获取集合中的元素，适合允许重复中奖的场景）。\n\n### Sorted Set（有序集合）\n\n#### 介绍\n\nSorted Set类似于Set，但和Set相比，Sorted Set增加了一个权重参数score，使得集合中的元素能够按score进行有序排列，还可以通过score的范围来获取元素的列表。有点像是Java中HashMap和TreeSet的结合体。\n\n![img](https://oss.javaguide.cn/github/javaguide/database/redis/image-20220719124437791.png)\n\n#### 常用命令\n\n| 命令                                          | 介绍                                                         |\n| --------------------------------------------- | ------------------------------------------------------------ |\n| ZADD key score1 member1 score2 member2 ...    | 向指定有序集合添加一个或多个元素                             |\n| ZCARD KEY                                     | 获取指定有序集合的元素数量                                   |\n| ZSCORE key member                             | 获取指定有序集合中指定元素的score值                        |\n| ZINTERSTORE destination numkeys key1 key2 ... | 将给定所有有序集合的交集存储在destination中，对相同元素对应的score值进行SUM聚合操作，numkeys为集合数量 |\n| ZUNIONSTORE destination numkeys key1 key2 ... | 求并集，其它和ZINTERSTORE类似                             |\n| ZDIFFSTORE destination numkeys key1 key2 ...  | 求差集，其它和ZINTERSTORE类似                              |\n| ZRANGE key start end                          | 获取指定有序集合start和end之间的元素（score从低到高）   |\n| ZREVRANGE key start end                       | 获取指定有序集合start和end之间的元素（score从高到底）   |\n| ZREVRANK key member                           | 获取指定有序集合中指定元素的排名(score从大到小排序)         |\n\n> 更多Redis Sorted Set命令以及详细使用指南，请查看Redis官网对应的[介绍](https://redis.io/commands/?group=sorted-set)。\n\n**基本操作**：\n\n```bash\n> ZADD myZset 2.0 value1 1.0 value2\n(integer) 2\n> ZCARD myZset\n2\n> ZSCORE myZset value1\n2.0\n> ZRANGE myZset 0 1\n1) \"value2\"\n2) \"value1\"\n> ZREVRANGE myZset 0 1\n1) \"value1\"\n2) \"value2\"\n> ZADD myZset2 4.0 value2 3.0 value3\n(integer) 2\n```\n\n- `myZset`:`value1`(2.0)、`value2`(1.0)。\n- `myZset2`：`value2`（4.0）、`value3`(3.0)。\n\n**获取指定元素的排名**：\n\n```bash\n> ZREVRANK myZset value1\n0\n> ZREVRANK myZset value2\n1\n```\n\n**求交集**：\n\n```bash\n> ZINTERSTORE myZset3 2 myZset myZset2\n1\n> ZRANGE myZset3 0 1 WITHSCORES\nvalue2\n5\n```\n\n**求并集**：\n\n```bash\n> ZUNIONSTORE myZset4 2 myZset myZset2\n3\n> ZRANGE myZset4 0 2 WITHSCORES\nvalue1\n2\nvalue3\n3\nvalue2\n5\n```\n\n**求差集**：\n\n```bash\n> ZDIFF 2 myZset myZset2 WITHSCORES\nvalue1\n2\n```\n\n#### 应用场景\n\n**需要随机获取数据源中的元素根据某个权重进行排序的场景**\n\n- 举例：各种排行榜比如直播间送礼物的排行榜、朋友圈的微信步数排行榜、王者荣耀中的段位排行榜、话题热度排行榜等等。\n- 相关命令：ZRANGE(从小到大排序)、ZREVRANGE（从大到小排序）、ZREVRANK(指定元素排名)。\n\n**需要存储的数据有优先级或者重要程度的场景**，比如优先级任务队列。\n\n- 举例：优先级任务队列。\n- 相关命令：ZRANGE(从小到大排序)、ZREVRANGE（从大到小排序）、ZREVRANK(指定元素排名)。\n\n## Redis3种特殊数据结构详解\n\n### Bitmap\n\n#### 介绍\n\nBitmap存储的是连续的二进制数字（0和1），通过Bitmap,只需要一个bit位来表示某个元素对应的值或者状态，key就是对应元素本身。我们知道8个bit可以组成一个byte，所以Bitmap本身会极大的节省储存空间。你可以将Bitmap看作是一个存储二进制数字（0和1）的数组，数组中每个元素的下标叫做offset（偏移量）。\n\n![img](https://oss.javaguide.cn/github/javaguide/database/redis/image-20220720194154133.png)\n\n#### 常用命令\n\n| 命令                                  | 介绍                                                         |\n| ------------------------------------- | ------------------------------------------------------------ |\n| SETBIT key offset value               | 设置指定offset位置的值                                     |\n| GETBIT key offset                     | 获取指定offset位置的值                                     |\n| BITCOUNT key start end                | 获取start和end之前值为1的元素个数                      |\n| BITOP operation destkey key1 key2 ... | 对一个或多个Bitmap进行运算，可用运算符有AND,OR,XOR以及NOT |\n\n**Bitmap基本操作演示**：\n\n```bash\n# SETBIT会返回之前位的值（默认是0）这里会生成7个位\n> SETBIT mykey 7 1\n(integer) 0\n> SETBIT mykey 7 0\n(integer) 1\n> GETBIT mykey 7\n(integer) 0\n> SETBIT mykey 6 1\n(integer) 0\n> SETBIT mykey 8 1\n(integer) 0\n# 通过bitcount统计被被设置为1的位的数量。\n> BITCOUNT mykey\n(integer) 2\n```\n\n#### 应用场景\n\n**需要保存状态信息（0/1即可表示）的场景**\n\n- 举例：用户签到情况、活跃用户情况、用户行为统计（比如是否点赞过某个视频）。\n- 相关命令：SETBIT、GETBIT、BITCOUNT、BITOP。\n\n### HyperLogLog\n\n#### 介绍\n\nHyperLogLog是一种有名的基数计数概率算法，基于LogLog Counting(LLC)优化改进得来，并不是Redis特有的，Redis只是实现了这个算法并提供了一些开箱即用的API。Redis提供的HyperLogLog占用空间非常非常小，只需要12k的空间就能存储接近2^64个不同元素。这是真的厉害，这就是数学的魅力么！并且，Redis对HyperLogLog的存储结构做了优化，采用两种方式计数：\n\n- **稀疏矩阵**：计数较少的时候，占用空间很小。\n- **稠密矩阵**：计数达到某个阈值的时候，占用12k的空间。\n\nRedis官方文档中有对应的详细说明：\n\n![img](https://oss.javaguide.cn/github/javaguide/database/redis/image-20220721091424563.png)\n\n基数计数概率算法为了节省内存并不会直接存储元数据，而是通过一定的概率统计方法预估基数值（集合中包含元素的个数）。因此，HyperLogLog的计数结果并不是一个精确值，存在一定的误差（标准误差为0.81%）。\n\n![img](https://oss.javaguide.cn/github/javaguide/database/redis/image-20220720194154133.png)\n\n> HyperLogLog的使用非常简单，但原理非常复杂。HyperLogLog的原理以及在Redis中的实现可以看这篇文章：[HyperLogLog算法的原理讲解以及Redis是如何应用它的](https://juejin.cn/post/6844903785744056333)。\n> 再推荐一个可以帮助理解HyperLogLog原理的工具：[Sketch of the Day: HyperLogLog — Cornerstone of a Big Data Infrastructure](http://content.research.neustar.biz/blog/hll.html)。\n\n#### 常用命令\n\nHyperLogLog相关的命令非常少，最常用的也就3个。\n\n| 命令                                      | 介绍                                                         |\n| ----------------------------------------- | ------------------------------------------------------------ |\n| PFADD key element1 element2 ...           | 添加一个或多个元素到HyperLogLog中                          |\n| PFCOUNT key1 key2                         | 获取一个或者多个HyperLogLog的唯一计数。                    |\n| PFMERGE destkey sourcekey1 sourcekey2 ... | 将多个HyperLogLog合并到destkey中，destkey会结合多个源，算出对应的唯一计数。|\n\n**HyperLogLog基本操作演示**：\n\n```bash\n> PFADD hll foo bar zap\n(integer) 1\n> PFADD hll zap zap zap\n(integer) 0\n> PFADD hll foo bar\n(integer) 0\n> PFCOUNT hll\n(integer) 3\n> PFADD some-other-hll 1 2 3\n(integer) 1\n> PFCOUNT hll some-other-hll\n(integer) 6\n> PFMERGE desthll hll some-other-hll\n\"OK\"\n> PFCOUNT desthll\n(integer) 6\n```\n\n#### 应用场景\n\n**数量量巨大（百万、千万级别以上）的计数场景**\n\n- 举例：热门网站每日/每周/每月访问ip数统计、热门帖子uv统计、\n- 相关命令：`PFADD`、`PFCOUNT`。\n\n### Geospatial index\n\n#### 介绍\n\nGeospatial index（地理空间索引，简称GEO）主要用于存储地理位置信息，基于Sorted Set实现。\n\n通过GEO我们可以轻松实现两个位置距离的计算、获取指定位置附近的元素等功能。\n\n![img](https://oss.javaguide.cn/github/javaguide/database/redis/image-20220720194359494.png)\n\n#### 常用命令\n\n| 命令                                             | 介绍                                                         |\n| ------------------------------------------------ | ------------------------------------------------------------ |\n| GEOADD key longitude1 latitude1 member1 ...      | 添加一个或多个元素对应的经纬度信息到GEO中                  |\n| GEOPOS key member1 member2 ...                   | 返回给定元素的经纬度信息                                     |\n| GEODIST key member1 member2 M/KM/FT/MI           | 返回两个给定元素之间的距离                                   |\n| GEORADIUS key longitude latitude radius distance | 获取指定位置附近distance范围内的其他元素，支持ASC(由近到远)、DESC（由远到近）、Count(数量)等参数|\n| GEORADIUSBYMEMBER key member radius distance     | 类似于GEORADIUS命令，只是参照的中心点是GEO中的元素       |\n\n**基本操作**：\n\n```bash\n> GEOADD personLocation 116.33 39.89 user1 116.34 39.90 user2 116.35 39.88 user3\n3\n> GEOPOS personLocation user1\n116.3299986720085144\n39.89000061669732844\n> GEODIST personLocation user1 user2 km\n1.4018\n```\n\n通过Redis可视化工具查看personLocation，果不其然，底层就是Sorted Set。\n\nGEO中存储的地理位置信息的经纬度数据通过GeoHash算法转换成了一个整数，这个整数作为Sorted Set的score(权重参数)使用。\n\n![img](https://oss.javaguide.cn/github/javaguide/database/redis/image-20220721201545147.png)\n\n**获取指定位置范围内的其他元素**：\n\n```bash\n> GEORADIUS personLocation 116.33 39.87 3 km\nuser3\nuser1\n> GEORADIUS personLocation 116.33 39.87 2 km\n> GEORADIUS personLocation 116.33 39.87 5 km\nuser3\nuser1\nuser2\n> GEORADIUSBYMEMBER personLocation user1 5 km\nuser3\nuser1\nuser2\n> GEORADIUSBYMEMBER personLocation user1 2 km\nuser1\nuser2\n```\n\n> GEORADIUS命令的底层原理解析可以看看阿里的这篇文章：[Redis到底是怎么实现“附近的人”这个功能的呢？](https://juejin.cn/post/6844903966061363207)。\n\n**移除元素**：\n\nGEO底层是Sorted Set，你可以对GEO使用Sorted Set相关的命令。\n\n```bash\n> ZREM personLocation user1\n1\n> ZRANGE personLocation 0 -1\nuser3\nuser2\n> ZSCORE personLocation user2\n4069879562983946\n```\n\n#### 应用场景\n\n**需要管理使用地理空间数据的场景**\n\n- 举例：附近的人。\n- 相关命令:GEOADD、GEORADIUS、GEORADIUSBYMEMBER\n\n## Redis事务\n\n### 如何使用Redis事务？\n\nRedis可以通过**MULTI，EXEC，DISCARD和WATCH**等命令来实现事务(transaction)功能。\n\n\n```bash\n> MULTI\nOK\n> SET PROJECT \"JavaGuide\"\nQUEUED\n> GET PROJECT\nQUEUED\n> EXEC\n1) OK\n2) \"JavaGuide\"\n```\n\n[MULTI](https://redis.io/commands/multi)命令后可以输入多个命令，Redis不会立即执行这些命令，而是将它们放到队列，当调用了[EXEC](https://redis.io/commands/exec)命令后，再执行所有的命令。这个过程是这样的：\n\n1. 开始事务（MULTI）；\n2. 命令入队(批量操作Redis的命令，先进先出（FIFO）的顺序执行)；\n3. 执行事务(EXEC)。\n\n你也可以通过[DISCARD](https://redis.io/commands/discard)命令取消一个事务，它会清空事务队列中保存的所有命令。\n\n```bash\n> MULTI\nOK\n> SET PROJECT \"JavaGuide\"\nQUEUED\n> GET PROJECT\nQUEUED\n> DISCARD\nOK\n```\n\n你可以通过[WATCH](https://redis.io/commands/watch)命令监听指定的Key，当调用EXEC命令执行事务时，如果一个被WATCH命令监视的Key被其他客户端/Session修改的话，整个事务都不会被执行。\n\n```bash\n# 客户端1\n> SET PROJECT \"RustGuide\"\nOK\n> WATCH PROJECT\nOK\n> MULTI\nOK\n> SET PROJECT \"JavaGuide\"\nQUEUED\n\n# 客户端2\n# 在客户端1执行EXEC命令提交事务之前修改PROJECT的值\n> SET PROJECT \"GoGuide\"\n\n# 客户端1\n# 修改失败，因为PROJECT的值被客户端2修改了\n> EXEC\n(nil)\n> GET PROJECT\n\"GoGuide\"\n```\n\n不过，如果WATCH与事务在同一个Session里，并且被**WATCH**监视的Key被修改的操作发生在事务内部，这个事务是可以被执行成功的\n\n>（相关issue：[WATCH命令碰到MULTI命令时的不同效果](https://github.com/Snailclimb/JavaGuide/issues/1714)）。\n\n事务内部修改WATCH监视的Key：\n\n\n```bash\n> SET PROJECT \"JavaGuide\"\nOK\n> WATCH PROJECT\nOK\n> MULTI\nOK\n> SET PROJECT \"JavaGuide1\"\nQUEUED\n> SET PROJECT \"JavaGuide2\"\nQUEUED\n> SET PROJECT \"JavaGuide3\"\nQUEUED\n> EXEC\n1) OK\n2) OK\n3) OK\n127.0.0.1:6379> GET PROJECT\n\"JavaGuide3\"\n```\n\n事务外部修改WATCH监视的Key：\n\n```bash\n> SET PROJECT \"JavaGuide\"\nOK\n> WATCH PROJECT\nOK\n> SET PROJECT \"JavaGuide2\"\nOK\n> MULTI\nOK\n> GET USER\nQUEUED\n> EXEC\n(nil)\n```\n\n> Redis官网相关介绍[https://redis.io/topics/transactions](https://redis.io/topics/transactions)\n\n### Redis事务支持原子性吗？\n\nRedis的事务和我们平时理解的关系型数据库的事务不同。我们知道事务具有四大特性：**原子性，隔离性，持久性，一致性**。\n\n1. **原子性（Atomicity）**：事务是最小的执行单位，不允许分割。事务的原子性确保动作要么全部完成，要么完全不起作用；\n2. **隔离性（Isolation）**：并发访问数据库时，一个用户的事务不被其他事务所干扰，各并发事务之间数据库是独立的；\n3. **持久性（Durability）**：一个事务被提交之后。它对数据库中数据的改变是持久的，即使数据库发生故障也不应该对其有任何影响。\n4. **一致性（Consistency）**：执行事务前后，数据保持一致，多个事务对同一个数据读取的结果是相同的；\n\nRedis事务在运行错误的情况下，除了执行过程中出现错误的命令外，其他命令都能正常执行。并且，Redis事务是不支持回滚（rollback）操作的。因此，Redis事务其实是不满足原子性的（而且不满足持久性）。\n\nRedis官网也解释了自己为啥不支持回滚。简单来说就是Redis开发者们觉得没必要支持回滚，这样更简单便捷并且性能更好。Redis开发者觉得即使命令执行错误也应该在开发过程中就被发现而不是生产过程中。\n\n![Redis为什么不支持回滚](https://oss.javaguide.cn/github/javaguide/database/redis/redis-rollback.png)\n\n你可以将Redis中的事务就理解为：**Redis事务提供了一种将多个命令请求打包的功能。然后，再按顺序执行打包的所有命令，并且不会被中途打断**。除了不满足原子性之外，事务中的每条命令都会与Redis服务器进行网络交互，这是比较浪费资源的行为。明明一次批量执行多个命令就可以了，这种操作实在是看不懂。因此，Redis事务是不建议在日常开发中使用的。\n\n> 相关issue:\n> - [issue452:关于Redis事务不满足原子性的问题](https://github.com/Snailclimb/JavaGuide/issues/452)。\n> - [Issue491:关于redis没有事务回滚？](https://github.com/Snailclimb/JavaGuide/issues/491)\n\n#### 如何解决Redis事务的缺陷？\n\nRedis从2.6版本开始支持执行Lua脚本，它的功能和事务非常类似。我们可以利用Lua脚本来批量执行多条Redis命令，这些Redis命令会被提交到Redis服务器一次性执行完成，大幅减小了网络开销。一段Lua脚本可以视作一条命令执行，一段Lua脚本执行过程中不会有其他脚本或Redis命令同时执行，保证了操作不会被其他指令插入或打扰。不过，如果Lua脚本运行时出错并中途结束，出错之后的命令是不会被执行的。并且，出错之前执行的命令是无法被撤销的，无法实现类似关系型数据库执行失败可以回滚的那种原子性效果。因此，**严格来说的话，通过Lua脚本来批量执行Redis命令实际也是不完全满足原子性的**。如果想要让Lua脚本中的命令全部执行，必须保证语句语法和命令都是对的。另外，Redis7.0新增了[Redis functions](https://redis.io/docs/manual/programmability/functions-intro/)特性，你可以将Redis functions看作是比Lua更强大的脚本。\n\n## Redis性能优化\n\n### 使用批量操作减少网络传输\n\n一个Redis命令的执行可以简化为以下4步：\n\n1. 发送命令\n2. 命令排队\n3. 命令执行\n4. 返回结果\n\n其中，第1步和第4步耗费时间之和称为**Round Trip Time（RTT,往返时间）**，也就是数据在网络上传输的时间。使用批量操作可以减少网络传输次数，进而有效减小网络开销，大幅减少RTT。\n\n#### 原生批量操作命令\n\nRedis中有一些原生支持批量操作的命令，比如：\n\n- mget(获取一个或多个指定key的值)、mset(设置一个或多个指定key的值)、\n- hmget(获取指定哈希表中一个或者多个指定字段的值)、hmset(同时将一个或多个field-value对设置到指定哈希表中)、\n- sadd（向指定集合添加一个或多个元素）\n- ......\n\n不过，在Redis官方提供的分片集群解决方案Redis Cluster下，使用这些原生批量操作命令可能会存在一些小问题需要解决。就比如说mget无法保证所有的key都在同一个**hash slot**（哈希槽）上，mget可能还是需要多次网络传输，原子操作也无法保证了。不过，相较于非批量操作，还是可以节省不少网络传输次数。整个步骤的简化版如下（通常由Redis客户端实现，无需我们自己再手动实现）：\n\n1. 找到key对应的所有hash slot；\n2. 分别向对应的Redis节点发起mget请求获取数据；\n3. 等待所有请求执行结束，重新组装结果数据，保持跟入参key的顺序一致，然后返回结果。\n\n如果想要解决这个多次网络传输的问题，比较常用的办法是自己维护key与slot的关系。不过这样不太灵活，虽然带来了性能提升，但同样让系统复杂性提升。\n\n> Redis Cluster并没有使用一致性哈希，采用的是**哈希槽分区**，每一个键值对都属于一个**hash slot**（哈希槽）。当客户端发送命令请求的时候，需要先根据key通过上面的计算公示找到的对应的哈希槽，然后再查询哈希槽和节点的映射关系，即可找到目标Redis节点。\n\n#### pipeline\n\n对于不支持批量操作的命令，我们可以利用pipeline(流水线)将一批Redis命令封装成一组，这些Redis命令会被一次性提交到Redis服务器，只需要一次网络传输。不过，需要注意控制一次批量操作的元素个数(例如500以内，实际也和元素字节数有关)，避免网络传输的数据量过大。与mget、mset等原生批量操作命令一样，pipeline同样在Redis Cluster上使用会存在一些小问题。原因类似，无法保证所有的key都在同一个hash slot（哈希槽）上。如果想要使用的话，客户端需要自己维护key与slot的关系。原生批量操作命令和pipeline的是有区别的，使用的时候需要注意：\n\n- 原生批量操作命令是原子操作，pipeline是非原子操作；\n- pipeline可以打包不同的命令，原生批量操作命令不可以；\n- 原生批量操作命令是Redis服务端支持实现的，而pipeline需要服务端和客户端的共同实现。\n\n另外，pipeline不适用于执行顺序有依赖关系的一批命令。就比如说，你需要将前一个命令的结果给后续的命令使用，pipeline就没办法满足你的需求了。对于这种需求，我们可以使用Lua脚本。\n\n#### Lua脚本\n\nLua脚本同样支持批量操作多条命令。一段Lua脚本可以视作一条命令执行，可以看作是原子操作。一段Lua脚本执行过程中不会有其他脚本或Redis命令同时执行，保证了操作不会被其他指令插入或打扰，这是pipeline所不具备的。并且，Lua脚本中支持一些简单的逻辑处理比如使用命令读取值并在Lua脚本中进行处理，这同样是pipeline所不具备的。不过，Redis Cluster下Lua脚本的原子操作也无法保证了，原因同样是无法保证所有的key都在同一个hashslot（哈希槽）上。\n\n### 大量key集中过期问题\n\n我在前面提到过：对于过期key，Redis采用的是**定期删除+惰性/懒汉式删除**策略。\n\n定期删除执行过程中，如果突然遇到大量过期key的话，客户端请求必须等待定期清理过期key任务线程执行完成，因为这个这个定期任务线程是在Redis主线程中执行的。这就导致客户端请求没办法被及时处理，响应速度会比较慢。如何解决呢？下面是两种常见的方法：\n\n1. 给key设置随机过期时间。\n2. 开启lazy-free（惰性删除/延迟释放）。lazy-free特性是Redis4.0开始引入的，指的是让Redis采用异步方式延迟释放key使用的内存，将该操作交给单独的子线程处理，避免阻塞主线程。\n\n个人建议不管是否开启lazy-free，我们都尽量给key设置随机过期时间。\n\n### Redis bigkey\n\n#### 什么是bigkey？\n\n简单来说，如果一个key对应的value所占用的内存比较大，那这个key就可以看作是bigkey。具体多大才算大呢？有一个不是特别精确的参考标准：string类型的value超过10kb，复合类型的value包含的元素超过5000个（对于复合类型的value来说，不一定包含的元素越多，占用的内存就越多）。\n\n#### bigkey有什么危害？\n\n除了会消耗更多的内存空间，bigkey对性能也会有比较大的影响。因此，我们应该尽量避免写入bigkey！\n\n#### 如何发现bigkey？\n\n**1、使用Redis自带的 --bigkeys参数来查找。**\n\n\n```bash\n# redis-cli -p 6379 --bigkeys\n\n# Scanning the entire keyspace to find biggest keys as well as\n# average sizes per key type.  You can use -i 0.1 to sleep 0.1 sec\n# per 100 SCAN commands (not usually needed).\n\n[00.00%] Biggest string found so far '\"ballcat:oauth:refresh_auth:f6cdb384-9a9d-4f2f-af01-dc3f28057c20\"' with 4437 bytes\n[00.00%] Biggest list   found so far '\"my-list\"' with 17 items\n\n-------- summary -------\n\nSampled 5 keys in the keyspace!\nTotal key length in bytes is 264 (avg len 52.80)\n\nBiggest   list found '\"my-list\"' has 17 items\nBiggest string found '\"ballcat:oauth:refresh_auth:f6cdb384-9a9d-4f2f-af01-dc3f28057c20\"' has 4437 bytes\n\n1 lists with 17 items (20.00% of keys, avg size 17.00)\n0 hashs with 0 fields (00.00% of keys, avg size 0.00)\n4 strings with 4831 bytes (80.00% of keys, avg size 1207.75)\n0 streams with 0 entries (00.00% of keys, avg size 0.00)\n0 sets with 0 members (00.00% of keys, avg size 0.00)\n0 zsets with 0 members (00.00% of keys, avg size 0.00\n```\n\n从这个命令的运行结果，我们可以看出：这个命令会扫描(Scan)Redis中的所有key，会对Redis的性能有一点影响。并且，这种方式只能找出每种数据结构top 1 bigkey（占用内存最大的string数据类型，包含元素最多的复合数据类型）。\n\n**2、分析RDB文件**\n\n通过分析RDB文件来找出bigkey。这种方案的前提是你的Redis采用的是RDB持久化。网上有现成的代码/工具可以直接拿来使用：\n\n- [redis-rdb-tools](https://github.com/sripathikrishnan/redis-rdb-tools)：Python语言写的用来分析Redis的RDB快照文件用的工具\n- [rdb_bigkeys](https://github.com/weiyanwei412/rdb_bigkeys):Go语言写的用来分析Redis的RDB快照文件用的工具，性能更好。\n\n### Redis内存碎片\n\n**相关问题**：\n\n1. 什么是内存碎片?为什么会有Redis内存碎片?\n2. 如何清理Redis内存碎片？\n\n> 参考答案：[Redis内存碎片详解](https://javaguide.cn/database/redis/redis-memory-fragmentation.html)。\n\n## Redis生产问题\n\n### 缓存穿透\n\n#### 什么是缓存穿透？\n\n缓存穿透说简单点就是大量请求的key是不合理的，**根本不存在于缓存中，也不存在于数据库中**。这就导致这些请求直接到了数据库上，根本没有经过缓存这一层，对数据库造成了巨大的压力，可能直接就被这么多请求弄宕机了。\n\n![缓存穿透](https://oss.javaguide.cn/github/javaguide/database/redis/redis-cache-penetration.png)\n\n举个例子：某个黑客故意制造一些非法的key发起大量请求，导致大量请求落到数据库，结果数据库上也没有查到对应的数据。也就是说这些请求最终都落到了数据库上，对数据库造成了巨大的压力。\n\n#### 有哪些解决办法？\n\n最基本的就是首先做好参数校验，一些不合法的参数请求直接抛出异常信息返回给客户端。比如查询的数据库id不能小于0、传入的邮箱格式不对的时候直接返回错误消息给客户端等等。\n\n**1）缓存无效key**\n\n如果缓存和数据库都查不到某个key的数据就写一个到Redis中去并设置过期时间，具体命令如下：`SET key value EX 10086`。这种方式可以解决请求的key变化不频繁的情况，如果黑客恶意攻击，每次构建不同的请求key，会导致Redis中缓存大量无效的key。很明显，这种方案并不能从根本上解决此问题。如果非要用这种方式来解决穿透问题的话，尽量将无效的key的过期时间设置短一点比如1分钟。\n\n另外，这里多说一嘴，一般情况下我们是这样设计key的：`表名:列名:主键名:主键值`。\n\n如果用Java代码展示的话，差不多是下面这样的：\n\n\n```java\npublic Object getObjectInclNullById(Integer id) {\n    // 从缓存中获取数据\n    Object cacheValue = cache.get(id);\n    // 缓存为空\n    if (cacheValue == null) {\n        // 从数据库中获取\n        Object storageValue = storage.get(key);\n        // 缓存空对象\n        cache.set(key, storageValue);\n        // 如果存储数据为空，需要设置一个过期时间(300秒)\n        if (storageValue == null) {\n            // 必须设置过期时间，否则有被攻击的风险\n            cache.expire(key, 60 * 5);\n        }\n        return storageValue;\n    }\n    return cacheValue;\n}\n```\n\n**2）布隆过滤器**\n\n布隆过滤器是一个非常神奇的数据结构，通过它我们可以非常方便地判断一个给定数据是否存在于海量数据中。我们需要的就是判断key是否合法，有没有感觉布隆过滤器就是我们想要找的那个“人”。\n\n具体是这样做的：把所有可能存在的请求的值都存放在布隆过滤器中，当用户请求过来，先判断用户发来的请求的值是否存在于布隆过滤器中。不存在的话，直接返回请求参数错误信息给客户端，存在的话才会走下面的流程。加入布隆过滤器之后的缓存处理流程图如下。\n\n![加入布隆过滤器之后的缓存处理流程图](https://oss.javaguide.cn/github/javaguide/database/redis/redis-cache-penetration-bloom-filter.png)\n\n但是，需要注意的是布隆过滤器可能会存在误判的情况。总结来说就是：**布隆过滤器说某个元素存在，小概率会误判。布隆过滤器说某个元素不在，那么这个元素一定不在。为什么会出现误判的情况呢?我们还要从布隆过滤器的原理来说**！我们先来看一下，当一个元素加入布隆过滤器中的时候，会进行哪些操作：\n\n1. 使用布隆过滤器中的哈希函数对元素值进行计算，得到哈希值（有几个哈希函数得到几个哈希值）。\n2. 根据得到的哈希值，在位数组中把对应下标的值置为1。\n\n我们再来看一下，当我们需要判断一个元素是否存在于布隆过滤器的时候，会进行哪些操作：\n\n1. 对给定元素再次进行相同的哈希计算；\n2. 得到值之后判断位数组中的每个元素是否都为1，如果值都为1，那么说明这个值在布隆过滤器中，如果存在一个值不为1，说明该元素不在布隆过滤器中。\n\n然后，一定会出现这样一种情况：**不同的字符串可能哈希出来的位置相同**。（可以适当增加位数组大小或者调整我们的哈希函数来降低概率）\n\n> 更多关于布隆过滤器的内容可以看我的这篇原创：[《不了解布隆过滤器？一文给你整的明明白白！》](https://javaguide.cn/cs-basics/data-structure/bloom-filter/)，强烈推荐，个人感觉网上应该找不到总结的这么明明白白的文章了。\n\n\n#### 总结\n\n缓存穿透是指查询一个一定不存在的数据。由于缓存不命中，并且出于容错考虑，如果从数据库查不到数据则不写入缓存，这将导致这个不存在的数据每次请求都要到数据库去查询，失去了缓存的意义。解决方案：\n1. 当我们从数据库找不到的时候，我们也将这个空对象设置到缓存里边去。下次再请求的时候，就可以从缓存里边获取了。\n这种情况我们一般会将空对象设置一个较短的过期时间。\n2. 布隆过滤器\n\n> [面试官：大量请求Redis不存在的数据，从而打倒数据库，你有什么方案？](https://mp.weixin.qq.com/s/soF3F8YYSbynK2lyofGMAg)\n\n### 缓存击穿\n\n#### 什么是缓存击穿？\n\n缓存击穿中，请求的key对应的是热点数据，该数据存在于数据库中，但不存在于缓存中（通常是因为缓存中的那份数据已经过期）。这就可能会导致瞬时大量的请求直接打到了数据库上，对数据库造成了巨大的压力，可能直接就被这么多请求弄宕机了。\n\n![缓存击穿](https://oss.javaguide.cn/github/javaguide/database/redis/redis-cache-breakdown.png)\n\n举个例子：秒杀进行过程中，缓存中的某个秒杀商品的数据突然过期，这就导致瞬时大量对该商品的请求直接落到数据库上，对数据库造成了巨大的压力。\n\n#### 有哪些解决办法？\n\n- 设置热点数据永不过期或者过期时间比较长。\n- 针对热点数据提前预热，将其存入缓存中并设置合理的过期时间比如秒杀场景下的数据在秒杀结束之前不过期。\n- 请求数据库写数据到缓存之前，先获取互斥锁，保证只有一个请求会落到数据库上，减少数据库的压力。\n\n#### 缓存穿透和缓存击穿有什么区别？\n\n缓存穿透中，请求的key既不存在于缓存中，也不存在于数据库中。\n\n缓存击穿中，请求的key对应的是热点数据，该数据存在于数据库中，但不存在于缓存中（通常是因为缓存中的那份数据已经过期）。\n\n#### 总结\n\n缓存击穿，就是说某个key非常热点，访问非常频繁，处于集中式高并发访问的情况，当这个key在失效的瞬间，大量的请求就击穿了缓存，直接请求数据库，就像是在一道屏障上凿开了一个洞。解决方案：\n1. 可以将热点数据设置为永远不过期\n2. 基于redis or zookeeper实现互斥锁，等待第一个请求构建完缓存之后，再释放锁，进而其它请求才能通过该key访问数据\n\n### 缓存雪崩\n\n#### 什么是缓存雪崩？\n\n实际上，缓存雪崩描述的就是这样一个简单的场景：**缓存在同一时间大面积的失效，导致大量的请求都直接落到了数据库上，对数据库造成了巨大的压力**。这就好比雪崩一样，摧枯拉朽之势，数据库的压力可想而知，可能直接就被这么多请求弄宕机了。另外，缓存服务宕机也会导致缓存雪崩现象，导致所有的请求都落到了数据库上。\n\n![缓存雪崩](https://oss.javaguide.cn/github/javaguide/database/redis/redis-cache-avalanche.png)\n\n举个例子：数据库中的大量数据在同一时间过期，这个时候突然有大量的请求需要访问这些过期的数据。这就导致大量的请求直接落到数据库上，对数据库造成了巨大的压力。\n\n#### 有哪些解决办法？\n\n**针对Redis服务不可用的情况：**\n\n1. 采用Redis集群，避免单机出现问题整个缓存服务都没办法使用。\n2. 限流，避免同时处理大量的请求。\n\n**针对热点缓存失效的情况：**\n\n1. 设置不同的失效时间比如随机设置缓存的失效时间。\n2. 缓存永不失效（不太推荐，实用性太差）。\n3. 设置二级缓存。\n\n#### 缓存雪崩和缓存击穿有什么区别？\n\n缓存雪崩和缓存击穿比较像，但缓存雪崩导致的原因是缓存中的大量或者所有数据失效，缓存击穿导致的原因主要是某个热点数据不存在与缓存中（通常是因为缓存中的那份数据已经过期）。\n\n#### 总结\n如果我们的缓存挂掉了，这意味着我们的全部请求都跑去数据库了。解决方案：\n1. 事发前：实现Redis的高可用(主从架构+Sentinel或者Redis Cluster)，尽量避免Redis挂掉这种情况发生。\n2. 事发中：万一Redis真的挂了，我们可以设置本地缓存(ehcache)+限流(hystrix)，尽量避免我们的数据库被干掉\n3. 事发后：redis持久化，重启后自动从磁盘上加载数据，快速恢复缓存数据\n\n> [Redis缓存击穿（失效）、缓存穿透、缓存雪崩怎么解决？](https://mp.weixin.qq.com/s/dig6ZcUzMvQZG0u-Bub2AQ)\n> [Redis缓存的常见异常及解决方案](https://mp.weixin.qq.com/s/pOUjLIgUSgbjoJ2KtfaQyw)\n> [常说的「缓存穿透」和「击穿」是什么](https://mp.weixin.qq.com/s/8cNZ2glJC6p3w0ogtgKQ-w)\n> [漫话：如何给女朋友解释什么是缓存穿透、缓存击穿、缓存雪崩](https://mp.weixin.qq.com/s/7h_IOg7RgR3bFscgbGnGFw)\n> [再也不怕，缓存雪崩、击穿、穿透！](https://mp.weixin.qq.com/s/rD6h874mPzUCRlE7l8CnUQ)\n> [一个Redis的雪崩和穿透问题](https://mp.weixin.qq.com/s/FDqctV8xun1fDxVmlFA45A)\n> [烂大街的缓存穿透、缓存击穿和缓存雪崩，你真的懂了？](https://mp.weixin.qq.com/s/5bz2-D-IglLHiwwmMLxohw)\n\n### 如何保证缓存和数据库数据的一致性？\n\n\n下面单独对**Cache Aside Pattern（旁路缓存模式）** 来聊聊。Cache Aside Pattern中遇到写请求是这样的：更新DB，然后直接删除cache。如果更新数据库成功，而删除缓存这一步失败的情况的话，简单说两个解决方案：\n\n1. **缓存失效时间变短（不推荐，治标不治本）**：我们让缓存数据的过期时间变短，这样的话缓存就会从数据库中加载数据。另外，这种解决办法对于先操作缓存后操作数据库的场景不适用。\n2. **增加cache更新重试机制（常用）**：如果cache服务当前不可用导致缓存删除失败的话，我们就隔一段时间进行重试，重试次数可以自己定。如果多次重试还是失败的话，我们可以把当前更新失败的key存入队列中，等缓存服务可用之后，再将缓存中对应的key删除即可。\n\n> 相关文章推荐：[缓存和数据库一致性问题，看这篇就够了-水滴与银弹](https://mp.weixin.qq.com/s?__biz=MzIyOTYxNDI5OA==&mid=2247487312&idx=1&sn=fa19566f5729d6598155b5c676eee62d&chksm=e8beb8e5dfc931f3e35655da9da0b61c79f2843101c130cf38996446975014f958a6481aacf1&scene=178&cur_album_id=1699766580538032128#rd)\n\n\n#### 总结\n\n读的时候先读缓存，缓存中没有数据的话就去数据库读取，然后再存入缓存中，同时返回响应。更新的时候，先更新数据库，然后再删除缓存。\n\n**双写一致方案**\n先删除缓存，后更新数据库：解决了缓存删除失败导致库与缓存不一致的问题，适用于并发量不高的业务场景。\n\n**缓存延时双删策略**\n在写库前后都进行Redis的删除操作，并且第二次删除通过延迟的方式进行,第一步：先删除缓存,第二步：再写入数据库,第三步：休眠xxx毫秒（根据具体的业务时间来定）,第四步：再次删除缓存。这种方案解决了高并发情况下，同时有读请求与写请求时导致的不一致问题。读取速度快，如果二次删除失败了，还是会导致缓存脏数据存在的；二次删除前面涉及到休眠，可能导致系统性能降低，可以采用异步的方式，再起一个线程来进行异步删除。在分布式系统中，缓存和数据库同时存在时，如果有写操作的时候，先操作数据库，再操作缓存。如下：\n\n1. 读取缓存中是否有相关数据\n2. 如果缓存中有相关数据value，则返回\n3. 如果缓存中没有相关数据，则从数据库读取相关数据放入缓存中key->value，再返回\n4. 如果有更新数据，则先更新数据，再删除缓存\n5. 为了保证第四步删除缓存成功，使用binlog异步删除\n6. 如果是主从数据库，binglog取自于从库\n7. 如果是一主多从，每个从库都要采集binlog，然后消费端收到最后一台binlog数据才删除缓存\n\n#### 相关文章\n\n- [如何保证缓存与数据库的双写一致性？](https://mp.weixin.qq.com/s/FldS8ynxoK8fD1QPVocnjA)\n- [如何保证数据库和缓存双写一致性？](https://mp.weixin.qq.com/s/1uJmVb_E980NWn_sCzM6mA)\n- [解决缓存和数据库双写数据一致性问题✳](https://blog.csdn.net/D812359/article/details/121645548)\n- [如何保证Redis缓存与数据库双写一致性？](https://mp.weixin.qq.com/s/5I4IQFYZDdeNulSZfEj79A)\n- [高并发场景下，到底先更新缓存还是先更新数据库？](https://mp.weixin.qq.com/s/JxdAEt4rfZp5KQwpxRPZlA)\n- [MySQL与Redis缓存的同步方案](https://mp.weixin.qq.com/s/_WCg3TDZCxRKPiVot1dG8Q)\n- [数据库跟缓存的双写一致性](https://mp.weixin.qq.com/s/bKtVlr9JZ1HCp1tgEEKENA)\n- [Redis和Mysql如何保证数据一致?](https://mp.weixin.qq.com/s/6WxbY-BOjX_5mwHNgoVl8g)\n- [高并发先操作数据库，还是先操作缓存？5个方案告诉你！](https://mp.weixin.qq.com/s/BwKTvDig4BjMqXb_O468Uw)\n- [Redis数据更新，是先更新数据库还是先更新缓存？](https://mp.weixin.qq.com/s/HVPsyNd7XxmxiVQ9Nza_TQ)\n- [缓存一致性方案](https://mp.weixin.qq.com/s/tpb6Xf4Vf6O6gxbZN3pWfA)\n- [如何保证mongodb和数据库双写数据一致性](https://mp.weixin.qq.com/s/RqQ2AyDZ5f8vvcP76TyuAA)\n\n## Redis的key过期策略\n\n我们在set key的时候，可以给它设置一个过期时间，比如expire key 60。指定这key60s后过期，60s后，redis是如何处理的？我们先来介绍几种过期策略：一般有**定时过期、惰性过期、定期过期**三种。\n\n### 定时过期\n\n每个设置过期时间的key都需要创建一个定时器，到过期时间就会立即对key进行清除。该策略可以立即清除过期的数据，对内存很友好；但是会占用大量的CPU资源去处理过期的数据，从而影响缓存的响应时间和吞吐量。\n\n### 惰性过期\n\n只有当访问一个key时，才会判断该key是否已过期，过期则清除。该策略可以最大化地节省CPU资源，却对内存非常不友好。极端情况可能出现大量的过期key没有再次被访问，从而不会被清除，占用大量内存。\n\n### 定期过期\n\n每隔一定的时间，会扫描一定数量的数据库的expires字典中一定数量的key，并清除其中已过期的key。该策略是前两者的一个折中方案。通过调整定时扫描的时间间隔和每次扫描的限定耗时，可以在不同情况下使得CPU和内存资源达到最优的平衡效果。expires字典会保存所有设置了过期时间的key的过期时间数据，其中，key是指向键空间中的某个键的指针，value是该键的毫秒精度的UNIX时间戳表示的过期时间。键空间是指该Redis集群中保存的所有键。\n\n### 总结\n\nRedis中同时使用了惰性过期和定期过期两种过期策略。假设Redis当前存放30万个key，并且都设置了过期时间，如果你每隔100ms就去检查这全部的key，CPU负载会特别高，最后可能会挂掉。因此，redis采取的是定期过期，每隔100ms就随机抽取一定数量的key来检查和删除的。但是呢，最后可能会有很多已经过期的key没被删除。这时候，redis采用惰性删除。在你获取某个key的时候，redis会检查一下，这个key如果设置了过期时间并且已经过期了，此时就会删除。但是，如果定期删除漏掉了很多过期的key，然后也没走惰性删除。就会有很多过期key积在内存内存，直接会导致内存爆的。或者有些时候，业务量大起来了，redis的key被大量使用，内存直接不够了，运维也忘记加大内存了。难道redis直接这样挂掉？不会的！Redis用8种内存淘汰策略保护自己~\n\n> [Redis过期key删除，那些不得不说的事情](https://mp.weixin.qq.com/s/iR8EgI9-p-BXjJEfTs3G7Q)\n> [Redis的过期数据会被立马删除么？](https://mp.weixin.qq.com/s/qJt0B9p0GeUkekK15xL-jw)\n\n## Redis内存淘汰策略\n\n1. **volatile-lru（least recently used）**：从已设置过期时间的数据集（server.db[i].expires）中挑选最近最少使用的数据淘汰(当内存不足以容纳新写入数据时，从设置了过期时间的key中使用LRU（最近最少使用）算法进行淘汰)\n2. **volatile-ttl**：从已设置过期时间的数据集（server.db[i].expires）中挑选将要过期的数据淘汰(当内存不足以容纳新写入数据时，在设置了过期时间的key中，根据过期时间进行淘汰，越早过期的优先被淘汰)\n3. **volatile-random**：从已设置过期时间的数据集（server.db[i].expires）中任意选择数据淘汰(当内存不足以容纳新写入数据时，从设置了过期时间的key中，随机淘汰数据)\n4. **allkeys-lru（least recently used）**：当内存不足以容纳新写入数据时，在键空间中，移除最近最少使用的key（这个是最常用的）(当内存不足以容纳新写入数据时，从所有key中使用LRU（最近最少使用）算法进行淘汰)\n5. **allkeys-random**：从数据集（server.db[i].dict）中任意选择数据淘汰(当内存不足以容纳新写入数据时，从所有key中随机淘汰数据)\n6. **no-eviction**：禁止驱逐数据，也就是说当内存不足以容纳新写入数据时，新写入操作会报错。这个应该没人使用吧！(默认策略，当内存不足以容纳新写入数据时，新写入操作会报错)\n\n4.0版本后增加以下两种：\n\n1. **volatile-lfu（least frequently used）**：从已设置过期时间的数据集（server.db[i].expires）中挑选最不经常使用的数据淘汰(当内存不足以容纳新写入数据时，在过期的key中，使用LFU（最少访问算法）进行删除key。)\n2. **allkeys-lfu（least frequently used）**：当内存不足以容纳新写入数据时，在键空间中，移除最不经常使用的key(当内存不足以容纳新写入数据时，从所有key中使用LFU算法进行淘汰)\n\n> [Redis内存满了怎么办？](https://mp.weixin.qq.com/s/-kKe_ss01CkLMRERyyjt1Q)\n> [内存耗尽后Redis会发生什么？](https://mp.weixin.qq.com/s/YqkVmIaDRV31-WrcW8K26g)\n\n\n## Redis持久化\n\n### AOF持久化\n\n**AOF（append only file）** 持久化，采用日志的形式来记录每个写操作，追加到AOF文件的末尾。Redis默认情况是不开启AOF的。重启时再重新执行AOF文件中的命令来恢复数据。它主要解决数据持久化的实时性问题。AOF是执行完命令后才记录日志的。为什么不先记录日志再执行命令呢？这是因为Redis在向AOF记录日志时，不会先对这些命令进行语法检查，如果先记录日志再执行命令，日志中可能记录了错误的命令，Redis使用日志回复数据时，可能会出错。正是因为执行完命令后才记录日志，所以不会阻塞当前的写操作。但是会存在两个风险：更执行完命令还没记录日志时，宕机了会导致数据丢失,AOF不会阻塞当前命令，但是可能会阻塞下一个操作。这两个风险最好的解决方案是折中\n\n#### 妙用AOF机制的三种写回策略(appendfsync)：\n\n- always:同步写回，每个子命令执行完，都立即将日志写回磁盘。\n- everysec:每个命令执行完，只是先把日志写到AOF内存缓冲区，每隔一秒同步到磁盘。\n- no:只是先把日志写到AOF内存缓冲区，有操作系统去决定何时写入磁盘。\n\nalways同步写回，可以基本保证数据不丢失，no策略则性能高但是数据可能会丢失，一般可以考虑折中选择everysec。如果接受的命令越来越多，AOF文件也会越来越大，文件过大还是会带来性能问题。日志文件过大怎么办呢？AOF重写机制！就是随着时间推移，AOF文件会有一些冗余的命令如：无效命令、过期数据的命令等等，AOF重写机制就是把它们合并为一个命令（类似批处理命令），从而达到精简压缩空间的目的。\n\n#### AOF重写会阻塞嘛？\nAOF日志是由主线程会写的，而重写则不一样，重写过程是由后台子进程bgrewriteaof完成。\n\n#### AOF优缺点\n\n- AOF的优点：数据的一致性和完整性更高，秒级数据丢失。\n- AOF的缺点：相同的数据集，AOF文件体积大于RDB文件。数据恢复也比较慢。\n\n\n### RDB\n\n因为AOF持久化方式，如果操作日志非常多的话，Redis恢复就很慢。有没有在宕机快速恢复的方法呢，有的，RDB！RDB，就是把内存数据以快照的形式保存到磁盘上。和AOF相比，它记录的是某一时刻的数据，，并不是操作。什么是快照?可以这样理解，给当前时刻的数据，拍一张照片，然后保存下来。RDB持久化，是指在指定的时间间隔内，执行指定次数的写操作，将内存中的数据集快照写入磁盘中，它是Redis默认的持久化方式。执行完操作后，在指定目录下会生成一个dump.rdb文件，Redis重启的时候，通过加载dump.rdb文件来恢复数据。RDB触发机制主要有以下几种：\n\n1. 手动触发：save(同步，会阻塞当前redis服务器)bgsave(异步，redis执行fork操作创建子进程)\n2. 自动触发：(save m n)m秒内数据集存在n次修改时，自动触发bgsave\n\nRDB通过bgsave命令的执行全量快照，可以避免阻塞主线程。basave命令会fork一个子进程，然后该子进程会负责创建RDB文件，而服务器进程会继续处理命令。请求快照时，数据能修改嘛？Redis接入操作系统的写时复制技术（copy-on-write，COW）,在执行快照的同时，正常处理写操作。虽然bgsave执行不会阻塞主线程，但是频繁执行全量快照也会带来性能开销。比如bgsave子进程需要通过fork操作从主线程创建出来，创建后不会阻塞主线程，但是创建过程是会阻塞主线程的。可以做增量快照。\n\n#### RDB优缺点\n\n- RDB的优点：与AOF相比，恢复大数据集的时候会更快，它适合大规模的数据恢复场景，如备份，全量复制等\n- RDB的缺点：没办法做到实时持久化/秒级持久化。\n\nRedis4.0开始支持RDB和AOF的混合持久化，就是内存快照以一定频率执行，两次快照之间，再使用AOF记录这期间的所有命令操作。\n\n### 如何选择RDB和AOF\n如果数据不能丢失，RDB和AOF混用。如果只作为缓存使用，可以承受几分钟的数据丢失的话，可以只使用RDB。如果只使用AOF，优先使用everysec的写回策略。\n\n#### 混合持久化\n既然RDB与AOF持久化都存在各自的缺点，那么有没有一种更好的持久化方式？接下来要介绍的是混合持久化。其实就是RDB与AOF的混合模式，这是Redis4之后新增的。\n1. 持久化方式\n混合持久化是通过aof-use-rdb-preamble参数来开启的。它的操作方式是这样的，在写入的时候先把数据以RDB的形式写入文件的开头，再将后续的写命令以AOF的格式追加到文件中。这样既能保证数据恢复时的速度，同时又能减少数据丢失的风险。\n2. 文件恢复\n那么混合持久化中是如何来进行数据恢复的呢？在Redis重启时，先加载RDB的内容，然后再重放增量AOF格式命令。这样就避免了AOF持久化时的全量加载，从而使加载速率得到大幅提升。\n\n\n### 相关文章\n\n- [同样是持久化，竟然有这么大的差别！](https://mp.weixin.qq.com/s/SVbVwHOAwL1RX0fa-rYGxg)\n- [如何让Redis更持久](https://mp.weixin.qq.com/s/G3ct5tWox5Qt4tLUDEpRuw)\n- [Redis宕机，数据丢了](https://mp.weixin.qq.com/s/RxhaZFnMAf7bAgYUtTGLuA)\n- [小伙用12张图讲明白了Redis持久化！](https://mp.weixin.qq.com/s/q7KEOA2Dy2Q5QpDX8FkPjg)\n- [彻底理解Redis的持久化和主从复制](https://mp.weixin.qq.com/s/5IBOKcoBxVoGSMrn3vBGOw)\n\n\n## Redis高可用\n\n\n### 哨兵\n\n主从模式中，一旦主节点由于故障不能提供服务，需要人工将从节点晋升为主节点，同时还要通知应用方更新主节点地址。显然，多数业务场景都不能接受这种故障处理方式。Redis从2.8开始正式提供了Redis Sentinel（哨兵）架构来解决这个问题。哨兵模式，由一个或多个Sentinel实例组成的Sentinel系统，它可以监视所有的Redis主节点和从节点，并在被监视的主节点进入下线状态时，自动将下线主服务器属下的某个从节点升级为新的主节点。但是呢，一个哨兵进程对Redis节点进行监控，就可能会出现问题（单点问题），因此，可以使用多个哨兵来进行监控Redis节点，并且各个哨兵之间还会进行监控。简单来说，哨兵模式就三个作用：\n\n- 发送命令，等待Redis服务器（包括主服务器和从服务器）返回监控其运行状态；\n- 哨兵监测到主节点宕机，会自动将从节点切换成主节点，然后通过发布订阅模式通知其他的从节点，修改配置文件，让它们切换主机；\n- 哨兵之间还会相互监控，从而达到高可用。\n\n#### 故障切换的过程是怎样的呢\n\n假设主服务器宕机，哨兵1先检测到这个结果，系统并不会马上进行failover过程，仅仅是哨兵1主观的认为主服务器不可用，这个现象成为主观下线。当后面的哨兵也检测到主服务器不可用，并且数量达到一定值时，那么哨兵之间就会进行一次投票，投票的结果由一个哨兵发起，进行failover操作。切换成功后，就会通过发布订阅模式，让各个哨兵把自己监控的从服务器实现切换主机，这个过程称为客观下线。这样对于客户端而言，一切都是透明的。哨兵的工作模式如下：每个Sentinel以每秒钟一次的频率向它所知的Master，Slave以及其他Sentinel实例发送一个PING命令。如果一个实例（instance）距离最后一次有效回复PING命令的时间超过down-after-milliseconds选项所指定的值，则这个实例会被Sentinel标记为主观下线。如果一个Master被标记为主观下线，则正在监视这个Master的所有Sentinel要以每秒一次的频率确认Master的确进入了主观下线状态。当有足够数量的Sentinel（大于等于配置文件指定的值）在指定的时间范围内确认Master的确进入了主观下线状态，则Master会被标记为客观下线。在一般情况下，每个Sentinel会以每10秒一次的频率向它已知的所有Master，Slave发送INFO命令。当Master被Sentinel标记为客观下线时，Sentinel向下线的Master的所有Slave发送INFO命令的频率会从10秒一次改为每秒一次，若没有足够数量的Sentinel同意Master已经下线，Master的客观下线状态就会被移除；若Master重新向Sentinel的PING命令返回有效回复，Master的主观下线状态就会被移除。\n\n### Cluster集群\n\n哨兵解决和主从不能自动故障恢复的问题，但是同时也存在难以扩容以及单机存储、读写能力受限的问题，并且集群之前都是一台redis都是全量的数据，这样所有的redis都冗余一份，就会大大消耗内存空间。集群模式实现了Redis数据的分布式存储，实现数据的分片，每个redis节点存储不同的内容，并且解决了在线的节点收缩（下线）和扩容（上线）问题。集群模式真正意义上实现了系统的高可用和高性能，但是集群同时进一步使系统变得越来越复杂，接下来我们来详细的了解集群的运作原理。\n\n### 相关文章\n\n- [Redis主库宕机如何快速恢复](https://mp.weixin.qq.com/s/PA31mNTzlQ2EGYEXfWagXQ)\n- [Redis官方的高可用性解决方案](https://mp.weixin.qq.com/s?__biz=Mzg2MDYzODI5Nw==&mid=2247494325&idx=1&sn=0458cd40672c1f3918efb47963e56c9c&source=41#wechat_redirect)\n- [一文把Redis主从复制、哨兵、Cluster三种模式摸透](https://mp.weixin.qq.com/s/BPcis9rPiWosFid5w0M40A)\n- [Redis主从、哨兵、Cluster集群一锅端！](https://mp.weixin.qq.com/s/U_5Tla4_XzlJsq3uRI2ifA)\n- [Redis的主从复制是如何做的？复制过程中也会产生各种问题](https://mp.weixin.qq.com/s/I3GimkIf27DL1uRqxImKWA)\n- [Redis中主、从库宕机如何恢复？](https://mp.weixin.qq.com/s/oRcOPBHwbimFak6CtooHEg)\n- [Redis高可用篇：你管这叫主从架构数据同步原理？](https://mp.weixin.qq.com/s/NEUdCfRtHma3mkJqFKym5A)\n- [如何从0到1构建一个稳定、高性能的Redis集群？](https://mp.weixin.qq.com/s/ZXz2IzbQjQJzCq_hkpkuEg)\n- [4种Redis集群方案介绍+优缺点对比](https://mp.weixin.qq.com/s/Po85M418zvos3pHev2q0Tg)\n- [详细剖析Redis三种集群策略](https://mp.weixin.qq.com/s/M1RymGVUqhQG0KnKupnDXQ)\n- [一文掌握，单机Redis、哨兵和Redis Cluster的搭建](https://mp.weixin.qq.com/s/m9K6acUUc41j44b8zPh28w)\n\n## 阿里官方Redis开发规范\n\n### 键值设计\n\n1. key名设计\n\n- 可读性和可管理性\n以业务名(或数据库名)为前缀(防止key冲突)，用冒号分隔，比如业务名:表名:id - ugc:video:1\n- 简洁性\n保证语义的前提下，控制key的长度，当key较多时，内存占用也不容忽视，例如：user:{uid}:friends:messages:{mid}简化为u:{uid}:fr:m:{mid}。\n不要包含特殊字符\n反例：包含空格、换行、单双引号以及其他转义字符\n\n2. value设计\n拒绝bigkey\n防止网卡流量、慢查询，string类型控制在10KB以内，hash、list、set、zset元素个数不要超过5000。\n反例：一个包含200万个元素的list。\n非字符串的bigkey，不要使用del删除，使用hscan、sscan、zscan方式渐进式删除，同时要注意防止bigkey过期时间自动删除问题(例如一个200万的zset设置1小时过期，会触发del操作，造成阻塞，而且该操作不会不出现在慢查询中(latency可查))，查找方法和删除方法\n选择适合的数据类型\n例如：实体类型(要合理控制和使用数据结构内存编码优化配置,例如ziplist，但也要注意节省内存和性能之间的平衡)\n反例：\nset user:1:name tomset user:1:age 19set user:1:favor football\n正例：\nhmset user:1 name tom age 19 favor football\n控制key的生命周期\nredis不是垃圾桶，建议使用expire设置过期时间(条件允许可以打散过期时间，防止集中过期)，不过期的数据重点关注idletime。\n\n### 命令使用\n\n1. O(N)命令关注N的数量\n例如hgetall、lrange、smembers、zrange、sinter等并非不能使用，但是需要明确N的值。有遍历的需求可以使用hscan、sscan、zscan代替。\n2. 禁用命令\n禁止线上使用keys、flushall、flushdb等，通过redis的rename机制禁掉命令，或者使用scan的方式渐进式处理。\n3. 合理使用select\nredis的多数据库较弱，使用数字进行区分，很多客户端支持较差，同时多业务用多数据库实际还是单线程处理，会有干扰。\n4. 使用批量操作提高效率\n原生命令：例如mget、mset。非原生命令：可以使用pipeline提高效率。但要注意控制一次批量操作的元素个数(例如500以内，实际也和元素字节数有关)。\n注意两者不同：原生是原子操作，pipeline是非原子操作。pipeline可以打包不同的命令，原生做不到,pipeline需要客户端和服务端同时支持。\n5. 不建议过多使用Redis事务功能\nRedis的事务功能较弱(不支持回滚)，而且集群版本(自研和官方)要求一次事务操作的key必须在一个slot上(可以使用hashtag功能解决)\n6. Redis集群版本在使用Lua上有特殊要求\n①所有key都应该由KEYS数组来传递，redis.call/pcall里面调用的redis命令，key的位置，必须是KEYS array,否则直接返回error，\"-ERR bad lua script for redis cluster, all the keys that the script uses should be passed using the KEYS arrayrn\"\n②所有key，必须在1个slot上，否则直接返回error, \"-ERR eval/evalsha command keys must in same slotrn\"\n7. monitor命令\n必要情况下使用monitor命令时，要注意不要长时间使用。\n\n### 客户端使用\n\n1. 避免多个应用使用一个Redis实例:不相干的业务拆分，公共数据做服务化。\n2. 使用连接池:可以有效控制连接，同时提高效率，标准使用方式：\n执行命令如下：\n```java\nJedis jedis = null;\ntry {\n    jedis = jedisPool.getResource();\n    //具体的命令\n    jedis.executeCommand()\n} catch (Exception e) {\n    logger.error(\"op key {} error: \" + e.getMessage(), key, e);\n} finally {\n    //注意这里不是关闭连接，在JedisPool模式下，Jedis会被归还给资源池。\n    if (jedis != null)\n        jedis.close();\n}\n```\n\n3. 熔断功能:高并发下建议客户端添加熔断功能(例如netflix hystrix)\n4. 合理的加密:设置合理的密码，如有必要可以使用SSL加密访问（阿里云Redis支持）\n5. 淘汰策略\n根据自身业务类型，选好maxmemory-policy(最大内存淘汰策略)，设置好过期时间。\n默认策略是volatile-lru，即超过最大内存后，在过期键中使用lru算法进行key的剔除，保证不过期数据不被删除，但是可能会出现OOM问题。\n其他策略如下：\nallkeys-lru：根据LRU算法删除键，不管数据有没有设置超时属性，直到腾出足够空间为止。\nallkeys-random：随机删除所有键，直到腾出足够空间为止。\nvolatile-random:随机删除过期键，直到腾出足够空间为止。\nvolatile-ttl：根据键值对象的ttl属性，删除最近将要过期数据。如果没有，回退到noeviction策略。\nnoeviction：不会剔除任何数据，拒绝所有写入操作并返回客户端错误信息\"(error) OOM command not allowed when used memory\"，此时Redis只响应读操作。\n\n### 相关工具\n\n1. 数据同步:redis间数据同步可以使用：redis-port\n2. big key搜索:redis大key搜索工具\n3. 热点key寻找\n内部实现使用monitor，所以建议短时间使用facebook的redis-faina 阿里云Redis已经在内核层面解决热点key问题\n\n### 删除bigkey\n\n下面操作可以使用pipeline加速。\nredis4.0已经支持key的异步删除，欢迎使用。\n\n1. Hash删除: hscan + hdel\n\n```java\npublic void delBigHash(String host, int port, String password, String bigHashKey) {\n    Jedis jedis = new Jedis(host, port);\n    if (password != null && !\"\".equals(password)) {\n        jedis.auth(password);\n    }\n    ScanParams scanParams = new ScanParams().count(100);\n    String cursor = \"0\";\n    do {\n        ScanResult<Entry<String, String>> scanResult = jedis.hscan(bigHashKey, cursor, scanParams);\n        List<Entry<String, String>> entryList = scanResult.getResult();\n        if (entryList != null && !entryList.isEmpty()) {\n            for (Entry<String, String> entry : entryList) {\n                jedis.hdel(bigHashKey, entry.getKey());\n            }\n        }\n        cursor = scanResult.getStringCursor();\n    } while (!\"0\".equals(cursor));\n\n    //删除bigkey\n    jedis.del(bigHashKey);\n}\n\n```\n2. List删除: ltrim\n\n```java\npublic void delBigList(String host, int port, String password, String bigListKey) {\n    Jedis jedis = new Jedis(host, port);\n    if (password != null && !\"\".equals(password)) {\n        jedis.auth(password);\n    }\n    long llen = jedis.llen(bigListKey);\n    int counter = 0;\n    int left = 100;\n    while (counter < llen) {\n        //每次从左侧截掉100个\n        jedis.ltrim(bigListKey, left, llen);\n        counter += left;\n    }\n    //最终删除key\n    jedis.del(bigListKey);\n}\n```\n\n3. Set删除: sscan + srem\n\n```java\npublic void delBigSet(String host, int port, String password, String bigSetKey) {\n    Jedis jedis = new Jedis(host, port);\n    if (password != null && !\"\".equals(password)) {\n        jedis.auth(password);\n    }\n    ScanParams scanParams = new ScanParams().count(100);\n    String cursor = \"0\";\n    do {\n        ScanResult<String> scanResult = jedis.sscan(bigSetKey, cursor, scanParams);\n        List<String> memberList = scanResult.getResult();\n        if (memberList != null && !memberList.isEmpty()) {\n            for (String member : memberList) {\n                jedis.srem(bigSetKey, member);\n            }\n        }\n        cursor = scanResult.getStringCursor();\n    } while (!\"0\".equals(cursor));\n\n    //删除bigkey\n    jedis.del(bigSetKey);\n}\n```\n\n4. SortedSet删除:zscan + zrem\n\n```java\npublic void delBigZset(String host, int port, String password, String bigZsetKey) {\n    Jedis jedis = new Jedis(host, port);\n    if (password != null && !\"\".equals(password)) {\n        jedis.auth(password);\n    }\n    ScanParams scanParams = new ScanParams().count(100);\n    String cursor = \"0\";\n    do {\n        ScanResult<Tuple> scanResult = jedis.zscan(bigZsetKey, cursor, scanParams);\n        List<Tuple> tupleList = scanResult.getResult();\n        if (tupleList != null && !tupleList.isEmpty()) {\n            for (Tuple tuple : tupleList) {\n                jedis.zrem(bigZsetKey, tuple.getElement());\n            }\n        }\n        cursor = scanResult.getStringCursor();\n    } while (!\"0\".equals(cursor));\n    //删除bigkey\n    jedis.del(bigZsetKey);\n}\n```\n\n> [阿里云Redis开发规范](https://developer.aliyun.com/article/531067)","categories":["数据库"]},{"title":"Oauth2","slug":"Oauth2","url":"/blog/posts/8c045b869342/","content":"\n> demo\n> fork from [oauth2-samples](https://github.com/lenve/oauth2-samples)\n>> | Demo                     | 文章                                                                                  |\n>> | :----------------------- |:------------------------------------------------------------------------------------|\n>> | authorization_code       | [这个案例写出来，还怕跟面试官扯不明白OAuth2登录流程？](https://mp.weixin.qq.com/s/GXMQI59U6uzmS-C0WQ5iUw)  |\n>> | client_credentials       | [死磕OAuth2，教练我要学全套的！](https://mp.weixin.qq.com/s/33Oxu6YMjwco3WRE07_IiQ)             |\n>> | implicit                 | [死磕OAuth2，教练我要学全套的！](https://mp.weixin.qq.com/s/33Oxu6YMjwco3WRE07_IiQ)             |\n>> | password                 | [死磕OAuth2，教练我要学全套的！](https://mp.weixin.qq.com/s/33Oxu6YMjwco3WRE07_IiQ)             |\n>> | authorization_code_redis | [OAuth2令牌存入Redis](https://mp.weixin.qq.com/s/cGopy8hDPtkn8Q7HUYabbA)                |\n>> | jwt                      | [让OAuth2和JWT在一起愉快玩耍](https://mp.weixin.qq.com/s/xEIWTduDqQuGL7lfiP735w)             |\n>> | oauth2-sso               | [Spring Boot+OAuth2，一个注解搞定单点登录！](https://mp.weixin.qq.com/s/EyAMTbKPqNNnEtZACIsMVw) |\n>> | github_login             | [分分钟让自己的网站接入GitHub第三方登录功能](https://mp.weixin.qq.com/s/tq4Q306J3hJFEtGL1EpOBA)       |\n>>\n\n---\n\n### OAuth2.0的四种模式\n\n#### 授权码模式\n这种方式是最常用的流程，安全性也最高，它适用于那些有后端的Web应用。授权码通过前端传送，令牌则是储存在后端，而且所有与资源服务器的通信都在后端完成。这样的前后端分离，可以避免令牌泄漏。令牌获取的流程如下：\n\n![授权码模式](https://mmbiz.qpic.cn/mmbiz_png/19cc2hfD2rCpPJB83SvgzosiboTJxftAhibgOZffmU9RnmNUusomvBtoUKaxEXIU1df2icbUZOwSUeG4G0DxWgjtQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)\n上图中涉及到两个角色，分别是客户端、认证中心，客户端负责拿令牌，认证中心负责发放令牌。但是不是所有客户端都有权限请求令牌的，需要事先在认证中心申请，比如微信并不是所有网站都能直接接入，而是要去微信后台开通这个权限。至少要提前向认证中心申请的几个参数如下：\n\n- **client_id**：客户端唯一id，认证中心颁发的唯一标识\n- **client_secret**：客户端的秘钥，相当于密码\n- **scope**：客户端的权限\n- **redirect_uri**：授权码模式使用的跳转uri，需要事先告知认证中心。\n\n**请求授权码**\n\n客户端需要向认证中心拿到授权码，比如第三方登录使用微信，扫一扫登录那一步就是向微信的认证中心获取授权码。请求的url如下：\n```\n/oauth/authorize?client_id=&response_type=code&scope=&redirect_uri=\n```\n上述这个url中携带的几个参数如下：\n\n- **client_id**：客户端的id，这个由认证中心分配，并不是所有的客户端都能随意接入认证中心\n- **response_type**：固定值为code，表示要求返回授权码。\n- **scope**：表示要求的授权范围，客户端的权限\n- **redirect_uri**：跳转的uri，认证中心同意或者拒绝授权跳转的地址，如果同意会在uri后面携带一个code=xxx，这就是授权码\n\n**返回授权码**\n\n请求授权码之后，认证中心会要求登录、是否同意授权，用户同意授权之后直接跳转到redirect_uri（这个需要事先在认证中心申请配置），授权码会携带在这个地址后面，如下：\n\n```\nhttp://xxxx?code=NMoj5y\n```\n上述链接中的NMoj5y就是授权码了。\n\n**请求令牌**\n\n客户端拿到授权码之后，直接携带授权码发送请求给认证中心获取令牌，请求的url如下：\n```\n/oauth/token?\n client_id=&\n client_secret=&\n grant_type=authorization_code&\n code=NMoj5y&\n redirect_uri=\n```\n相同的参数同上，不同参数解析如下：\n\n- grant_type：授权类型，授权码固定的值为authorization_code\n- code：这个就是上一步获取的授权码\n\n**返回令牌**\n\n认证中心收到令牌请求之后，通过之后，会返回一段JSON数据，其中包含了令牌access_token，如下：\n\n```json\n{    \n  \"access_token\":\"ACCESS_TOKEN\",\n  \"token_type\":\"bearer\",\n  \"expires_in\":2592000,\n  \"refresh_token\":\"REFRESH_TOKEN\",\n  \"scope\":\"read\",\n  \"uid\":100101\n}\n```\naccess_token则是颁发的令牌，refresh_token是刷新令牌，一旦令牌失效则携带这个令牌进行刷新。\n\n#### 简化模式\n\n这种模式不常用，主要针对那些无后台的系统，直接通过web跳转授权，流程如下图：\n\n![简化模式](https://mmbiz.qpic.cn/mmbiz_png/19cc2hfD2rCpPJB83SvgzosiboTJxftAhxGsEsTxPIovmxbYqEqregcCE7o0h7fvcjkGSrdtXqUxFfs4EwqbegQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)\n\n这种方式把令牌直接传给前端，是很不安全的。因此，只能用于一些安全要求不高的场景，并且令牌的有效期必须非常短，通常就是会话期间（session）有效，浏览器关掉，令牌就失效了。\n\n**请求令牌**\n\n客户端直接请求令牌，请求的url如下：\n```\n/oauth/authorize?\n  response_type=token&\n  client_id=CLIENT_ID&\n  redirect_uri=CALLBACK_URL&\n  scope=\n```\n这个url正是授权码模式中获取授权码的url，各个参数解析如下：\n\n- **client_id**：客户端的唯一Id\n- **response_type**：简化模式的固定值为token\n- **scope**：客户端的权限\n- **redirect_uri**：跳转的uri，这里后面携带的直接是令牌，不是授权码了。\n\n**返回令牌**\n\n认证中心认证通过后，会跳转到redirect_uri，并且后面携带着令牌，链接如下：\n```\nhttps://xxxx#token=NPmdj5\n```\ntoken=NPmdj5这一段后面携带的就是认证中心携带的，令牌为NPmdj5。\n\n#### 密码模式\n\n密码模式也很简单，直接通过用户名、密码获取令牌，流程如下：\n![密码模式](https://mmbiz.qpic.cn/mmbiz_png/19cc2hfD2rCpPJB83SvgzosiboTJxftAhxGsEsTxPIovmxbYqEqregcCE7o0h7fvcjkGSrdtXqUxFfs4EwqbegQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)\n\n**请求令牌**\n\n认证中心要求客户端输入用户名、密码，认证成功则颁发令牌，请求的url如下：\n```\n/oauth/token?\n  grant_type=password&\n  username=&\n  password=&\n  client_id=&\n  client_secret=\n```\n参数解析如下：\n\n- **grant_type**：授权类型，密码模式固定值为password\n- **username**：用户名\n- **password**：密码\n- **client_id**：客户端id\n- **client_secret**：客户端的秘钥\n\n**返回令牌**\n\n上述认证通过，直接返回JSON数据，不需要跳转，如下：\n```json\n{\n  \"access_token\":\"ACCESS_TOKEN\",\n  \"token_type\":\"bearer\",\n  \"expires_in\":2592000,\n  \"refresh_token\":\"REFRESH_TOKEN\",\n  \"scope\":\"read\",\n  \"uid\":100101\n}\n```\naccess_token则是颁发的令牌，refresh_token是刷新令牌，一旦令牌失效则携带这个令牌进行刷新。\n\n#### 客户端模式\n\n适用于没有前端的命令行应用，即在命令行下请求令牌。\n这种方式给出的令牌，是针对第三方应用的，而不是针对用户的，即有可能多个用户共享同一个令牌。流程如下：\n\n![客户端模式](https://mmbiz.qpic.cn/mmbiz_png/19cc2hfD2rCpPJB83SvgzosiboTJxftAhxGsEsTxPIovmxbYqEqregcCE7o0h7fvcjkGSrdtXqUxFfs4EwqbegQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)\n\n**请求令牌**\n\n请求的url为如下：\n```\n/oauth/token?\ngrant_type=client_credentials&\nclient_id=&\nclient_secret=\n```\n参数解析如下：\n\n- **grant_type**：授权类型，客户端模式固定值为client_credentials\n- **client_id**：客户端id\n- **client_secret**：客户端秘钥\n\n**返回令牌**\n\n认证成功后直接返回令牌，格式为JSON数据，如下：\n```json\n{\n    \"access_token\": \"ACCESS_TOKEN\",\n    \"token_type\": \"bearer\",\n    \"expires_in\": 7200,\n    \"scope\": \"all\"\n}\n```\n\n### access_token VS refresh_token\n\n#### 介绍\n\nToken作为用户获取受保护资源的凭证，必须设置一个过期时间，否则一次登录便可永久使用，认证功能就失去了意义。但是矛盾在于：过期时间设置得太长，用户数据的安全性将大打折扣，过期时间设置得太短，用户就必须每隔一段时间重新登录，以获取新的凭证，这会极大挫伤用户的积极性。针对这一问题，我们可以利用Access / Refresh Token这一概念来平衡Token安全性和用户体验。\n\n众所周知,在OAuth 2.0授权协议中,有两个令牌token,分别是access_token和refresh_token。我们先看下面两者的介绍：access_token-访问令牌,它是一个用来访问受保护资源的凭证。refresh_token-刷新令牌,它是一个用来获取access token的凭证。这两个令牌的主要区别如下:\n\n- access_token时效短,refresh_token时效长,比如access_token有效期1个小时,refresh_token有效期1天。\n- access_token是授权服务器一定颁发的,而refresh_token却是可选的。\n- access_token过期后,可以使用refresh_token重新获取,而refresh_token过期后就只能重新授权了,也没有refresh_refresh_token。\n- access_token和资源服务器和授权服务器交互,而refresh_token只和授权服务器交互。\n- access_token颁发后可以直接使用,而使用refresh_token需要客户端秘钥client_secret。\n\n简单来说：Access Token即“访问令牌”，是客户端向资源服务器换取资源的凭证；Refresh Token即“刷新令牌”，是客户端向认证服务器换取Access Token的凭证。\n\n接下来,我们继续看两个令牌在下面场景的应用,假设有一个用户需要在后台管理界面上操作6个小时。\n\n1. 颁发一个有效性很长的access_token,比如6个小时,或者可以更长,这样用户只需要刚开始登录一次,access_token可以一直使用,直到access_token过期,然后重复,这种是不安全的,access_token的时效太长,也就失去了本身的意义。\n\n2. 颁发一个1小时有效期的access_token,过期后重新登录授权,这样用户需要登录6次,安全倒是有了,但是用户体验极差。\n\n3. 颁发1小时有效期的access_token和6小时有效期的refresh_token,当access_token过期后（或者快要过期的时候）,使用refresh_token获取一个新的access_token,直到refresh_token过期,用户重新登录,这样整个过程中，用户只需要登录一次,用户体验好。**access_token泄露了怎么办?没关系,它很快就会过期。refresh_token泄露了怎么办?没关系,使用refresh_token是需要客户端秘钥client_secret的**。\n\n4. 用户登录后,在后台管理页面上操作1个小时后,离开了一段时间,然后5个小时后,回到管理页面继续操作,此时refresh_token有效期6个小时,一直没有过期,也就可以换取新的access_token,用户在这个过程中,可以不用重复登录。但是在一些安全要求较高的系统中,第二次操作是需要重新登录的,即使refresh_token没有过期,因为中间有几个小时,用户是没有操作的,系统猜测用户已离开,并关闭会话。\n\n所以得出的结论是,refresh_token是一个很巧妙地设计,提升了用户体验的同时,又保证了安全性。另外,在OAuth 2.0安全最佳实践中,推荐refresh_token是一次性的,什么意思呢?使用refresh_token获取access_token时,同时会返回一个新的refresh_token,之前的refresh_token就会失效,但是两个refresh_token的绝对过期时间是一样的,所以不会存在refresh_token快过期就获取一个新的,然后重复,永不过期的情况。\n\n#### Access / Refresh Token如何使用？\n\n1. 用户提供身份信息（一般是用户名密码），利用客户端向认证服务器换取Refresh Token和Access Token；\n\n2. 客户端携带Access Token访问资源服务器，资源服务器识别Access Token并返回资源；\n\n3. 当Access Token过期或失效，客户端再一次访问资源服务器，资源服务器返回“无效token”报错；\n\n4. 客户端通过Refresh Token向认证服务器换取Access Token，认证服务器返回新的Access Token。\n\n用一个现实生活中的比喻来解释Access/Refresh Token的使用过程：假设我在网上预定了一家酒店。如果要入住这家酒店，我必须出示身份证和订单。酒店前台会登记相关证件和订单信息，确认无误后会给我一张票据和一张房卡（票据记录我需要入住多少天，而房卡则让我有当天的入住权）。以上场景中，“身份证和订单”是我的用户名密码，“票据/房卡”是Refresh/Access Token，“前台”是认证服务器，“房间”是资源服务器。在整个入住过程中，“身份证和订单”只在前台使用一次；实际能进入房间的是“房卡”，但是房卡只有一天的有效期；如果房卡过期，我需要凭“票据”去前台刷新“房卡”，获取第二天的入住权。将Token拆分成两个，就是为了解决安全性和用户体验方面的矛盾—Access Token使用频繁，且与用户数据直接关联，安全性方面比较敏感，因此有效期设置得较短，即使Access Token泄漏也将很快失效。利用过期时间较短这个特性，也可以及时更新用户的访问权限（比如管理员缩小了的某员工访问公司数据的权限，当Token过期后换取的新Access Token将立马缩小其访问数据的权限）。而Refresh Token仅用于获取新的Access Token，使用频率较低，不与用户数据直接关联，过期时间允许设置得长一些。这样就解决了用户反复登录的问题。\n\n\n### 相关文章\n\n- [妹子始终没搞懂OAuth 2.0，今天整合Spring Cloud Security一次说明白！](https://mp.weixin.qq.com/s/i8hvrKPSCwlzpmt_p52ZbA)\n- [快速接入GitHub、QQ第三方登录真有那么难吗？](https://mp.weixin.qq.com/s/l1vll9aSL1IzjsI-DhbtUw)\n- [Spring Security OAuth2整合企业微信扫码登录](https://mp.weixin.qq.com/s/S7NNeiPJAEtNQtypxrWcmw)\n- [实战：画了几张图，终于把OAuth2搞清楚了](https://mp.weixin.qq.com/s/r0H5zsm2H0AZui5nfeElRw)","categories":["技术栈"]},{"title":"ElasticSearch详解","slug":"ElasticSearch详解","url":"/blog/posts/38f9e76ec464/","content":"\n|                                                              | **[第1章 Elasticsearch概述](https://blog.csdn.net/u011863024/article/details/115721328#1_Elasticsearch_33)** |                                                              |\n| ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |\n| [01-开篇](https://blog.csdn.net/u011863024/article/details/115721328#01_35) | [02-技术选型](https://blog.csdn.net/u011863024/article/details/115721328#02_53) | [03-教学大纲](https://blog.csdn.net/u011863024/article/details/115721328#03_94) |\n|                                                              |                                                              |                                                              |\n|                                                              | **[第2章 Elasticsearch入门](https://blog.csdn.net/u011863024/article/details/115721328#2_Elasticsearch_104)** |                                                              |\n| [04-入门-环境准备](https://blog.csdn.net/u011863024/article/details/115721328#04_106) | [05-入门-RESTful & JSON](https://blog.csdn.net/u011863024/article/details/115721328#05RESTful__JSON_152) | [06-入门-Postman客户端工具](https://blog.csdn.net/u011863024/article/details/115721328#06Postman_163) |\n| [07-入门-倒排索引](https://blog.csdn.net/u011863024/article/details/115721328#07_172) | [08-入门-HTTP-索引-创建](https://blog.csdn.net/u011863024/article/details/115721328#08HTTP_195) | [09-入门-HTTP-索引-查询 & 删除](https://blog.csdn.net/u011863024/article/details/115721328#09HTTP___241) |\n| [10-入门-HTTP-文档-创建（Put & Post）](https://blog.csdn.net/u011863024/article/details/115721328#10HTTPPut__Post_316) | [11-入门-HTTP-查询-主键查询 & 全查询](https://blog.csdn.net/u011863024/article/details/115721328#11HTTP___392) | [12-入门-HTTP-全量修改 & 局部修改 & 删除](https://blog.csdn.net/u011863024/article/details/115721328#12HTTP_____485) |\n| [13-入门-HTTP-条件查询 & 分页查询 & 查询排序](https://blog.csdn.net/u011863024/article/details/115721328#13HTTP_____618) | [14-入门-HTTP-多条件查询 & 范围查询](https://blog.csdn.net/u011863024/article/details/115721328#14HTTP___1264) | [15-入门-HTTP-全文检索 & 完全匹配 & 高亮查询](https://blog.csdn.net/u011863024/article/details/115721328#15HTTP_____1522) |\n| [16-入门-HTTP-聚合查询](https://blog.csdn.net/u011863024/article/details/115721328#16HTTP_1810) | [17-入门-HTTP-映射关系](https://blog.csdn.net/u011863024/article/details/115721328#17HTTP_2049) | [18-入门-JavaAPI-环境准备](https://blog.csdn.net/u011863024/article/details/115721328#18JavaAPI_2369) |\n| [19-入门-JavaAPI-索引-创建](https://blog.csdn.net/u011863024/article/details/115721328#19JavaAPI_2441) | [20-入门-JavaAPI-索引-查询 & 删除](https://blog.csdn.net/u011863024/article/details/115721328#20JavaAPI___2486) | [21-入门-JavaAPI-文档-新增 & 修改](https://blog.csdn.net/u011863024/article/details/115721328#21JavaAPI___2569) |\n| [22-入门-JavaAPI-文档-查询 & 删除](https://blog.csdn.net/u011863024/article/details/115721328#22JavaAPI___2727) | [23-入门-JavaAPI-文档-批量新增 & 批量删除](https://blog.csdn.net/u011863024/article/details/115721328#23JavaAPI___2799) | [24-入门-JavaAPI-文档-高级查询-全量查询](https://blog.csdn.net/u011863024/article/details/115721328#24JavaAPI_2882) |\n| [25-入门-JavaAPI-文档-高级查询-分页查询 & 条件查询 & 查询排序](https://blog.csdn.net/u011863024/article/details/115721328#25JavaAPI_____2982) | [26-入门-JavaAPI-文档-高级查询-组合查询 & 范围查询](https://blog.csdn.net/u011863024/article/details/115721328#26JavaAPI___3173) | [27-入门-JavaAPI-文档-高级查询-模糊查询 & 高亮查询](https://blog.csdn.net/u011863024/article/details/115721328#27JavaAPI___3317) |\n| [28-入门-JavaAPI-文档-高级查询-最大值查询 & 分组查询](https://blog.csdn.net/u011863024/article/details/115721328#28JavaAPI___3477) |                                                              |                                                              |\n|                                                              |                                                              |                                                              |\n|                                                              | **[第3章 Elasticsearch环境](https://blog.csdn.net/u011863024/article/details/115721328#3_Elasticsearch_3587)** |                                                              |\n| [29-环境-简介](https://blog.csdn.net/u011863024/article/details/115721328#29_3589) | [30-环境-Windows集群部署](https://blog.csdn.net/u011863024/article/details/115721328#30Windows_3622) | [31-环境-Linux单节点部署](https://blog.csdn.net/u011863024/article/details/115721328#31Linux_3821) |\n| [32-环境-Linux集群部署](https://blog.csdn.net/u011863024/article/details/115721328#32Linux_3945) |                                                              |                                                              |\n|                                                              |                                                              |                                                              |\n|                                                              | **[第4章 Elasticsearch进阶](https://blog.csdn.net/u011863024/article/details/115721328#4_Elasticsearch_4073)** |                                                              |\n| [33-进阶-核心概念](https://blog.csdn.net/u011863024/article/details/115721328#33_4075) | [34-进阶-系统架构-简介](https://blog.csdn.net/u011863024/article/details/115721328#34_4156) | [35-进阶-单节点集群](https://blog.csdn.net/u011863024/article/details/115721328#35_4169) |\n| [36-进阶-故障转移](https://blog.csdn.net/u011863024/article/details/115721328#36_4209) | [37-进阶-水平扩容](https://blog.csdn.net/u011863024/article/details/115721328#37_4228) | [38-进阶-应对故障](https://blog.csdn.net/u011863024/article/details/115721328#38_4288) |\n| [39-进阶-路由计算 & 分片控制](https://blog.csdn.net/u011863024/article/details/115721328#39___4323) | [40-进阶-数据写流程](https://blog.csdn.net/u011863024/article/details/115721328#40_4352) | [41-进阶-数据读流程](https://blog.csdn.net/u011863024/article/details/115721328#41_4372) |\n| [42-进阶-更新流程 & 批量操作流程](https://blog.csdn.net/u011863024/article/details/115721328#42___4378) | [43-进阶-倒排索引](https://blog.csdn.net/u011863024/article/details/115721328#43_4428) | [44-进阶-文档搜索](https://blog.csdn.net/u011863024/article/details/115721328#44_4501) |\n| [45-进阶-文档刷新 & 文档刷写 & 文档合并](https://blog.csdn.net/u011863024/article/details/115721328#45_____4557) | [46-进阶-文档分析](https://blog.csdn.net/u011863024/article/details/115721328#46_4689) | [47-进阶-文档控制](https://blog.csdn.net/u011863024/article/details/115721328#47_5124) |\n| [48-进阶-文档展示-Kibana](https://blog.csdn.net/u011863024/article/details/115721328#48Kibana_5311) |                                                              |                                                              |\n|                                                              |                                                              |                                                              |\n|                                                              | **[第5章 Elasticsearch集成](https://blog.csdn.net/u011863024/article/details/115721328#5_Elasticsearch_5340)** |                                                              |\n| [49-框架集成-SpringData-整体介绍](https://blog.csdn.net/u011863024/article/details/115721328#49SpringData_5342) | [50-框架集成-SpringData-代码功能集成](https://blog.csdn.net/u011863024/article/details/115721328#50SpringData_5374) | [51-框架集成-SpringData-集成测试-索引操作](https://blog.csdn.net/u011863024/article/details/115721328#51SpringData_5598) |\n| [52-框架集成-SpringData-集成测试-文档操作](https://blog.csdn.net/u011863024/article/details/115721328#52SpringData_5639) | [53-框架集成-SpringData-集成测试-文档搜索](https://blog.csdn.net/u011863024/article/details/115721328#53SpringData_5749) | [54-框架集成-SparkStreaming-集成](https://blog.csdn.net/u011863024/article/details/115721328#54SparkStreaming_5800) |\n| [55-框架集成-Flink-集成](https://blog.csdn.net/u011863024/article/details/115721328#55Flink_5931) |                                                              |                                                              |\n|                                                              |                                                              |                                                              |\n|                                                              | **[第6章 Elasticsearch优化](https://blog.csdn.net/u011863024/article/details/115721328#6_Elasticsearch_6060)** |                                                              |\n| [56-优化-硬件选择](https://blog.csdn.net/u011863024/article/details/115721328#56_6062) | [57-优化-分片策略](https://blog.csdn.net/u011863024/article/details/115721328#57_6084) | [58-优化-路由选择](https://blog.csdn.net/u011863024/article/details/115721328#58_6119) |\n| [59-优化-写入速度优化](https://blog.csdn.net/u011863024/article/details/115721328#59_6140) | [60-优化-内存设置](https://blog.csdn.net/u011863024/article/details/115721328#60_6191) | [61-优化-重要配置](https://blog.csdn.net/u011863024/article/details/115721328#61_6214) |\n|                                                              |                                                              |                                                              |\n|                                                              | **[第7章 Elasticsearch面试题](https://blog.csdn.net/u011863024/article/details/115721328#7_Elasticsearch_6228)** |                                                              |\n| [62-面试题](https://blog.csdn.net/u011863024/article/details/115721328#62_6230) |                                                              |                                                              |\n\n# 第1章 [Elasticsearch](https://so.csdn.net/so/search?q=Elasticsearch&spm=1001.2101.3001.7020)概述\n\n## 01-开篇\n\n[教学视频](https://www.bilibili.com/video/BV1hh411D7sb)\n\n结构化数据\n\n![img](https://img-blog.csdnimg.cn/img_convert/a421a414ea8fb8e25c466add5b00d31f.png)\n\n非结构化数据\n\n![img](https://img-blog.csdnimg.cn/img_convert/109d3c62c1f36b4f352170488b516709.png)\n\n半结构化数据\n\n![img](https://img-blog.csdnimg.cn/img_convert/48742ba6bbdc47e0dae2754027f30a31.png)\n\n## 02-技术选型\n\n### Elasticsearch是什么\n\nThe Elastic Stack,包括Elasticsearch、Kibana、Beats和Logstash（也称为ELK Stack）。能够安全可靠地获取任何来源、任何格式的数据，然后实时地对数据进行搜索、分析和可视化。\n\nElaticsearch，简称为ES，ES是一个**开源的高扩展的分布式全文搜索引擎**，是整个ElasticStack技术栈的核心。\n\n它可以近乎实时的存储、检索数据；本身扩展性很好，可以扩展到上百台服务器，处理PB级别的数据。\n\n> elastic\n> 英 [ɪˈlæstɪk] 美 [ɪˈlæstɪk]\n> n. 橡皮圈(或带);松紧带\n> adj. 橡皮圈(或带)的;有弹性的;有弹力的;灵活的;可改变的;可伸缩的\n\n### 全文搜索引擎\n\nGoogle，百度类的网站搜索，它们都是根据网页中的关键字生成索引，我们在搜索的时候输入关键字，它们会将该关键字即索引匹配到的所有网页返回；还有常见的项目中应用日志的搜索等等。对于这些非结构化的数据文本，关系型数据库搜索不是能很好的支持。\n\n一般传统数据库，全文检索都实现的很鸡肋，因为一般也没人用数据库存文本字段。进行全文检索需要扫描整个表，如果数据量大的话即使对SQL的语法优化，也收效甚微。建立了索引，但是维护起来也很麻烦，对于insert和update操作都会重新构建索引。\n\n基于以上原因可以分析得出，在一些生产环境中，使用常规的搜索方式，性能是非常差的：\n\n- 搜索的数据对象是大量的非结构化的文本数据。\n- 文件记录量达到数十万或数百万个甚至更多。\n- 支持大量基于交互式文本的查询。\n- 需求非常灵活的全文搜索查询。\n- 对高度相关的搜索结果的有特殊需求，但是没有可用的关系数据库可以满足。\n- 对不同记录类型、非文本数据操作或安全事务处理的需求相对较少的情况。为了解决结构化数据搜索和非结构化数据搜索性能问题，我们就需要专业，健壮，强大的全文搜索引擎。\n\n**这里说到的全文搜索引擎指的是目前广泛应用的主流搜索引擎**。它的工作原理是计算机索引程序通过扫描文章中的每一个词，对每一个词建立一个索引，指明该词在文章中出现的次数和位置，当用户查询时，检索程序就根据事先建立的索引进行查找，并将查找的结果反馈给用户的检索方式。这个过程类似于通过字典中的检索字表查字的过程。\n\n### Elasticsearch应用案例\n\n- GitHub:2013年初，抛弃了Solr，采取Elasticsearch来做PB级的搜索。“GitHub使用Elasticsearch搜索20TB的数据，包括13亿文件和1300亿行代码”。\n- 维基百科：启动以Elasticsearch为基础的核心搜索架构\n- 百度：目前广泛使用Elasticsearch作为文本数据分析，采集百度所有服务器上的各类指标数据及用户自定义数据，通过对各种数据进行多维分析展示，辅助定位分析实例异常或业务层面异常。目前覆盖百度内部20多个业务线（包括云分析、网盟、预测、文库、直达号、钱包、风控等），单集群最大100台机器，200个ES节点，每天导入30TB+数据。\n- 新浪：使用Elasticsearch分析处理32亿条实时日志。\n- 阿里：使用Elasticsearch构建日志采集和分析体系。\n- Stack Overflow：解决Bug问题的网站，全英文，编程人员交流的网站。\n\n## 03-教学大纲\n\n- 第1章 Elasticsearch概述\n- 第2章 Elasticsearch入门\n- 第3章 Elasticsearch环境\n- 第4章 Elasticsearch进阶\n- 第5章 Elasticsearch集成\n- 第6章 Elasticsearch优化\n- 第7章 Elasticsearch面试题\n\n# 第2章 Elasticsearch入门\n\n## 04-入门-环境准备\n\n[官方网址](https://www.elastic.co/cn/)\n\n[官方文档](https://www.elastic.co/guide/index.html)\n\n[Elasticsearch 7.8.0下载页面](https://www.elastic.co/cn/downloads/past-releases/elasticsearch-7-8-0)\n\nWindows版的Elasticsearch压缩包，解压即安装完毕，解压后的Elasticsearch的目录结构如下 ：\n\n| 目录    | 含义           |\n| ------- | -------------- |\n| bin     | 可执行脚本目录 |\n| config  | 配置目录       |\n| jdk     | 内置 JDK 目录  |\n| lib     | 类库           |\n| logs    | 日志目录       |\n| modules | 模块目录       |\n| plugins | 插件目录       |\n\n解压后，进入bin文件目录，点击elasticsearch.bat文件启动ES服务。\n\n注意：9300端口为Elasticsearch集群间组件的通信端口，9200端口为浏览器访问的http协议[RESTful](https://so.csdn.net/so/search?q=RESTful&spm=1001.2101.3001.7020)端口。\n\n打开浏览器，输入地址：http://localhost:9200，测试返回结果，返回结果如下：\n\n```json\n{\n  \"name\" : \"DESKTOP-LNJQ0VF\",\n  \"cluster_name\" : \"elasticsearch\",\n  \"cluster_uuid\" : \"nCZqBhfdT1-pw8Yas4QU9w\",\n  \"version\" : {\n    \"number\" : \"7.8.0\",\n    \"build_flavor\" : \"default\",\n    \"build_type\" : \"zip\",\n    \"build_hash\" : \"757314695644ea9a1dc2fecd26d1a43856725e65\",\n    \"build_date\" : \"2020-06-14T19:35:50.234439Z\",\n    \"build_snapshot\" : false,\n    \"lucene_version\" : \"8.5.1\",\n    \"minimum_wire_compatibility_version\" : \"6.8.0\",\n    \"minimum_index_compatibility_version\" : \"6.0.0-beta1\"\n  },\n  \"tagline\" : \"You Know, for Search\"\n}\n\n```\n\n## 05-入门-RESTful & JSON\n\nREST指的是一组架构约束条件和原则。满足这些约束条件和原则的应用程序或设计就是RESTful。Web应用程序最重要的REST原则是，客户端和服务器之间的交互在请求之间是无状态的。从客户端到服务器的每个请求都必须包含理解请求所必需的信息。如果服务器在请求之间的任何时间点重启，客户端不会得到通知。此外，无状态请求可以由任何可用服务器回答，这十分适合云计算之类的环境。客户端可以缓存数据以改进性能。\n\n在服务器端，应用程序状态和功能可以分为各种资源。资源是一个有趣的概念实体，它向客户端公开。资源的例子有：应用程序对象、数据库记录、算法等等。每个资源都使用URI(Universal Resource Identifier)得到一个唯一的地址。所有资源都共享统一的接口，以便在客户端和服务器之间传输状态。使用的是标准的HTTP方法，比如GET、PUT、POST和DELETE。\n\n在REST样式的Web服务中，每个资源都有一个地址。资源本身都是方法调用的目\n标，方法列表对所有资源都是一样的。这些方法都是标准方法，包括HTTP GET、POST、PUT、DELETE，还可能包括HEAD和OPTIONS。简单的理解就是，**如果想要访问互联网上的资源，就必须向资源所在的服务器发出请求，请求体中必须包含资源的网络路径，以及对资源进行的操作(增删改查)**。\n\nREST样式的Web服务若有返回结果，大多数以JSON字符串形式返回。\n\n## 06-入门-[Postman](https://so.csdn.net/so/search?q=Postman&spm=1001.2101.3001.7020)客户端工具\n\n如果直接通过浏览器向Elasticsearch服务器发请求，那么需要在发送的请求中包含\nHTTP标准的方法，而HTTP的大部分特性且仅支持GET和POST方法。所以为了能方便地进行客户端的访问，可以使用Postman软件Postman是一款强大的网页调试工具，提供功能强大的Web API和HTTP请求调试。\n\n软件功能强大，界面简洁明晰、操作方便快捷，设计得很人性化。Postman中文版能够发送任何类型的HTTP请求(GET,HEAD,POST,PUT…)，不仅能够表单提交，且可以附带任意类型请求体。\n\n[Postman下载页面](https://www.postman.com/downloads/)\n\n## 07-入门-倒排索引\n\n正排索引（传统）\n\n| id   | content              |\n| ---- | -------------------- |\n| 1001 | my name is zhang san |\n| 1002 | my name is li si     |\n\n倒排索引\n\n| keyword | id         |\n| ------- | ---------- |\n| name    | 1001, 1002 |\n| zhang   | 1001       |\n\nElasticsearch是**面向文档型数据库**，一条数据在这里就是一个文档。为了方便大家理解，我们将Elasticsearch里存储文档数据和关系型数据库MySQL存储数据的概念进行一个类比\n\n![img](https://img-blog.csdnimg.cn/img_convert/146a779da01f53e7f7a8d53132d3c7cf.png)\n\nES里的Index可以看做一个库，而Types相当于表，Documents则相当于表的行。这里Types的概念已经被逐渐弱化，Elasticsearch6.X中，一个index下已经只能包含一个type，Elasticsearch7.X中,Type的概念已经被删除了。\n\n## 08-入门-HTTP-索引-创建\n\n对比关系型数据库，创建索引就等同于创建数据库。\n\n在Postman中，向ES服务器发PUT请求：http://127.0.0.1:9200/shopping\n\n请求后，服务器返回响应：\n\n```json\n{\n    \"acknowledged\": true,//响应结果\n    \"shards_acknowledged\": true,//分片结果\n    \"index\": \"shopping\"//索引名称\n}\n\n```\n\n后台日志：\n\n```cmd\n[2021-04-08T13:57:06,954][INFO ][o.e.c.m.MetadataCreateIndexService] [DESKTOP-LNJQ0VF] [shopping] creating index, cause [api], templates [], shards [1]/[1], mappings []\n\n```\n\n如果重复发PUT请求：http://127.0.0.1:9200/shopping 添加索引，会返回错误信息:\n\n```json\n{\n    \"error\": {\n        \"root_cause\": [\n            {\n                \"type\": \"resource_already_exists_exception\",\n                \"reason\": \"index [shopping/J0WlEhh4R7aDrfIc3AkwWQ] already exists\",\n                \"index_uuid\": \"J0WlEhh4R7aDrfIc3AkwWQ\",\n                \"index\": \"shopping\"\n            }\n        ],\n        \"type\": \"resource_already_exists_exception\",\n        \"reason\": \"index [shopping/J0WlEhh4R7aDrfIc3AkwWQ] already exists\",\n        \"index_uuid\": \"J0WlEhh4R7aDrfIc3AkwWQ\",\n        \"index\": \"shopping\"\n    },\n    \"status\": 400\n}\n\n```\n\n## 09-入门-HTTP-索引-查询 & 删除\n\n### 查看所有索引\n\n在Postman中，向ES服务器发GET请求：http://127.0.0.1:9200/_cat/indices?v\n\n这里请求路径中的_cat表示查看的意思，indices表示索引，所以整体含义就是查看当前ES服务器中的所有索引，就好像MySQL中的show tables的感觉，服务器响应结果如下:\n\n```\nhealth status index    uuid                   pri rep docs.count docs.deleted store.size pri.store.size\nyellow open   shopping J0WlEhh4R7aDrfIc3AkwWQ   1   1          0            0       208b           208b\n\n```\n\n| 表头           | 含义                                                         |\n| -------------- | ------------------------------------------------------------ |\n| health         | 当前服务器健康状态： green(集群完整) yellow(单点正常、集群不完整) red(单点不正常) |\n| status         | 索引打开、关闭状态                                           |\n| index          | 索引名                                                       |\n| uuid           | 索引统一编号                                                 |\n| pri            | 主分片数量                                                   |\n| rep            | 副本数量                                                     |\n| docs.count     | 可用文档数量                                                 |\n| docs.deleted   | 文档删除状态（逻辑删除）                                     |\n| store.size     | 主分片和副分片整体占空间大小                                 |\n| pri.store.size | 主分片占空间大小                                             |\n\n### 查看单个索引\n\n在Postman中，向ES服务器发GET请求：http://127.0.0.1:9200/shopping\n\n返回结果如下：\n\n```json\n{\n    \"shopping\": {//索引名\n        \"aliases\": {},//别名\n        \"mappings\": {},//映射\n        \"settings\": {//设置\n            \"index\": {//设置 - 索引\n                \"creation_date\": \"1617861426847\",//设置 - 索引 - 创建时间\n                \"number_of_shards\": \"1\",//设置 - 索引 - 主分片数量\n                \"number_of_replicas\": \"1\",//设置 - 索引 - 主分片数量\n                \"uuid\": \"J0WlEhh4R7aDrfIc3AkwWQ\",//设置 - 索引 - 主分片数量\n                \"version\": {//设置 - 索引 - 主分片数量\n                    \"created\": \"7080099\"\n                },\n                \"provided_name\": \"shopping\"//设置 - 索引 - 主分片数量\n            }\n        }\n    }\n}\n\n```\n\n### 删除索引\n\n在Postman中，向ES服务器发DELETE请求：http://127.0.0.1:9200/shopping\n\n返回结果如下：\n\n```json\n{\n    \"acknowledged\": true\n}\n\n```\n\n再次查看所有索引，GET http://127.0.0.1:9200/_cat/indices?v，返回结果如下：\n\n```\nhealth status index uuid pri rep docs.count docs.deleted store.size pri.store.size\n\n```\n\n成功删除。\n\n## 10-入门-HTTP-文档-创建（Put & Post）\n\n假设索引已经创建好了，接下来我们来创建文档，并添加数据。这里的文档可以类比为关系型数据库中的表数据，添加的数据格式为JSON格式\n\n在Postman中，向ES服务器发POST请求：http://127.0.0.1:9200/shopping/_doc，请求体JSON内容为：\n\n```json\n{\n    \"title\":\"小米手机\",\n    \"category\":\"小米\",\n    \"images\":\"http://www.gulixueyuan.com/xm.jpg\",\n    \"price\":3999.00\n}\n\n```\n\n![img](https://img-blog.csdnimg.cn/img_convert/20d54cba223bd9d70ea356d3e40a8161.png)\n\n注意，此处发送请求的方式必须为POST，不能是PUT，否则会发生错误。\n\n返回结果：\n\n```json\n{\n    \"_index\": \"shopping\",//索引\n    \"_type\": \"_doc\",//类型-文档\n    \"_id\": \"ANQqsHgBaKNfVnMbhZYU\",//唯一标识，可以类比为 MySQL 中的主键，随机生成\n    \"_version\": 1,//版本\n    \"result\": \"created\",//结果，这里的 create 表示创建成功\n    \"_shards\": {//\n        \"total\": 2,//分片 - 总数\n        \"successful\": 1,//分片 - 总数\n        \"failed\": 0//分片 - 总数\n    },\n    \"_seq_no\": 0,\n    \"_primary_term\": 1\n}\n\n```\n\n上面的数据创建后，由于没有指定数据唯一性标识（ID），默认情况下，ES服务器会随机生成一个。\n\n如果想要自定义唯一性标识，需要在创建时指定： http://127.0.0.1:9200/shopping/_doc/1，请求体JSON内容为：\n\n```json\n{\n    \"title\":\"小米手机\",\n    \"category\":\"小米\",\n    \"images\":\"http://www.gulixueyuan.com/xm.jpg\",\n    \"price\":3999.00\n}\n\n```\n\n返回结果如下：\n\n```json\n{\n    \"_index\": \"shopping\",\n    \"_type\": \"_doc\",\n    \"_id\": \"1\",//<------------------自定义唯一性标识\n    \"_version\": 1,\n    \"result\": \"created\",\n    \"_shards\": {\n        \"total\": 2,\n        \"successful\": 1,\n        \"failed\": 0\n    },\n    \"_seq_no\": 1,\n    \"_primary_term\": 1\n}\n\n```\n\n**此处需要注意：如果增加数据时明确数据主键，那么请求方式也可以为PUT。**\n\n## 11-入门-HTTP-查询-[主键](https://so.csdn.net/so/search?q=主键&spm=1001.2101.3001.7020)查询 & 全查询\n\n查看文档时，需要指明文档的唯一性标识，类似于MySQL中数据的主键查询\n\n在Postman中，向ES服务器发GET请求：http://127.0.0.1:9200/shopping/_doc/1。\n\n返回结果如下：\n\n```json\n{\n    \"_index\": \"shopping\",\n    \"_type\": \"_doc\",\n    \"_id\": \"1\",\n    \"_version\": 1,\n    \"_seq_no\": 1,\n    \"_primary_term\": 1,\n    \"found\": true,\n    \"_source\": {\n        \"title\": \"小米手机\",\n        \"category\": \"小米\",\n        \"images\": \"http://www.gulixueyuan.com/xm.jpg\",\n        \"price\": 3999\n    }\n}\n\n```\n\n查找不存在的内容，向ES服务器发GET请求：http://127.0.0.1:9200/shopping/_doc/1001。\n\n返回结果如下：\n\n```json\n{\n    \"_index\": \"shopping\",\n    \"_type\": \"_doc\",\n    \"_id\": \"1001\",\n    \"found\": false\n}\n\n```\n\n查看索引下所有数据，向ES服务器发GET请求：http://127.0.0.1:9200/shopping/_search。\n\n返回结果如下：\n\n```json\n{\n    \"took\": 133,\n    \"timed_out\": false,\n    \"_shards\": {\n        \"total\": 1,\n        \"successful\": 1,\n        \"skipped\": 0,\n        \"failed\": 0\n    },\n    \"hits\": {\n        \"total\": {\n            \"value\": 2,\n            \"relation\": \"eq\"\n        },\n        \"max_score\": 1,\n        \"hits\": [\n            {\n                \"_index\": \"shopping\",\n                \"_type\": \"_doc\",\n                \"_id\": \"ANQqsHgBaKNfVnMbhZYU\",\n                \"_score\": 1,\n                \"_source\": {\n                    \"title\": \"小米手机\",\n                    \"category\": \"小米\",\n                    \"images\": \"http://www.gulixueyuan.com/xm.jpg\",\n                    \"price\": 3999\n                }\n            },\n            {\n                \"_index\": \"shopping\",\n                \"_type\": \"_doc\",\n                \"_id\": \"1\",\n                \"_score\": 1,\n                \"_source\": {\n                    \"title\": \"小米手机\",\n                    \"category\": \"小米\",\n                    \"images\": \"http://www.gulixueyuan.com/xm.jpg\",\n                    \"price\": 3999\n                }\n            }\n        ]\n    }\n}\n\n```\n\n## 12-入门-HTTP-全量修改 & 局部修改 & 删除\n\n### 全量修改\n\n和新增文档一样，输入相同的URL地址请求，如果请求体变化，会将原有的数据内容覆盖\n\n在Postman中，向ES服务器发POST请求：http://127.0.0.1:9200/shopping/_doc/1\n\n请求体JSON内容为:\n\n```json\n{\n    \"title\":\"华为手机\",\n    \"category\":\"华为\",\n    \"images\":\"http://www.gulixueyuan.com/hw.jpg\",\n    \"price\":1999.00\n}\n\n```\n\n修改成功后，服务器响应结果：\n\n```json\n{\n    \"_index\": \"shopping\",\n    \"_type\": \"_doc\",\n    \"_id\": \"1\",\n    \"_version\": 2,\n    \"result\": \"updated\",//<-----------updated 表示数据被更新\n    \"_shards\": {\n        \"total\": 2,\n        \"successful\": 1,\n        \"failed\": 0\n    },\n    \"_seq_no\": 2,\n    \"_primary_term\": 1\n}\n\n```\n\n### 局部修改\n\n修改数据时，也可以只修改某一给条数据的局部信息\n\n在Postman中，向ES服务器发POST请求：http://127.0.0.1:9200/shopping/_update/1。\n\n请求体JSON内容为:\n\n```json\n{\n\t\"doc\": {\n\t\t\"title\":\"小米手机\",\n\t\t\"category\":\"小米\"\n\t}\n}\n\n```\n\n返回结果如下：\n\n```json\n{\n    \"_index\": \"shopping\",\n    \"_type\": \"_doc\",\n    \"_id\": \"1\",\n    \"_version\": 3,\n    \"result\": \"updated\",//<-----------updated 表示数据被更新\n    \"_shards\": {\n        \"total\": 2,\n        \"successful\": 1,\n        \"failed\": 0\n    },\n    \"_seq_no\": 3,\n    \"_primary_term\": 1\n}\n\n```\n\n在Postman中，向ES服务器发GET请求：http://127.0.0.1:9200/shopping/_doc/1，查看修改内容：\n\n```json\n{\n    \"_index\": \"shopping\",\n    \"_type\": \"_doc\",\n    \"_id\": \"1\",\n    \"_version\": 3,\n    \"_seq_no\": 3,\n    \"_primary_term\": 1,\n    \"found\": true,\n    \"_source\": {\n        \"title\": \"小米手机\",\n        \"category\": \"小米\",\n        \"images\": \"http://www.gulixueyuan.com/hw.jpg\",\n        \"price\": 1999\n    }\n}\n\n```\n\n### 删除\n\n删除一个文档不会立即从磁盘上移除，它只是被标记成已删除（逻辑删除）。\n\n在Postman中，向ES服务器发DELETE请求：http://127.0.0.1:9200/shopping/_doc/1\n\n返回结果：\n\n```json\n{\n    \"_index\": \"shopping\",\n    \"_type\": \"_doc\",\n    \"_id\": \"1\",\n    \"_version\": 4,\n    \"result\": \"deleted\",//<---删除成功\n    \"_shards\": {\n        \"total\": 2,\n        \"successful\": 1,\n        \"failed\": 0\n    },\n    \"_seq_no\": 4,\n    \"_primary_term\": 1\n}\n\n```\n\n在Postman中，向ES服务器发GET请求：http://127.0.0.1:9200/shopping/_doc/1，查看是否删除成功：\n\n```json\n{\n    \"_index\": \"shopping\",\n    \"_type\": \"_doc\",\n    \"_id\": \"1\",\n    \"found\": false\n}\n\n```\n\n## 13-入门-HTTP-条件查询 & 分页查询 & 查询排序\n\n### 条件查询\n\n假设有以下文档内容，（在Postman中，向ES服务器发GET请求：http://127.0.0.1:9200/shopping/_search）：\n\n```json\n{\n    \"took\": 5,\n    \"timed_out\": false,\n    \"_shards\": {\n        \"total\": 1,\n        \"successful\": 1,\n        \"skipped\": 0,\n        \"failed\": 0\n    },\n    \"hits\": {\n        \"total\": {\n            \"value\": 6,\n            \"relation\": \"eq\"\n        },\n        \"max_score\": 1,\n        \"hits\": [\n            {\n                \"_index\": \"shopping\",\n                \"_type\": \"_doc\",\n                \"_id\": \"ANQqsHgBaKNfVnMbhZYU\",\n                \"_score\": 1,\n                \"_source\": {\n                    \"title\": \"小米手机\",\n                    \"category\": \"小米\",\n                    \"images\": \"http://www.gulixueyuan.com/xm.jpg\",\n                    \"price\": 3999\n                }\n            },\n            {\n                \"_index\": \"shopping\",\n                \"_type\": \"_doc\",\n                \"_id\": \"A9R5sHgBaKNfVnMb25Ya\",\n                \"_score\": 1,\n                \"_source\": {\n                    \"title\": \"小米手机\",\n                    \"category\": \"小米\",\n                    \"images\": \"http://www.gulixueyuan.com/xm.jpg\",\n                    \"price\": 1999\n                }\n            },\n            {\n                \"_index\": \"shopping\",\n                \"_type\": \"_doc\",\n                \"_id\": \"BNR5sHgBaKNfVnMb7pal\",\n                \"_score\": 1,\n                \"_source\": {\n                    \"title\": \"小米手机\",\n                    \"category\": \"小米\",\n                    \"images\": \"http://www.gulixueyuan.com/xm.jpg\",\n                    \"price\": 1999\n                }\n            },\n            {\n                \"_index\": \"shopping\",\n                \"_type\": \"_doc\",\n                \"_id\": \"BtR6sHgBaKNfVnMbX5Y5\",\n                \"_score\": 1,\n                \"_source\": {\n                    \"title\": \"华为手机\",\n                    \"category\": \"华为\",\n                    \"images\": \"http://www.gulixueyuan.com/xm.jpg\",\n                    \"price\": 1999\n                }\n            },\n            {\n                \"_index\": \"shopping\",\n                \"_type\": \"_doc\",\n                \"_id\": \"B9R6sHgBaKNfVnMbZpZ6\",\n                \"_score\": 1,\n                \"_source\": {\n                    \"title\": \"华为手机\",\n                    \"category\": \"华为\",\n                    \"images\": \"http://www.gulixueyuan.com/xm.jpg\",\n                    \"price\": 1999\n                }\n            },\n            {\n                \"_index\": \"shopping\",\n                \"_type\": \"_doc\",\n                \"_id\": \"CdR7sHgBaKNfVnMbsJb9\",\n                \"_score\": 1,\n                \"_source\": {\n                    \"title\": \"华为手机\",\n                    \"category\": \"华为\",\n                    \"images\": \"http://www.gulixueyuan.com/xm.jpg\",\n                    \"price\": 1999\n                }\n            }\n        ]\n    }\n}\n\n```\n\n#### URL带参查询\n\n**查找category为小米的文档**，在Postman中，向ES服务器发GET请求：http://127.0.0.1:9200/shopping/_search?q=category:小米，返回结果如下：\n\n```json\n{\n    \"took\": 94,\n    \"timed_out\": false,\n    \"_shards\": {\n        \"total\": 1,\n        \"successful\": 1,\n        \"skipped\": 0,\n        \"failed\": 0\n    },\n    \"hits\": {\n        \"total\": {\n            \"value\": 3,\n            \"relation\": \"eq\"\n        },\n        \"max_score\": 1.3862942,\n        \"hits\": [\n            {\n                \"_index\": \"shopping\",\n                \"_type\": \"_doc\",\n                \"_id\": \"ANQqsHgBaKNfVnMbhZYU\",\n                \"_score\": 1.3862942,\n                \"_source\": {\n                    \"title\": \"小米手机\",\n                    \"category\": \"小米\",\n                    \"images\": \"http://www.gulixueyuan.com/xm.jpg\",\n                    \"price\": 3999\n                }\n            },\n            {\n                \"_index\": \"shopping\",\n                \"_type\": \"_doc\",\n                \"_id\": \"A9R5sHgBaKNfVnMb25Ya\",\n                \"_score\": 1.3862942,\n                \"_source\": {\n                    \"title\": \"小米手机\",\n                    \"category\": \"小米\",\n                    \"images\": \"http://www.gulixueyuan.com/xm.jpg\",\n                    \"price\": 1999\n                }\n            },\n            {\n                \"_index\": \"shopping\",\n                \"_type\": \"_doc\",\n                \"_id\": \"BNR5sHgBaKNfVnMb7pal\",\n                \"_score\": 1.3862942,\n                \"_source\": {\n                    \"title\": \"小米手机\",\n                    \"category\": \"小米\",\n                    \"images\": \"http://www.gulixueyuan.com/xm.jpg\",\n                    \"price\": 1999\n                }\n            }\n        ]\n    }\n}\n```\n\n上述为URL带参数形式查询，这很容易让不善者心怀恶意，或者参数值出现中文会出现乱码情况。为了避免这些情况，我们可用使用带JSON请求体请求进行查询。\n\n#### 请求体带参查询\n\n接下带JSON请求体，还是**查找category为小米的文档**，在Postman中，向ES服务器发GET请求：http://127.0.0.1:9200/shopping/_search，附带JSON体如下：\n\n```json\n{\n\t\"query\":{\n\t\t\"match\":{\n\t\t\t\"category\":\"小米\"\n\t\t}\n\t}\n}\n\n```\n\n返回结果如下：\n\n```json\n{\n    \"took\": 3,\n    \"timed_out\": false,\n    \"_shards\": {\n        \"total\": 1,\n        \"successful\": 1,\n        \"skipped\": 0,\n        \"failed\": 0\n    },\n    \"hits\": {\n        \"total\": {\n            \"value\": 3,\n            \"relation\": \"eq\"\n        },\n        \"max_score\": 1.3862942,\n        \"hits\": [\n            {\n                \"_index\": \"shopping\",\n                \"_type\": \"_doc\",\n                \"_id\": \"ANQqsHgBaKNfVnMbhZYU\",\n                \"_score\": 1.3862942,\n                \"_source\": {\n                    \"title\": \"小米手机\",\n                    \"category\": \"小米\",\n                    \"images\": \"http://www.gulixueyuan.com/xm.jpg\",\n                    \"price\": 3999\n                }\n            },\n            {\n                \"_index\": \"shopping\",\n                \"_type\": \"_doc\",\n                \"_id\": \"A9R5sHgBaKNfVnMb25Ya\",\n                \"_score\": 1.3862942,\n                \"_source\": {\n                    \"title\": \"小米手机\",\n                    \"category\": \"小米\",\n                    \"images\": \"http://www.gulixueyuan.com/xm.jpg\",\n                    \"price\": 1999\n                }\n            },\n            {\n                \"_index\": \"shopping\",\n                \"_type\": \"_doc\",\n                \"_id\": \"BNR5sHgBaKNfVnMb7pal\",\n                \"_score\": 1.3862942,\n                \"_source\": {\n                    \"title\": \"小米手机\",\n                    \"category\": \"小米\",\n                    \"images\": \"http://www.gulixueyuan.com/xm.jpg\",\n                    \"price\": 1999\n                }\n            }\n        ]\n    }\n}\n\n```\n\n#### 带请求体方式的查找所有内容\n\n**查找所有文档内容**，也可以这样，在Postman中，向ES服务器发GET请求：http://127.0.0.1:9200/shopping/_search，附带JSON体如下：\n\n```json\n{\n\t\"query\":{\n\t\t\"match_all\":{}\n\t}\n}\n\n```\n\n则返回所有文档内容：\n\n```json\n{\n    \"took\": 2,\n    \"timed_out\": false,\n    \"_shards\": {\n        \"total\": 1,\n        \"successful\": 1,\n        \"skipped\": 0,\n        \"failed\": 0\n    },\n    \"hits\": {\n        \"total\": {\n            \"value\": 6,\n            \"relation\": \"eq\"\n        },\n        \"max_score\": 1,\n        \"hits\": [\n            {\n                \"_index\": \"shopping\",\n                \"_type\": \"_doc\",\n                \"_id\": \"ANQqsHgBaKNfVnMbhZYU\",\n                \"_score\": 1,\n                \"_source\": {\n                    \"title\": \"小米手机\",\n                    \"category\": \"小米\",\n                    \"images\": \"http://www.gulixueyuan.com/xm.jpg\",\n                    \"price\": 3999\n                }\n            },\n            {\n                \"_index\": \"shopping\",\n                \"_type\": \"_doc\",\n                \"_id\": \"A9R5sHgBaKNfVnMb25Ya\",\n                \"_score\": 1,\n                \"_source\": {\n                    \"title\": \"小米手机\",\n                    \"category\": \"小米\",\n                    \"images\": \"http://www.gulixueyuan.com/xm.jpg\",\n                    \"price\": 1999\n                }\n            },\n            {\n                \"_index\": \"shopping\",\n                \"_type\": \"_doc\",\n                \"_id\": \"BNR5sHgBaKNfVnMb7pal\",\n                \"_score\": 1,\n                \"_source\": {\n                    \"title\": \"小米手机\",\n                    \"category\": \"小米\",\n                    \"images\": \"http://www.gulixueyuan.com/xm.jpg\",\n                    \"price\": 1999\n                }\n            },\n            {\n                \"_index\": \"shopping\",\n                \"_type\": \"_doc\",\n                \"_id\": \"BtR6sHgBaKNfVnMbX5Y5\",\n                \"_score\": 1,\n                \"_source\": {\n                    \"title\": \"华为手机\",\n                    \"category\": \"华为\",\n                    \"images\": \"http://www.gulixueyuan.com/xm.jpg\",\n                    \"price\": 1999\n                }\n            },\n            {\n                \"_index\": \"shopping\",\n                \"_type\": \"_doc\",\n                \"_id\": \"B9R6sHgBaKNfVnMbZpZ6\",\n                \"_score\": 1,\n                \"_source\": {\n                    \"title\": \"华为手机\",\n                    \"category\": \"华为\",\n                    \"images\": \"http://www.gulixueyuan.com/xm.jpg\",\n                    \"price\": 1999\n                }\n            },\n            {\n                \"_index\": \"shopping\",\n                \"_type\": \"_doc\",\n                \"_id\": \"CdR7sHgBaKNfVnMbsJb9\",\n                \"_score\": 1,\n                \"_source\": {\n                    \"title\": \"华为手机\",\n                    \"category\": \"华为\",\n                    \"images\": \"http://www.gulixueyuan.com/xm.jpg\",\n                    \"price\": 1999\n                }\n            }\n        ]\n    }\n}\n\n```\n\n#### 查询指定字段\n\n**如果你想查询指定字段**，在Postman中，向ES服务器发GET请求：http://127.0.0.1:9200/shopping/_search，附带JSON体如下：\n\n```json\n{\n\t\"query\":{\n\t\t\"match_all\":{}\n\t},\n\t\"_source\":[\"title\"]\n}\n\n```\n\n返回结果如下：\n\n```json\n{\n    \"took\": 5,\n    \"timed_out\": false,\n    \"_shards\": {\n        \"total\": 1,\n        \"successful\": 1,\n        \"skipped\": 0,\n        \"failed\": 0\n    },\n    \"hits\": {\n        \"total\": {\n            \"value\": 6,\n            \"relation\": \"eq\"\n        },\n        \"max_score\": 1,\n        \"hits\": [\n            {\n                \"_index\": \"shopping\",\n                \"_type\": \"_doc\",\n                \"_id\": \"ANQqsHgBaKNfVnMbhZYU\",\n                \"_score\": 1,\n                \"_source\": {\n                    \"title\": \"小米手机\"\n                }\n            },\n            {\n                \"_index\": \"shopping\",\n                \"_type\": \"_doc\",\n                \"_id\": \"A9R5sHgBaKNfVnMb25Ya\",\n                \"_score\": 1,\n                \"_source\": {\n                    \"title\": \"小米手机\"\n                }\n            },\n            {\n                \"_index\": \"shopping\",\n                \"_type\": \"_doc\",\n                \"_id\": \"BNR5sHgBaKNfVnMb7pal\",\n                \"_score\": 1,\n                \"_source\": {\n                    \"title\": \"小米手机\"\n                }\n            },\n            {\n                \"_index\": \"shopping\",\n                \"_type\": \"_doc\",\n                \"_id\": \"BtR6sHgBaKNfVnMbX5Y5\",\n                \"_score\": 1,\n                \"_source\": {\n                    \"title\": \"华为手机\"\n                }\n            },\n            {\n                \"_index\": \"shopping\",\n                \"_type\": \"_doc\",\n                \"_id\": \"B9R6sHgBaKNfVnMbZpZ6\",\n                \"_score\": 1,\n                \"_source\": {\n                    \"title\": \"华为手机\"\n                }\n            },\n            {\n                \"_index\": \"shopping\",\n                \"_type\": \"_doc\",\n                \"_id\": \"CdR7sHgBaKNfVnMbsJb9\",\n                \"_score\": 1,\n                \"_source\": {\n                    \"title\": \"华为手机\"\n                }\n            }\n        ]\n    }\n}\n\n```\n\n### 分页查询\n\n在Postman中，向ES服务器发GET请求：http://127.0.0.1:9200/shopping/_search，附带JSON体如下：\n\n```json\n{\n\t\"query\":{\n\t\t\"match_all\":{}\n\t},\n\t\"from\":0,\n\t\"size\":2\n}\n\n```\n\n返回结果如下：\n\n```java\n{\n    \"took\": 1,\n    \"timed_out\": false,\n    \"_shards\": {\n        \"total\": 1,\n        \"successful\": 1,\n        \"skipped\": 0,\n        \"failed\": 0\n    },\n    \"hits\": {\n        \"total\": {\n            \"value\": 6,\n            \"relation\": \"eq\"\n        },\n        \"max_score\": 1,\n        \"hits\": [\n            {\n                \"_index\": \"shopping\",\n                \"_type\": \"_doc\",\n                \"_id\": \"ANQqsHgBaKNfVnMbhZYU\",\n                \"_score\": 1,\n                \"_source\": {\n                    \"title\": \"小米手机\",\n                    \"category\": \"小米\",\n                    \"images\": \"http://www.gulixueyuan.com/xm.jpg\",\n                    \"price\": 3999\n                }\n            },\n            {\n                \"_index\": \"shopping\",\n                \"_type\": \"_doc\",\n                \"_id\": \"A9R5sHgBaKNfVnMb25Ya\",\n                \"_score\": 1,\n                \"_source\": {\n                    \"title\": \"小米手机\",\n                    \"category\": \"小米\",\n                    \"images\": \"http://www.gulixueyuan.com/xm.jpg\",\n                    \"price\": 1999\n                }\n            }\n        ]\n    }\n}\n```\n\n### 查询排序\n\n如果你想通过排序查出价格最高的手机，在Postman中，向ES服务器发GET请求：http://127.0.0.1:9200/shopping/_search，附带JSON体如下：\n\n```json\n{\n\t\"query\":{\n\t\t\"match_all\":{}\n\t},\n\t\"sort\":{\n\t\t\"price\":{\n\t\t\t\"order\":\"desc\"\n\t\t}\n\t}\n}\n\n```\n\n返回结果如下：\n\n```json\n{\n    \"took\": 96,\n    \"timed_out\": false,\n    \"_shards\": {\n        \"total\": 1,\n        \"successful\": 1,\n        \"skipped\": 0,\n        \"failed\": 0\n    },\n    \"hits\": {\n        \"total\": {\n            \"value\": 6,\n            \"relation\": \"eq\"\n        },\n        \"max_score\": null,\n        \"hits\": [\n            {\n                \"_index\": \"shopping\",\n                \"_type\": \"_doc\",\n                \"_id\": \"ANQqsHgBaKNfVnMbhZYU\",\n                \"_score\": null,\n                \"_source\": {\n                    \"title\": \"小米手机\",\n                    \"category\": \"小米\",\n                    \"images\": \"http://www.gulixueyuan.com/xm.jpg\",\n                    \"price\": 3999\n                },\n                \"sort\": [\n                    3999\n                ]\n            },\n            {\n                \"_index\": \"shopping\",\n                \"_type\": \"_doc\",\n                \"_id\": \"A9R5sHgBaKNfVnMb25Ya\",\n                \"_score\": null,\n                \"_source\": {\n                    \"title\": \"小米手机\",\n                    \"category\": \"小米\",\n                    \"images\": \"http://www.gulixueyuan.com/xm.jpg\",\n                    \"price\": 1999\n                },\n                \"sort\": [\n                    1999\n                ]\n            },\n            {\n                \"_index\": \"shopping\",\n                \"_type\": \"_doc\",\n                \"_id\": \"BNR5sHgBaKNfVnMb7pal\",\n                \"_score\": null,\n                \"_source\": {\n                    \"title\": \"小米手机\",\n                    \"category\": \"小米\",\n                    \"images\": \"http://www.gulixueyuan.com/xm.jpg\",\n                    \"price\": 1999\n                },\n                \"sort\": [\n                    1999\n                ]\n            },\n            {\n                \"_index\": \"shopping\",\n                \"_type\": \"_doc\",\n                \"_id\": \"BtR6sHgBaKNfVnMbX5Y5\",\n                \"_score\": null,\n                \"_source\": {\n                    \"title\": \"华为手机\",\n                    \"category\": \"华为\",\n                    \"images\": \"http://www.gulixueyuan.com/xm.jpg\",\n                    \"price\": 1999\n                },\n                \"sort\": [\n                    1999\n                ]\n            },\n            {\n                \"_index\": \"shopping\",\n                \"_type\": \"_doc\",\n                \"_id\": \"B9R6sHgBaKNfVnMbZpZ6\",\n                \"_score\": null,\n                \"_source\": {\n                    \"title\": \"华为手机\",\n                    \"category\": \"华为\",\n                    \"images\": \"http://www.gulixueyuan.com/xm.jpg\",\n                    \"price\": 1999\n                },\n                \"sort\": [\n                    1999\n                ]\n            },\n            {\n                \"_index\": \"shopping\",\n                \"_type\": \"_doc\",\n                \"_id\": \"CdR7sHgBaKNfVnMbsJb9\",\n                \"_score\": null,\n                \"_source\": {\n                    \"title\": \"华为手机\",\n                    \"category\": \"华为\",\n                    \"images\": \"http://www.gulixueyuan.com/xm.jpg\",\n                    \"price\": 1999\n                },\n                \"sort\": [\n                    1999\n                ]\n            }\n        ]\n    }\n}\n\n```\n\n## 14-入门-HTTP-多条件查询 & 范围查询\n\n### 多条件查询\n\n假设想找出小米牌子，价格为3999元的。（must相当于数据库的&&）\n\n在Postman中，向ES服务器发GET请求：http://127.0.0.1:9200/shopping/_search，附带JSON体如下：\n\n```json\n{\n\t\"query\":{\n\t\t\"bool\":{\n\t\t\t\"must\":[{\n\t\t\t\t\"match\":{\n\t\t\t\t\t\"category\":\"小米\"\n\t\t\t\t}\n\t\t\t},{\n\t\t\t\t\"match\":{\n\t\t\t\t\t\"price\":3999.00\n\t\t\t\t}\n\t\t\t}]\n\t\t}\n\t}\n}\n\n```\n\n返回结果如下：\n\n```json\n{\n    \"took\": 134,\n    \"timed_out\": false,\n    \"_shards\": {\n        \"total\": 1,\n        \"successful\": 1,\n        \"skipped\": 0,\n        \"failed\": 0\n    },\n    \"hits\": {\n        \"total\": {\n            \"value\": 1,\n            \"relation\": \"eq\"\n        },\n        \"max_score\": 2.3862944,\n        \"hits\": [\n            {\n                \"_index\": \"shopping\",\n                \"_type\": \"_doc\",\n                \"_id\": \"ANQqsHgBaKNfVnMbhZYU\",\n                \"_score\": 2.3862944,\n                \"_source\": {\n                    \"title\": \"小米手机\",\n                    \"category\": \"小米\",\n                    \"images\": \"http://www.gulixueyuan.com/xm.jpg\",\n                    \"price\": 3999\n                }\n            }\n        ]\n    }\n}\n\n```\n\n假设想找出小米和华为的牌子。（should相当于数据库的||）\n\n在Postman中，向ES服务器发GET请求：http://127.0.0.1:9200/shopping/_search，附带JSON体如下：\n\n```json\n{\n\t\"query\":{\n\t\t\"bool\":{\n\t\t\t\"should\":[{\n\t\t\t\t\"match\":{\n\t\t\t\t\t\"category\":\"小米\"\n\t\t\t\t}\n\t\t\t},{\n\t\t\t\t\"match\":{\n\t\t\t\t\t\"category\":\"华为\"\n\t\t\t\t}\n\t\t\t}]\n\t\t},\n        \"filter\":{\n            \"range\":{\n                \"price\":{\n                    \"gt\":2000\n                }\n            }\n        }\n\t}\n}\n\n```\n\n返回结果如下：\n\n```json\n{\n    \"took\": 8,\n    \"timed_out\": false,\n    \"_shards\": {\n        \"total\": 1,\n        \"successful\": 1,\n        \"skipped\": 0,\n        \"failed\": 0\n    },\n    \"hits\": {\n        \"total\": {\n            \"value\": 6,\n            \"relation\": \"eq\"\n        },\n        \"max_score\": 1.3862942,\n        \"hits\": [\n            {\n                \"_index\": \"shopping\",\n                \"_type\": \"_doc\",\n                \"_id\": \"ANQqsHgBaKNfVnMbhZYU\",\n                \"_score\": 1.3862942,\n                \"_source\": {\n                    \"title\": \"小米手机\",\n                    \"category\": \"小米\",\n                    \"images\": \"http://www.gulixueyuan.com/xm.jpg\",\n                    \"price\": 3999\n                }\n            },\n            {\n                \"_index\": \"shopping\",\n                \"_type\": \"_doc\",\n                \"_id\": \"A9R5sHgBaKNfVnMb25Ya\",\n                \"_score\": 1.3862942,\n                \"_source\": {\n                    \"title\": \"小米手机\",\n                    \"category\": \"小米\",\n                    \"images\": \"http://www.gulixueyuan.com/xm.jpg\",\n                    \"price\": 1999\n                }\n            },\n            {\n                \"_index\": \"shopping\",\n                \"_type\": \"_doc\",\n                \"_id\": \"BNR5sHgBaKNfVnMb7pal\",\n                \"_score\": 1.3862942,\n                \"_source\": {\n                    \"title\": \"小米手机\",\n                    \"category\": \"小米\",\n                    \"images\": \"http://www.gulixueyuan.com/xm.jpg\",\n                    \"price\": 1999\n                }\n            },\n            {\n                \"_index\": \"shopping\",\n                \"_type\": \"_doc\",\n                \"_id\": \"BtR6sHgBaKNfVnMbX5Y5\",\n                \"_score\": 1.3862942,\n                \"_source\": {\n                    \"title\": \"华为手机\",\n                    \"category\": \"华为\",\n                    \"images\": \"http://www.gulixueyuan.com/xm.jpg\",\n                    \"price\": 1999\n                }\n            },\n            {\n                \"_index\": \"shopping\",\n                \"_type\": \"_doc\",\n                \"_id\": \"B9R6sHgBaKNfVnMbZpZ6\",\n                \"_score\": 1.3862942,\n                \"_source\": {\n                    \"title\": \"华为手机\",\n                    \"category\": \"华为\",\n                    \"images\": \"http://www.gulixueyuan.com/xm.jpg\",\n                    \"price\": 1999\n                }\n            },\n            {\n                \"_index\": \"shopping\",\n                \"_type\": \"_doc\",\n                \"_id\": \"CdR7sHgBaKNfVnMbsJb9\",\n                \"_score\": 1.3862942,\n                \"_source\": {\n                    \"title\": \"华为手机\",\n                    \"category\": \"华为\",\n                    \"images\": \"http://www.gulixueyuan.com/xm.jpg\",\n                    \"price\": 1999\n                }\n            }\n        ]\n    }\n}\n\n```\n\n### 范围查询\n\n假设想找出小米和华为的牌子，价格大于2000元的手机。\n\n在Postman中，向ES服务器发GET请求：http://127.0.0.1:9200/shopping/_search，附带JSON体如下：\n\n```json\n{\n\t\"query\":{\n\t\t\"bool\":{\n\t\t\t\"should\":[{\n\t\t\t\t\"match\":{\n\t\t\t\t\t\"category\":\"小米\"\n\t\t\t\t}\n\t\t\t},{\n\t\t\t\t\"match\":{\n\t\t\t\t\t\"category\":\"华为\"\n\t\t\t\t}\n\t\t\t}],\n            \"filter\":{\n            \t\"range\":{\n                \t\"price\":{\n                    \t\"gt\":2000\n                \t}\n\t            }\n    \t    }\n\t\t}\n\t}\n}\n\n```\n\n返回结果如下：\n\n```json\n{\n    \"took\": 72,\n    \"timed_out\": false,\n    \"_shards\": {\n        \"total\": 1,\n        \"successful\": 1,\n        \"skipped\": 0,\n        \"failed\": 0\n    },\n    \"hits\": {\n        \"total\": {\n            \"value\": 1,\n            \"relation\": \"eq\"\n        },\n        \"max_score\": 1.3862942,\n        \"hits\": [\n            {\n                \"_index\": \"shopping\",\n                \"_type\": \"_doc\",\n                \"_id\": \"ANQqsHgBaKNfVnMbhZYU\",\n                \"_score\": 1.3862942,\n                \"_source\": {\n                    \"title\": \"小米手机\",\n                    \"category\": \"小米\",\n                    \"images\": \"http://www.gulixueyuan.com/xm.jpg\",\n                    \"price\": 3999\n                }\n            }\n        ]\n    }\n}\n\n```\n\n## 15-入门-HTTP-全文检索 & 完全匹配 & 高亮查询\n\n### 全文检索\n\n这功能像搜索引擎那样，如品牌输入“小华”，返回结果带回品牌有“小米”和华为的。\n\n在Postman中，向ES服务器发GET请求：http://127.0.0.1:9200/shopping/_search，附带JSON体如下：\n\n```json\n{\n\t\"query\":{\n\t\t\"match\":{\n\t\t\t\"category\" : \"小华\"\n\t\t}\n\t}\n}\n\n```\n\n返回结果如下：\n\n```json\n{\n    \"took\": 7,\n    \"timed_out\": false,\n    \"_shards\": {\n        \"total\": 1,\n        \"successful\": 1,\n        \"skipped\": 0,\n        \"failed\": 0\n    },\n    \"hits\": {\n        \"total\": {\n            \"value\": 6,\n            \"relation\": \"eq\"\n        },\n        \"max_score\": 0.6931471,\n        \"hits\": [\n            {\n                \"_index\": \"shopping\",\n                \"_type\": \"_doc\",\n                \"_id\": \"ANQqsHgBaKNfVnMbhZYU\",\n                \"_score\": 0.6931471,\n                \"_source\": {\n                    \"title\": \"小米手机\",\n                    \"category\": \"小米\",\n                    \"images\": \"http://www.gulixueyuan.com/xm.jpg\",\n                    \"price\": 3999\n                }\n            },\n            {\n                \"_index\": \"shopping\",\n                \"_type\": \"_doc\",\n                \"_id\": \"A9R5sHgBaKNfVnMb25Ya\",\n                \"_score\": 0.6931471,\n                \"_source\": {\n                    \"title\": \"小米手机\",\n                    \"category\": \"小米\",\n                    \"images\": \"http://www.gulixueyuan.com/xm.jpg\",\n                    \"price\": 1999\n                }\n            },\n            {\n                \"_index\": \"shopping\",\n                \"_type\": \"_doc\",\n                \"_id\": \"BNR5sHgBaKNfVnMb7pal\",\n                \"_score\": 0.6931471,\n                \"_source\": {\n                    \"title\": \"小米手机\",\n                    \"category\": \"小米\",\n                    \"images\": \"http://www.gulixueyuan.com/xm.jpg\",\n                    \"price\": 1999\n                }\n            },\n            {\n                \"_index\": \"shopping\",\n                \"_type\": \"_doc\",\n                \"_id\": \"BtR6sHgBaKNfVnMbX5Y5\",\n                \"_score\": 0.6931471,\n                \"_source\": {\n                    \"title\": \"华为手机\",\n                    \"category\": \"华为\",\n                    \"images\": \"http://www.gulixueyuan.com/xm.jpg\",\n                    \"price\": 1999\n                }\n            },\n            {\n                \"_index\": \"shopping\",\n                \"_type\": \"_doc\",\n                \"_id\": \"B9R6sHgBaKNfVnMbZpZ6\",\n                \"_score\": 0.6931471,\n                \"_source\": {\n                    \"title\": \"华为手机\",\n                    \"category\": \"华为\",\n                    \"images\": \"http://www.gulixueyuan.com/xm.jpg\",\n                    \"price\": 1999\n                }\n            },\n            {\n                \"_index\": \"shopping\",\n                \"_type\": \"_doc\",\n                \"_id\": \"CdR7sHgBaKNfVnMbsJb9\",\n                \"_score\": 0.6931471,\n                \"_source\": {\n                    \"title\": \"华为手机\",\n                    \"category\": \"华为\",\n                    \"images\": \"http://www.gulixueyuan.com/xm.jpg\",\n                    \"price\": 1999\n                }\n            }\n        ]\n    }\n}\n\n```\n\n### 完全匹配\n\n在Postman中，向ES服务器发GET请求：http://127.0.0.1:9200/shopping/_search，附带JSON体如下：\n\n```json\n{\n\t\"query\":{\n\t\t\"match_phrase\":{\n\t\t\t\"category\" : \"为\"\n\t\t}\n\t}\n}\n\n```\n\n返回结果如下：\n\n```json\n{\n    \"took\": 2,\n    \"timed_out\": false,\n    \"_shards\": {\n        \"total\": 1,\n        \"successful\": 1,\n        \"skipped\": 0,\n        \"failed\": 0\n    },\n    \"hits\": {\n        \"total\": {\n            \"value\": 3,\n            \"relation\": \"eq\"\n        },\n        \"max_score\": 0.6931471,\n        \"hits\": [\n            {\n                \"_index\": \"shopping\",\n                \"_type\": \"_doc\",\n                \"_id\": \"BtR6sHgBaKNfVnMbX5Y5\",\n                \"_score\": 0.6931471,\n                \"_source\": {\n                    \"title\": \"华为手机\",\n                    \"category\": \"华为\",\n                    \"images\": \"http://www.gulixueyuan.com/xm.jpg\",\n                    \"price\": 1999\n                }\n            },\n            {\n                \"_index\": \"shopping\",\n                \"_type\": \"_doc\",\n                \"_id\": \"B9R6sHgBaKNfVnMbZpZ6\",\n                \"_score\": 0.6931471,\n                \"_source\": {\n                    \"title\": \"华为手机\",\n                    \"category\": \"华为\",\n                    \"images\": \"http://www.gulixueyuan.com/xm.jpg\",\n                    \"price\": 1999\n                }\n            },\n            {\n                \"_index\": \"shopping\",\n                \"_type\": \"_doc\",\n                \"_id\": \"CdR7sHgBaKNfVnMbsJb9\",\n                \"_score\": 0.6931471,\n                \"_source\": {\n                    \"title\": \"华为手机\",\n                    \"category\": \"华为\",\n                    \"images\": \"http://www.gulixueyuan.com/xm.jpg\",\n                    \"price\": 1999\n                }\n            }\n        ]\n    }\n}\n\n```\n\n### 高亮查询\n\n在Postman中，向ES服务器发GET请求：http://127.0.0.1:9200/shopping/_search，附带JSON体如下：\n\n```json\n{\n\t\"query\":{\n\t\t\"match_phrase\":{\n\t\t\t\"category\" : \"为\"\n\t\t}\n\t},\n    \"highlight\":{\n        \"fields\":{\n            \"category\":{}//<----高亮这字段\n        }\n    }\n}\n\n```\n\n返回结果如下：\n\n```json\n{\n    \"took\": 100,\n    \"timed_out\": false,\n    \"_shards\": {\n        \"total\": 1,\n        \"successful\": 1,\n        \"skipped\": 0,\n        \"failed\": 0\n    },\n    \"hits\": {\n        \"total\": {\n            \"value\": 3,\n            \"relation\": \"eq\"\n        },\n        \"max_score\": 0.6931471,\n        \"hits\": [\n            {\n                \"_index\": \"shopping\",\n                \"_type\": \"_doc\",\n                \"_id\": \"BtR6sHgBaKNfVnMbX5Y5\",\n                \"_score\": 0.6931471,\n                \"_source\": {\n                    \"title\": \"华为手机\",\n                    \"category\": \"华为\",\n                    \"images\": \"http://www.gulixueyuan.com/xm.jpg\",\n                    \"price\": 1999\n                },\n                \"highlight\": {\n                    \"category\": [\n                        \"华<em>为</em>\"//<------高亮一个为字。\n                    ]\n                }\n            },\n            {\n                \"_index\": \"shopping\",\n                \"_type\": \"_doc\",\n                \"_id\": \"B9R6sHgBaKNfVnMbZpZ6\",\n                \"_score\": 0.6931471,\n                \"_source\": {\n                    \"title\": \"华为手机\",\n                    \"category\": \"华为\",\n                    \"images\": \"http://www.gulixueyuan.com/xm.jpg\",\n                    \"price\": 1999\n                },\n                \"highlight\": {\n                    \"category\": [\n                        \"华<em>为</em>\"\n                    ]\n                }\n            },\n            {\n                \"_index\": \"shopping\",\n                \"_type\": \"_doc\",\n                \"_id\": \"CdR7sHgBaKNfVnMbsJb9\",\n                \"_score\": 0.6931471,\n                \"_source\": {\n                    \"title\": \"华为手机\",\n                    \"category\": \"华为\",\n                    \"images\": \"http://www.gulixueyuan.com/xm.jpg\",\n                    \"price\": 1999\n                },\n                \"highlight\": {\n                    \"category\": [\n                        \"华<em>为</em>\"\n                    ]\n                }\n            }\n        ]\n    }\n}\n\n```\n\n## 16-入门-HTTP-聚合查询\n\n聚合允许使用者对es文档进行统计分析，类似与关系型数据库中的groupby，当然还有很多其他的聚合，例如取最大值max、平均值avg等等。\n\n接下来按price字段进行分组：\n\n在Postman中，向ES服务器发GET请求：http://127.0.0.1:9200/shopping/_search，附带JSON体如下：\n\n```json\n{\n\t\"aggs\":{//聚合操作\n\t\t\"price_group\":{//名称，随意起名\n\t\t\t\"terms\":{//分组\n\t\t\t\t\"field\":\"price\"//分组字段\n\t\t\t}\n\t\t}\n\t}\n}\n\n```\n\n返回结果如下：\n\n```json\n{\n    \"took\": 63,\n    \"timed_out\": false,\n    \"_shards\": {\n        \"total\": 1,\n        \"successful\": 1,\n        \"skipped\": 0,\n        \"failed\": 0\n    },\n    \"hits\": {\n        \"total\": {\n            \"value\": 6,\n            \"relation\": \"eq\"\n        },\n        \"max_score\": 1,\n        \"hits\": [\n            {\n                \"_index\": \"shopping\",\n                \"_type\": \"_doc\",\n                \"_id\": \"ANQqsHgBaKNfVnMbhZYU\",\n                \"_score\": 1,\n                \"_source\": {\n                    \"title\": \"小米手机\",\n                    \"category\": \"小米\",\n                    \"images\": \"http://www.gulixueyuan.com/xm.jpg\",\n                    \"price\": 3999\n                }\n            },\n            {\n                \"_index\": \"shopping\",\n                \"_type\": \"_doc\",\n                \"_id\": \"A9R5sHgBaKNfVnMb25Ya\",\n                \"_score\": 1,\n                \"_source\": {\n                    \"title\": \"小米手机\",\n                    \"category\": \"小米\",\n                    \"images\": \"http://www.gulixueyuan.com/xm.jpg\",\n                    \"price\": 1999\n                }\n            },\n            {\n                \"_index\": \"shopping\",\n                \"_type\": \"_doc\",\n                \"_id\": \"BNR5sHgBaKNfVnMb7pal\",\n                \"_score\": 1,\n                \"_source\": {\n                    \"title\": \"小米手机\",\n                    \"category\": \"小米\",\n                    \"images\": \"http://www.gulixueyuan.com/xm.jpg\",\n                    \"price\": 1999\n                }\n            },\n            {\n                \"_index\": \"shopping\",\n                \"_type\": \"_doc\",\n                \"_id\": \"BtR6sHgBaKNfVnMbX5Y5\",\n                \"_score\": 1,\n                \"_source\": {\n                    \"title\": \"华为手机\",\n                    \"category\": \"华为\",\n                    \"images\": \"http://www.gulixueyuan.com/xm.jpg\",\n                    \"price\": 1999\n                }\n            },\n            {\n                \"_index\": \"shopping\",\n                \"_type\": \"_doc\",\n                \"_id\": \"B9R6sHgBaKNfVnMbZpZ6\",\n                \"_score\": 1,\n                \"_source\": {\n                    \"title\": \"华为手机\",\n                    \"category\": \"华为\",\n                    \"images\": \"http://www.gulixueyuan.com/xm.jpg\",\n                    \"price\": 1999\n                }\n            },\n            {\n                \"_index\": \"shopping\",\n                \"_type\": \"_doc\",\n                \"_id\": \"CdR7sHgBaKNfVnMbsJb9\",\n                \"_score\": 1,\n                \"_source\": {\n                    \"title\": \"华为手机\",\n                    \"category\": \"华为\",\n                    \"images\": \"http://www.gulixueyuan.com/xm.jpg\",\n                    \"price\": 1999\n                }\n            }\n        ]\n    },\n    \"aggregations\": {\n        \"price_group\": {\n            \"doc_count_error_upper_bound\": 0,\n            \"sum_other_doc_count\": 0,\n            \"buckets\": [\n                {\n                    \"key\": 1999,\n                    \"doc_count\": 5\n                },\n                {\n                    \"key\": 3999,\n                    \"doc_count\": 1\n                }\n            ]\n        }\n    }\n}\n\n```\n\n上面返回结果会附带原始数据的。若不想要不附带原始数据的结果，在Postman中，向ES服务器发GET请求：http://127.0.0.1:9200/shopping/_search 附带JSON体如下：\n\n```json\n{\n\t\"aggs\":{\n\t\t\"price_group\":{\n\t\t\t\"terms\":{\n\t\t\t\t\"field\":\"price\"\n\t\t\t}\n\t\t}\n\t},\n    \"size\":0\n}\n\n```\n\n返回结果如下：\n\n```json\n{\n    \"took\": 60,\n    \"timed_out\": false,\n    \"_shards\": {\n        \"total\": 1,\n        \"successful\": 1,\n        \"skipped\": 0,\n        \"failed\": 0\n    },\n    \"hits\": {\n        \"total\": {\n            \"value\": 6,\n            \"relation\": \"eq\"\n        },\n        \"max_score\": null,\n        \"hits\": []\n    },\n    \"aggregations\": {\n        \"price_group\": {\n            \"doc_count_error_upper_bound\": 0,\n            \"sum_other_doc_count\": 0,\n            \"buckets\": [\n                {\n                    \"key\": 1999,\n                    \"doc_count\": 5\n                },\n                {\n                    \"key\": 3999,\n                    \"doc_count\": 1\n                }\n            ]\n        }\n    }\n}\n\n```\n\n若想对所有手机价格求**平均值**。\n\n在Postman中，向ES服务器发GET请求：http://127.0.0.1:9200/shopping/_search，附带JSON体如下：\n\n```json\n{\n\t\"aggs\":{\n\t\t\"price_avg\":{//名称，随意起名\n\t\t\t\"avg\":{//求平均\n\t\t\t\t\"field\":\"price\"\n\t\t\t}\n\t\t}\n\t},\n    \"size\":0\n}\n\n```\n\n返回结果如下：\n\n```json\n{\n    \"took\": 14,\n    \"timed_out\": false,\n    \"_shards\": {\n        \"total\": 1,\n        \"successful\": 1,\n        \"skipped\": 0,\n        \"failed\": 0\n    },\n    \"hits\": {\n        \"total\": {\n            \"value\": 6,\n            \"relation\": \"eq\"\n        },\n        \"max_score\": null,\n        \"hits\": []\n    },\n    \"aggregations\": {\n        \"price_avg\": {\n            \"value\": 2332.3333333333335\n        }\n    }\n}\n\n```\n\n## 17-入门-HTTP-映射关系\n\n有了索引库，等于有了数据库中的database。\n\n接下来就需要建索引库(index)中的映射了，类似于数据库(database)中的表结构(table)。\n\n创建数据库表需要设置字段名称，类型，长度，约束等；索引库也一样，需要知道这个类型下有哪些字段，每个字段有哪些约束信息，这就叫做映射(mapping)。\n\n先创建一个索引：\n\n```json\n# PUT http://127.0.0.1:9200/user\n\n```\n\n返回结果：\n\n```json\n{\n    \"acknowledged\": true,\n    \"shards_acknowledged\": true,\n    \"index\": \"user\"\n}\n\n```\n\n**创建映射**\n\n```json\n# PUT http://127.0.0.1:9200/user/_mapping\n\n{\n    \"properties\": {\n        \"name\":{\n        \t\"type\": \"text\",\n        \t\"index\": true\n        },\n        \"sex\":{\n        \t\"type\": \"keyword\",\n        \t\"index\": true\n        },\n        \"tel\":{\n        \t\"type\": \"keyword\",\n        \t\"index\": false\n        }\n    }\n}\n\n```\n\n返回结果如下：\n\n```json\n{\n    \"acknowledged\": true\n}\n\n```\n\n**查询映射**\n\n```json\n#GET http://127.0.0.1:9200/user/_mapping\n\n```\n\n返回结果如下：\n\n```json\n{\n    \"user\": {\n        \"mappings\": {\n            \"properties\": {\n                \"name\": {\n                    \"type\": \"text\"\n                },\n                \"sex\": {\n                    \"type\": \"keyword\"\n                },\n                \"tel\": {\n                    \"type\": \"keyword\",\n                    \"index\": false\n                }\n            }\n        }\n    }\n}\n\n```\n\n增加数据\n\n```json\n#PUT http://127.0.0.1:9200/user/_create/1001\n{\n\t\"name\":\"小米\",\n\t\"sex\":\"男的\",\n\t\"tel\":\"1111\"\n}\n\n```\n\n返回结果如下：\n\n```json\n{\n    \"_index\": \"user\",\n    \"_type\": \"_doc\",\n    \"_id\": \"1001\",\n    \"_version\": 1,\n    \"result\": \"created\",\n    \"_shards\": {\n        \"total\": 2,\n        \"successful\": 1,\n        \"failed\": 0\n    },\n    \"_seq_no\": 0,\n    \"_primary_term\": 1\n}\n\n```\n\n查找name含有”小“数据：\n\n```json\n#GET http://127.0.0.1:9200/user/_search\n{\n\t\"query\":{\n\t\t\"match\":{\n\t\t\t\"name\":\"小\"\n\t\t}\n\t}\n}\n\n```\n\n返回结果如下：\n\n```json\n{\n    \"took\": 495,\n    \"timed_out\": false,\n    \"_shards\": {\n        \"total\": 1,\n        \"successful\": 1,\n        \"skipped\": 0,\n        \"failed\": 0\n    },\n    \"hits\": {\n        \"total\": {\n            \"value\": 1,\n            \"relation\": \"eq\"\n        },\n        \"max_score\": 0.2876821,\n        \"hits\": [\n            {\n                \"_index\": \"user\",\n                \"_type\": \"_doc\",\n                \"_id\": \"1001\",\n                \"_score\": 0.2876821,\n                \"_source\": {\n                    \"name\": \"小米\",\n                    \"sex\": \"男的\",\n                    \"tel\": \"1111\"\n                }\n            }\n        ]\n    }\n}\n\n```\n\n查找sex含有”男“数据：\n\n```json\n#GET http://127.0.0.1:9200/user/_search\n{\n\t\"query\":{\n\t\t\"match\":{\n\t\t\t\"sex\":\"男\"\n\t\t}\n\t}\n}\n\n```\n\n返回结果如下：\n\n```json\n{\n    \"took\": 1,\n    \"timed_out\": false,\n    \"_shards\": {\n        \"total\": 1,\n        \"successful\": 1,\n        \"skipped\": 0,\n        \"failed\": 0\n    },\n    \"hits\": {\n        \"total\": {\n            \"value\": 0,\n            \"relation\": \"eq\"\n        },\n        \"max_score\": null,\n        \"hits\": []\n    }\n}\n\n```\n\n找不想要的结果，只因创建映射时\"sex\"的类型为\"keyword\"。\n\n\"sex\"只能完全为”男的“，才能得出原数据。\n\n```json\n#GET http://127.0.0.1:9200/user/_search\n{\n\t\"query\":{\n\t\t\"match\":{\n\t\t\t\"sex\":\"男的\"\n\t\t}\n\t}\n}\n\n```\n\n返回结果如下：\n\n```json\n{\n    \"took\": 2,\n    \"timed_out\": false,\n    \"_shards\": {\n        \"total\": 1,\n        \"successful\": 1,\n        \"skipped\": 0,\n        \"failed\": 0\n    },\n    \"hits\": {\n        \"total\": {\n            \"value\": 1,\n            \"relation\": \"eq\"\n        },\n        \"max_score\": 0.2876821,\n        \"hits\": [\n            {\n                \"_index\": \"user\",\n                \"_type\": \"_doc\",\n                \"_id\": \"1001\",\n                \"_score\": 0.2876821,\n                \"_source\": {\n                    \"name\": \"小米\",\n                    \"sex\": \"男的\",\n                    \"tel\": \"1111\"\n                }\n            }\n        ]\n    }\n}\n\n```\n\n查询电话\n\n```json\n# GET http://127.0.0.1:9200/user/_search\n{\n\t\"query\":{\n\t\t\"match\":{\n\t\t\t\"tel\":\"11\"\n\t\t}\n\t}\n}\n\n```\n\n返回结果如下：\n\n```json\n{\n    \"error\": {\n        \"root_cause\": [\n            {\n                \"type\": \"query_shard_exception\",\n                \"reason\": \"failed to create query: Cannot search on field [tel] since it is not indexed.\",\n                \"index_uuid\": \"ivLnMfQKROS7Skb2MTFOew\",\n                \"index\": \"user\"\n            }\n        ],\n        \"type\": \"search_phase_execution_exception\",\n        \"reason\": \"all shards failed\",\n        \"phase\": \"query\",\n        \"grouped\": true,\n        \"failed_shards\": [\n            {\n                \"shard\": 0,\n                \"index\": \"user\",\n                \"node\": \"4P7dIRfXSbezE5JTiuylew\",\n                \"reason\": {\n                    \"type\": \"query_shard_exception\",\n                    \"reason\": \"failed to create query: Cannot search on field [tel] since it is not indexed.\",\n                    \"index_uuid\": \"ivLnMfQKROS7Skb2MTFOew\",\n                    \"index\": \"user\",\n                    \"caused_by\": {\n                        \"type\": \"illegal_argument_exception\",\n                        \"reason\": \"Cannot search on field [tel] since it is not indexed.\"\n                    }\n                }\n            }\n        ]\n    },\n    \"status\": 400\n}\n\n```\n\n报错只因创建映射时\"tel\"的\"index\"为false。\n\n## 18-入门-JavaAPI-环境准备\n\n新建Maven工程。\n\n添加依赖：\n\n```xml\n<dependencies>\n    <dependency>\n        <groupId>org.elasticsearch</groupId>\n        <artifactId>elasticsearch</artifactId>\n        <version>7.8.0</version>\n    </dependency>\n    <!-- elasticsearch的客户端-->\n    <dependency>\n        <groupId>org.elasticsearch.client</groupId>\n        <artifactId>elasticsearch-rest-high-level-client</artifactId>\n        <version>7.8.0</version>\n    </dependency>\n    <!-- elasticsearch依赖2.x的log4j-->\n    <dependency>\n        <groupId>org.apache.logging.log4j</groupId>\n        <artifactId>log4j-api</artifactId>\n        <version>2.8.2</version>\n    </dependency>\n    <dependency>\n        <groupId>org.apache.logging.log4j</groupId>\n        <artifactId>log4j-core</artifactId>\n        <version>2.8.2</version>\n    </dependency>\n    <dependency>\n        <groupId>com.fasterxml.jackson.core</groupId>\n        <artifactId>jackson-databind</artifactId>\n        <version>2.9.9</version>\n    </dependency>\n    <!-- junit单元测试 -->\n    <dependency>\n        <groupId>junit</groupId>\n        <artifactId>junit</artifactId>\n        <version>4.12</version>\n    </dependency>\n</dependencies>\n\n```\n\nHelloElasticsearch\n\n```java\nimport java.io.IOException;\n\nimport org.apache.http.HttpHost;\nimport org.elasticsearch.client.RestClient;\nimport org.elasticsearch.client.RestHighLevelClient;\n\npublic class HelloElasticsearch {\n\n\tpublic static void main(String[] args) throws IOException {\n\t\t// 创建客户端对象\n\t\tRestHighLevelClient client = new RestHighLevelClient(\n\t\t\t\tRestClient.builder(new HttpHost(\"localhost\", 9200, \"http\")));\n//\t\t...\n\t\tSystem.out.println(client);\n\n\t\t// 关闭客户端连接\n\t\tclient.close();\n\t}\n}\n\n```\n\n## 19-入门-JavaAPI-索引-创建\n\n```java\nimport org.apache.http.HttpHost;\nimport org.elasticsearch.action.admin.indices.create.CreateIndexRequest;\nimport org.elasticsearch.action.admin.indices.create.CreateIndexResponse;\nimport org.elasticsearch.client.RequestOptions;\nimport org.elasticsearch.client.RestClient;\nimport org.elasticsearch.client.RestHighLevelClient;\n\nimport java.io.IOException;\n\npublic class CreateIndex {\n\n    public static void main(String[] args) throws IOException {\n        // 创建客户端对象\n        RestHighLevelClient client = new RestHighLevelClient(\n                RestClient.builder(new HttpHost(\"localhost\", 9200, \"http\")));\n\n        // 创建索引 - 请求对象\n        CreateIndexRequest request = new CreateIndexRequest(\"user2\");\n        // 发送请求，获取响应\n        CreateIndexResponse response = client.indices().create(request,\n                RequestOptions.DEFAULT);\n        boolean acknowledged = response.isAcknowledged();\n        // 响应状态\n        System.out.println(\"操作状态 = \" + acknowledged);\n\n        // 关闭客户端连接\n        client.close();\n    }\n\n}\n\n```\n\n后台打印：\n\n```\\\n四月 09, 2021 2:12:08 下午 org.elasticsearch.client.RestClient logResponse\n警告: request [PUT http://localhost:9200/user2?master_timeout=30s&include_type_name=true&timeout=30s] returned 1 warnings: [299 Elasticsearch-7.8.0-757314695644ea9a1dc2fecd26d1a43856725e65 \"[types removal] Using include_type_name in create index requests is deprecated. The parameter will be removed in the next major version.\"]\n操作状态 = true\n\nProcess finished with exit code 0\n\n```\n\n## 20-入门-JavaAPI-索引-查询 & 删除\n\n### 查询\n\n```java\nimport org.apache.http.HttpHost;\n\nimport org.elasticsearch.client.RequestOptions;\nimport org.elasticsearch.client.RestClient;\nimport org.elasticsearch.client.RestHighLevelClient;\nimport org.elasticsearch.client.indices.GetIndexRequest;\nimport org.elasticsearch.client.indices.GetIndexResponse;\n\nimport java.io.IOException;\n\npublic class SearchIndex {\n    public static void main(String[] args) throws IOException {\n        // 创建客户端对象\n        RestHighLevelClient client = new RestHighLevelClient(\n                RestClient.builder(new HttpHost(\"localhost\", 9200, \"http\")));\n\n        // 查询索引 - 请求对象\n        GetIndexRequest request = new GetIndexRequest(\"user2\");\n        // 发送请求，获取响应\n        GetIndexResponse response = client.indices().get(request,\n                RequestOptions.DEFAULT);\n        \n        System.out.println(\"aliases:\"+response.getAliases());\n        System.out.println(\"mappings:\"+response.getMappings());\n        System.out.println(\"settings:\"+response.getSettings());\n\n        client.close();\n    }\n}\n\n```\n\n后台打印：\n\n```\naliases:{user2=[]}\nmappings:{user2=org.elasticsearch.cluster.metadata.MappingMetadata@ad700514}\nsettings:{user2={\"index.creation_date\":\"1617948726976\",\"index.number_of_replicas\":\"1\",\"index.number_of_shards\":\"1\",\"index.provided_name\":\"user2\",\"index.uuid\":\"UGZ1ntcySnK6hWyP2qoVpQ\",\"index.version.created\":\"7080099\"}}\n\nProcess finished with exit code 0\n\n```\n\n### 删除\n\n```java\nimport org.apache.http.HttpHost;\nimport org.elasticsearch.action.admin.indices.delete.DeleteIndexRequest;\nimport org.elasticsearch.action.support.master.AcknowledgedResponse;\nimport org.elasticsearch.client.RequestOptions;\nimport org.elasticsearch.client.RestClient;\nimport org.elasticsearch.client.RestHighLevelClient;\n\nimport java.io.IOException;\n\npublic class DeleteIndex {\n    public static void main(String[] args) throws IOException {\n        RestHighLevelClient client = new RestHighLevelClient(\n                RestClient.builder(new HttpHost(\"localhost\", 9200, \"http\")));\n        // 删除索引 - 请求对象\n        DeleteIndexRequest request = new DeleteIndexRequest(\"user2\");\n        // 发送请求，获取响应\n        AcknowledgedResponse response = client.indices().delete(request,RequestOptions.DEFAULT);\n        // 操作结果\n        System.out.println(\"操作结果 ： \" + response.isAcknowledged());\n        client.close();\n    }\n}\n\n```\n\n后台打印：\n\n```\n操作结果：true\n\nProcess finished with exit code 0\n\n```\n\n## 21-入门-JavaAPI-文档-新增 & 修改\n\n### 重构\n\n上文由于频繁使用以下连接Elasticsearch和关闭它的代码，于是**个人**对它进行重构。\n\n```java\npublic class SomeClass {\n    public static void main(String[] args) throws IOException {\n        RestHighLevelClient client = new RestHighLevelClient(\n                RestClient.builder(new HttpHost(\"localhost\", 9200, \"http\")));\n\t\t\n        ...\n        \n        client.close();\n    }\n}\n\n```\n\n重构后的代码：\n\n```java\nimport org.elasticsearch.client.RestHighLevelClient;\n\npublic interface ElasticsearchTask {\n\n    void doSomething(RestHighLevelClient client) throws Exception;\n\n}\n\npublic class ConnectElasticsearch{\n\n    public static void connect(ElasticsearchTask task){\n        // 创建客户端对象\n        RestHighLevelClient client = new RestHighLevelClient(\n                RestClient.builder(new HttpHost(\"localhost\", 9200, \"http\")));\n        try {\n            task.doSomething(client);\n            // 关闭客户端连接\n            client.close();\n        } catch (Exception e) {\n            e.printStackTrace();\n        }\n    }\n}\n\n```\n\n接下来，如果想让Elasticsearch完成一些操作，就编写一个lambda式即可。\n\n```java\npublic class SomeClass {\n\n    public static void main(String[] args) {\n        ConnectElasticsearch.connect(client -> {\n\t\t\t//do something\n        });\n    }\n}\n\n\n```\n\n### 新增\n\n```java\nimport com.fasterxml.jackson.databind.ObjectMapper;\nimport com.lun.elasticsearch.hello.ConnectElasticsearch;\nimport com.lun.elasticsearch.model.User;\nimport org.elasticsearch.action.index.IndexRequest;\nimport org.elasticsearch.action.index.IndexResponse;\nimport org.elasticsearch.client.RequestOptions;\nimport org.elasticsearch.common.xcontent.XContentType;\n\npublic class InsertDoc {\n\n    public static void main(String[] args) {\n        ConnectElasticsearch.connect(client -> {\n            // 新增文档 - 请求对象\n            IndexRequest request = new IndexRequest();\n            // 设置索引及唯一性标识\n            request.index(\"user\").id(\"1001\");\n\n            // 创建数据对象\n            User user = new User();\n            user.setName(\"zhangsan\");\n            user.setAge(30);\n            user.setSex(\"男\");\n\n            ObjectMapper objectMapper = new ObjectMapper();\n            String productJson = objectMapper.writeValueAsString(user);\n            // 添加文档数据，数据格式为 JSON 格式\n            request.source(productJson, XContentType.JSON);\n            // 客户端发送请求，获取响应对象\n            IndexResponse response = client.index(request, RequestOptions.DEFAULT);\n            3.打印结果信息\n            System.out.println(\"_index:\" + response.getIndex());\n            System.out.println(\"_id:\" + response.getId());\n            System.out.println(\"_result:\" + response.getResult());\n        });\n    }\n}\n\n```\n\n后台打印：\n\n```\n_index:user\n_id:1001\n_result:UPDATED\n\nProcess finished with exit code 0\n\n```\n\n### 修改\n\n```java\nimport com.lun.elasticsearch.hello.ConnectElasticsearch;\nimport org.elasticsearch.action.update.UpdateRequest;\nimport org.elasticsearch.action.update.UpdateResponse;\nimport org.elasticsearch.client.RequestOptions;\nimport org.elasticsearch.common.xcontent.XContentType;\n\npublic class UpdateDoc {\n\n    public static void main(String[] args) {\n        ConnectElasticsearch.connect(client -> {\n            // 修改文档 - 请求对象\n            UpdateRequest request = new UpdateRequest();\n            // 配置修改参数\n            request.index(\"user\").id(\"1001\");\n            // 设置请求体，对数据进行修改\n            request.doc(XContentType.JSON, \"sex\", \"女\");\n            // 客户端发送请求，获取响应对象\n            UpdateResponse response = client.update(request, RequestOptions.DEFAULT);\n            System.out.println(\"_index:\" + response.getIndex());\n            System.out.println(\"_id:\" + response.getId());\n            System.out.println(\"_result:\" + response.getResult());\n        });\n    }\n\n}\n\n```\n\n后台打印：\n\n```\n_index:user\n_id:1001\n_result:UPDATED\n\nProcess finished with exit code 0\n\n```\n\n## 22-入门-JavaAPI-文档-查询 & 删除\n\n### 查询\n\n```java\nimport com.lun.elasticsearch.hello.ConnectElasticsearch;\nimport org.elasticsearch.action.get.GetRequest;\nimport org.elasticsearch.action.get.GetResponse;\nimport org.elasticsearch.client.RequestOptions;\n\npublic class GetDoc {\n\n    public static void main(String[] args) {\n        ConnectElasticsearch.connect(client -> {\n            //1.创建请求对象\n            GetRequest request = new GetRequest().index(\"user\").id(\"1001\");\n            //2.客户端发送请求，获取响应对象\n            GetResponse response = client.get(request, RequestOptions.DEFAULT);\n            3.打印结果信息\n            System.out.println(\"_index:\" + response.getIndex());\n            System.out.println(\"_type:\" + response.getType());\n            System.out.println(\"_id:\" + response.getId());\n            System.out.println(\"source:\" + response.getSourceAsString());\n        });\n    }\n}\n\n\n```\n\n后台打印：\n\n```\n_index:user\n_type:_doc\n_id:1001\nsource:{\"name\":\"zhangsan\",\"age\":30,\"sex\":\"男\"}\n\nProcess finished with exit code 0\n\n```\n\n### 删除\n\n```java\nimport com.lun.elasticsearch.hello.ConnectElasticsearch;\nimport org.elasticsearch.action.delete.DeleteRequest;\nimport org.elasticsearch.action.delete.DeleteResponse;\nimport org.elasticsearch.client.RequestOptions;\n\npublic class DeleteDoc {\n    public static void main(String[] args) {\n        ConnectElasticsearch.connect(client -> {\n            //创建请求对象\n            DeleteRequest request = new DeleteRequest().index(\"user\").id(\"1001\");\n            //客户端发送请求，获取响应对象\n            DeleteResponse response = client.delete(request, RequestOptions.DEFAULT);\n            //打印信息\n            System.out.println(response.toString());\n        });\n    }\n}\n\n```\n\n后台打印：\n\n```\nDeleteResponse[index=user,type=_doc,id=1001,version=16,result=deleted,shards=ShardInfo{total=2, successful=1, failures=[]}]\n\nProcess finished with exit code 0\n\n```\n\n## 23-入门-JavaAPI-文档-批量新增 & 批量删除\n\n### 批量新增\n\n```java\nimport com.lun.elasticsearch.hello.ConnectElasticsearch;\nimport org.elasticsearch.action.bulk.BulkRequest;\nimport org.elasticsearch.action.bulk.BulkResponse;\nimport org.elasticsearch.action.index.IndexRequest;\nimport org.elasticsearch.client.RequestOptions;\nimport org.elasticsearch.common.xcontent.XContentType;\n\npublic class BatchInsertDoc {\n\n    public static void main(String[] args) {\n        ConnectElasticsearch.connect(client -> {\n            //创建批量新增请求对象\n            BulkRequest request = new BulkRequest();\n            request.add(new\n                    IndexRequest().index(\"user\").id(\"1001\").source(XContentType.JSON, \"name\",\n                    \"zhangsan\"));\n            request.add(new\n                    IndexRequest().index(\"user\").id(\"1002\").source(XContentType.JSON, \"name\",\n                            \"lisi\"));\n            request.add(new\n                    IndexRequest().index(\"user\").id(\"1003\").source(XContentType.JSON, \"name\",\n                    \"wangwu\"));\n            //客户端发送请求，获取响应对象\n            BulkResponse responses = client.bulk(request, RequestOptions.DEFAULT);\n            //打印结果信息\n            System.out.println(\"took:\" + responses.getTook());\n            System.out.println(\"items:\" + responses.getItems());\n        });\n    }\n}\n\n```\n\n后台打印\n\n```\ntook:294ms\nitems:[Lorg.elasticsearch.action.bulk.BulkItemResponse;@2beee7ff\n\nProcess finished with exit code 0\n\n```\n\n### 批量删除\n\n```java\nimport com.lun.elasticsearch.hello.ConnectElasticsearch;\nimport org.elasticsearch.action.bulk.BulkRequest;\nimport org.elasticsearch.action.bulk.BulkResponse;\nimport org.elasticsearch.action.delete.DeleteRequest;\nimport org.elasticsearch.client.RequestOptions;\n\npublic class BatchDeleteDoc {\n    public static void main(String[] args) {\n        ConnectElasticsearch.connect(client -> {\n            //创建批量删除请求对象\n            BulkRequest request = new BulkRequest();\n            request.add(new DeleteRequest().index(\"user\").id(\"1001\"));\n            request.add(new DeleteRequest().index(\"user\").id(\"1002\"));\n            request.add(new DeleteRequest().index(\"user\").id(\"1003\"));\n            //客户端发送请求，获取响应对象\n            BulkResponse responses = client.bulk(request, RequestOptions.DEFAULT);\n            //打印结果信息\n            System.out.println(\"took:\" + responses.getTook());\n            System.out.println(\"items:\" + responses.getItems());\n        });\n    }\n}\n\n```\n\n后台打印\n\n```\ntook:108ms\nitems:[Lorg.elasticsearch.action.bulk.BulkItemResponse;@7b02881e\n\nProcess finished with exit code 0\n\n\n```\n\n## 24-入门-JavaAPI-文档-高级查询-全量查询\n\n先批量增加数据\n\n```java\npublic class BatchInsertDoc {\n\n    public static void main(String[] args) {\n        ConnectElasticsearch.connect(client -> {\n            //创建批量新增请求对象\n            BulkRequest request = new BulkRequest();\n            request.add(new IndexRequest().index(\"user\").id(\"1001\").source(XContentType.JSON, \"name\", \"zhangsan\", \"age\", \"10\", \"sex\",\"女\"));\n            request.add(new IndexRequest().index(\"user\").id(\"1002\").source(XContentType.JSON, \"name\", \"lisi\", \"age\", \"30\", \"sex\",\"女\"));\n            request.add(new IndexRequest().index(\"user\").id(\"1003\").source(XContentType.JSON, \"name\", \"wangwu1\", \"age\", \"40\", \"sex\",\"男\"));\n            request.add(new IndexRequest().index(\"user\").id(\"1004\").source(XContentType.JSON, \"name\", \"wangwu2\", \"age\", \"20\", \"sex\",\"女\"));\n            request.add(new IndexRequest().index(\"user\").id(\"1005\").source(XContentType.JSON, \"name\", \"wangwu3\", \"age\", \"50\", \"sex\",\"男\"));\n            request.add(new IndexRequest().index(\"user\").id(\"1006\").source(XContentType.JSON, \"name\", \"wangwu4\", \"age\", \"20\", \"sex\",\"男\"));\n            //客户端发送请求，获取响应对象\n            BulkResponse responses = client.bulk(request, RequestOptions.DEFAULT);\n            //打印结果信息\n            System.out.println(\"took:\" + responses.getTook());\n            System.out.println(\"items:\" + responses.getItems());\n        });\n    }\n}\n\n```\n\n后台打印\n\n```\ntook:168ms\nitems:[Lorg.elasticsearch.action.bulk.BulkItemResponse;@2beee7ff\n\nProcess finished with exit code 0\n\n```\n\n**查询所有索引数据**\n\n```java\nimport com.lun.elasticsearch.hello.ConnectElasticsearch;\nimport org.elasticsearch.action.search.SearchRequest;\nimport org.elasticsearch.action.search.SearchResponse;\nimport org.elasticsearch.client.RequestOptions;\nimport org.elasticsearch.index.query.QueryBuilders;\nimport org.elasticsearch.search.SearchHit;\nimport org.elasticsearch.search.SearchHits;\nimport org.elasticsearch.search.builder.SearchSourceBuilder;\n\npublic class QueryDoc {\n\n    public static void main(String[] args) {\n        ConnectElasticsearch.connect(client -> {\n            // 创建搜索请求对象\n            SearchRequest request = new SearchRequest();\n            request.indices(\"user\");\n            // 构建查询的请求体\n            SearchSourceBuilder sourceBuilder = new SearchSourceBuilder();\n            // 查询所有数据\n            sourceBuilder.query(QueryBuilders.matchAllQuery());\n            request.source(sourceBuilder);\n            SearchResponse response = client.search(request, RequestOptions.DEFAULT);\n            // 查询匹配\n            SearchHits hits = response.getHits();\n            System.out.println(\"took:\" + response.getTook());\n            System.out.println(\"timeout:\" + response.isTimedOut());\n            System.out.println(\"total:\" + hits.getTotalHits());\n            System.out.println(\"MaxScore:\" + hits.getMaxScore());\n            System.out.println(\"hits========>>\");\n            for (SearchHit hit : hits) {\n            //输出每条查询的结果信息\n                System.out.println(hit.getSourceAsString());\n            }\n            System.out.println(\"<<========\");\n        });\n    }\n\n}\n\n```\n\n后台打印\n\n```\ntook:2ms\ntimeout:false\ntotal:6 hits\nMaxScore:1.0\nhits========>>\n{\"name\":\"zhangsan\",\"age\":\"10\",\"sex\":\"女\"}\n{\"name\":\"lisi\",\"age\":\"30\",\"sex\":\"女\"}\n{\"name\":\"wangwu1\",\"age\":\"40\",\"sex\":\"男\"}\n{\"name\":\"wangwu2\",\"age\":\"20\",\"sex\":\"女\"}\n{\"name\":\"wangwu3\",\"age\":\"50\",\"sex\":\"男\"}\n{\"name\":\"wangwu4\",\"age\":\"20\",\"sex\":\"男\"}\n<<========\n\nProcess finished with exit code 0\n\n```\n\n## 25-入门-JavaAPI-文档-高级查询-分页查询 & 条件查询 & 查询排序\n\n### 条件查询\n\n```java\nimport com.lun.elasticsearch.hello.ConnectElasticsearch;\nimport com.lun.elasticsearch.hello.ElasticsearchTask;\nimport org.elasticsearch.action.search.SearchRequest;\nimport org.elasticsearch.action.search.SearchResponse;\nimport org.elasticsearch.client.RequestOptions;\nimport org.elasticsearch.index.query.QueryBuilders;\nimport org.elasticsearch.search.SearchHit;\nimport org.elasticsearch.search.SearchHits;\nimport org.elasticsearch.search.builder.SearchSourceBuilder;\nimport org.elasticsearch.search.sort.SortOrder;\n\npublic class QueryDoc {\n    \n\tpublic static final ElasticsearchTask SEARCH_BY_CONDITION = client -> {\n        // 创建搜索请求对象\n        SearchRequest request = new SearchRequest();\n        request.indices(\"user\");\n        // 构建查询的请求体\n        SearchSourceBuilder sourceBuilder = new SearchSourceBuilder();\n        sourceBuilder.query(QueryBuilders.termQuery(\"age\", \"30\"));\n        request.source(sourceBuilder);\n        SearchResponse response = client.search(request, RequestOptions.DEFAULT);\n        // 查询匹配\n        SearchHits hits = response.getHits();\n        System.out.println(\"took:\" + response.getTook());\n        System.out.println(\"timeout:\" + response.isTimedOut());\n        System.out.println(\"total:\" + hits.getTotalHits());\n        System.out.println(\"MaxScore:\" + hits.getMaxScore());\n        System.out.println(\"hits========>>\");\n        for (SearchHit hit : hits) {\n            //输出每条查询的结果信息\n            System.out.println(hit.getSourceAsString());\n        }\n        System.out.println(\"<<========\");\n    };\n    \n    public static void main(String[] args) {\n        ConnectElasticsearch.connect(SEARCH_BY_CONDITION);\n    }\n}\n\n```\n\n后台打印\n\n```\ntook:1ms\ntimeout:false\ntotal:1 hits\nMaxScore:1.0\nhits========>>\n{\"name\":\"lisi\",\"age\":\"30\",\"sex\":\"女\"}\n<<========\n\n```\n\n### 分页查询\n\n```java\nimport com.lun.elasticsearch.hello.ConnectElasticsearch;\nimport com.lun.elasticsearch.hello.ElasticsearchTask;\nimport org.elasticsearch.action.search.SearchRequest;\nimport org.elasticsearch.action.search.SearchResponse;\nimport org.elasticsearch.client.RequestOptions;\nimport org.elasticsearch.index.query.QueryBuilders;\nimport org.elasticsearch.search.SearchHit;\nimport org.elasticsearch.search.SearchHits;\nimport org.elasticsearch.search.builder.SearchSourceBuilder;\nimport org.elasticsearch.search.sort.SortOrder;\n\npublic class QueryDoc {\n    \n\tpublic static final ElasticsearchTask SEARCH_BY_PAGING = client -> {\n        // 创建搜索请求对象\n        SearchRequest request = new SearchRequest();\n        request.indices(\"user\");\n        // 构建查询的请求体\n        SearchSourceBuilder sourceBuilder = new SearchSourceBuilder();\n        sourceBuilder.query(QueryBuilders.matchAllQuery());\n        // 分页查询\n        // 当前页其实索引(第一条数据的顺序号)， from\n        sourceBuilder.from(0);\n\n        // 每页显示多少条 size\n        sourceBuilder.size(2);\n        request.source(sourceBuilder);\n        SearchResponse response = client.search(request, RequestOptions.DEFAULT);\n        // 查询匹配\n        SearchHits hits = response.getHits();\n        System.out.println(\"took:\" + response.getTook());\n        System.out.println(\"timeout:\" + response.isTimedOut());\n        System.out.println(\"total:\" + hits.getTotalHits());\n        System.out.println(\"MaxScore:\" + hits.getMaxScore());\n        System.out.println(\"hits========>>\");\n        for (SearchHit hit : hits) {\n            //输出每条查询的结果信息\n            System.out.println(hit.getSourceAsString());\n        }\n        System.out.println(\"<<========\");\n    };\n    \n    public static void main(String[] args) {\n        ConnectElasticsearch.connect(SEARCH_BY_CONDITION);\n    }\n\n}\n\n```\n\n后台打印\n\n```\ntook:1ms\ntimeout:false\ntotal:6 hits\nMaxScore:1.0\nhits========>>\n{\"name\":\"zhangsan\",\"age\":\"10\",\"sex\":\"女\"}\n{\"name\":\"lisi\",\"age\":\"30\",\"sex\":\"女\"}\n<<========\n\n```\n\n### 查询排序\n\n```java\nimport com.lun.elasticsearch.hello.ConnectElasticsearch;\nimport com.lun.elasticsearch.hello.ElasticsearchTask;\nimport org.elasticsearch.action.search.SearchRequest;\nimport org.elasticsearch.action.search.SearchResponse;\nimport org.elasticsearch.client.RequestOptions;\nimport org.elasticsearch.index.query.QueryBuilders;\nimport org.elasticsearch.search.SearchHit;\nimport org.elasticsearch.search.SearchHits;\nimport org.elasticsearch.search.builder.SearchSourceBuilder;\nimport org.elasticsearch.search.sort.SortOrder;\n\npublic class QueryDoc {\n    \n\tpublic static final ElasticsearchTask SEARCH_WITH_ORDER = client -> {\n        // 创建搜索请求对象\n        SearchRequest request = new SearchRequest();\n        request.indices(\"user\");\n\n        // 构建查询的请求体\n        SearchSourceBuilder sourceBuilder = new SearchSourceBuilder();\n        sourceBuilder.query(QueryBuilders.matchAllQuery());\n        // 排序\n        sourceBuilder.sort(\"age\", SortOrder.ASC);\n        request.source(sourceBuilder);\n        SearchResponse response = client.search(request, RequestOptions.DEFAULT);\n        // 查询匹配\n        SearchHits hits = response.getHits();\n        System.out.println(\"took:\" + response.getTook());\n        System.out.println(\"timeout:\" + response.isTimedOut());\n        System.out.println(\"total:\" + hits.getTotalHits());\n        System.out.println(\"MaxScore:\" + hits.getMaxScore());\n        System.out.println(\"hits========>>\");\n        for (SearchHit hit : hits) {\n        //输出每条查询的结果信息\n            System.out.println(hit.getSourceAsString());\n        }\n        System.out.println(\"<<========\");\n    };\n\n    public static void main(String[] args) {\n        ConnectElasticsearch.connect(SEARCH_WITH_ORDER);\n    }\n\n}\n\n\n```\n\n后台打印\n\n```\ntook:1ms\ntimeout:false\ntotal:6 hits\nMaxScore:NaN\nhits========>>\n{\"name\":\"zhangsan\",\"age\":\"10\",\"sex\":\"女\"}\n{\"name\":\"wangwu2\",\"age\":\"20\",\"sex\":\"女\"}\n{\"name\":\"wangwu4\",\"age\":\"20\",\"sex\":\"男\"}\n{\"name\":\"lisi\",\"age\":\"30\",\"sex\":\"女\"}\n{\"name\":\"wangwu1\",\"age\":\"40\",\"sex\":\"男\"}\n{\"name\":\"wangwu3\",\"age\":\"50\",\"sex\":\"男\"}\n<<========\n\n```\n\n## 26-入门-JavaAPI-文档-高级查询-组合查询 & 范围查询\n\n### 组合查询\n\n```java\nimport com.lun.elasticsearch.hello.ConnectElasticsearch;\nimport com.lun.elasticsearch.hello.ElasticsearchTask;\nimport org.elasticsearch.action.search.SearchRequest;\nimport org.elasticsearch.action.search.SearchResponse;\nimport org.elasticsearch.client.RequestOptions;\nimport org.elasticsearch.index.query.BoolQueryBuilder;\nimport org.elasticsearch.index.query.QueryBuilders;\nimport org.elasticsearch.search.SearchHit;\nimport org.elasticsearch.search.SearchHits;\nimport org.elasticsearch.search.builder.SearchSourceBuilder;\nimport org.elasticsearch.search.sort.SortOrder;\n\npublic class QueryDoc {\n    \n\tpublic static final ElasticsearchTask SEARCH_BY_BOOL_CONDITION = client -> {\n        // 创建搜索请求对象\n        SearchRequest request = new SearchRequest();\n        request.indices(\"user\");\n        // 构建查询的请求体\n        SearchSourceBuilder sourceBuilder = new SearchSourceBuilder();\n        BoolQueryBuilder boolQueryBuilder = QueryBuilders.boolQuery();\n        // 必须包含\n        boolQueryBuilder.must(QueryBuilders.matchQuery(\"age\", \"30\"));\n        // 一定不含\n        boolQueryBuilder.mustNot(QueryBuilders.matchQuery(\"name\", \"zhangsan\"));\n        // 可能包含\n        boolQueryBuilder.should(QueryBuilders.matchQuery(\"sex\", \"男\"));\n        sourceBuilder.query(boolQueryBuilder);\n        request.source(sourceBuilder);\n        SearchResponse response = client.search(request, RequestOptions.DEFAULT);\n        // 查询匹配\n        SearchHits hits = response.getHits();\n        System.out.println(\"took:\" + response.getTook());\n        System.out.println(\"timeout:\" + response.isTimedOut());\n        System.out.println(\"total:\" + hits.getTotalHits());\n        System.out.println(\"MaxScore:\" + hits.getMaxScore());\n        System.out.println(\"hits========>>\");\n        for (SearchHit hit : hits) {\n            //输出每条查询的结果信息\n            System.out.println(hit.getSourceAsString());\n        }\n        System.out.println(\"<<========\");\n\n    };\n\n    public static void main(String[] args) {\n        ConnectElasticsearch.connect(SEARCH_BY_BOOL_CONDITION);\n    }\n}\n\n```\n\n后台打印\n\n```\ntook:28ms\ntimeout:false\ntotal:1 hits\nMaxScore:1.0\nhits========>>\n{\"name\":\"lisi\",\"age\":\"30\",\"sex\":\"女\"}\n<<========\n\nProcess finished with exit code 0\n\n```\n\n### 范围查询\n\n```java\nimport com.lun.elasticsearch.hello.ConnectElasticsearch;\nimport com.lun.elasticsearch.hello.ElasticsearchTask;\nimport org.elasticsearch.action.search.SearchRequest;\nimport org.elasticsearch.action.search.SearchResponse;\nimport org.elasticsearch.client.RequestOptions;\nimport org.elasticsearch.index.query.BoolQueryBuilder;\nimport org.elasticsearch.index.query.QueryBuilders;\nimport org.elasticsearch.index.query.RangeQueryBuilder;\nimport org.elasticsearch.search.SearchHit;\nimport org.elasticsearch.search.SearchHits;\nimport org.elasticsearch.search.builder.SearchSourceBuilder;\nimport org.elasticsearch.search.sort.SortOrder;\n\npublic class QueryDoc {\n    \n\tpublic static final ElasticsearchTask SEARCH_BY_RANGE = client -> {\n        // 创建搜索请求对象\n        SearchRequest request = new SearchRequest();\n        request.indices(\"user\");\n        // 构建查询的请求体\n        SearchSourceBuilder sourceBuilder = new SearchSourceBuilder();\n        RangeQueryBuilder rangeQuery = QueryBuilders.rangeQuery(\"age\");\n        // 大于等于\n        //rangeQuery.gte(\"30\");\n        // 小于等于\n        rangeQuery.lte(\"40\");\n        sourceBuilder.query(rangeQuery);\n        request.source(sourceBuilder);\n        SearchResponse response = client.search(request, RequestOptions.DEFAULT);\n        // 查询匹配\n        SearchHits hits = response.getHits();\n        System.out.println(\"took:\" + response.getTook());\n        System.out.println(\"timeout:\" + response.isTimedOut());\n        System.out.println(\"total:\" + hits.getTotalHits());\n        System.out.println(\"MaxScore:\" + hits.getMaxScore());\n        System.out.println(\"hits========>>\");\n        for (SearchHit hit : hits) {\n        //输出每条查询的结果信息\n            System.out.println(hit.getSourceAsString());\n        }\n        System.out.println(\"<<========\");\n    };\n\n    public static void main(String[] args) {\n        ConnectElasticsearch.connect(SEARCH_BY_RANGE);\n    }\n\n}\n\n\n```\n\n后台打印\n\n```\ntook:1ms\ntimeout:false\ntotal:5 hits\nMaxScore:1.0\nhits========>>\n{\"name\":\"zhangsan\",\"age\":\"10\",\"sex\":\"女\"}\n{\"name\":\"lisi\",\"age\":\"30\",\"sex\":\"女\"}\n{\"name\":\"wangwu1\",\"age\":\"40\",\"sex\":\"男\"}\n{\"name\":\"wangwu2\",\"age\":\"20\",\"sex\":\"女\"}\n{\"name\":\"wangwu4\",\"age\":\"20\",\"sex\":\"男\"}\n<<========\n\nProcess finished with exit code 0\n\n```\n\n## 27-入门-JavaAPI-文档-高级查询-模糊查询 & 高亮查询\n\n### 模糊查询\n\n```java\nimport com.lun.elasticsearch.hello.ConnectElasticsearch;\nimport com.lun.elasticsearch.hello.ElasticsearchTask;\nimport org.elasticsearch.action.search.SearchRequest;\nimport org.elasticsearch.action.search.SearchResponse;\nimport org.elasticsearch.client.RequestOptions;\nimport org.elasticsearch.common.unit.Fuzziness;\nimport org.elasticsearch.index.query.BoolQueryBuilder;\nimport org.elasticsearch.index.query.QueryBuilders;\nimport org.elasticsearch.index.query.RangeQueryBuilder;\nimport org.elasticsearch.search.SearchHit;\nimport org.elasticsearch.search.SearchHits;\nimport org.elasticsearch.search.builder.SearchSourceBuilder;\nimport org.elasticsearch.search.sort.SortOrder;\n\npublic class QueryDoc {\n    \n    public static final ElasticsearchTask SEARCH_BY_FUZZY_CONDITION = client -> {\n        // 创建搜索请求对象\n        SearchRequest request = new SearchRequest();\n        request.indices(\"user\");\n        // 构建查询的请求体\n        SearchSourceBuilder sourceBuilder = new SearchSourceBuilder();\n        sourceBuilder.query(QueryBuilders.fuzzyQuery(\"name\",\"wangwu\").fuzziness(Fuzziness.ONE));\n        request.source(sourceBuilder);\n        SearchResponse response = client.search(request, RequestOptions.DEFAULT);\n        // 查询匹配\n        SearchHits hits = response.getHits();\n        System.out.println(\"took:\" + response.getTook());\n        System.out.println(\"timeout:\" + response.isTimedOut());\n        System.out.println(\"total:\" + hits.getTotalHits());\n        System.out.println(\"MaxScore:\" + hits.getMaxScore());\n        System.out.println(\"hits========>>\");\n        for (SearchHit hit : hits) {\n            //输出每条查询的结果信息\n            System.out.println(hit.getSourceAsString());\n        }\n        System.out.println(\"<<========\");\n    };\n\n\n    public static void main(String[] args) {\n//        ConnectElasticsearch.connect(SEARCH_ALL);\n//        ConnectElasticsearch.connect(SEARCH_BY_CONDITION);\n//        ConnectElasticsearch.connect(SEARCH_BY_PAGING);\n//        ConnectElasticsearch.connect(SEARCH_WITH_ORDER);\n//        ConnectElasticsearch.connect(SEARCH_BY_BOOL_CONDITION);\n//        ConnectElasticsearch.connect(SEARCH_BY_RANGE);\n        ConnectElasticsearch.connect(SEARCH_BY_FUZZY_CONDITION);\n    }\n\n}\n\n```\n\n后台打印\n\n```\ntook:152ms\ntimeout:false\ntotal:4 hits\nMaxScore:1.2837042\nhits========>>\n{\"name\":\"wangwu1\",\"age\":\"40\",\"sex\":\"男\"}\n{\"name\":\"wangwu2\",\"age\":\"20\",\"sex\":\"女\"}\n{\"name\":\"wangwu3\",\"age\":\"50\",\"sex\":\"男\"}\n{\"name\":\"wangwu4\",\"age\":\"20\",\"sex\":\"男\"}\n<<========\n\nProcess finished with exit code 0\n\n```\n\n### 高亮查询\n\n```java\nimport com.lun.elasticsearch.hello.ConnectElasticsearch;\nimport com.lun.elasticsearch.hello.ElasticsearchTask;\nimport org.elasticsearch.action.search.SearchRequest;\nimport org.elasticsearch.action.search.SearchResponse;\nimport org.elasticsearch.client.RequestOptions;\nimport org.elasticsearch.common.unit.Fuzziness;\nimport org.elasticsearch.index.query.BoolQueryBuilder;\nimport org.elasticsearch.index.query.QueryBuilders;\nimport org.elasticsearch.index.query.RangeQueryBuilder;\nimport org.elasticsearch.index.query.TermsQueryBuilder;\nimport org.elasticsearch.search.SearchHit;\nimport org.elasticsearch.search.SearchHits;\nimport org.elasticsearch.search.builder.SearchSourceBuilder;\nimport org.elasticsearch.search.fetch.subphase.highlight.HighlightBuilder;\nimport org.elasticsearch.search.fetch.subphase.highlight.HighlightField;\nimport org.elasticsearch.search.sort.SortOrder;\n\nimport java.util.Map;\n\npublic class QueryDoc {\n    \n    public static final ElasticsearchTask SEARCH_WITH_HIGHLIGHT = client -> {\n        // 高亮查询\n        SearchRequest request = new SearchRequest().indices(\"user\");\n        //2.创建查询请求体构建器\n        SearchSourceBuilder sourceBuilder = new SearchSourceBuilder();\n        //构建查询方式：高亮查询\n        TermsQueryBuilder termsQueryBuilder =\n                QueryBuilders.termsQuery(\"name\",\"zhangsan\");\n        //设置查询方式\n        sourceBuilder.query(termsQueryBuilder);\n        //构建高亮字段\n        HighlightBuilder highlightBuilder = new HighlightBuilder();\n        highlightBuilder.preTags(\"<font color='red'>\");//设置标签前缀\n        highlightBuilder.postTags(\"</font>\");//设置标签后缀\n        highlightBuilder.field(\"name\");//设置高亮字段\n        //设置高亮构建对象\n        sourceBuilder.highlighter(highlightBuilder);\n        //设置请求体\n        request.source(sourceBuilder);\n        //3.客户端发送请求，获取响应对象\n        SearchResponse response = client.search(request, RequestOptions.DEFAULT);\n        //4.打印响应结果\n        SearchHits hits = response.getHits();\n        System.out.println(\"took::\"+response.getTook());\n        System.out.println(\"time_out::\"+response.isTimedOut());\n        System.out.println(\"total::\"+hits.getTotalHits());\n        System.out.println(\"max_score::\"+hits.getMaxScore());\n        System.out.println(\"hits::::>>\");\n        for (SearchHit hit : hits) {\n            String sourceAsString = hit.getSourceAsString();\n            System.out.println(sourceAsString);\n            //打印高亮结果\n            Map<String, HighlightField> highlightFields = hit.getHighlightFields();\n            System.out.println(highlightFields);\n        }\n        System.out.println(\"<<::::\");\n    };\n\n\n    public static void main(String[] args) {\n        ConnectElasticsearch.connect(SEARCH_WITH_HIGHLIGHT);\n    }\n\n}\n\n```\n\n后台打印\n\n```\ntook::672ms\ntime_out::false\ntotal::1 hits\nmax_score::1.0\nhits::::>>\n{\"name\":\"zhangsan\",\"age\":\"10\",\"sex\":\"女\"}\n{name=[name], fragments[[<font color='red'>zhangsan</font>]]}\n<<::::\n\nProcess finished with exit code 0\n\n```\n\n## 28-入门-JavaAPI-文档-高级查询-最大值查询 & 分组查询\n\n### 最大值查询\n\n```java\nimport com.lun.elasticsearch.hello.ConnectElasticsearch;\nimport com.lun.elasticsearch.hello.ElasticsearchTask;\nimport org.elasticsearch.action.search.SearchRequest;\nimport org.elasticsearch.action.search.SearchResponse;\nimport org.elasticsearch.client.RequestOptions;\nimport org.elasticsearch.common.unit.Fuzziness;\nimport org.elasticsearch.index.query.BoolQueryBuilder;\nimport org.elasticsearch.index.query.QueryBuilders;\nimport org.elasticsearch.index.query.RangeQueryBuilder;\nimport org.elasticsearch.index.query.TermsQueryBuilder;\nimport org.elasticsearch.search.SearchHit;\nimport org.elasticsearch.search.SearchHits;\nimport org.elasticsearch.search.aggregations.AggregationBuilders;\nimport org.elasticsearch.search.builder.SearchSourceBuilder;\nimport org.elasticsearch.search.fetch.subphase.highlight.HighlightBuilder;\nimport org.elasticsearch.search.fetch.subphase.highlight.HighlightField;\nimport org.elasticsearch.search.sort.SortOrder;\n\nimport java.util.Map;\n\npublic class QueryDoc {\n    \n    public static final ElasticsearchTask SEARCH_WITH_MAX = client -> {\n        // 高亮查询\n        SearchRequest request = new SearchRequest().indices(\"user\");\n        SearchSourceBuilder sourceBuilder = new SearchSourceBuilder();\n        sourceBuilder.aggregation(AggregationBuilders.max(\"maxAge\").field(\"age\"));\n        //设置请求体\n        request.source(sourceBuilder);\n        //3.客户端发送请求，获取响应对象\n        SearchResponse response = client.search(request, RequestOptions.DEFAULT);\n        //4.打印响应结果\n        SearchHits hits = response.getHits();\n        System.out.println(response);\n    };\n\n    public static void main(String[] args) {\n        ConnectElasticsearch.connect(SEARCH_WITH_MAX);\n    }\n\n}\n\n```\n\n后台打印\n\n```\n{\"took\":16,\"timed_out\":false,\"_shards\":{\"total\":1,\"successful\":1,\"skipped\":0,\"failed\":0},\"hits\":{\"total\":{\"value\":6,\"relation\":\"eq\"},\"max_score\":1.0,\"hits\":[{\"_index\":\"user\",\"_type\":\"_doc\",\"_id\":\"1001\",\"_score\":1.0,\"_source\":{\"name\":\"zhangsan\",\"age\":\"10\",\"sex\":\"女\"}},{\"_index\":\"user\",\"_type\":\"_doc\",\"_id\":\"1002\",\"_score\":1.0,\"_source\":{\"name\":\"lisi\",\"age\":\"30\",\"sex\":\"女\"}},{\"_index\":\"user\",\"_type\":\"_doc\",\"_id\":\"1003\",\"_score\":1.0,\"_source\":{\"name\":\"wangwu1\",\"age\":\"40\",\"sex\":\"男\"}},{\"_index\":\"user\",\"_type\":\"_doc\",\"_id\":\"1004\",\"_score\":1.0,\"_source\":{\"name\":\"wangwu2\",\"age\":\"20\",\"sex\":\"女\"}},{\"_index\":\"user\",\"_type\":\"_doc\",\"_id\":\"1005\",\"_score\":1.0,\"_source\":{\"name\":\"wangwu3\",\"age\":\"50\",\"sex\":\"男\"}},{\"_index\":\"user\",\"_type\":\"_doc\",\"_id\":\"1006\",\"_score\":1.0,\"_source\":{\"name\":\"wangwu4\",\"age\":\"20\",\"sex\":\"男\"}}]},\"aggregations\":{\"max#maxAge\":{\"value\":50.0}}}\n\nProcess finished with exit code 0\n\n```\n\n### 分组查询\n\n```java\nimport com.lun.elasticsearch.hello.ConnectElasticsearch;\nimport com.lun.elasticsearch.hello.ElasticsearchTask;\nimport org.elasticsearch.action.search.SearchRequest;\nimport org.elasticsearch.action.search.SearchResponse;\nimport org.elasticsearch.client.RequestOptions;\nimport org.elasticsearch.common.unit.Fuzziness;\nimport org.elasticsearch.index.query.BoolQueryBuilder;\nimport org.elasticsearch.index.query.QueryBuilders;\nimport org.elasticsearch.index.query.RangeQueryBuilder;\nimport org.elasticsearch.index.query.TermsQueryBuilder;\nimport org.elasticsearch.search.SearchHit;\nimport org.elasticsearch.search.SearchHits;\nimport org.elasticsearch.search.aggregations.AggregationBuilders;\nimport org.elasticsearch.search.builder.SearchSourceBuilder;\nimport org.elasticsearch.search.fetch.subphase.highlight.HighlightBuilder;\nimport org.elasticsearch.search.fetch.subphase.highlight.HighlightField;\nimport org.elasticsearch.search.sort.SortOrder;\n\nimport java.util.Map;\n\npublic class QueryDoc {\n\n\tpublic static final ElasticsearchTask SEARCH_WITH_GROUP = client -> {\n        SearchRequest request = new SearchRequest().indices(\"user\");\n        SearchSourceBuilder sourceBuilder = new SearchSourceBuilder();\n        sourceBuilder.aggregation(AggregationBuilders.terms(\"age_groupby\").field(\"age\"));\n        //设置请求体\n        request.source(sourceBuilder);\n        //3.客户端发送请求，获取响应对象\n        SearchResponse response = client.search(request, RequestOptions.DEFAULT);\n        //4.打印响应结果\n        SearchHits hits = response.getHits();\n        System.out.println(response);\n    };\n\n    public static void main(String[] args) {\n        ConnectElasticsearch.connect(SEARCH_WITH_GROUP);\n    }\n\n}\n\n```\n\n后台打印\n\n```\n{\"took\":10,\"timed_out\":false,\"_shards\":{\"total\":1,\"successful\":1,\"skipped\":0,\"failed\":0},\"hits\":{\"total\":{\"value\":6,\"relation\":\"eq\"},\"max_score\":1.0,\"hits\":[{\"_index\":\"user\",\"_type\":\"_doc\",\"_id\":\"1001\",\"_score\":1.0,\"_source\":{\"name\":\"zhangsan\",\"age\":\"10\",\"sex\":\"女\"}},{\"_index\":\"user\",\"_type\":\"_doc\",\"_id\":\"1002\",\"_score\":1.0,\"_source\":{\"name\":\"lisi\",\"age\":\"30\",\"sex\":\"女\"}},{\"_index\":\"user\",\"_type\":\"_doc\",\"_id\":\"1003\",\"_score\":1.0,\"_source\":{\"name\":\"wangwu1\",\"age\":\"40\",\"sex\":\"男\"}},{\"_index\":\"user\",\"_type\":\"_doc\",\"_id\":\"1004\",\"_score\":1.0,\"_source\":{\"name\":\"wangwu2\",\"age\":\"20\",\"sex\":\"女\"}},{\"_index\":\"user\",\"_type\":\"_doc\",\"_id\":\"1005\",\"_score\":1.0,\"_source\":{\"name\":\"wangwu3\",\"age\":\"50\",\"sex\":\"男\"}},{\"_index\":\"user\",\"_type\":\"_doc\",\"_id\":\"1006\",\"_score\":1.0,\"_source\":{\"name\":\"wangwu4\",\"age\":\"20\",\"sex\":\"男\"}}]},\"aggregations\":{\"lterms#age_groupby\":{\"doc_count_error_upper_bound\":0,\"sum_other_doc_count\":0,\"buckets\":[{\"key\":20,\"doc_count\":2},{\"key\":10,\"doc_count\":1},{\"key\":30,\"doc_count\":1},{\"key\":40,\"doc_count\":1},{\"key\":50,\"doc_count\":1}]}}}\n\nProcess finished with exit code 0\n\n```\n\n# 第3章 Elasticsearch环境\n\n## 29-环境-简介\n\n### 单机 & 集群\n\n单台Elasticsearch服务器提供服务，往往都有最大的负载能力，超过这个阈值，服务器\n性能就会大大降低甚至不可用，所以生产环境中，一般都是运行在指定服务器集群中。\n除了负载能力，单点服务器也存在其他问题：\n\n- 单台机器存储容量有限\n- 单服务器容易出现单点故障，无法实现高可用\n- 单服务的并发处理能力有限\n\n配置服务器集群时，集群中节点数量没有限制，大于等于2个节点就可以看做是集群了。一\n般出于高性能及高可用方面来考虑集群中节点数量都是3个以上\n\n总之，集群能提高性能，增加容错。\n\n### 集群 Cluster\n\n**一个集群就是由一个或多个服务器节点组织在一起，共同持有整个的数据，并一起提供索引和搜索功能**。一个Elasticsearch集群有一个唯一的名字标识，这个名字默认就是”elasticsearch”。这个名字是重要的，因为一个节点只能通过指定某个集群的名字，来加入这个集群。\n\n### 节点Node\n\n集群中包含很多服务器，一个节点就是其中的一个服务器。作为集群的一部分，它存储数据，参与集群的索引和搜索功能。\n\n一个节点也是由一个名字来标识的，默认情况下，这个名字是一个随机的漫威漫画角色的名字，这个名字会在启动的时候赋予节点。这个名字对于管理工作来说挺重要的，因为在这个管理过程中，你会去确定网络中的哪些服务器对应于Elasticsearch集群中的哪些节点。\n\n一个节点可以通过配置集群名称的方式来加入一个指定的集群。默认情况下，每个节点都会被安排加入到一个叫做“elasticsearch”的集群中，这意味着，如果你在你的网络中启动了若干个节点，并假定它们能够相互发现彼此，它们将会自动地形成并加入到一个叫做“elasticsearch”的集群中。\n\n在一个集群里，只要你想，可以拥有任意多个节点。而且，如果当前你的网络中没有运\n行任何Elasticsearch节点，这时启动一个节点，会默认创建并加入一个叫做“elasticsearch”的\n集群。\n\n## 30-环境-Windows集群部署\n\n### 部署集群\n\n一、创建elasticsearch-cluster文件夹\n\n创建elasticsearch-7.8.0-cluster文件夹，在内部复制三个elasticsearch服务。\n\n![img](https://img-blog.csdnimg.cn/img_convert/6d6022b6d30a9e2c1a4e18092d5f130f.png)\n\n二、修改集群文件目录中每个节点的config/elasticsearch.yml配置文件\n\n**node-1001节点**\n\n```yaml\n#节点1的配置信息：\n#集群名称，节点之间要保持一致\ncluster.name: my-elasticsearch\n#节点名称，集群内要唯一\nnode.name: node-1001\nnode.master: true\nnode.data: true\n#ip地址\nnetwork.host: localhost\n#http端口\nhttp.port: 1001\n#tcp监听端口\ntransport.tcp.port: 9301\n#discovery.seed_hosts: [\"localhost:9301\", \"localhost:9302\",\"localhost:9303\"]\n#discovery.zen.fd.ping_timeout: 1m\n#discovery.zen.fd.ping_retries: 5\n#集群内的可以被选为主节点的节点列表\n#cluster.initial_master_nodes: [\"node-1\", \"node-2\",\"node-3\"]\n#跨域配置\n#action.destructive_requires_name: true\nhttp.cors.enabled: true\nhttp.cors.allow-origin: \"*\"\n\n```\n\n**node-1002节点**\n\n```yaml\n#节点2的配置信息：\n#集群名称，节点之间要保持一致\ncluster.name: my-elasticsearch\n#节点名称，集群内要唯一\nnode.name: node-1002\nnode.master: true\nnode.data: true\n#ip地址\nnetwork.host: localhost\n#http端口\nhttp.port: 1002\n#tcp监听端口\ntransport.tcp.port: 9302\ndiscovery.seed_hosts: [\"localhost:9301\"]\ndiscovery.zen.fd.ping_timeout: 1m\ndiscovery.zen.fd.ping_retries: 5\n#集群内的可以被选为主节点的节点列表\n#cluster.initial_master_nodes: [\"node-1\", \"node-2\",\"node-3\"]\n#跨域配置\n#action.destructive_requires_name: true\nhttp.cors.enabled: true\nhttp.cors.allow-origin: \"*\"\n\n```\n\n**node-1003节点**\n\n```yaml\n#节点3的配置信息：\n#集群名称，节点之间要保持一致\ncluster.name: my-elasticsearch\n#节点名称，集群内要唯一\nnode.name: node-1003\nnode.master: true\nnode.data: true\n#ip地址\nnetwork.host: localhost\n#http端口\nhttp.port: 1003\n#tcp监听端口\ntransport.tcp.port: 9303\n#候选主节点的地址，在开启服务后可以被选为主节点\ndiscovery.seed_hosts: [\"localhost:9301\", \"localhost:9302\"]\ndiscovery.zen.fd.ping_timeout: 1m\ndiscovery.zen.fd.ping_retries: 5\n#集群内的可以被选为主节点的节点列表\n#cluster.initial_master_nodes: [\"node-1\", \"node-2\",\"node-3\"]\n#跨域配置\n#action.destructive_requires_name: true\nhttp.cors.enabled: true\nhttp.cors.allow-origin: \"*\"\n\n```\n\n三、如果有必要，删除每个节点中的data目录中所有内容。\n\n### 启动集群\n\n分别依次双击执行节点的bin/elasticsearch.bat,启动节点服务器（可以编写一个脚本启动），启动后，会自动加入指定名称的集群。\n\n### 测试集群\n\n一、用Postman，查看集群状态\n\n1. `GET http://127.0.0.1:1001/_cluster/health`\n2. `GET http://127.0.0.1:1002/_cluster/health`\n3. `GET http://127.0.0.1:1003/_cluster/health`\n\n返回结果皆为如下：\n\n```json\n{\n    \"cluster_name\": \"my-application\",\n    \"status\": \"green\",\n    \"timed_out\": false,\n    \"number_of_nodes\": 3,\n    \"number_of_data_nodes\": 3,\n    \"active_primary_shards\": 0,\n    \"active_shards\": 0,\n    \"relocating_shards\": 0,\n    \"initializing_shards\": 0,\n    \"unassigned_shards\": 0,\n    \"delayed_unassigned_shards\": 0,\n    \"number_of_pending_tasks\": 0,\n    \"number_of_in_flight_fetch\": 0,\n    \"task_max_waiting_in_queue_millis\": 0,\n    \"active_shards_percent_as_number\": 100.0\n}\n\n```\n\n**status字段**指示着当前集群在总体上是否工作正常。它的三种颜色含义如下：\n\n1. green：所有的主分片和副本分片都正常运行。\n2. yellow：所有的主分片都正常运行，但不是所有的副本分片都正常运行。\n3. red：有主分片没能正常运行。\n\n二、用Postman，在一节点增加索引，另一节点获取索引\n\n向集群中的node-1001节点增加索引：\n\n```json\n#PUT http://127.0.0.1:1001/user\n\n```\n\n返回结果：\n\n```json\n{\n    \"acknowledged\": true,\n    \"shards_acknowledged\": true,\n    \"index\": \"user\"\n}\n\n```\n\n向集群中的node-1003节点获取索引：\n\n```json\n#GET http://127.0.0.1:1003/user\n\n```\n\n返回结果：\n\n```json\n{\n    \"user\": {\n        \"aliases\": {},\n        \"mappings\": {},\n        \"settings\": {\n            \"index\": {\n                \"creation_date\": \"1617993035885\",\n                \"number_of_shards\": \"1\",\n                \"number_of_replicas\": \"1\",\n                \"uuid\": \"XJKERwQlSJ6aUxZEN2EV0w\",\n                \"version\": {\n                    \"created\": \"7080099\"\n                },\n                \"provided_name\": \"user\"\n            }\n        }\n    }\n}\n\n```\n\n如果在1003创建索引，同样在1001也能获取索引信息，这就是集群能力。\n\n## 31-环境-Linux单节点部署\n\n### 软件安装\n\n一、下载软件\n\n[下载Linux版的Elasticsearch](https://www.elastic.co/cn/downloads/past-releases/elasticsearch-7-8-0)\n\n二、解压软件\n\n```shell\n# 解压缩\ntar -zxvf elasticsearch-7.8.0-linux-x86_64.tar.gz -C /opt/module\n# 改名\nmv elasticsearch-7.8.0 es\n\n```\n\n三、创建用户\n\n因为安全问题，Elasticsearch不允许root用户直接运行，所以要创建新用户，在root用户中创建新用户。\n\n```shell\nuseradd es #新增es用户\npasswd es #为es用户设置密码\nuserdel -r es #如果错了，可以删除再加\nchown -R es:es /opt/module/es #文件夹所有者\n\n```\n\n四、修改配置文件\n\n修改/opt/module/es/config/elasticsearch.yml文件。\n\n```yaml\n# 加入如下配置\ncluster.name: elasticsearch\nnode.name: node-1\nnetwork.host: 0.0.0.0\nhttp.port: 9200\ncluster.initial_master_nodes: [\"node-1\"]\n\n```\n\n修改/etc/security/limits.conf\n\n```\n# 在文件末尾中增加下面内容\n# 每个进程可以打开的文件数的限制\nes soft nofile 65536\nes hard nofile 65536\n\n```\n\n修改/etc/security/limits.d/20-nproc.conf\n\n```\n# 在文件末尾中增加下面内容\n# 每个进程可以打开的文件数的限制\nes soft nofile 65536\nes hard nofile 65536\n# 操作系统级别对每个用户创建的进程数的限制\n* hard nproc 4096\n# 注： * 带表Linux所有用户名称\n\n```\n\n修改/etc/sysctl.conf\n\n```\n# 在文件中增加下面内容\n# 一个进程可以拥有的VMA(虚拟内存区域)的数量,默认值为65536\nvm.max_map_count=655360\n\n```\n\n重新加载\n\n```\nsysctl -p\n\n```\n\n### 启动软件\n\n使用ES用户启动\n\n```\ncd /opt/module/es/\n#启动\nbin/elasticsearch\n#后台启动\nbin/elasticsearch -d\n\n```\n\n启动时，会动态生成文件，如果文件所属用户不匹配，会发生错误，需要重新进行修改用户和用户组\n\n![img](https://img-blog.csdnimg.cn/img_convert/75e24cd1fbc430577681bb4f8a0d3e2b.png)\n\n关闭防火墙\n\n```\n#暂时关闭防火墙\nsystemctl stop firewalld\n#永久关闭防火墙\nsystemctl enable firewalld.service #打开防火墙永久性生效，重启后不会复原\nsystemctl disable firewalld.service #关闭防火墙，永久性生效，重启后不会复原\n\n```\n\n### 测试软件\n\n浏览器中输入地址： http://linux1:9200/\n\n![img](https://img-blog.csdnimg.cn/img_convert/477065a2cb499a32871549c21e2fc487.png)\n\n## 32-环境-Linux集群部署\n\n### 软件安装\n\n一、下载软件\n\n[下载Linux版的Elasticsearch](https://www.elastic.co/cn/downloads/past-releases/elasticsearch-7-8-0)\n\n二、解压软件\n\n```shell\n# 解压缩\ntar -zxvf elasticsearch-7.8.0-linux-x86_64.tar.gz -C /opt/module\n# 改名\nmv elasticsearch-7.8.0 es-cluster\n\n```\n\n将软件分发到其他节点：linux2,linux3\n\n三、创建用户\n\n因为安全问题，Elasticsearch不允许root用户直接运行，所以要创建新用户，在root用户中创建新用户。\n\n```shell\nuseradd es #新增es用户\npasswd es #为es用户设置密码\nuserdel -r es #如果错了，可以删除再加\nchown -R es:es /opt/module/es #文件夹所有者\n\n```\n\n四、修改配置文件\n\n修改/opt/module/es/config/elasticsearch.yml文件，分发文件。\n\n```yaml\n# 加入如下配置\n#集群名称\ncluster.name: cluster-es\n#节点名称，每个节点的名称不能重复\nnode.name: node-1\n#ip地址，每个节点的地址不能重复\nnetwork.host: linux1\n#是不是有资格主节点\nnode.master: true\nnode.data: true\nhttp.port: 9200\n# head插件需要这打开这两个配置\nhttp.cors.allow-origin: \"*\"\nhttp.cors.enabled: true\nhttp.max_content_length: 200mb\n#es7.x之后新增的配置，初始化一个新的集群时需要此配置来选举master\ncluster.initial_master_nodes: [\"node-1\"]\n#es7.x之后新增的配置，节点发现\ndiscovery.seed_hosts: [\"linux1:9300\",\"linux2:9300\",\"linux3:9300\"]\ngateway.recover_after_nodes: 2\nnetwork.tcp.keep_alive: true\nnetwork.tcp.no_delay: true\ntransport.tcp.compress: true\n#集群内同时启动的数据任务个数，默认是2个\ncluster.routing.allocation.cluster_concurrent_rebalance: 16\n#添加或删除节点及负载均衡时并发恢复的线程个数，默认4个\ncluster.routing.allocation.node_concurrent_recoveries: 16\n#初始化数据恢复时，并发恢复线程的个数，默认4个\ncluster.routing.allocation.node_initial_primaries_recoveries: 16\n\n```\n\n修改/etc/security/limits.conf，分发文件\n\n```\n# 在文件末尾中增加下面内容\nes soft nofile 65536\nes hard nofile 65536\n\n```\n\n修改/etc/security/limits.d/20-nproc.conf，分发文件\n\n```\n# 在文件末尾中增加下面内容\nes soft nofile 65536\nes hard nofile 65536\n\\* hard nproc 4096\n\\# 注：*代表Linux所有用户名称\n\n```\n\n修改/etc/sysctl.conf\n\n```\n# 在文件中增加下面内容\nvm.max_map_count=655360\n\n```\n\n重新加载\n\n```\nsysctl -p\n\n```\n\n### 启动软件\n\n分别在不同节点上启动ES软件\n\n```\ncd /opt/module/es-cluster\n#启动\nbin/elasticsearch\n#后台启动\nbin/elasticsearch -d\n\n```\n\n### 测试集群\n\n![img](https://img-blog.csdnimg.cn/img_convert/0412e37cb5249d1ff0e813ee87f49a50.png)\n\n# 第4章 Elasticsearch进阶\n\n## 33-进阶-核心概念\n\n![img](https://img-blog.csdnimg.cn/img_convert/146a779da01f53e7f7a8d53132d3c7cf.png)\n\n### 索引Index\n\n一个索引就是一个拥有几分相似特征的文档的集合。比如说，你可以有一个客户数据的索引，另一个产品目录的索引，还有一个订单数据的索引。一个索引由一个名字来标识（必须全部是小写字母），并且当我们要对这个索引中的文档进行索引、搜索、更新和删除（CRUD）的时候，都要使用到这个名字。在一个集群中，可以定义任意多的索引。\n\n能搜索的数据必须索引，这样的好处是可以提高查询速度，比如：新华字典前面的目录就是索引的意思，目录可以提高查询速度。\n\n**Elasticsearch索引的精髓：一切设计都是为了提高搜索的性能。**\n\n### 类型Type\n\n在一个索引中，你可以定义一种或多种类型。\n\n一个类型是你的索引的一个逻辑上的分类/分区，其语义完全由你来定。通常，会为具\n有一组共同字段的文档定义一个类型。不同的版本，类型发生了不同的变化。\n\n| 版本 | Type                                            |\n| ---- | ----------------------------------------------- |\n| 5.x  | 支持多种 type                                   |\n| 6.x  | 只能有一种 type                                 |\n| 7.x  | 默认不再支持自定义索引类型（默认类型为： _doc） |\n\n### 文档Document\n\n一个文档是一个可被索引的基础信息单元，也就是一条数据。\n\n比如：你可以拥有某一个客户的文档，某一个产品的一个文档，当然，也可以拥有某个订单的一个文档。文档以JSON（Javascript Object Notation）格式来表示，而JSON是一个到处存在的互联网数据交互格式。\n\n在一个index/type里面，你可以存储任意多的文档。\n\n### 字段Field\n\n相当于是数据表的字段，对文档数据根据不同属性进行的分类标识。\n\n### 映射Mapping\n\nmapping是处理数据的方式和规则方面做一些限制，如：某个字段的数据类型、默认值、分析器、是否被索引等等。这些都是映射里面可以设置的，其它就是处理ES里面数据的一些使用规则设置也叫做映射，按着最优规则处理数据对性能提高很大，因此才需要建立映射，并且需要思考如何建立映射才能对性能更好。\n\n### 分片Shards\n\n一个索引可以存储超出单个节点硬件限制的大量数据。比如，一个具有10亿文档数据\n的索引占据1TB的磁盘空间，而任一节点都可能没有这样大的磁盘空间。或者单个节点处理搜索请求，响应太慢。为了解决这个问题，**Elasticsearch提供了将索引划分成多份的能力，每一份就称之为分片**。当你创建一个索引的时候，你可以指定你想要的分片的数量。**每个分片本身也是一个功能完善并且独立的“索引”**，这个“索引”可以被放置到集群中的任何节点上。\n\n分片很重要，主要有两方面的原因：\n\n1. 允许你水平分割/扩展你的内容容量。\n2. 允许你在分片之上进行分布式的、并行的操作，进而提高性能/吞吐量。\n\n至于一个分片怎样分布，它的文档怎样聚合和搜索请求，是完全由Elasticsearch管理的，对于作为用户的你来说，这些都是透明的，无需过分关心。\n\n被混淆的概念是，一个Lucene索引我们在Elasticsearch称作分片。一个Elasticsearch索引是分片的集合。当Elasticsearch在索引中搜索的时候，他发送查询到每一个属于索引的分片（Lucene索引），然后合并每个分片的结果到一个全局的结果集。\n\nLucene是Apache软件基金会Jakarta项目组的一个子项目，提供了一个简单却强大的应用程式接口，能够做全文索引和搜寻。在Java开发环境里Lucene是一个成熟的免费开源工具。就其本身而言，Lucene是当前以及最近几年最受欢迎的免费Java信息检索程序库。但Lucene只是一个提供全文搜索功能类库的核心工具包，而真正使用它还需要一个完善的服务框架搭建起来进行应用。\n\n目前市面上流行的搜索引擎软件，主流的就两款：Elasticsearch和Solr,这两款都是基于Lucene搭建的，可以独立部署启动的搜索引擎服务软件。由于内核相同，所以两者除了服务器安装、部署、管理、集群以外，对于数据的操作修改、添加、保存、查询等等都十分类似。\n\n### 副本Replicas\n\n在一个网络/云的环境里，失败随时都可能发生，在某个分片/节点不知怎么的就处于\n离线状态，或者由于任何原因消失了，这种情况下，有一个故障转移机制是非常有用并且是强烈推荐的。为此目的，Elasticsearch允许你创建分片的一份或多份拷贝，这些拷贝叫做复制分片(副本)。\n\n复制分片之所以重要，有两个主要原因：\n\n- 在分片/节点失败的情况下，**提供了高可用性**。因为这个原因，注意到复制分片从不与原/主要（original/primary）分片置于同一节点上是非常重要的。\n- 扩展你的搜索量/吞吐量，因为搜索可以在所有的副本上并行运行。\n\n总之，每个索引可以被分成多个分片。一个索引也可以被复制0次（意思是没有复制）或多次。一旦复制了，每个索引就有了主分片（作为复制源的原来的分片）和复制分片（主分片的拷贝）之别。\n\n分片和复制的数量可以在索引创建的时候指定。在索引创建之后，你可以在任何时候动态地改变复制的数量，但是你事后不能改变分片的数量。\n\n默认情况下，Elasticsearch中的每个索引被分片1个主分片和1个复制，这意味着，如果你的集群中至少有两个节点，你的索引将会有1个主分片和另外1个复制分片（1个完全拷贝），这样的话每个索引总共就有2个分片，我们需要根据索引需要确定分片个数。\n\n### 分配Allocation\n\n将分片分配给某个节点的过程，包括分配主分片或者副本。如果是副本，还包含从主分片复制数据的过程。这个过程是由master节点完成的。\n\n## 34-进阶-系统架构-简介\n\n![img](https://img-blog.csdnimg.cn/img_convert/e4d13427545dc174eb9ccface85c1f0c.png)\n\n一个运行中的Elasticsearch实例称为一个节点，而集群是由一个或者多个拥有相同\ncluster.name配置的节点组成，它们共同承担数据和负载的压力。当有节点加入集群中或者从集群中移除节点时，集群将会重新平均分布所有的数据。\n\n当一个节点被选举成为主节点时，它将负责管理集群范围内的所有变更，例如增加、\n删除索引，或者增加、删除节点等。而主节点并不需要涉及到文档级别的变更和搜索等操作，所以当集群只拥有一个主节点的情况下，即使流量的增加它也不会成为瓶颈。任何节点都可以成为主节点。我们的示例集群就只有一个节点，所以它同时也成为了主节点。\n\n作为用户，我们可以将请求发送到集群中的任何节点，包括主节点。每个节点都知道\n任意文档所处的位置，并且能够将我们的请求直接转发到存储我们所需文档的节点。无论我们将请求发送到哪个节点，它都能负责从各个包含我们所需文档的节点收集回数据，并将最终结果返回給客户端。Elasticsearch对这一切的管理都是透明的。\n\n## 35-进阶-单节点集群\n\n我们在包含一个空节点的集群内创建名为users的索引，为了演示目的，我们将分配3个主分片和一份副本（每个主分片拥有一个副本分片）。\n\n```json\n#PUT http://127.0.0.1:1001/users\n{\n    \"settings\" : {\n        \"number_of_shards\" : 3,\n        \"number_of_replicas\" : 1\n    }\n}\n\n```\n\n集群现在是拥有一个索引的单节点集群。所有3个主分片都被分配在node-1。\n\n![img](https://img-blog.csdnimg.cn/img_convert/54ee6753be248cc7d345b38a0eae7d96.png)\n\n通过elasticsearch-head插件（一个Chrome插件）查看集群情况。\n\n![img](https://img-blog.csdnimg.cn/img_convert/e8b15d0b243d486e91f478a220da63bf.png)\n\n- 集群健康值:yellow(3of6)：表示当前集群的全部主分片都正常运行，但是副本分片没有全部处在正常状态。\n- ![img](https://img-blog.csdnimg.cn/img_convert/489b6de480112879a00067b793bde685.png)：3个主分片正常。\n- ![img](https://img-blog.csdnimg.cn/img_convert/3ce9a78d26ee762f0a7a8abf7817a58e.png)：3个副本分片都是Unassigned，它们都没有被分配到任何节点。在同一个节点上既保存原始数据又保存副本是没有意义的，因为一旦失去了那个节点，我们也将丢失该节点上的所有副本数据。\n\n当前集群是正常运行的，但存在丢失数据的风险。\n\n------\n\n**elasticsearch-head chrome插件安装**\n\n[插件获取网址](https://github.com/mobz/elasticsearch-head)，下载压缩包，解压后将内容放入自定义命名为elasticsearch-head文件夹。\n\n接着点击Chrome右上角选项->工具->管理扩展（或则地址栏输入chrome://extensions/），选择打开“开发者模式”，让后点击“加载已解压得扩展程序”，选择elasticsearch-head/_site，即可完成chrome插件安装。\n\n\n## 36-进阶-故障转移\n\n当集群中只有一个节点在运行时，意味着会有一个单点故障问题——没有冗余。幸运的是，我们只需再启动一个节点即可防止数据丢失。当你在同一台机器上启动了第二个节点时，只要它和第一个节点有同样的cluster.name配置，它就会自动发现集群并加入到其中。但是在不同机器上启动节点的时候，为了加入到同一集群，你需要配置一个可连接到的单播主机列表。之所以配置为使用单播发现，以防止节点无意中加入集群。只有在同一台机器上\n运行的节点才会自动组成集群。\n\n如果启动了第二个节点，集群将会拥有两个节点:所有主分片和副本分片都已被分配。\n\n![img](https://img-blog.csdnimg.cn/img_convert/bf76cb1bfbdf07555918d9055817ab44.png)\n\n通过elasticsearch-head插件查看集群情况\n\n![img](https://img-blog.csdnimg.cn/img_convert/18db400822b83e727d6206f486b7b2ea.png)\n\n- 集群健康值:green(3of6)：表示所有6个分片（包括3个主分片和3个副本分片）都在正常运行。\n- ![img](https://img-blog.csdnimg.cn/img_convert/e485d8263a4aa3a94af0be951bd5a241.png)：3个主分片正常。\n- ![img](https://img-blog.csdnimg.cn/img_convert/e485d8263a4aa3a94af0be951bd5a241.png)：第二个节点加入到集群后，3个副本分片将会分配到这个节点上——每个主分片对应一个副本分片。这意味着当集群内任何一个节点出现问题时，我们的数据都完好无损。所有新近被索引的文档都将会保存在主分片上，然后被并行的复制到对应的副本分片上。这就保证了我们既可以从主分片又可以从副本分片上获得文档。\n\n## 37-进阶-水平扩容\n\n怎样为我们的正在增长中的应用程序按需扩容呢？当启动了第三个节点，我们的集群将会拥有三个节点的集群:为了分散负载而对分片进行重新分配。\n\n![img](https://img-blog.csdnimg.cn/img_convert/d527e26aa2bccdf54b11410024eadc92.png)\n\n通过elasticsearch-head插件查看集群情况。\n\n![img](https://img-blog.csdnimg.cn/img_convert/6985fe14454c1269204478320d089bd7.png)\n\n- 集群健康值:green(3of6)：表示所有6个分片（包括3个主分片和3个副本分片）都在正常运行。\n- ![img](https://img-blog.csdnimg.cn/img_convert/9494419153adb44bedb395ac5d7bc488.png)Node1和Node2上各有一个分片被迁移到了新的Node3节点，现在每个节点上都拥有2个分片，而不是之前的3个。这表示每个节点的硬件资源（CPU,RAM,I/O）将被更少的分片所共享，每个分片的性能将会得到提升。\n\n分片是一个功能完整的搜索引擎，它拥有使用一个节点上的所有资源的能力。我们这个拥有6个分片（3个主分片和3个副本分片）的索引可以最大扩容到6个节点，每个节点上存在一个分片，并且每个分片拥有所在节点的全部资源。\n\n**但是如果我们想要扩容超过6个节点怎么办呢？**\n\n主分片的数目在索引创建时就已经确定了下来。实际上，这个数目定义了这个索引能够\n存储 的最大数据量。（实际大小取决于你的数据、硬件和使用场景。）但是，读操作——\n搜索和返回数据——可以同时被主分片或副本分片所处理，所以当你拥有越多的副本分片\n时，也将拥有越高的吞吐量。\n\n在运行中的集群上是可以动态调整副本分片数目的，我们可以按需伸缩集群。让我们把\n副本数从默认的1增加到2。\n\n```json\n#PUT http://127.0.0.1:1001/users/_settings\n\n{\n    \"number_of_replicas\" : 2\n}\n\n```\n\nusers索引现在拥有9个分片：3个主分片和6个副本分片。这意味着我们可以将集群\n扩容到9个节点，每个节点上一个分片。相比原来3个节点时，集群搜索性能可以提升3倍。\n\n![img](https://img-blog.csdnimg.cn/img_convert/97fd01e34e5d8df23d226c4fef157801.png)\n\n通过elasticsearch-head插件查看集群情况：\n\n![img](https://img-blog.csdnimg.cn/img_convert/8bf9dbf0cec5b7875bf8aa9d17a9a67c.png)\n\n当然，如果只是在相同节点数目的集群上增加更多的副本分片并不能提高性能，因为每\n个分片从节点上获得的资源会变少。你需要增加更多的硬件资源来提升吞吐量。\n\n但是更多的副本分片数提高了数据冗余量：按照上面的节点配置，我们可以在失去 2 个节点\n的情况下不丢失任何数据。\n\n## 38-进阶-应对故障\n\n我们关闭第一个节点，这时集群的状态为:关闭了一个节点后的集群。\n\n![img](https://img-blog.csdnimg.cn/img_convert/44e841004be934e6bce08187ca3852bb.png)\n\n我们关闭的节点是一个主节点。而集群必须拥有一个主节点来保证正常工作，所以发生\n的第一件事情就是选举一个新的主节点：Node2。在我们关闭Node1的同时也失去了主\n分片1和2，并且在缺失主分片的时候索引也不能正常工作。如果此时来检查集群的状况，我们看到的状态将会为red：不是所有主分片都在正常工作。\n\n幸运的是，在其它节点上存在着这两个主分片的完整副本，所以新的主节点立即将这些分片在Node2和Node3上对应的副本分片提升为主分片，此时集群的状态将会为yellow。这个提升主分片的过程是瞬间发生的，如同按下一个开关一般。\n\n![img](https://img-blog.csdnimg.cn/img_convert/e956bda7e0d005699e27760d4193d101.png)\n\n**为什么我们集群状态是yellow而不是green呢？**\n\n虽然我们拥有所有的三个主分片，但是同时设置了每个主分片需要对应2份副本分片，而此\n时只存在一份副本分片。所以集群不能为green的状态，不过我们不必过于担心：如果我\n们同样关闭了Node2，我们的程序依然可以保持在不丢任何数据的情况下运行，因为\nNode3为每一个分片都保留着一份副本。\n\n如果想回复原来的样子，要确保Node-1的配置文件有如下配置：\n\n```yaml\ndiscovery.seed_hosts: [\"localhost:9302\", \"localhost:9303\"]\n\n```\n\n集群可以将缺失的副本分片再次进行分配，那么集群的状态也将恢复成之前的状态。如果Node1依然拥有着之前的分片，它将尝试去重用它们，同时仅从主分片复制发生了修改的数据文件。和之前的集群相比，只是Master节点切换了。\n\n![img](https://img-blog.csdnimg.cn/img_convert/37eea6a8dae7ba908312f2ebf0eced11.png)\n\n## 39-进阶-路由计算 & 分片控制\n\n### 路由计算\n\n当索引一个文档的时候，文档会被存储到一个主分片中。Elasticsearch如何知道一个\n文档应该存放到哪个分片中呢？当我们创建文档时，它如何决定这个文档应当被存储在分片1还是分片2中呢？首先这肯定不会是随机的，否则将来要获取文档的时候我们就不知道从何处寻找了。实际上，这个过程是根据下面这个公式决定的：\n\n```\nshard = hash(routing) % number_of_primary_shards\n\n```\n\nrouting是一个可变值，默认是文档的_id，也可以设置成一个自定义的值。routing通过hash函数生成一个数字，然后这个数字再除以number_of_primary_shards（主分片的数量）后得到余数。这个分布在0到number_of_primary_shards-1之间的余数，就是我们所寻求的文档所在分片的位置。\n\n![img](https://img-blog.csdnimg.cn/img_convert/9c34e8603887c2bed475416e3b67cd9a.png)\n\n这就解释了为什么我们要在创建索引的时候就确定好主分片的数量并且永远不会改变这个数量:因为如果数量变化了，那么所有之前路由的值都会无效，文档也再也找不到了。\n\n所有的文档API(get.index.delete、bulk,update以及mget）都接受一个叫做routing的路由参数，通过这个参数我们可以自定义文档到分片的映射。一个自定义的路由参数可以用来确保所有相关的文档—一例如所有属于同一个用户的文档——都被存储到同一个分片中。\n\n### 分片控制\n\n我们可以发送请求到集群中的任一节点。每个节点都有能力处理任意请求。每个节点都知道集群中任一文档位置，所以可以直接将请求转发到需要的节点上。在下面的例子中，如果将所有的请求发送到Node 1001，我们将其称为协调节点**coordinating node**。\n\n![img](https://img-blog.csdnimg.cn/img_convert/3940d6cdb197259368542b86384911a4.png)\n\n当发送请求的时候，为了扩展负载，更好的做法是轮询集群中所有的节点。\n\n## 40-进阶-数据写流程\n\n新建、索引和删除请求都是写操作，必须在主分片上面完成之后才能被复制到相关的副本分片。\n\n![img](https://img-blog.csdnimg.cn/img_convert/418356a32516c222a8d366df021276c2.png)\n\n在客户端收到成功响应时，文档变更已经在主分片和所有副本分片执行完成，变更是安全的。有一些可选的**请求参数**允许您影响这个过程，可能以数据安全为代价提升性能。这些选项很少使用，因为Elasticsearch已经很快，但是为了完整起见，请参考下文：\n\n1. consistency\n\n- 即一致性。在默认设置下，即使仅仅是在试图执行一个写操作之前，主分片都会要求必须要有规定数量quorum（或者换种说法，也即必须要有大多数）的分片副本处于活跃可用状态，才会去执行写操作（其中分片副本可以是主分片或者副本分片）。这是为了避免在发生网络分区故障（network partition）的时候进行写操作，进而导致数据不一致。规定数量即：**int((primary+number_of_replicas)/2)+1**\n- consistency参数的值可以设为：\n  - one：只要主分片状态ok就允许执行写操作。\n  - all：必须要主分片和所有副本分片的状态没问题才允许执行写操作。\n  - quorum：默认值为quorum,即大多数的分片副本状态没问题就允许执行写操作。\n- 注意，规定数量的计算公式中number_of_replicas指的是在索引设置中的设定副本分片数，而不是指当前处理活动状态的副本分片数。如果你的索引设置中指定了当前索引拥有3个副本分片，那规定数量的计算结果即：**int((1 primary + 3 replicas) / 2) + 1 = 3**，如果此时你只启动两个节点，那么处于活跃状态的分片副本数量就达不到规定数量，也因此您将无法索引和删除任何文档。\n\n1. timeout\n   - 如果没有足够的副本分片会发生什么？Elasticsearch会等待，希望更多的分片出现。默认情况下，它最多等待1分钟。如果你需要，你可以使用timeout参数使它更早终止：100是100毫秒，30s是30秒。\n\n新索引默认有1个副本分片，这意味着为满足规定数量应该需要两个活动的分片副本。但是，这些默认的设置会阻止我们在单一节点上做任何事情。为了避免这个问题，要求只有当number_of_replicas大于1的时候，规定数量才会执行。\n\n## 41-进阶-数据读流程\n\n![img](https://img-blog.csdnimg.cn/img_convert/7139df83ee6f7a59c5d3252d34cc8762.png)\n\n在处理读取请求时，协调结点在每次请求的时候都会通过轮询所有的副本分片来达到负载均衡。在文档被检索时，已经被索引的文档可能已经存在于主分片上但是还没有复制到副本分片。在这种情况下，副本分片可能会报告文档不存在，但是主分片可能成功返回文档。一旦索引请求成功返回给用户，文档在主分片和副本分片都是可用的。\n\n## 42-进阶-更新流程 & 批量操作流程\n\n### 更新流程\n\n部分更新一个文档结合了先前说明的读取和写入流程：\n\n![img](https://img-blog.csdnimg.cn/img_convert/ca41993144aee98311671278725437cd.png)\n\n部分更新一个文档的步骤如下：\n\n1. 客户端向Node 1发送更新请求。\n2. 它将请求转发到主分片所在的Node 3。\n3. Node 3从主分片检索文档，修改_source字段中的JSON，并且尝试重新索引主分片的文档。如果文档已经被另一个进程修改,它会重试步骤3,超过retry_on_conflict次后放弃。\n4. 如果Node 3成功地更新文档，它将新版本的文档并行转发到Node1和Node2上的副本分片，重新建立索引。一旦所有副本分片都返回成功，Node3向协调节点也返回成功，协调节点向客户端返回成功。\n\n当主分片把更改转发到副本分片时，它不会转发更新请求。相反，它转发完整文档的新版本。请记住，这些更改将会异步转发到副本分片，并且不能保证它们以发送它们相同的顺序到达。如果Elasticsearch仅转发更改请求，则可能以错误的顺序应用更改，导致得到损坏的文档。\n\n### 批量操作流程\n\n**mget和bulkAPI的模式类似于单文档模式**。区别在于协调节点知道每个文档存在于哪个分片中。它将整个多文档请求分解成每个分片的多文档请求，并且将这些请求并行转发到每个参与节点。\n\n协调节点一旦收到来自每个节点的应答，就将每个节点的响应收集整理成单个响应，返回给客户端。\n\n![img](https://img-blog.csdnimg.cn/img_convert/b90ea9c79138d8361ca339cff205fdb0.png)\n\n**用单个 mget 请求取回多个文档所需的步骤顺序:**\n\n1. 客户端向Node1发送mget请求。\n2. Node1为每个分片构建多文档获取请求，然后并行转发这些请求到托管在每个所需的主分片或者副本分片的节点上。一旦收到所有答复，Node1构建响应并将其返回给客户端。\n\n可以对docs数组中每个文档设置routing参数。\n\nbulk API，允许在单个批量请求中执行多个创建、索引、删除和更新请求。\n\n![img](https://img-blog.csdnimg.cn/img_convert/83499315a7b8ab81471a88f3e142f0a8.png)\n\n**bulk API按如下步骤顺序执行：**\n\n1. 客户端向Node1发送bulk请求。\n2. Node 1为每个节点创建一个批量请求，并将这些请求并行转发到每个包含主分片的节点主机。\n3. 主分片一个接一个按顺序执行每个操作。当每个操作成功时,主分片并行转发新文档（或删除）到副本分片，然后执行下一个操作。一旦所有的副本分片报告所有操作成功，该节点将向协调节点报告成功，协调节点将这些响应收集整理并返回给客户端。\n\n## 43-进阶-倒排索引\n\n分片是Elasticsearch最小的工作单元。但是究竟什么是一个分片，它是如何工作的？\n\n传统的数据库每个字段存储单个值，但这对全文检索并不够。文本字段中的每个单词需要被搜索，对数据库意味着需要单个字段有索引多值的能力。最好的支持是一个字段多个值需求的数据结构是**倒排索引**。\n\n### 倒排索引原理\n\nElasticsearch使用一种称为倒排索引的结构，它适用于快速的全文搜索。\n\n见其名，知其意，有倒排索引，肯定会对应有正向索引。正向索引（forward index），反向索引（inverted index）更熟悉的名字是**倒排索引**。\n\n所谓的**正向索引**，就是搜索引擎会将待搜索的文件都对应一个文件ID，搜索时将这个ID和搜索关键字进行对应，形成K-V对，然后对关键字进行统计计数。（统计？？下文有解释）\n\n![img](https://img-blog.csdnimg.cn/img_convert/cba02cc6d7c5f054dfe5d58fafac9a6a.png)\n\n但是互联网上收录在搜索引擎中的文档的数目是个天文数字，这样的索引结构根本无法满足实时返回排名结果的要求。所以，搜索引擎会将正向索引重新构建为倒排索引，即把文件ID对应到关键词的映射转换为关键词到文件ID的映射，每个关键词都对应着一系列的文件，这些文件中都出现这个关键词。\n\n![img](https://img-blog.csdnimg.cn/img_convert/a1f52e96e0ac218b5024d708202afba4.png)\n\n### 倒排索引的例子\n\n一个倒排索引由文档中所有不重复词的列表构成，对于其中每个词，有一个包含它的文档列表。例如，假设我们有两个文档，每个文档的content域包含如下内容：\n\n- The quick brown fox jumped over the lazy dog\n- Quick brown foxes leap over lazy dogs in summer\n\n为了创建倒排索引，我们首先将每个文档的content域拆分成单独的词（我们称它为词条或tokens)，创建一个包含所有不重复词条的排序列表，然后列出每个词条出现在哪个文档。结果如下所示：\n\n![img](https://img-blog.csdnimg.cn/img_convert/3cc642e9bae776c3e617f9d117d41e21.png)\n\n现在，如果我们想搜索`quick` `brown`，我们只需要查找包含每个词条的文档：\n\n![img](https://img-blog.csdnimg.cn/img_convert/f26aaa01e011edfa68736956b2f1ddea.png)\n\n两个文档都匹配，但是第一个文档比第二个匹配度更高。如果我们使用仅计算匹配词条数量的简单相似性算法，那么我们可以说，对于我们查询的相关性来讲，第一个文档比第二个文档更佳。\n\n但是，我们目前的倒排索引有一些问题：\n\n- `Quick`和`quick`以独立的词条出现，然而用户可能认为它们是相同的词。\n- `fox`和`foxes`非常相似，就像`dog`和`dogs`；他们有相同的词根。\n- `jumped`和`leap`，尽管没有相同的词根，但他们的意思很相近。他们是同义词。\n\n使用前面的索引搜索`+Quick` `+fox`不会得到任何匹配文档。(记住，＋前缀表明这个词必须存在）。\n\n只有同时出现`Quick`和`fox` 的文档才满足这个查询条件，但是第一个文档包含`quick` `fox` ，第二个文档包含`Quick` `foxes` 。\n\n我们的用户可以合理的期望两个文档与查询匹配。我们可以做的更好。\n\n如果我们将词条规范为标准模式，那么我们可以找到与用户搜索的词条不完全一致，但具有足够相关性的文档。例如：\n\n- `Quick`可以小写化为`quick`。\n- `foxes`可以词干提取变为词根的格式为`fox`。类似的，`dogs`可以为提取为`dog`。\n- `jumped`和`leap`是同义词，可以索引为相同的单词`jump` 。\n\n现在索引看上去像这样：\n\n![img](https://img-blog.csdnimg.cn/img_convert/19813d1918c89461303377444cf85c8c.png)\n\n这还远远不够。我们搜索`+Quick` `+fox` 仍然会失败，因为在我们的索引中，已经没有`Quick`了。但是，如果我们对搜索的字符串使用与content域相同的标准化规则，会变成查询`+quick` `+fox`，这样两个文档都会匹配！分词和标准化的过程称为**分析**，这非常重要。你只能搜索在索引中出现的词条，所以索引文本和查询字符串必须标准化为相同的格式。\n\n## 44-进阶-文档搜索\n\n### 不可改变的倒排索引\n\n早期的全文检索会为整个文档集合建立一个很大的倒排索引并将其写入到磁盘。一旦新的索引就绪，旧的就会被其替换，这样最近的变化便可以被检索到。\n\n倒排索引被写入磁盘后是不可改变的：它永远不会修改。\n\n- 不需要锁。如果你从来不更新索引，你就不需要担心多进程同时修改数据的问题。\n- 一旦索引被读入内核的文件系统缓存，便会留在哪里，由于其不变性。只要文件系统缓存中还有足够的空间，那么大部分读请求会直接请求内存，而不会命中磁盘。这提供了很大的性能提升。\n- 其它缓存(像filter缓存)，在索引的生命周期内始终有效。它们不需要在每次数据改变时被重建，因为数据不会变化。\n- 写入单个大的倒排索引允许数据被压缩，减少磁盘IO和需要被缓存到内存的索引的使用量。\n\n当然，一个不变的索引也有不好的地方。主要事实是它是不可变的!你不能修改它。如果你需要让一个新的文档可被搜索，你需要重建整个索引。这要么对一个索引所能包含的数据量造成了很大的限制，要么对索引可被更新的频率造成了很大的限制。\n\n### 动态更新索引\n\n如何在保留不变性的前提下实现倒排索引的更新？\n\n答案是：用更多的索引。通过增加新的补充索引来反映新近的修改，而不是直接重写整个倒排索引。每一个倒排索引都会被轮流查询到,从最早的开始查询完后再对结果进行合并。\n\nElasticsearch基于Lucene，这个java库引入了**按段搜索**的概念。每一段本身都是一个倒排索引，但索引在Lucene中除表示所有段的集合外，还增加了提交点的概念—一个列出了所有已知段的文件。\n\n![img](https://img-blog.csdnimg.cn/img_convert/9ee1adbb2d55e710257e01b812a6d8cf.png)\n\n按段搜索会以如下流程执行：\n\n一、新文档被收集到内存索引缓存。\n\n![img](https://img-blog.csdnimg.cn/img_convert/9d499fde966ee9825fa5a424d8357489.png)\n\n二、不时地,缓存被提交。\n\n1. 一个新的段，一个追加的倒排索引，被写入磁盘。\n2. 一个新的包含新段名字的提交点被写入磁盘。\n3. 磁盘进行同步，所有在文件系统缓存中等待的写入都刷新到磁盘，以确保它们被写入物理文件\n\n三、新的段被开启，让它包含的文档可见以被搜索。\n\n四、内存缓存被清空，等待接收新的文档。\n\n![img](https://img-blog.csdnimg.cn/img_convert/f74828ff58cc4635a97e88706a221e50.png)\n\n当一个查询被触发，所有已知的段按顺序被查询。词项统计会对所有段的结果进行聚合，以保证每个词和每个文档的关联都被准确计算。这种方式可以用相对较低的成本将新文档添加到索引。\n\n段是不可改变的，所以既不能从把文档从旧的段中移除，也不能修改旧的段来进行反映文档的更新。取而代之的是，每个提交点会包含一个.del文件，文件中会列出这些被删除文档的段信息。\n\n当一个**文档被“删除”**时，它实际上只是在.del文件中被标记删除。一个被标记删除的文档仍然可以被查询匹配到，但它会在最终结果被返回前从结果集中移除。\n\n**文档更新**也是类似的操作方式:当一个文档被更新时，旧版本文档被标记删除，文档的新版本被索引到一个新的段中。可能两个版本的文档都会被一个查询匹配到，但被删除的那个旧版本文档在结果集返回前就已经被移除。\n\n## 45-进阶-文档刷新 & 文档刷写 & 文档合并\n\n![img](https://img-blog.csdnimg.cn/img_convert/b3b31c1e592d5aa794e7c9fcb259c924.png)\n\n![img](https://img-blog.csdnimg.cn/img_convert/521c25f0f16247240234d1b8eb3c5f25.png)\n\n### 近实时搜索\n\n随着按段（per-segment）搜索的发展，一个新的文档从索引到可被搜索的延迟显著降低了。新文档在几分钟之内即可被检索，但这样还是不够快。磁盘在这里成为了瓶颈。**提交（Commiting）一个新的段到磁盘需要一个fsync来确保段被物理性地写入磁盘**，这样在断电的时候就不会丢失数据。但是fsync操作代价很大；如果每次索引一个文档都去执行一次的话会造成很大的性能问题。\n\n我们需要的是一个更轻量的方式来使一个文档可被搜索，这意味着fsync要从整个过程中被移除。在Elasticsearch和磁盘之间是**文件系统缓存**。像之前描述的一样，在内存索引缓冲区中的文档会被写入到一个新的段中。但是这里新段会被先写入到文件系统缓存—这一步代价会比较低，稍后再被刷新到磁盘—这一步代价比较高。不过只要文件已经在缓存中，就可以像其它文件一样被打开和读取了。\n\n![img](https://img-blog.csdnimg.cn/img_convert/a679d4f5f4bfa6913a53316251beef2a.png)\n\nLucene允许新段被写入和打开，使其包含的文档在未进行一次完整提交时便对搜索可见。这种方式比进行一次提交代价要小得多，并且在不影响性能的前提下可以被频繁地执行。\n\n![img](https://img-blog.csdnimg.cn/img_convert/673d3a77e254fa3a5a6f5293ffb125ab.png)\n\n在Elasticsearch中，写入和打开一个新段的轻量的过程叫做refresh。默认情况下每个分片会每秒自动刷新一次。这就是为什么我们说Elasticsearch是近实时搜索：文档的变化并不是立即对搜索可见，但会在一秒之内变为可见。\n\n这些行为可能会对新用户造成困惑：他们索引了一个文档然后尝试搜索它，但却没有搜到。这个问题的解决办法是用refresh API执行一次手动刷新：/usersl_refresh\n\n尽管刷新是比提交轻量很多的操作，它还是会有性能开销。当写测试的时候，手动刷新很有用，但是不要在生产环境下每次索引一个文档都去手动刷新。相反，你的应用需要意识到Elasticsearch 的近实时的性质，并接受它的不足。\n\n并不是所有的情况都需要每秒刷新。可能你正在使用Elasticsearch索引大量的日志文件，你可能想优化索引速度而不是近实时搜索，可以通过设置refresh_interval，降低每个索引的刷新频率\n\n```json\n{\n    \"settings\": {\n    \t\"refresh_interval\": \"30s\"\n    }\n}\n\n```\n\nrefresh_interval可以在既存索引上进行动态更新。在生产环境中，当你正在建立一个大的新索引时，可以先关闭自动刷新，待开始使用该索引时，再把它们调回来。\n\n```\n# 关闭自动刷新\nPUT /users/_settings\n{ \"refresh_interval\": -1 }\n\n# 每一秒刷新\nPUT /users/_settings\n{ \"refresh_interval\": \"1s\" }\n\n```\n\n### 持久化变更\n\n如果没有用fsync把数据从文件系统缓存刷（flush）到硬盘，我们不能保证数据在断电甚至是程序正常退出之后依然存在。为了保证Elasticsearch的可靠性，需要确保数据变化被持久化到磁盘。在动态更新索引，我们说一次完整的提交会将段刷到磁盘，并写入一个包含所有段列表的提交点。Elasticsearch在启动或重新打开一个索引的过程中使用这个提交点来判断哪些段隶属于当前分片。\n\n即使通过每秒刷新(refresh）实现了近实时搜索，我们仍然需要经常进行完整提交来确保能从失败中恢复。但在两次提交之间发生变化的文档怎么办?我们也不希望丢失掉这些数据。Elasticsearch增加了一个translog，或者叫事务日志，在每一次对Elasticsearch进行操作时均进行了日志记录。\n\n整个流程如下:\n\n一、一个文档被索引之后，就会被添加到内存缓冲区，并且追加到了translog\n\n![img](https://img-blog.csdnimg.cn/img_convert/baeab48c8d6b87660ac4fb954e9c9731.png)\n\n二、刷新（refresh）使分片每秒被刷新（refresh）一次：\n\n- 这些在内存缓冲区的文档被写入到一个新的段中，且没有进行fsync操作。\n- 这个段被打开，使其可被搜索。\n- 内存缓冲区被清空。\n\n![img](https://img-blog.csdnimg.cn/img_convert/17be4247e6b23f31b1e589c70d61e817.png)\n\n三、这个进程继续工作，更多的文档被添加到内存缓冲区和追加到事务日志。\n\n![img](https://img-blog.csdnimg.cn/img_convert/4b5c4a3a3ffb4c84625bb283f6a67018.png)\n\n四、每隔一段时间—例如translog变得越来越大，索引被刷新（flush）；一个新的translog被创建，并且一个全量提交被执行。\n\n- 所有在内存缓冲区的文档都被写入一个新的段。\n- 缓冲区被清空。\n- 一个提交点被写入硬盘。\n- 文件系统缓存通过fsync被刷新（flush）。\n- 老的translog被删除。\n\ntranslog提供所有还没有被刷到磁盘的操作的一个持久化纪录。当Elasticsearch启动的时候，它会从磁盘中使用最后一个提交点去恢复己知的段，并且会重放translog中所有在最后一次提交后发生的变更操作。\n\ntranslog也被用来提供实时CRUD。当你试着通过ID查询、更新、删除一个文档，它会在尝试从相应的段中检索之前，首先检查translog任何最近的变更。这意味着它总是能够实时地获取到文档的最新版本。\n\n![img](https://img-blog.csdnimg.cn/img_convert/11c7d2cc05244e669eb8402dd8049de9.png)\n\n执行一个提交并且截断translog的行为在Elasticsearch被称作一次flush。分片每30分钟被自动刷新（flush)，或者在translog太大的时候也会刷新。\n\n你很少需要自己手动执行flush操作，通常情况下，自动刷新就足够了。这就是说，在重启节点或关闭索引之前执行flush有益于你的索引。当Elasticsearch尝试恢复或重新打开一个索引，它需要重放translog中所有的操作，所以如果日志越短，恢复越快。\n\ntranslog的目的是保证操作不会丢失，在文件被fsync到磁盘前，被写入的文件在重启之后就会丢失。默认translog是每5秒被fsync刷新到硬盘，或者在每次写请求完成之后执行（e.g.index,delete,update,bulk）。这个过程在主分片和复制分片都会发生。最终，基本上，这意味着在整个请求被fsync到主分片和复制分片的translog之前，你的客户端不会得到一个200 OK响应。\n\n在每次请求后都执行一个fsync会带来一些性能损失，尽管实践表明这种损失相对较小（特别是bulk导入，它在一次请求中平摊了大量文档的开销）。\n\n但是对于一些大容量的偶尔丢失几秒数据问题也并不严重的集群，使用异步的fsync还是比较有益的。比如，写入的数据被缓存到内存中，再每5秒执行一次fsync。如果你决定使用异步translog的话，你需要保证在发生crash时，丢失掉sync_interval时间段的数据也无所谓。请在决定前知晓这个特性。如果你不确定这个行为的后果，最好是使用默认的参数{“index.translog.durability”:“request”}来避免数据丢失。\n\n### 段合并\n\n由于自动刷新流程每秒会创建一个新的段，这样会导致短时间内的段数量暴增。而段数目太多会带来较大的麻烦。每一个段都会消耗文件句柄、内存和cpu运行周期。更重要的是，每个搜索请求都必须轮流检查每个段；所以段越多，搜索也就越慢。\n\nElasticsearch通过在后台进行段合并来解决这个问题。小的段被合并到大的段，然后这些大的段再被合并到更大的段。\n\n段合并的时候会将那些旧的已删除文档从文件系统中清除。被删除的文档（或被更新文档的旧版本）不会被拷贝到新的大段中。\n\n启动段合并不需要你做任何事。进行索引和搜索时会自动进行。\n\n一、当索引的时候，刷新（refresh）操作会创建新的段并将段打开以供搜索使用。\n\n二、合并进程选择一小部分大小相似的段，并且在后台将它们合并到更大的段中。这并不会中断索引和搜索。\n\n![img](https://img-blog.csdnimg.cn/img_convert/c907ca35bd7c0393d46aec2c7038af19.png)\n\n三、一旦合并结束，老的段被删除\n\n- 新的段被刷新(flush)到了磁盘。\n- 写入一个包含新段且排除旧的和较小的段的新提交点。\n- 新的段被打开用来搜索。老的段被删除。\n\n![img](https://img-blog.csdnimg.cn/img_convert/a00cc1c19652c47fcfb663aaf337a41b.png)\n\n合并大的段需要消耗大量的I/O和CPU资源，如果任其发展会影响搜索性能。Elasticsearch在默认情况下会对合并流程进行资源限制，所以搜索仍然有足够的资源很好地执行。\n\n## 46-进阶-文档分析\n\n分析包含下面的过程：\n\n- 将一块文本分成适合于倒排索引的独立的词条。\n- 将这些词条统一化为标准格式以提高它们的“可搜索性”，或者recall。\n\n分析器执行上面的工作。分析器实际上是将三个功能封装到了一个包里：\n\n- 字符过滤器：首先，字符串按顺序通过每个字符过滤器。他们的任务是在分词前整理字符串。一个字符过滤器可以用来去掉HTML，或者将&转化成and。\n- 分词器：其次，字符串被分词器分为单个的词条。一个简单的分词器遇到空格和标点的时候，可能会将文本拆分成词条。\n- Token过滤器：最后，词条按顺序通过每个token过滤器。这个过程可能会改变词条（例如，小写化Quick），删除词条（例如，像a，and，the等无用词），或者增加词条（例如，像jump和leap这种同义词）\n\n### 内置分析器\n\nElasticsearch还附带了可以直接使用的预包装的分析器。接下来我们会列出最重要的分析器。为了证明它们的差异，我们看看每个分析器会从下面的字符串得到哪些词条：\n\n```\n\"Set the shape to semi-transparent by calling set_trans(5)\"\n\n```\n\n- 标准分析器\n\n标准分析器是Elasticsearch默认使用的分析器。它是分析各种语言文本最常用的选择。它根据Unicode 联盟定义的单词边界划分文本。删除绝大部分标点。最后，将词条小写。它会产生：\n\n```\nset, the, shape, to, semi, transparent, by, calling, set_trans, 5\n\n```\n\n- 简单分析器\n\n简单分析器在任何不是字母的地方分隔文本，将词条小写。它会产生：\n\n```\nset, the, shape, to, semi, transparent, by, calling, set, trans\n\n```\n\n- 空格分析器\n\n空格分析器在空格的地方划分文本。它会产生:\n\n```\nSet, the, shape, to, semi-transparent, by, calling, set_trans(5)\n\n```\n\n- 语言分析器\n\n特定语言分析器可用于很多语言。它们可以考虑指定语言的特点。例如，英语分析器附带了一组英语无用词（常用单词，例如and或者the,它们对相关性没有多少影响），它们会被删除。由于理解英语语法的规则，这个分词器可以提取英语单词的词干。\n\n英语分词器会产生下面的词条：\n\n```\nset, shape, semi, transpar, call, set_tran, 5\n\n```\n\n注意看transparent、calling和set_trans已经变为词根格式。\n\n### 分析器使用场景\n\n当我们索引一个文档，它的全文域被分析成词条以用来创建倒排索引。但是，当我们在全文域搜索的时候，我们需要将查询字符串通过相同的分析过程，以保证我们搜索的词条格式与索引中的词条格式一致。\n\n全文查询，理解每个域是如何定义的，因此它们可以做正确的事：\n\n- 当你查询一个全文域时，会对查询字符串应用相同的分析器，以产生正确的搜索词条列表。\n- 当你查询一个精确值域时，不会分析查询字符串，而是搜索你指定的精确值。\n\n### 测试分析器\n\n有些时候很难理解分词的过程和实际被存储到索引中的词条，特别是你刚接触Elasticsearch。为了理解发生了什么，你可以使用analyze API来看文本是如何被分析的。在消息体里，指定分析器和要分析的文本。\n\n```json\n#GET http://localhost:9200/_analyze\n{\n    \"analyzer\": \"standard\",\n    \"text\": \"Text to analyze\"\n}\n\n```\n\n结果中每个元素代表一个单独的词条：\n\n```json\n{\n    \"tokens\": [\n        {\n            \"token\": \"text\", \n            \"start_offset\": 0, \n            \"end_offset\": 4, \n            \"type\": \"<ALPHANUM>\", \n            \"position\": 1\n        }, \n        {\n            \"token\": \"to\", \n            \"start_offset\": 5, \n            \"end_offset\": 7, \n            \"type\": \"<ALPHANUM>\", \n            \"position\": 2\n        }, \n        {\n            \"token\": \"analyze\", \n            \"start_offset\": 8, \n            \"end_offset\": 15, \n            \"type\": \"<ALPHANUM>\", \n            \"position\": 3\n        }\n    ]\n}\n\n```\n\n- token是实际存储到索引中的词条。\n- start_ offset 和end_ offset指明字符在原始字符串中的位置。\n- position指明词条在原始文本中出现的位置。\n\n### 指定分析器\n\n当Elasticsearch在你的文档中检测到一个新的字符串域，它会自动设置其为一个全文字符串域，使用标准分析器对它进行分析。你不希望总是这样。可能你想使用一个不同的分析器，适用于你的数据使用的语言。有时候你想要一个字符串域就是一个字符串域，不使用分析，直接索引你传入的精确值，例如用户ID或者一个内部的状态域或标签。要做到这一点，我们必须手动指定这些域的映射。\n\n（细粒度指定分析器）\n\n### IK分词器\n\n首先通过Postman发送GET请求查询分词效果\n\n```json\n# GET http://localhost:9200/_analyze\n{\n\t\"text\":\"测试单词\"\n}\n\n```\n\nES 的默认分词器无法识别中文中测试、单词这样的词汇，而是简单的将每个字拆完分为一个词。\n\n```json\n{\n    \"tokens\": [\n        {\n            \"token\": \"测\", \n            \"start_offset\": 0, \n            \"end_offset\": 1, \n            \"type\": \"<IDEOGRAPHIC>\", \n            \"position\": 0\n        }, \n        {\n            \"token\": \"试\", \n            \"start_offset\": 1, \n            \"end_offset\": 2, \n            \"type\": \"<IDEOGRAPHIC>\", \n            \"position\": 1\n        }, \n        {\n            \"token\": \"单\", \n            \"start_offset\": 2, \n            \"end_offset\": 3, \n            \"type\": \"<IDEOGRAPHIC>\", \n            \"position\": 2\n        }, \n        {\n            \"token\": \"词\", \n            \"start_offset\": 3, \n            \"end_offset\": 4, \n            \"type\": \"<IDEOGRAPHIC>\", \n            \"position\": 3\n        }\n    ]\n}\n\n```\n\n这样的结果显然不符合我们的使用要求，所以我们需要下载ES对应版本的中文分词器。\n\n[IK中文分词器下载网址](https://github.com/medcl/elasticsearch-analysis-ik/releases/tag/v7.8.0)\n\n将解压后的后的文件夹放入ES根目录下的plugins目录下，重启ES即可使用。\n\n我们这次加入新的查询参数\"analyzer\":“ik_max_word”。\n\n```json\n# GET http://localhost:9200/_analyze\n{\n\t\"text\":\"测试单词\",\n\t\"analyzer\":\"ik_max_word\"\n}\n\n```\n\n- ik_max_word：会将文本做最细粒度的拆分。\n- ik_smart：会将文本做最粗粒度的拆分。\n\n使用中文分词后的结果为：\n\n```json\n{\n    \"tokens\": [\n        {\n            \"token\": \"测试\", \n            \"start_offset\": 0, \n            \"end_offset\": 2, \n            \"type\": \"CN_WORD\", \n            \"position\": 0\n        }, \n        {\n            \"token\": \"单词\", \n            \"start_offset\": 2, \n            \"end_offset\": 4, \n            \"type\": \"CN_WORD\", \n            \"position\": 1\n        }\n    ]\n}\n\n```\n\nES中也可以进行扩展词汇，首先查询\n\n```json\n#GET http://localhost:9200/_analyze\n\n{\n    \"text\":\"弗雷尔卓德\",\n    \"analyzer\":\"ik_max_word\"\n}\n\n```\n\n仅仅可以得到每个字的分词结果，我们需要做的就是使分词器识别到弗雷尔卓德也是一个词语。\n\n```json\n{\n    \"tokens\": [\n        {\n            \"token\": \"弗\",\n            \"start_offset\": 0,\n            \"end_offset\": 1,\n            \"type\": \"CN_CHAR\",\n            \"position\": 0\n        },\n        {\n            \"token\": \"雷\",\n            \"start_offset\": 1,\n            \"end_offset\": 2,\n            \"type\": \"CN_CHAR\",\n            \"position\": 1\n        },\n        {\n            \"token\": \"尔\",\n            \"start_offset\": 2,\n            \"end_offset\": 3,\n            \"type\": \"CN_CHAR\",\n            \"position\": 2\n        },\n        {\n            \"token\": \"卓\",\n            \"start_offset\": 3,\n            \"end_offset\": 4,\n            \"type\": \"CN_CHAR\",\n            \"position\": 3\n        },\n        {\n            \"token\": \"德\",\n            \"start_offset\": 4,\n            \"end_offset\": 5,\n            \"type\": \"CN_CHAR\",\n            \"position\": 4\n        }\n    ]\n}\n\n```\n\n1. 首先进入ES根目录中的plugins文件夹下的ik文件夹，进入config目录，创建custom.dic文件，写入“弗雷尔卓德”。\n2. 同时打开IKAnalyzer.cfg.xml文件，将新建的custom.dic配置其中。\n3. 重启ES服务器。\n\n```xml\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<!DOCTYPE properties SYSTEM \"http://java.sun.com/dtd/properties.dtd\">\n<properties>\n\t<comment>IK Analyzer扩展配置</comment>\n\t<!--用户可以在这里配置自己的扩展字典 -->\n\t<entry key=\"ext_dict\">custom.dic</entry>\n\t <!--用户可以在这里配置自己的扩展停止词字典-->\n\t<entry key=\"ext_stopwords\"></entry>\n\t<!--用户可以在这里配置远程扩展字典 -->\n\t<!-- <entry key=\"remote_ext_dict\">words_location</entry> -->\n\t<!--用户可以在这里配置远程扩展停止词字典-->\n\t<!-- <entry key=\"remote_ext_stopwords\">words_location</entry> -->\n</properties>\n\n```\n\n扩展后再次查询\n\n```json\n# GET http://localhost:9200/_analyze\n{\n\t\"text\":\"测试单词\",\n\t\"analyzer\":\"ik_max_word\"\n}\n\n```\n\n返回结果如下：\n\n```json\n{\n    \"tokens\": [\n        {\n            \"token\": \"弗雷尔卓德\",\n            \"start_offset\": 0,\n            \"end_offset\": 5,\n            \"type\": \"CN_WORD\",\n            \"position\": 0\n        }\n    ]\n}\n\n```\n\n### 自定义分析器\n\n虽然Elasticsearch带有一些现成的分析器，然而在分析器上Elasticsearch真正的强大之处在于，你可以通过在一个适合你的特定数据的设置之中组合字符过滤器、分词器、词汇单元过滤器来创建自定义的分析器。在分析与分析器我们说过，一个分析器就是在一个包里面组合了三种函数的一个包装器，三种函数按照顺序被执行：\n\n#### 字符过滤器\n\n字符过滤器用来整理一个尚未被分词的字符串。例如，如果我们的文本是HTML格式的，它会包含像`<p>`或者`<div>`这样的HTML标签，这些标签是我们不想索引的。我们可以使用html清除字符过滤器来移除掉所有的HTML标签，并且像把`Á`转换为相对应的Unicode字符Á 这样，转换HTML实体。一个分析器可能有0个或者多个字符过滤器。\n\n#### 分词器\n\n一个分析器必须有一个唯一的分词器。分词器把字符串分解成单个词条或者词汇单元。标准分析器里使用的标准分词器把一个字符串根据单词边界分解成单个词条，并且移除掉大部分的标点符号，然而还有其他不同行为的分词器存在。\n\n例如，关键词分词器完整地输出接收到的同样的字符串，并不做任何分词。空格分词器只根据空格分割文本。正则分词器根据匹配正则表达式来分割文本。\n\n#### 词单元过滤器\n\n经过分词，作为结果的词单元流会按照指定的顺序通过指定的词单元过滤器。词单元过滤器可以修改、添加或者移除词单元。我们已经提到过lowercase和stop词过滤器，但是在Elasticsearch里面还有很多可供选择的词单元过滤器。词干过滤器把单词遏制为词干。ascii_folding过滤器移除变音符，把一个像\"très”这样的词转换为“tres”。\n\nngram和edge_ngram词单元过滤器可以产生适合用于部分匹配或者自动补全的词单元。\n\n#### 自定义分析器例子\n\n接下来，我们看看如何创建自定义的分析器：\n\n```json\n#PUT http://localhost:9200/my_index\n\n{\n    \"settings\": {\n        \"analysis\": {\n            \"char_filter\": {\n                \"&_to_and\": {\n                    \"type\": \"mapping\", \n                    \"mappings\": [\n                        \"&=> and \"\n                    ]\n                }\n            }, \n            \"filter\": {\n                \"my_stopwords\": {\n                    \"type\": \"stop\", \n                    \"stopwords\": [\n                        \"the\", \n                        \"a\"\n                    ]\n                }\n            }, \n            \"analyzer\": {\n                \"my_analyzer\": {\n                    \"type\": \"custom\", \n                    \"char_filter\": [\n                        \"html_strip\", \n                        \"&_to_and\"\n                    ], \n                    \"tokenizer\": \"standard\", \n                    \"filter\": [\n                        \"lowercase\", \n                        \"my_stopwords\"\n                    ]\n                }\n            }\n        }\n    }\n}\n\n```\n\n索引被创建以后，使用analyze API来测试这个新的分析器：\n\n```json\n# GET http://127.0.0.1:9200/my_index/_analyze\n{\n    \"text\":\"The quick & brown fox\",\n    \"analyzer\": \"my_analyzer\"\n}\n\n```\n\n返回结果为：\n\n```json\n{\n    \"tokens\": [\n        {\n            \"token\": \"quick\",\n            \"start_offset\": 4,\n            \"end_offset\": 9,\n            \"type\": \"<ALPHANUM>\",\n            \"position\": 1\n        },\n        {\n            \"token\": \"and\",\n            \"start_offset\": 10,\n            \"end_offset\": 11,\n            \"type\": \"<ALPHANUM>\",\n            \"position\": 2\n        },\n        {\n            \"token\": \"brown\",\n            \"start_offset\": 12,\n            \"end_offset\": 17,\n            \"type\": \"<ALPHANUM>\",\n            \"position\": 3\n        },\n        {\n            \"token\": \"fox\",\n            \"start_offset\": 18,\n            \"end_offset\": 21,\n            \"type\": \"<ALPHANUM>\",\n            \"position\": 4\n        }\n    ]\n}\n\n```\n\n## 47-进阶-文档控制\n\n### 文档冲突\n\n当我们使用index API更新文档，可以一次性读取原始文档，做我们的修改，然后重新索引整个文档。最近的索引请求将获胜：无论最后哪一个文档被索引，都将被唯一存储在Elasticsearch中。如果其他人同时更改这个文档，他们的更改将丢失。\n\n很多时候这是没有问题的。也许我们的主数据存储是一个关系型数据库，我们只是将数据复制到Elasticsearch中并使其可被搜索。也许两个人同时更改相同的文档的几率很小。或者对于我们的业务来说偶尔丢失更改并不是很严重的问题。\n\n但有时丢失了一个变更就是非常严重的。试想我们使用Elasticsearch存储我们网上商城商品库存的数量，每次我们卖一个商品的时候，我们在Elasticsearch中将库存数量减少。有一天，管理层决定做一次促销。突然地，我们一秒要卖好几个商品。假设有两个web程序并行运行，每一个都同时处理所有商品的销售。\n\n![img](https://img-blog.csdnimg.cn/img_convert/49ca2ec50db3ddd0fcd1f364ac600b96.png)\n\nweb_1对stock_count所做的更改已经丢失，因为web_2不知道它的stock_count的拷贝已经过期。结果我们会认为有超过商品的实际数量的库存，因为卖给顾客的库存商品并不存在，我们将让他们非常失望。\n\n变更越频繁，读数据和更新数据的间隙越长，也就越可能丢失变更。在数据库领域中，有两种方法通常被用来确保并发更新时变更不会丢失：\n\n- 悲观并发控制：这种方法被关系型数据库广泛使用，它假定有变更冲突可能发生，因此阻塞访问资源以防止冲突。一个典型的例子是读取一行数据之前先将其锁住，确保只有放置锁的线程能够对这行数据进行修改。\n- 乐观并发控制：Elasticsearch中使用的这种方法假定冲突是不可能发生的，并且不会阻塞正在尝试的操作。然而，如果源数据在读写当中被修改，更新将会失败。应用程序接下来将决定该如何解决冲突。例如，可以重试更新、使用新的数据、或者将相关情况报告给用户。\n\n### 乐观并发控制\n\nElasticsearch是分布式的。当文档创建、更新或删除时，新版本的文档必须复制到集群中的其他节点。Elasticsearch也是异步和并发的，这意味着这些复制请求被并行发送，并且到达目的地时也许顺序是乱的。Elasticsearch需要一种方法确保文档的旧版本不会覆盖新的版本。\n\n当我们之前讨论index,GET和DELETE请求时，我们指出每个文档都有一个_version（版本号），当文档被修改时版本号递增。Elasticsearch使用这个version号来确保变更以正确顺序得到执行。如果旧版本的文档在新版本之后到达，它可以被简单的忽略。\n\n我们可以利用version号来确保应用中相互冲突的变更不会导致数据丢失。我们通过指定想要修改文档的version号来达到这个目的。如果该版本不是当前版本号，我们的请求将会失败。\n\n老的版本es使用version，但是新版本不支持了，会报下面的错误，提示我们用if_seq_no和if_primary_term\n\n创建索引\n\n```json\n#PUT http://127.0.0.1:9200/shopping/_create/1001\n\n```\n\n返回结果\n\n```json\n{\n    \"_index\": \"shopping\",\n    \"_type\": \"_doc\",\n    \"_id\": \"1001\",\n    \"_version\": 1,\n    \"result\": \"created\",\n    \"_shards\": {\n        \"total\": 2,\n        \"successful\": 1,\n        \"failed\": 0\n    },\n    \"_seq_no\": 10,\n    \"_primary_term\": 15\n}\n\n```\n\n更新数据\n\n```json\n#POST http://127.0.0.1:9200/shopping/_update/1001\n{\n    \"doc\":{\n        \"title\":\"华为手机\"\n    }\n}\n\n```\n\n返回结果：\n\n```json\n{\n    \"_index\": \"shopping\",\n    \"_type\": \"_doc\",\n    \"_id\": \"1001\",\n    \"_version\": 2,\n    \"result\": \"updated\",\n    \"_shards\": {\n        \"total\": 2,\n        \"successful\": 1,\n        \"failed\": 0\n    },\n    \"_seq_no\": 11,\n    \"_primary_term\": 15\n}\n\n```\n\n旧版本使用的防止冲突更新方法：\n\n```json\n#POST http://127.0.0.1:9200/shopping/_update/1001?version=1\n{\n    \"doc\":{\n        \"title\":\"华为手机2\"\n    }\n}\n\n```\n\n返回结果：\n\n```json\n{\n    \"error\": {\n        \"root_cause\": [\n            {\n                \"type\": \"action_request_validation_exception\",\n                \"reason\": \"Validation Failed: 1: internal versioning can not be used for optimistic concurrency control. Please use `if_seq_no` and `if_primary_term` instead;\"\n            }\n        ],\n        \"type\": \"action_request_validation_exception\",\n        \"reason\": \"Validation Failed: 1: internal versioning can not be used for optimistic concurrency control. Please use `if_seq_no` and `if_primary_term` instead;\"\n    },\n    \"status\": 400\n}\n\n```\n\n新版本使用的防止冲突更新方法：\n\n```json\n#POST http://127.0.0.1:9200/shopping/_update/1001?if_seq_no=11&if_primary_term=15\n{\n    \"doc\":{\n        \"title\":\"华为手机2\"\n    }\n}\n\n```\n\n返回结果：\n\n```json\n{\n    \"_index\": \"shopping\",\n    \"_type\": \"_doc\",\n    \"_id\": \"1001\",\n    \"_version\": 3,\n    \"result\": \"updated\",\n    \"_shards\": {\n        \"total\": 2,\n        \"successful\": 1,\n        \"failed\": 0\n    },\n    \"_seq_no\": 12,\n    \"_primary_term\": 16\n}\n\n```\n\n### 外部系统版本控制\n\n一个常见的设置是使用其它数据库作为主要的数据存储，使用Elasticsearch做数据检索，这意味着主数据库的所有更改发生时都需要被复制到Elasticsearch，如果多个进程负责这一数据同步，你可能遇到类似于之前描述的并发问题。\n\n如果你的主数据库已经有了版本号，或一个能作为版本号的字段值比如timestamp，那么你就可以在Elasticsearch中通过增加version_type=extermal到查询字符串的方式重用这些相同的版本号，版本号必须是大于零的整数，且小于9.2E+18，一个Java中long类型的正值。\n\n外部版本号的处理方式和我们之前讨论的内部版本号的处理方式有些不同，Elasticsearch不是检查当前_version和请求中指定的版本号是否相同，而是检查当前_version是否小于指定的版本号。如果请求成功，外部的版本号作为文档的新_version进行存储。\n\n```json\n#POST http://127.0.0.1:9200/shopping/_doc/1001?version=300&version_type=external\n{\n\t\"title\":\"华为手机2\"\n}\n\n```\n\n返回结果：\n\n```json\n{\n    \"_index\": \"shopping\",\n    \"_type\": \"_doc\",\n    \"_id\": \"1001\",\n    \"_version\": 300,\n    \"result\": \"updated\",\n    \"_shards\": {\n        \"total\": 2,\n        \"successful\": 1,\n        \"failed\": 0\n    },\n    \"_seq_no\": 13,\n    \"_primary_term\": 16\n}\n\n```\n\n## 48-进阶-文档展示-Kibana\n\nKibana是一个免费且开放的用户界面，能够让你对Elasticsearch数据进行可视化，并让你在Elastic Stack中进行导航。你可以进行各种操作，从跟踪查询负载，到理解请求如何流经你的整个应用，都能轻松完成。\n\n[Kibana下载网址](https://artifacts.elastic.co/downloads/kibana/kibana-7.8.0-windows-x86_64.zip)\n\n一、解压缩下载的zip文件。\n\n二、修改config/kibana.yml文件。\n\n```yaml\n# 默认端口\nserver.port: 5601\n# ES服务器的地址\nelasticsearch.hosts: [\"http://localhost:9200\"]\n# 索引名\nkibana.index: \".kibana\"\n# 支持中文\ni18n.locale: \"zh-CN\"\n\n```\n\n三、Windows环境下执行bin/kibana.bat文件。（首次启动有点耗时）\n\n四、通过浏览器访问：http://localhost:5601。\n\n![img](https://img-blog.csdnimg.cn/img_convert/d066ba5e26916624b5d056d04c5580ac.png)\n\n# 第5章 Elasticsearch集成\n\n## 49-框架集成-SpringData-整体介绍\n\nSpring Data是一个用于简化数据库、非关系型数据库、索引库访问，并支持云服务的开源框架。其主要目标是使得对数据的访问变得方便快捷，并支持map-reduce框架和云计算数据服务。Spring Data可以极大的简化JPA(Elasticsearch…)的写法，可以在几乎不用写实现的情况下，实现对数据的访问和操作。除了CRUD外，还包括如分页、排序等一些常用的功能。\n\n[Spring Data的官网](https://spring.io/projects/spring-data)\n\nSpring Data常用的功能模块如下：\n\n- Spring Data JDBC\n- Spring Data JPA\n- Spring Data LDAP\n- Spring Data MongoDB\n- Spring Data Redis\n- Spring Data R2DBC\n- Spring Data REST\n- Spring Data for Apache Cassandra\n- Spring Data for Apache Geode\n- Spring Data for Apache Solr\n- Spring Data for Pivotal GemFire\n- Spring Data Couchbase\n- Spring Data Elasticsearch\n- Spring Data Envers\n- Spring Data Neo4j\n- Spring Data JDBC Extensions\n- Spring for Apache Hadoop\n\n### Spring Data Elasticsearch介绍\n\nSpring Data Elasticsearch基于Spring Data API简化Elasticsearch操作，将原始操作Elasticsearch的客户端API进行封装。Spring Data为Elasticsearch项目提供集成搜索引擎。Spring Data Elasticsearch POJO的关键功能区域为中心的模型与Elastichsearch交互文档和轻松地编写一个存储索引库数据访问层。\n\n[Spring Data Elasticsearch官网](https://spring.io/projects/spring-data-elasticsearch)\n\n## 50-框架集成-SpringData-代码功能集成\n\n一、创建Maven项目。\n\n二、修改pom文件，增加依赖关系。\n\n```xml\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<project xmlns=\"http://maven.apache.org/POM/4.0.0\"\n         xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n         xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">\n    <modelVersion>4.0.0</modelVersion>\n\n    <parent>\n        <groupId>org.springframework.boot</groupId>\n        <artifactId>spring-boot-starter-parent</artifactId>\n        <version>2.3.6.RELEASE</version>\n        <relativePath/>\n    </parent>\n\n    <groupId>com.lun</groupId>\n    <artifactId>SpringDataWithES</artifactId>\n    <version>1.0.0-SNAPSHOT</version>\n\n    <properties>\n        <maven.compiler.source>8</maven.compiler.source>\n        <maven.compiler.target>8</maven.compiler.target>\n    </properties>\n    <dependencies>\n        <dependency>\n            <groupId>org.projectlombok</groupId>\n            <artifactId>lombok</artifactId>\n        </dependency>\n        <dependency>\n            <groupId>org.springframework.boot</groupId>\n            <artifactId>spring-boot-starter-data-elasticsearch</artifactId>\n        </dependency>\n        <dependency>\n            <groupId>org.springframework.boot</groupId>\n            <artifactId>spring-boot-devtools</artifactId>\n            <scope>runtime</scope>\n            <optional>true</optional>\n        </dependency>\n        <dependency>\n            <groupId>org.springframework.boot</groupId>\n            <artifactId>spring-boot-starter-test</artifactId>\n            <scope>test</scope>\n        </dependency>\n        <dependency>\n            <groupId>org.springframework.boot</groupId>\n            <artifactId>spring-boot-test</artifactId>\n        </dependency>\n        <dependency>\n            <groupId>junit</groupId>\n            <artifactId>junit</artifactId>\n        </dependency>\n        <dependency>\n            <groupId>org.springframework</groupId>\n            <artifactId>spring-test</artifactId>\n        </dependency>\n    </dependencies>\n</project>\n\n```\n\n三、增加配置文件。\n\n在resources目录中增加application.properties文件\n\n```properties\n# es 服务地址\nelasticsearch.host=127.0.0.1\n# es 服务端口\nelasticsearch.port=9200\n# 配置日志级别,开启 debug 日志\nlogging.level.com.atguigu.es=debug\n\n```\n\n四、Spring Boot主程序。\n\n```java\nimport org.springframework.boot.SpringApplication;\nimport org.springframework.boot.autoconfigure.SpringBootApplication;\n\n@SpringBootApplication\npublic class MainApplication {\n    public static void main(String[] args) {\n        SpringApplication.run(MainApplication.class, args);\n    }\n}\n\n```\n\n五、数据实体类。\n\n```java\nimport lombok.AllArgsConstructor;\nimport lombok.Data;\nimport lombok.NoArgsConstructor;\nimport lombok.ToString;\nimport org.springframework.data.annotation.Id;\nimport org.springframework.data.elasticsearch.annotations.Document;\nimport org.springframework.data.elasticsearch.annotations.Field;\nimport org.springframework.data.elasticsearch.annotations.FieldType;\n\n@Data\n@NoArgsConstructor\n@AllArgsConstructor\n@ToString\n@Document(indexName = \"shopping\", shards = 3, replicas = 1)\npublic class Product {\n    //必须有 id,这里的 id 是全局唯一的标识，等同于 es 中的\"_id\"\n    @Id\n    private Long id;//商品唯一标识\n\n    /**\n     * type : 字段数据类型\n     * analyzer : 分词器类型\n     * index : 是否索引(默认:true)\n     * Keyword : 短语,不进行分词\n     */\n    @Field(type = FieldType.Text, analyzer = \"ik_max_word\")\n    private String title;//商品名称\n\n    @Field(type = FieldType.Keyword)\n    private String category;//分类名称\n\n    @Field(type = FieldType.Double)\n    private Double price;//商品价格\n\n    @Field(type = FieldType.Keyword, index = false)\n    private String images;//图片地址\n}\n\n```\n\n六、配置类\n\n- ElasticsearchRestTemplate是spring-data-elasticsearch项目中的一个类,和其他spring项目中的template类似。\n- 在新版的spring-data-elasticsearch中，ElasticsearchRestTemplate代替了原来的ElasticsearchTemplate。\n- 原因是ElasticsearchTemplate基于TransportClient，TransportClient即将在8.x以后的版本中移除。所以，我们推荐使用ElasticsearchRestTemplate。\n- ElasticsearchRestTemplate基于RestHighLevelClient客户端的。需要自定义配置类，继承AbstractElasticsearchConfiguration，并实现elasticsearchClient()抽象方法，创建RestHighLevelClient对象。\n\nAbstractElasticsearchConfiguration源码：\n\n```json\npackage org.springframework.data.elasticsearch.config;\n\nimport org.elasticsearch.client.RestHighLevelClient;\nimport org.springframework.context.annotation.Bean;\nimport org.springframework.data.elasticsearch.core.ElasticsearchOperations;\nimport org.springframework.data.elasticsearch.core.ElasticsearchRestTemplate;\nimport org.springframework.data.elasticsearch.core.convert.ElasticsearchConverter;\n\n/**\n * @author Christoph Strobl\n * @author Peter-Josef Meisch\n * @since 3.2\n * @see ElasticsearchConfigurationSupport\n */\npublic abstract class AbstractElasticsearchConfiguration extends ElasticsearchConfigurationSupport {\n\n\t//需重写本方法\n\tpublic abstract RestHighLevelClient elasticsearchClient();\n\n\t@Bean(name = { \"elasticsearchOperations\", \"elasticsearchTemplate\" })\n\tpublic ElasticsearchOperations elasticsearchOperations(ElasticsearchConverter elasticsearchConverter) {\n\t\treturn new ElasticsearchRestTemplate(elasticsearchClient(), elasticsearchConverter);\n\t}\n}\n\n```\n\n需要自定义配置类，继承AbstractElasticsearchConfiguration，并实现elasticsearchClient()抽象方法，创建RestHighLevelClient对象。\n\n```java\nimport lombok.Data;\nimport org.apache.http.HttpHost;\nimport org.elasticsearch.client.RestClient;\nimport org.elasticsearch.client.RestClientBuilder;\nimport org.elasticsearch.client.RestHighLevelClient;\nimport org.springframework.boot.context.properties.ConfigurationProperties;\nimport org.springframework.context.annotation.Configuration;\nimport org.springframework.data.elasticsearch.config.AbstractElasticsearchConfiguration;\n\n@ConfigurationProperties(prefix = \"elasticsearch\")\n@Configuration\n@Data\npublic class ElasticsearchConfig extends AbstractElasticsearchConfiguration{\n\n    private String host ;\n    private Integer port ;\n    //重写父类方法\n    @Override\n    public RestHighLevelClient elasticsearchClient() {\n        RestClientBuilder builder = RestClient.builder(new HttpHost(host, port));\n        RestHighLevelClient restHighLevelClient = new\n                RestHighLevelClient(builder);\n        return restHighLevelClient;\n    }\n}\n\n```\n\n七、DAO 数据访问对象\n\n```java\nimport com.lun.model.Product;\nimport org.springframework.data.elasticsearch.repository.ElasticsearchRepository;\nimport org.springframework.stereotype.Repository;\n\n@Repository\npublic interface ProductDao extends ElasticsearchRepository<Product, Long>{\n\n}\n\n```\n\n## 51-框架集成-SpringData-集成测试-索引操作\n\n```java\nimport com.lun.model.Product;\nimport org.junit.Test;\nimport org.junit.runner.RunWith;\nimport org.springframework.beans.factory.annotation.Autowired;\nimport org.springframework.boot.test.context.SpringBootTest;\nimport org.springframework.data.elasticsearch.core.ElasticsearchRestTemplate;\nimport org.springframework.test.context.junit4.SpringRunner;\n\n@RunWith(SpringRunner.class)\n@SpringBootTest\npublic class SpringDataESIndexTest {\n    //注入 ElasticsearchRestTemplate\n    @Autowired\n    private ElasticsearchRestTemplate elasticsearchRestTemplate;\n    //创建索引并增加映射配置\n    @Test\n    public void createIndex(){\n        //创建索引，系统初始化会自动创建索引\n        System.out.println(\"创建索引\");\n    }\n\n    @Test\n    public void deleteIndex(){\n        //创建索引，系统初始化会自动创建索引\n        boolean flg = elasticsearchRestTemplate.deleteIndex(Product.class);\n        System.out.println(\"删除索引 = \" + flg);\n    }\n}\n\n```\n\n用Postman检测有没有创建和删除。\n\n```json\n#GET http://localhost:9200/_cat/indices?v \n\n```\n\n## 52-框架集成-SpringData-集成测试-文档操作\n\n```java\nimport com.lun.dao.ProductDao;\nimport com.lun.model.Product;\nimport org.junit.Test;\nimport org.junit.runner.RunWith;\nimport org.springframework.beans.factory.annotation.Autowired;\nimport org.springframework.boot.test.context.SpringBootTest;\nimport org.springframework.data.domain.Page;\nimport org.springframework.data.domain.PageRequest;\nimport org.springframework.data.domain.Sort;\nimport org.springframework.test.context.junit4.SpringRunner;\n\nimport java.util.ArrayList;\nimport java.util.List;\n\n@RunWith(SpringRunner.class)\n@SpringBootTest\npublic class SpringDataESProductDaoTest {\n\n    @Autowired\n    private ProductDao productDao;\n    /**\n     * 新增\n     */\n    @Test\n    public void save(){\n        Product product = new Product();\n        product.setId(2L);\n        product.setTitle(\"华为手机\");\n        product.setCategory(\"手机\");\n        product.setPrice(2999.0);\n        product.setImages(\"http://www.atguigu/hw.jpg\");\n        productDao.save(product);\n    }\n    //POSTMAN, GET http://localhost:9200/product/_doc/2\n\n    //修改\n    @Test\n    public void update(){\n        Product product = new Product();\n        product.setId(2L);\n        product.setTitle(\"小米 2 手机\");\n        product.setCategory(\"手机\");\n        product.setPrice(9999.0);\n        product.setImages(\"http://www.atguigu/xm.jpg\");\n        productDao.save(product);\n    }\n    //POSTMAN, GET http://localhost:9200/product/_doc/2\n\n\n    //根据 id 查询\n    @Test\n    public void findById(){\n        Product product = productDao.findById(2L).get();\n        System.out.println(product);\n    }\n\n    @Test\n    public void findAll(){\n        Iterable<Product> products = productDao.findAll();\n        for (Product product : products) {\n            System.out.println(product);\n        }\n    }\n\n    //删除\n    @Test\n    public void delete(){\n        Product product = new Product();\n        product.setId(2L);\n        productDao.delete(product);\n    }\n    //POSTMAN, GET http://localhost:9200/product/_doc/2\n\n    //批量新增\n    @Test\n    public void saveAll(){\n        List<Product> productList = new ArrayList<>();\n        for (int i = 0; i < 10; i++) {\n            Product product = new Product();\n            product.setId(Long.valueOf(i));\n            product.setTitle(\"[\"+i+\"]小米手机\");\n            product.setCategory(\"手机\");\n            product.setPrice(1999.0 + i);\n            product.setImages(\"http://www.atguigu/xm.jpg\");\n            productList.add(product);\n        }\n        productDao.saveAll(productList);\n    }\n\n    //分页查询\n    @Test\n    public void findByPageable(){\n        //设置排序(排序方式，正序还是倒序，排序的id)\n        Sort sort = Sort.by(Sort.Direction.DESC,\"id\");\n        int currentPage=0;//当前页，第一页从0开始，1表示第二页\n        int pageSize = 5;//每页显示多少条\n        //设置查询分页\n        PageRequest pageRequest = PageRequest.of(currentPage, pageSize,sort);\n        //分页查询\n        Page<Product> productPage = productDao.findAll(pageRequest);\n        for (Product Product : productPage.getContent()) {\n            System.out.println(Product);\n        }\n    }\n}\n\n```\n\n## 53-框架集成-SpringData-集成测试-文档搜索\n\n```java\nimport com.lun.dao.ProductDao;\nimport com.lun.model.Product;\nimport org.elasticsearch.index.query.QueryBuilders;\nimport org.elasticsearch.index.query.TermQueryBuilder;\nimport org.junit.Test;\nimport org.junit.runner.RunWith;\nimport org.springframework.beans.factory.annotation.Autowired;\nimport org.springframework.boot.test.context.SpringBootTest;\nimport org.springframework.data.domain.PageRequest;\nimport org.springframework.test.context.junit4.SpringRunner;\n\n@RunWith(SpringRunner.class)\n@SpringBootTest\npublic class SpringDataESSearchTest {\n\n    @Autowired\n    private ProductDao productDao;\n    /**\n     * term 查询\n     * search(termQueryBuilder) 调用搜索方法，参数查询构建器对象\n     */\n    @Test\n    public void termQuery(){\n        TermQueryBuilder termQueryBuilder = QueryBuilders.termQuery(\"title\", \"小米\");\n                Iterable<Product> products = productDao.search(termQueryBuilder);\n        for (Product product : products) {\n            System.out.println(product);\n        }\n    }\n    /**\n     * term 查询加分页\n     */\n    @Test\n    public void termQueryByPage(){\n        int currentPage= 0 ;\n        int pageSize = 5;\n        //设置查询分页\n        PageRequest pageRequest = PageRequest.of(currentPage, pageSize);\n        TermQueryBuilder termQueryBuilder = QueryBuilders.termQuery(\"title\", \"小米\");\n                Iterable<Product> products =\n                        productDao.search(termQueryBuilder,pageRequest);\n        for (Product product : products) {\n            System.out.println(product);\n        }\n    }\n\n}\n\n```\n\n## 54-框架集成-SparkStreaming-集成\n\nSpark Streaming是Spark core API的扩展，支持实时数据流的处理，并且具有可扩展，高吞吐量，容错的特点。数据可以从许多来源获取,如Kafka，Flume，Kinesis或TCP sockets，并且可以使用复杂的算法进行处理，这些算法使用诸如map，reduce，join和window等高级函数表示。最后，处理后的数据可以推送到文件系统，数据库等。实际上，您可以将Spark的机器学习和图形处理算法应用于数据流。\n\n一、创建Maven项目。\n\n二、修改pom文件，增加依赖关系。\n\n```xml\n<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<project\n    xmlns=\"http://maven.apache.org/POM/4.0.0\"\n    xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">\n    <modelVersion>4.0.0</modelVersion>\n    <groupId>com.lun.es</groupId>\n    <artifactId>sparkstreaming-elasticsearch</artifactId>\n    <version>1.0</version>\n    <properties>\n        <maven.compiler.source>8</maven.compiler.source>\n        <maven.compiler.target>8</maven.compiler.target>\n    </properties>\n    <dependencies>\n        <dependency>\n            <groupId>org.apache.spark</groupId>\n            <artifactId>spark-core_2.12</artifactId>\n            <version>3.0.0</version>\n        </dependency>\n        <dependency>\n            <groupId>org.apache.spark</groupId>\n            <artifactId>spark-streaming_2.12</artifactId>\n            <version>3.0.0</version>\n        </dependency>\n        <dependency>\n            <groupId>org.elasticsearch</groupId>\n            <artifactId>elasticsearch</artifactId>\n            <version>7.8.0</version>\n        </dependency>\n        <!-- elasticsearch 的客户端 -->\n        <dependency>\n            <groupId>org.elasticsearch.client</groupId>\n            <artifactId>elasticsearch-rest-high-level-client</artifactId>\n            <version>7.8.0</version>\n        </dependency>\n        <!-- elasticsearch 依赖 2.x 的 log4j -->\n        <dependency>\n            <groupId>org.apache.logging.log4j</groupId>\n            <artifactId>log4j-api</artifactId>\n            <version>2.8.2</version>\n        </dependency>\n        <dependency>\n            <groupId>org.apache.logging.log4j</groupId>\n            <artifactId>log4j-core</artifactId>\n            <version>2.8.2</version>\n        </dependency>\n        <!-- <dependency>-->\n        <!-- <groupId>com.fasterxml.jackson.core</groupId>-->\n        <!-- <artifactId>jackson-databind</artifactId>-->\n        <!-- <version>2.11.1</version>-->\n        <!-- </dependency>-->\n        <!-- &lt;!&ndash; junit 单元测试 &ndash;&gt;-->\n        <!-- <dependency>-->\n        <!-- <groupId>junit</groupId>-->\n        <!-- <artifactId>junit</artifactId>-->\n        <!-- <version>4.12</version>-->\n        <!-- </dependency>-->\n    </dependencies>\n</project>\n\n```\n\n三、功能实现\n\n```scala\nimport org.apache.http.HttpHost\nimport org.apache.spark.SparkConf\nimport org.apache.spark.streaming.dstream.ReceiverInputDStream\nimport org.apache.spark.streaming.{Seconds, StreamingContext}\nimport org.elasticsearch.action.index.IndexRequest\nimport org.elasticsearch.client.indices.CreateIndexRequest\nimport org.elasticsearch.client.{RequestOptions, RestClient, RestHighLevelClient}\nimport org.elasticsearch.common.xcontent.XContentType\nimport java.util.Date\n\nobject SparkStreamingESTest {\n\n    def main(args: Array[String]): Unit = {\n        val sparkConf = new SparkConf().setMaster(\"local[*]\").setAppName(\"ESTest\")\n        val ssc = new StreamingContext(sparkConf, Seconds(3))\n        val ds: ReceiverInputDStream[String] = ssc.socketTextStream(\"localhost\", 9999)\n        ds.foreachRDD(\n            rdd => {\n                println(\"*************** \" + new Date())\n                rdd.foreach(\n                    data => {\n                        val client = new RestHighLevelClient(RestClient.builder(new HttpHost(\"localhost\", 9200, \"http\")));\n                        // 新增文档 - 请求对象\n                        val request = new IndexRequest();\n                        \n                        // 设置索引及唯一性标识\n                        val ss = data.split(\" \")\n                        println(\"ss = \" + ss.mkString(\",\"))\n                        request.index(\"sparkstreaming\").id(ss(0));\n                        \n                        val productJson =\n                            s\"\"\"\n                            | { \"data\":\"${ss(1)}\" }\n                            |\"\"\".stripMargin;\n                        \n                        // 添加文档数据，数据格式为 JSON 格式\n                        request.source(productJson,XContentType.JSON);\n                        \n                        // 客户端发送请求，获取响应对象\n                        val response = client.index(request,\n                        RequestOptions.DEFAULT);\n                        System.out.println(\"_index:\" + response.getIndex());\n                        System.out.println(\"_id:\" + response.getId());\n                        System.out.println(\"_result:\" + response.getResult());\n                        client.close()\n                    }\n                )\n            }\n        )\n        ssc.start()\n        ssc.awaitTermination()\n    }\n}\n\n```\n\n## 55-框架集成-Flink-集成\n\nApache Spark是一-种基于内存的快速、通用、可扩展的大数据分析计算引擎。Apache Spark掀开了内存计算的先河，以内存作为赌注，贏得了内存计算的飞速发展。但是在其火热的同时，开发人员发现，在Spark中，计算框架普遍存在的缺点和不足依然没有完全解决，而这些问题随着5G时代的来临以及决策者对实时数据分析结果的迫切需要而凸显的更加明显：\n\n- 乱序数据，迟到数据\n- 低延迟，高吞吐，准确性\n- 容错性\n- 数据精准一次性处理（Exactly-Once）\n\nApache Flink是一个框架和分布式处理引擎，用于对无界和有界数据流进行有状态计算。在Spark火热的同时，也默默地发展自己，并尝试着解决其他计算框架的问题。慢慢地，随着这些问题的解决，Flink慢慢被绝大数程序员所熟知并进行大力推广，阿里公司在2015年改进Flink，并创建了内部分支Blink，目前服务于阿里集团内部搜索、推荐、广告和蚂蚁等大量核心实时业务。\n\n一、创建Maven项目。\n\n二、修改pom文件，增加相关依赖类库。\n\n```xml\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<project\n    xmlns=\"http://maven.apache.org/POM/4.0.0\"\n    xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\nxsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0\nhttp://maven.apache.org/xsd/maven-4.0.0.xsd\">\n    <modelVersion>4.0.0</modelVersion>\n    <groupId>com.lun.es</groupId>\n    <artifactId>flink-elasticsearch</artifactId>\n    <version>1.0</version>\n    <properties>\n        <maven.compiler.source>8</maven.compiler.source>\n        <maven.compiler.target>8</maven.compiler.target>\n    </properties>\n    <dependencies>\n        <dependency>\n            <groupId>org.apache.flink</groupId>\n            <artifactId>flink-scala_2.12</artifactId>\n            <version>1.12.0</version>\n        </dependency>\n        <dependency>\n            <groupId>org.apache.flink</groupId>\n            <artifactId>flink-streaming-scala_2.12</artifactId>\n            <version>1.12.0</version>\n        </dependency>\n        <dependency>\n            <groupId>org.apache.flink</groupId>\n            <artifactId>flink-clients_2.12</artifactId>\n            <version>1.12.0</version>\n        </dependency>\n        <dependency>\n            <groupId>org.apache.flink</groupId>\n            <artifactId>flink-connector-elasticsearch7_2.11</artifactId>\n            <version>1.12.0</version>\n        </dependency>\n        <!-- jackson -->\n        <dependency>\n            <groupId>com.fasterxml.jackson.core</groupId>\n            <artifactId>jackson-core</artifactId>\n            <version>2.11.1</version>\n        </dependency>\n    </dependencies>\n</project>\n\n```\n\n三、功能实现\n\n```java\nimport org.apache.flink.api.common.functions.RuntimeContext;\nimport org.apache.flink.streaming.api.datastream.DataStreamSource;\nimport org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\nimport org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSinkFunction;\nimport org.apache.flink.streaming.connectors.elasticsearch.RequestIndexer;\nimport org.apache.flink.streaming.connectors.elasticsearch7.ElasticsearchSink;\nimport org.apache.http.HttpHost;\nimport org.elasticsearch.action.index.IndexRequest;\nimport org.elasticsearch.client.Requests;\nimport java.util.ArrayList;\nimport java.util.HashMap;\nimport java.util.List;\nimport java.util.Map;\n\npublic class FlinkElasticsearchSinkTest {\n\n\tpublic static void main(String[] args) throws Exception {\n\n\t\tStreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n\t\tDataStreamSource<String> source = env.socketTextStream(\"localhost\", 9999);\n\t\tList<HttpHost> httpHosts = new ArrayList<>();\n\t\thttpHosts.add(new HttpHost(\"127.0.0.1\", 9200, \"http\"));\n\t\t//httpHosts.add(new HttpHost(\"10.2.3.1\", 9200, \"http\"));\n\n\t\t// use a ElasticsearchSink.Builder to create an ElasticsearchSink\n\t\tElasticsearchSink.Builder<String> esSinkBuilder = new ElasticsearchSink.Builder<>(httpHosts, \n\t\t\tnew ElasticsearchSinkFunction<String>() {\n\t\t\t\tpublic IndexRequest createIndexRequest(String element) {\n\t\t\t\t\tMap<String, String> json = new HashMap<>();\n\t\t\t\t\tjson.put(\"data\", element);\n\t\t\t\t\treturn Requests.indexRequest()\n\t\t\t\t\t\t.index(\"my-index\")\n\t\t\t\t\t\t//.type(\"my-type\")\n\t\t\t\t\t\t.source(json);\n\t\t\t\t}\n\n\t\t\t\t@Override\n\t\t\t\tpublic void process(String element, RuntimeContext ctx, RequestIndexer indexer) {\n\t\t\t\t\tindexer.add(createIndexRequest(element));\n\t\t\t\t}\n\t\t\t}\n\t\t);\n\t\t\n\t\t// configuration for the bulk requests; this instructs the sink to emit after every element, otherwise they would be buffered\n\t\tesSinkBuilder.setBulkFlushMaxActions(1);\n\n\t\t// provide a RestClientFactory for custom configuration on the internally createdREST client\n\t\t// esSinkBuilder.setRestClientFactory(\n\t\t// restClientBuilder -> {\n\t\t\t// restClientBuilder.setDefaultHeaders(...)\n\t\t\t// restClientBuilder.setMaxRetryTimeoutMillis(...)\n\t\t\t// restClientBuilder.setPathPrefix(...)\n\t\t\t// restClientBuilder.setHttpClientConfigCallback(...)\n\t\t// }\n\t\t// );\n\t\tsource.addSink(esSinkBuilder.build());\n\t\tenv.execute(\"flink-es\");\n\t}\n}\n\n```\n\n# 第6章 Elasticsearch优化\n\n## 56-优化-硬件选择\n\nElasticsearch的基础是Lucene，所有的索引和文档数据是存储在本地的磁盘中，具体的路径可在ES的配置文件…/config/elasticsearch.yml中配置，如下：\n\n```yaml\n#\n# Path to directory where to store the data (separate multiple locations by comma):\n#\npath.data: /path/to/data\n#\n# Path to log files:\n#\npath.logs: /path/to/logs\n\n```\n\n磁盘在现代服务器上通常都是瓶颈。Elasticsearch重度使用磁盘，你的磁盘能处理的吞吐量越大，你的节点就越稳定。这里有一些优化磁盘I/O的技巧：\n\n- 使用SSD就像其他地方提过的，他们比机械磁盘优秀多了。\n- 使用RAID0。条带化RAID会提高磁盘IO，代价显然就是当一块硬盘故障时整个就故障了。不要使用镜像或者奇偶校验RAID，因为副本已经提供了这个功能。\n- 另外，使用多块硬盘，并允许Elasticsearch通过多个pathdata目录配置把数据条带化分配到它们上面。\n- 不要使用远程挂载的存储，比如NFS或者SMB/CIFS。这个引入的延迟对性能来说完全是背道而驰的。\n\n## 57-优化-分片策略\n\n### 合理设置分片数\n\n分片和副本的设计为ES提供了支持分布式和故障转移的特性，但并不意味着分片和副本是可以无限分配的。而且索引的分片完成分配后由于索引的路由机制，我们是不能重新修改分片数的。\n\n可能有人会说，我不知道这个索引将来会变得多大，并且过后我也不能更改索引的大小，所以为了保险起见，还是给它设为1000个分片吧。但是需要知道的是，一个分片并不是没有代价的。需要了解：\n\n- 一个分片的底层即为一个Lucene索引，会消耗一定文件句柄、内存、以及 CPU运转。\n- 每一个搜索请求都需要命中索引中的每一个分片，如果每一个分片都处于不同的节点还好，但如果多个分片都需要在同一个节点上竞争使用相同的资源就有些糟糕了。\n- 用于计算相关度的词项统计信息是基于分片的。如果有许多分片，每一个都只有很少的数据会导致很低的相关度。\n\n一个业务索引具体需要分配多少分片可能需要架构师和技术人员对业务的增长有个预先的判断，横向扩展应当分阶段进行。为下一阶段准备好足够的资源。只有当你进入到下一个阶段，你才有时间思考需要作出哪些改变来达到这个阶段。一般来说，我们遵循一些原则：\n\n- 控制每个分片占用的硬盘容量不超过ES的最大JVM的堆空间设置（一般设置不超过32G，参考下文的JVM设置原则），因此，如果索引的总容量在500G左右，那分片大小在16个左右即可；当然，最好同时考虑原则2。\n- 考虑一下node数量，一般一个节点有时候就是一台物理机，如果分片数过多，大大超过了节点数，很可能会导致一个节点上存在多个分片，一旦该节点故障，即使保持了1个以上的副本，同样有可能会导致数据丢失，集群无法恢复。所以，一般都设置分片数不超过节点数的3倍。\n- 主分片，副本和节点最大数之间数量，我们分配的时候可以参考以下关系：\n  `节点数<=主分片数*（副本数+1）`\n\n### 推迟分片分配\n\n对于节点瞬时中断的问题，默认情况，集群会等待一分钟来查看节点是否会重新加入，如果这个节点在此期间重新加入，重新加入的节点会保持其现有的分片数据，不会触发新的分片分配。这样就可以减少ES在自动再平衡可用分片时所带来的极大开销。\n\n通过修改参数delayed_timeout，可以延长再均衡的时间，可以全局设置也可以在索引级别进行修改：\n\n```json\n#PUT /_all/_settings\n{\n\t\"settings\": {\n\t\t\"index.unassigned.node_left.delayed_timeout\": \"5m\"\n\t}\n}\n\n```\n\n## 58-优化-路由选择\n\n当我们查询文档的时候，Elasticsearch如何知道一个文档应该存放到哪个分片中呢？它其实是通过下面这个公式来计算出来：\n\n```\nshard = hash(routing) % number_of_primary_shards\n\n```\n\nrouting默认值是文档的id，也可以采用自定义值，比如用户id。\n\n### 不带routing查询\n\n在查询的时候因为不知道要查询的数据具体在哪个分片上，所以整个过程分为2个步骤\n\n- 分发：请求到达协调节点后，协调节点将查询请求分发到每个分片上。\n- 聚合：协调节点搜集到每个分片上查询结果，在将查询的结果进行排序，之后给用户返回结果。\n\n### 带routing查询\n\n查询的时候，可以直接根据routing信息定位到某个分配查询，不需要查询所有的分配，经过协调节点排序。向上面自定义的用户查询，如果routing设置为userid的话，就可以直接查询出数据来，效率提升很多。\n\n## 59-优化-写入速度优化\n\nES 的默认配置，是综合了数据可靠性、写入速度、搜索实时性等因素。实际使用时，我们需要根据公司要求，进行偏向性的优化。\n\n针对于搜索性能要求不高，但是对写入要求较高的场景，我们需要尽可能的选择恰当写优化策略。综合来说，可以考虑以下几个方面来提升写索引的性能：\n\n- 加大Translog Flush，目的是降低Iops、Writeblock。\n- 增加Index Refesh间隔，目的是减少Segment Merge的次数。\n- 调整Bulk线程池和队列。\n- 优化节点间的任务分布。\n- 优化Lucene层的索引建立，目的是降低CPU及IO。\n\n### 优化存储设备\n\nES 是一种密集使用磁盘的应用，在段合并的时候会频繁操作磁盘，所以对磁盘要求较高，当磁盘速度提升之后，集群的整体性能会大幅度提高。\n\n### 合理使用合并\n\nLucene以段的形式存储数据。当有新的数据写入索引时，Lucene就会自动创建一个新的段。\n\n随着数据量的变化，段的数量会越来越多，消耗的多文件句柄数及CPU就越多，查询效率就会下降。\n\n由于Lucene段合并的计算量庞大，会消耗大量的I/O，所以ES默认采用较保守的策略，让后台定期进行段合并。\n\n### 减少Refresh的次数\n\nLucene在新增数据时，采用了延迟写入的策略，默认情况下索引的refresh_interval为1秒。\n\nLucene将待写入的数据先写到内存中，超过1秒（默认）时就会触发一次Refresh，然后Refresh会把内存中的的数据刷新到操作系统的文件缓存系统中。\n\n如果我们对搜索的实效性要求不高，可以将Refresh周期延长，例如30秒。\n\n这样还可以有效地减少段刷新次数，但这同时意味着需要消耗更多的Heap内存。\n\n### 加大Flush设置\n\nFlush的主要目的是把文件缓存系统中的段持久化到硬盘，当Translog的数据量达到512MB或者30分钟时，会触发一次Flush。\n\nindex.translog.flush_threshold_size参数的默认值是512MB，我们进行修改。\n\n增加参数值意味着文件缓存系统中可能需要存储更多的数据，所以我们需要为操作系统的文件缓存系统留下足够的空间。\n\n### 减少副本的数量\n\nES为了保证集群的可用性，提供了Replicas（副本）支持，然而每个副本也会执行分析、索引及可能的合并过程，所以Replicas的数量会严重影响写索引的效率。\n\n当写索引时，需要把写入的数据都同步到副本节点，副本节点越多，写索引的效率就越慢。\n\n如果我们需要大批量进行写入操作，可以先禁止Replica复制，设置\nindex.number_of_replicas:0关闭副本。在写入完成后，Replica修改回正常的状态。\n\n## 60-优化-内存设置\n\nES默认安装后设置的内存是1GB，对于任何一个现实业务来说，这个设置都太小了。如果是通过解压安装的ES，则在ES安装文件中包含一个jvm.option文件，添加如下命令来设置ES的堆大小，Xms表示堆的初始大小，Xmx表示可分配的最大内存，都是1GB。\n\n确保Xmx和Xms的大小是相同的，其目的是为了能够在Java垃圾回收机制清理完堆区后不需要重新分隔计算堆区的大小而浪费资源，可以减轻伸缩堆大小带来的压力。\n\n假设你有一个64G内存的机器，按照正常思维思考，你可能会认为把64G内存都给ES比较好，但现实是这样吗，越大越好？虽然内存对ES来说是非常重要的，但是答案是否定的！\n\n因为ES堆内存的分配需要满足以下两个原则：\n\n- 不要超过物理内存的50%：Lucene的设计目的是把底层OS里的数据缓存到内存中。Lucene的段是分别存储到单个文件中的，这些文件都是不会变化的，所以很利于缓存，同时操作系统也会把这些段文件缓存起来，以便更快的访问。如果我们设置的堆内存过大，Lucene可用的内存将会减少，就会严重影响降低Lucene的全文本查询性能。\n- 堆内存的大小最好不要超过32GB：在Java中，所有对象都分配在堆上，然后有一个KlassPointer指针指向它的类元数据。这个指针在64位的操作系统上为64位，64位的操作系统可以使用更多的内存（2^64）。在32位的系统上为32位，32位的操作系统的最大寻址空间为4GB（2^32）。但是64位的指针意味着更大的浪费，因为你的指针本身大了。浪费内存不算，更糟糕的是，更大的指针在主内存和缓存器（例如LLC,L1等）之间移动数据的时候，会占用更多的带宽。\n\n最终我们都会采用31G设置\n\n- -Xms 31g\n- -Xmx 31g\n\n假设你有个机器有128GB的内存，你可以创建两个节点，每个节点内存分配不超过32GB。也就是说不超过64GB内存给ES的堆内存，剩下的超过64GB的内存给Lucene。\n\n## 61-优化-重要配置\n\n| 参数名                             | 参数值        | 说明                                                         |\n| ---------------------------------- | ------------- | ------------------------------------------------------------ |\n| cluster.name                       | elasticsearch | 配置ES的集群名称，默认值是ES，建议改成与所存数据相关的名称，ES会自动发现在同一网段下的集群名称相同的节点。 |\n| node.name                          | node-1        | 集群中的节点名，在同一个集群中不能重复。节点的名称一旦设置，就不能再改变了。当然，也可以设置成服务器的主机名称，例如node.name:${HOSTNAME}。 |\n| node.master                        | true          | 指定该节点是否有资格被选举成为Master节点，默认是True，如果被设置为True，则只是有资格成为Master节点，具体能否成为Master节点，需要通过选举产生。 |\n| node.data                          | true          | 指定该节点是否存储索引数据，默认为True。数据 的增、删、改、查都是在Data节点完成的。 |\n| index.number_of_shards             | 1             | 设置都索引分片个数，默认是1片。也可以在创建索引时设置该值，具体设置为多大都值要根据数据量的大小来定。如果数据量不大，则设置成1时效率最高 |\n| index.number_of_replicas           | 1             | 设置默认的索引副本个数，默认为1个。副本数越多，集群的可用性越好，但是写索引时需要同步的数据越多。 |\n| transport.tcp.compress             | true          | 设置在节点间传输数据时是否压缩，默认为False， 不压缩        |\n| discovery.zen.minimum_master_nodes | 1             | 设置在选举Master节点时需要参与的最少的候选主节点数，默认为1。如果使用默认值，则当网络不稳定时有可能会出现脑裂。合理的数值为(master_eligible_nodes/2)+1，其中master_eligible_nodes表示集群中的候选主节点数 |\n| discovery.zen.ping.timeout         | 3s            | 设置在集群中自动发现其他节点时Ping连接的超时时间，默认为3秒。在较差的网络环境下需要设置得大一点，防止因误判该节点的存活状态而导致分片的转移 |\n\n# 第7章 Elasticsearch面试题\n\n## 62-面试题\n\n### 为什么要使用Elasticsearch？\n\n系统中的数据，随着业务的发展，时间的推移，将会非常多，而业务中往往采用模糊查询进行数据的搜索，而模糊查询会导致查询引擎放弃索引，导致系统查询数据时都是全表扫描，在百万级别的数据库中，查询效率是非常低下的，而我们使用ES做一个全文索引，将经常查询的系统功能的某些字段，比如说电商系统的商品表中商品名，描述、价格还有id这些字段我们放入ES索引库里，可以提高查询速度。\n\n### Elasticsearch的master选举流程？\n\n- Elasticsearch的选主是ZenDiscovery模块负责的，主要包含Ping（节点之间通过这个RPC来发现彼此）和Unicast（单播模块包含-一个主机列表以控制哪些节点需要ping通）这两部分。\n- 对所有可以成为master的节点（node master:true）根据nodeId字典排序，每次选举每个节点都把自己所知道节点排一次序，然后选出第一个（第0位）节点，暂且认为它是master节点。\n- 如果对某个节点的投票数达到一定的值（可以成为master节点数n/2+1）并且该节点自己也选举自己，那这个节点就是master。否则重新选举一直到满足上述条件。\n- master节点的职责主要包括集群、节点和索引的管理，不负责文档级别的管理；data节点可以关闭http功能。\n\n### Elasticsearch集群脑裂问题？\n\n“脑裂”问题可能的成因：\n\n- 网络问题：集群间的网络延迟导致一些节点访问不到master,认为master挂掉了从而选举出新的master,并对master上的分片和副本标红，分配新的主分片。\n- 节点负载：主节点的角色既为master又为data,访问量较大时可能会导致ES停止响应造成大面积延迟，此时其他节点得不到主节点的响应认为主节点挂掉了，会重新选取主节点。\n- 内存回收：data节点上的ES进程占用的内存较大，引发JVM的大规模内存回收，造成ES进程失去响应。\n\n脑裂问题解决方案：\n\n- 减少误判：discovery.zen ping_timeout节点状态的响应时间，默认为3s，可以适当调大，如果master在该响应时间的范围内没有做出响应应答，判断该节点已经挂掉了。调大参数（如6s，discovery.zen.ping_timeout:6），可适当减少误判。\n- 选举触发：discovery.zen.minimum._master_nodes:1，该参數是用于控制选举行为发生的最小集群主节点数量。当备选主节点的个數大于等于该参数的值，且备选主节点中有该参数个节点认为主节点挂了，进行选举。官方建议为(n /2)+1,n为主节点个数（即有资格成为主节点的节点个数）。\n- 角色分离：即master节点与data节点分离，限制角色\n  - 主节点配置为：node master: true，node data: false\n  - 从节点配置为：node master: false，node data: true\n\n### Elasticsearch索引文档的流程？\n\n![img](https://img-blog.csdnimg.cn/img_convert/1bdc6c30d1be9b1bff83a683c64d2ac7.png)\n\n- 协调节点默认使用文档ID参与计算（也支持通过routing），以便为路由提供合适的分片：shard = hash(document_id) % (num_of_primary_shards)\n- 当分片所在的节点接收到来自协调节点的请求后，会将请求写入到MemoryBuffer，然后定时（默认是每隔1秒）写入到FilesystemCache，这个从MemoryBuffer到FilesystemCache的过程就叫做refresh；\n- 当然在某些情况下，存在Momery Buffer和FilesystemCache的数据可能会丢失，ES是通过translog的机制来保证数据的可靠性的。其实现机制是接收到请求后，同时也会写入到translog中，当Filesystemcache中的数据写入到磁盘中时，才会清除掉，这个过程叫做flush；\n- 在flush过程中，内存中的缓冲将被清除，内容被写入一个新段，段的fsync将创建一个新的提交点，并将内容刷新到磁盘，旧的translog将被删除并开始一个新的translog。\n- flush触发的时机是定时触发（默认30分钟）或者translog变得太大（默认为512M）时；\n\n### Elasticsearch更新和删除文档的流程？\n\n- 删除和更新也都是写操作，但是Elasticsearch中的文档是不可变的，因此不能被删除或者改动以展示其变更；\n- 磁盘上的每个段都有一个相应的.del文件。当删除请求发送后，文档并没有真的被删除，而是在.del文件中被标记为删除。该文档依然能匹配查询，但是会在结果中被过滤掉。当段合并时，在.del文件中被标记为删除的文档将不会被写入新段。\n- 在新的文档被创建时，Elasticsearch会为该文档指定一个版本号，当执行更新时，旧版本的文档在.del文件中被标记为删除，新版本的文档被索引到一个新段。旧版本的文档依然能匹配查询，但是会在结果中被过滤掉。\n\n### Elasticsearch搜索的流程？\n\n![img](https://img-blog.csdnimg.cn/img_convert/053a14eee04ace7b4e5aec0ce53a5284.png)\n\n- 搜索被执行成一个两阶段过程，我们称之为Query Then Fetch；\n- 在初始查询阶段时，查询会广播到索引中每一个分片拷贝（主分片或者副本分片）。每个分片在本地执行搜索并构建一个匹配文档的大小为from+size的优先队列。PS：在搜索的时候是会查询Filesystem Cache的，但是有部分数据还在Memory Buffer，所以搜索是近实时的。\n- 每个分片返回各自优先队列中所有文档的ID和排序值给协调节点，它合并这些值到自己的优先队列中来产生一个全局排序后的结果列表。\n- 接下来就是取回阶段，协调节点辨别出哪些文档需要被取回并向相关的分片提交多个GET请求。每个分片加载并丰富文档，如果有需要的话，接着返回文档给协调节点。一旦所有的文档都被取回了，协调节点返回结果给客户端。\n- Query Then Fetch的搜索类型在文档相关性打分的时候参考的是本分片的数据，这样在文档数量较少的时候可能不够准确，DFS Query Then Fetch增加了一个预查询的处理，询问Term和Document frequency，这个评分更准确，但是性能会变差。\n\n### Elasticsearch在部署时，对Linux的设置有哪些优化方法？\n\n- 64GB内存的机器是非常理想的，但是32GB和16GB机器也是很常见的。少于8GB会适得其反。\n- 如果你要在更快的CPUs和更多的核心之间选择，选择更多的核心更好。多个内核提供的额外并发远胜过稍微快一点点的时钟频率。\n- 如果你负担得起SSD，它将远远超出任何旋转介质。基于SSD的节点，查询和索引性能都有提升。如果你负担得起，SSD是一个好的选择。\n- 即使数据中心们近在咫尺，也要避免集群跨越多个数据中心。绝对要避免集群跨越大的地理距离。\n- 请确保运行你应用程序的JVM和服务器的JVM是完全一样的。在Elasticsearch的几个地方，使用Java的本地序列化。\n- 通过设置gateway.recover_after_nodes、gateway.expected_nodes、gateway.recover_after_time可以在集群重启的时候避免过多的分片交换，这可能会让数据恢复从数个小时缩短为几秒钟。\n- Elasticsearch默认被配置为使用单播发现，以防止节点无意中加入集群。只有在同一台机器上运行的节点才会自动组成集群。最好使用单播代替组播。\n- 不要随意修改垃圾回收器（CMS）和各个线程池的大小。\n- 把你的内存的（少于）一半给Lucene（但不要超过32GB！），通过ES_HEAP_SIZE环境变量设置。\n- 内存交换到磁盘对服务器性能来说是致命的。如果内存交换到磁盘上，一个100微秒的操作可能变成10毫秒。再想想那么多10微秒的操作时延累加起来。不难看出swapping对于性能是多么可怕。\n- Lucene使用了大量的文件。同时，Elasticsearch在节点和HTTP客户端之间进行通信也使用了大量的套接字。所有这一切都需要足够的文件描述符。你应该增加你的文件描述符，设置一个很大的值，如64,000。\n\n### GC方面，在使用Elasticsearch时要注意什么？\n\n倒排词典的索引需要常驻内存，无法GC，需要监控data node 上 segment memory增长趋势。\n\n各类缓存，field cache, filter cache, indexing cache, bulk queue等等，要设置合理的大小，并且要应该根据最坏的情况来看heap是否够用，也就是各类缓存全部占满的时候，还有heap空间可以分配给其他任务吗？避免采用clear cache等“自欺欺人”的方式来释放内存。\n\n避免返回大量结果集的搜索与聚合。确实需要大量拉取数据的场景，可以采用scan & scroll api来实现。\n\ncluster stats驻留内存并无法水平扩展，超大规模集群可以考虑分拆成多个集群通过tribe node连接。\n\n想知道heap够不够，必须结合实际应用场景，并对集群的heap使用情况做持续的监控。\n\n### Elasticsearch对于大数据量（上亿量级）的聚合如何实现？\n\nElasticsearch提供的首个近似聚合是cardinality度量。它提供一个字段的基数，即该字段的 distinct或者unique值的数目。它是基于HLL算法的。HLL会先对我们的输入作哈希运算，然后根据哈希运算的结果中的bits做概率估算从而得到基数。其特点是：可配置的精度，用来控制内存的使用（更精确＝更多内存）；小的数据集精度是非常高的；我们可以通过配置参数，来设置去重需要的固定内存使用量。无论数千还是数十亿的唯一值，内存使用量只与你配置的精确度相关。\n\n### 在并发情况下，Elasticsearch如果保证读写一致？\n\n- 可以通过版本号使用乐观并发控制，以确保新版本不会被旧版本覆盖，由应用层来处理具体的冲突；\n- 另外对于写操作，一致性级别支持quorum/one/all，默认为quorum，即只有当大多数分片可用时才允许写操作。但即使大多数可用，也可能存在因为网络等原因导致写入副本失败，这样该副本被认为故障，分片将会在一个不同的节点上重建。\n- 对于读操作，可以设置replication为sync(默认)，这使得操作在主分片和副本分片都完成后才会返回；如果设置replication为async时，也可以通过设置搜索请求参数_preference为primary来查询主分片，确保文档是最新版本。\n\n### 如何监控Elasticsearch集群状态？\n\n1. elasticsearch-head插件。\n2. 通过Kibana监控Elasticsearch。你可以实时查看你的集群健康状态和性能，也可以分析过去的集群、索引和节点指标\n\n### 是否了解字典树？\n\n字典树又称单词查找树，Trie树，是一种树形结构，是一种哈希树的变种。典型应用是用于统计，排序和保存大量的字符串（但不仅限于字符串），所以经常被搜索引擎系统用于文本词频统计。它的优点是：利用字符串的公共前缀来减少查询时间，最大限度地减少无谓的字符串比较，查询效率比哈希树高。\n\nTrie的核心思想是空间换时间，利用字符串的公共前缀来降低查询时间的开销以达到提高效率的目的。它有3个基本性质：\n\n- 根节点不包含字符，除根节点外每一个节点都只包含一个字符。\n- 从根节点到某一节点，路径上经过的字符连接起来，为该节点对应的字符串。\n- 每个节点的所有子节点包含的字符都不相同。\n\n对于中文的字典树，每个节点的子节点用一个哈希表存储，这样就不用浪费太大的空间，而且查询速度上可以保留哈希的复杂度O(1)。\n\n### Elasticsearch中的集群、节点、索引、文档、类型是什么？\n\n- 集群是一个或多个节点（服务器）的集合，它们共同保存您的整个数据，并提供跨所有节点的联合索引和搜索功能。群集由唯一名称标识，默认情况下为\"elasticsearch\"。此名称很重要，因为如果节点设置为按名称加入群集，则该节点只能是群集的一部分。\n- 节点是属于集群一部分的单个服务器。它存储数据并参与群集索引和搜索功能。\n- 索引就像关系数据库中的“数据库”。它有一个定义多种类型的映射。索引是逻辑名称空间，映射到一个或多个主分片，并且可以有零个或多个副本分片。MySQL=>数据库，Elasticsearch=>索引。\n- 文档类似于关系数据库中的一行。不同之处在于索引中的每个文档可以具有不同的结构(字段)，但是对于通用字段应该具有相同的数据类型。MySQL=>Databases=>Tables=>Columns/Rows，Elasticsearch=>Indices=>Types=>具有属性的文档Doc。\n- 类型是索引的逻辑类别/分区，其语义完全取决于用户。\n\n### Elasticsearch中的倒排索引是什么？\n\n倒排索引是搜索引擎的核心。搜索引擎的主要目标是在查找发生搜索条件的文档时提供快速搜索。ES中的倒排索引其实就是lucene的倒排索引，区别于传统的正向索引，倒排索引会再存储数据时将关键词和数据进行关联，保存到倒排表中，然后查询时，将查询内容进行分词后在倒排表中进行查询，最后匹配数据即可。\n\n\n# 相关文章\n\n- [ElasticSearch自定义中文分词插件开发介绍](https://mp.weixin.qq.com/s/UjVH0IhSrbyk9MTKi7Nwhg)\n- [一篇就懂Elasticsearch|原力计划](https://mp.weixin.qq.com/s/W1jy-ECW_2hsvtNkrTy5eg)\n- [Elasticsearch为什么能做到快速检索？秘密在这里！](https://mp.weixin.qq.com/s/7OmlaV7yrod2_5FWyq-Y4Q)\n- [手把手教你使用Elasticsearch实现海量级数据搜索（上）](https://mp.weixin.qq.com/s/WavG7tl-xZDn7hfw24vDuw)\n- [手把手教你在CentOS上安装ELK，对服务器日志进行收集](https://mp.weixin.qq.com/s/LX2cKF6wH1IvlxmCSnWgCw)\n- [2万字详解，彻底讲透全文搜索引擎Elasticsearch](https://mp.weixin.qq.com/s/ZBsv3o5XfQq7gZkqa2yA6Q)\n- [ElasticSearch深度分页详解](https://mp.weixin.qq.com/s/Quoym4438irm4Uexb40asw)\n- [来聊一聊ElasticSearch最新版的Java客户端](https://mp.weixin.qq.com/s/nOVXS4XjNwig_2wNhaCYVw)","categories":["数据库"]},{"title":"Dubbo","slug":"Dubbo","url":"/blog/posts/566ad91076fb/","content":"\n## Dubbo基础\n\n### 什么是Dubbo?\n\n![img](https://my-blog-to-use.oss-cn-beijing.aliyuncs.com/2020-8/427f2168-1930-4c14-8760-415fac8db1d0-20200802184737978.png)\n\n[Apache Dubbo](https://github.com/apache/dubbo)|ˈdʌbəʊ|是一款高性能、轻量级的开源Java RPC框架。根据 [Dubbo官方文档](https://dubbo.apache.org/zh/)的介绍，Dubbo提供了六大核心能力\n\n1. 面向接口代理的高性能RPC调用。\n2. 智能容错和负载均衡。\n3. 服务自动注册和发现。\n4. 高度可扩展能力。\n5. 运行期流量调度。\n6. 可视化的服务治理与运维。\n\n![Dubbo提供的六大核心能力](https://oss.javaguide.cn/源码/dubbo/dubbo提供的六大核心能力.png)\n\n简单来说就是：**Dubbo不光可以帮助我们调用远程服务，还提供了一些其他开箱即用的功能比如智能负载均衡**。Dubbo目前已经有接近34.4k的Star。Dubbo是由阿里开源，后来加入了Apache。正是由于Dubbo的出现，才使得越来越多的公司开始使用以及接受分布式架构。\n\n### 为什么要用Dubbo?\n\n随着互联网的发展，网站的规模越来越大，用户数量越来越多。单一应用架构、垂直应用架构无法满足我们的需求，这个时候分布式服务架构就诞生了。分布式服务架构下，系统被拆分成不同的服务比如短信服务、安全服务，每个服务独立提供系统的某个核心服务。\n\n我们可以使用Java RMI（Java Remote Method Invocation）、Hessian这种支持远程调用的框架来简单地暴露和引用远程服务。但是当服务越来越多之后，服务调用关系越来越复杂。当应用访问压力越来越大后，负载均衡以及服务监控的需求也迫在眉睫。我们可以用F5这类硬件来做负载均衡，但这样增加了成本，并且存在单点故障的风险。\n\n不过，Dubbo的出现让上述问题得到了解决。**Dubbo帮助我们解决了什么问题呢**？\n\n1. **负载均衡**：同一个服务部署在不同的机器时该调用哪一台机器上的服务。\n2. **服务调用链路生成**：随着系统的发展，服务越来越多，服务间依赖关系变得错踪复杂，甚至分不清哪个应用要在哪个应用之前启动，架构师都不能完整的描述应用的架构关系。Dubbo可以为我们解决服务之间互相是如何调用的。\n3. **服务访问压力以及时长统计、资源调度和治理**：基于访问压力实时管理集群容量，提高集群利用率。\n4. ......\n\n![img](https://my-blog-to-use.oss-cn-beijing.aliyuncs.com/18-9-26/43050183.jpg)\n\n另外，Dubbo除了能够应用在分布式系统中，也可以应用在现在比较火的微服务系统中。不过，由于Spring Cloud在微服务中应用更加广泛，所以，我觉得一般我们提Dubbo的话，大部分是分布式系统的情况。\n\n## 分布式基础\n\n### 什么是分布式?\n\n分布式或者说SOA分布式重要的就是面向服务，说简单的分布式就是我们把整个系统拆分成不同的服务然后将这些服务放在不同的服务器上减轻单体服务的压力提高并发量和性能。比如电商系统可以简单地拆分成订单系统、商品系统、登录系统等等，拆分之后的每个服务可以部署在不同的机器上，如果某一个服务的访问量比较大的话也可以将这个服务同时部署在多台机器上。\n\n![分布式事务示意图](https://oss.javaguide.cn/java-guide-blog/分布式事务示意图.png)\n\n### 为什么要分布式?\n\n从开发角度来讲单体应用的代码都集中在一起，而分布式系统的代码根据业务被拆分。所以，每个团队可以负责一个服务的开发，这样提升了开发效率。另外，代码根据业务拆分之后更加便于维护和扩展。另外，我觉得将系统拆分成分布式之后不光便于系统扩展和维护，更能提高整个系统的性能。你想一想嘛？把整个系统拆分成不同的服务/系统，然后每个服务/系统单独部署在一台服务器上，是不是很大程度上提高了系统性能呢？\n\n## Dubbo架构\n\n### Dubbo架构中的核心角色有哪些？\n\n[官方文档中的框架设计章节](https://dubbo.apache.org/zh/docs/v2.7/dev/design/)已经介绍的非常详细了，我这里把一些比较重要的点再提一下。\n\n![dubbo-relation](https://oss.javaguide.cn/源码/dubbo/dubbo-relation.jpg)\n\n上述节点简单介绍以及他们之间的关系：\n\n- **Container**：服务运行容器，负责加载、运行服务提供者。必须。\n- **Provider**：暴露服务的服务提供方，会向注册中心注册自己提供的服务。必须。\n- **Consumer**：调用远程服务的服务消费方，会向注册中心订阅自己所需的服务。必须。\n- **Registry**：服务注册与发现的注册中心。注册中心会返回服务提供者地址列表给消费者。非必须。\n- **Monitor**：统计服务的调用次数和调用时间的监控中心。服务消费者和提供者会定时发送统计数据到监控中心。非必须。\n\n### Dubbo中的Invoker概念了解么？\n\nInvoker是Dubbo领域模型中非常重要的一个概念，你如果阅读过Dubbo源码的话，你会无数次看到这玩意。就比如下面我要说的负载均衡这块的源码中就有大量Invoker的身影。简单来说，Invoker就是Dubbo对远程调用的抽象。\n\n![dubbo_rpc_invoke.jpg](https://oss.javaguide.cn/java-guide-blog/dubbo_rpc_invoke.jpg)\n\n按照Dubbo官方的话来说，Invoker分为\n\n- 服务提供Invoker\n- 服务消费Invoker\n\n假如我们需要调用一个远程方法，我们需要动态代理来屏蔽远程调用的细节吧！我们屏蔽掉的这些细节就依赖对应的Invoker实现，Invoker实现了真正的远程服务调用。\n\n### Dubbo的工作原理了解么？\n\n下图是Dubbo的整体设计，从下至上分为十层，各层均为单向依赖。\n\n> 左边淡蓝背景的为服务消费方使用的接口，右边淡绿色背景的为服务提供方使用的接口，位于中轴线上的为双方都用到的接口。\n\n![dubbo-framework](https://oss.javaguide.cn/source-code/dubbo/dubbo-framework.jpg)\n\n- **config配置层**：Dubbo相关的配置。支持代码配置，同时也支持基于Spring来做配置，以Service Config,Reference Config为中心\n- **proxy服务代理层**：调用远程方法像调用本地的方法一样简单的一个关键，真实调用过程依赖代理类，以ServiceProxy为中心。\n- **registry注册中心层**：封装服务地址的注册与发现。\n- **cluster路由层**：封装多个提供者的路由及负载均衡，并桥接注册中心，以Invoker为中心。\n- **monitor监控层**：RPC调用次数和调用时间监控，以Statistics为中心。\n- **protocol远程调用层**：封装RPC调用，以Invocation,Result为中心。\n- **exchange信息交换层**：封装请求响应模式，同步转异步，以Request,Response为中心。\n- **transport网络传输层**：抽象mina和netty为统一接口，以Message为中心。\n- **serialize数据序列化层**：对需要在网络传输的数据进行序列化。\n\n### Dubbo的SPI机制了解么？如何扩展Dubbo中的默认实现？\n\nSPI（Service Provider Interface）机制被大量用在开源项目中，它可以帮助我们动态寻找服务/功能（比如负载均衡策略）的实现。SPI的具体原理是这样的：我们将接口的实现类放在配置文件中，我们在程序运行过程中读取配置文件，通过反射加载实现类。这样，我们可以在运行的时候，动态替换接口的实现类。和IoC的解耦思想是类似的。Java本身就提供了SPI机制的实现。不过，Dubbo没有直接用，而是对Java原生的SPI机制进行了增强，以便更好满足自己的需求。\n\n**那我们如何扩展Dubbo中的默认实现呢**？\n\n比如说我们想要实现自己的负载均衡策略，我们创建对应的实现类XxxLoadBalance实现LoadBalance接口或者AbstractLoadBalance类。\n\n```java\npackage com.xxx;\n \nimport org.apache.dubbo.rpc.cluster.LoadBalance;\nimport org.apache.dubbo.rpc.Invoker;\nimport org.apache.dubbo.rpc.Invocation;\nimport org.apache.dubbo.rpc.RpcException;\n \npublic class XxxLoadBalance implements LoadBalance {\n    public <T> Invoker<T> select(List<Invoker<T>> invokers, Invocation invocation) throws RpcException {\n        // ...\n    }\n}\n```\n\n我们将这个实现类的路径写入到resources目录下的META-INF/dubbo/org.apache.dubbo.rpc.cluster.LoadBalance文件中即可。\n\n```java\nsrc\n |-main\n    |-java\n        |-com\n            |-xxx\n                |-XxxLoadBalance.java(实现LoadBalance接口)\n    |-resources\n        |-META-INF\n            |-dubbo\n                |-org.apache.dubbo.rpc.cluster.LoadBalance(纯文本文件，内容为：xxx=com.xxx.XxxLoadBalance)\n```\norg.apache.dubbo.rpc.cluster.LoadBalance\n\n```text\nxxx=com.xxx.XxxLoadBalance\n```\n\n其他还有很多可供扩展的选择，你可以在[官方文档](https://cn.dubbo.apache.org/zh-cn/overview/home/)中找到。\n\n### Dubbo的微内核架构了解吗？\n\nDubbo采用微内核（Microkernel）+插件（Plugin）模式，简单来说就是微内核架构。微内核只负责组装插件。\n\n**何为微内核架构呢**？《软件架构模式》这本书是这样介绍的：\n\n> 微内核架构模式（有时被称为插件架构模式）是实现基于产品应用程序的一种自然模式。基于产品的应用程序是已经打包好并且拥有不同版本，可作为第三方插件下载的。然后，很多公司也在开发、发布自己内部商业应用像有版本号、说明及可加载插件式的应用软件（这也是这种模式的特征）。微内核系统可让用户添加额外的应用如插件，到核心应用，继而提供了可扩展性和功能分离的用法。\n\n微内核架构包含两类组件：**核心系统（coresystem）**和**插件模块（plug-inmodules）**。\n\n![img](https://oss.javaguide.cn/source-code/dubbo/微内核架构示意图.png)\n\n核心系统提供系统所需核心能力，插件模块可以扩展系统的功能。因此，基于微内核架构的系统，非常易于扩展功能。我们常见的一些IDE，都可以看作是基于微内核架构设计的。绝大多数IDE比如IDEA、VSCode都提供了插件来丰富自己的功能。正是因为Dubbo基于微内核架构，才使得我们可以随心所欲替换Dubbo的功能点。比如你觉得Dubbo的序列化模块实现的不满足自己要求，没关系,你自己实现一个序列化模块就好了。通常情况下，微核心都会采用Factory、IoC、OSGi等方式管理插件生命周期。Dubbo不想依赖Spring等IoC容器，也不想自己造一个小的IoC容器（过度设计），因此采用了一种最简单的Factory方式管理插件：**JDK标准的SPI扩展机制**（java.util.ServiceLoader）。\n\n### 关于Dubbo架构的一些自测小问题\n\n#### 注册中心的作用了解么？\n\n注册中心负责服务地址的注册与查找，相当于目录服务，服务提供者和消费者只在启动时与注册中心交互。\n\n#### 服务提供者宕机后，注册中心会做什么？\n\n注册中心会立即推送事件通知消费者。\n\n#### 监控中心的作用呢？\n\n监控中心负责统计各服务调用次数，调用时间等。\n\n#### 注册中心和监控中心都宕机的话，服务都会挂掉吗？\n\n不会。两者都宕机也不影响已运行的提供者和消费者，消费者在本地缓存了提供者列表。注册中心和监控中心都是可选的，服务消费者可以直连服务提供者。\n\n## Dubbo的负载均衡策略\n\n### 什么是负载均衡？\n\n先来看一下稍微官方点的解释。下面这段话摘自维基百科对负载均衡的定义：\n\n> 负载均衡改善了跨多个计算资源（例如计算机，计算机集群，网络链接，中央处理单元或磁盘驱动）的工作负载分布。负载平衡旨在优化资源使用，最大化吞吐量，最小化响应时间，并避免任何单个资源的过载。使用具有负载平衡而不是单个组件的多个组件可以通过冗余提高可靠性和可用性。负载平衡通常涉及专用软件或硬件。\n\n我们的系统中的某个服务的访问量特别大，我们将这个服务部署在了多台服务器上，当客户端发起请求的时候，多台服务器都可以处理这个请求。那么，如何正确选择处理该请求的服务器就很关键。假如，你就要一台服务器来处理该服务的请求，那该服务部署在多台服务器的意义就不复存在了。负载均衡就是为了避免单个服务器响应同一请求，容易造成服务器宕机、崩溃等问题，我们从负载均衡的这四个字就能明显感受到它的意义。\n\n### Dubbo提供的负载均衡策略有哪些？\n\n在集群负载均衡时，Dubbo提供了多种均衡策略，默认为random随机调用。我们还可以自行扩展负载均衡策略（参考DubboSPI机制）。\n\n在Dubbo中，所有负载均衡实现类均继承自AbstractLoadBalance，该类实现了LoadBalance接口，并封装了一些公共的逻辑。\n\n```java\npublic abstract class AbstractLoadBalance implements LoadBalance {\n\n    static int calculateWarmupWeight(int uptime, int warmup, int weight) {\n    }\n\n    @Override\n    public <T> Invoker<T> select(List<Invoker<T>> invokers, URL url, Invocation invocation) {\n    }\n\n    protected abstract <T> Invoker<T> doSelect(List<Invoker<T>> invokers, URL url, Invocation invocation);\n\n    int getWeight(Invoker<?> invoker, Invocation invocation) {\n\n    }\n}\n```\n\nAbstractLoadBalance的实现类有下面这些：\n\n![img](https://oss.javaguide.cn/java-guide-blog/image-20210326105257812.png)\n\n\n\n#### RandomLoadBalance\n\n根据权重随机选择（对加权随机算法的实现）。这是Dubbo默认采用的一种负载均衡策略。\n\nRandomLoadBalance具体的实现原理非常简单，假如有两个提供相同服务的服务器S1,S2，S1的权重为7，S2的权重为3。\n\n我们把这些权重值分布在坐标区间会得到：S1->[0,7)，S2->[7,10)。我们生成[0,10)之间的随机数，随机数落到对应的区间，我们就选择对应的服务器来处理请求。\n\n![RandomLoadBalance](https://oss.javaguide.cn/java-guide-blog/%20RandomLoadBalance.png)\n\nRandomLoadBalance的源码非常简单，简单花几分钟时间看一下。\n\n>以下源码来自Dubbomaster分支上的最新的版本2.7.9。\n\n```java\npublic class RandomLoadBalance extends AbstractLoadBalance {\n\n    public static final String NAME = \"random\";\n\n    @Override\n    protected <T> Invoker<T> doSelect(List<Invoker<T>> invokers, URL url, Invocation invocation) {\n\n        int length = invokers.size();\n        boolean sameWeight = true;\n        int[] weights = new int[length]; \n        int totalWeight = 0;\n        // 下面这个for循环的主要作用就是计算所有该服务的提供者的权重之和totalWeight（），\n        // 除此之外，还会检测每个服务提供者的权重是否相同\n        for (int i = 0; i < length; i++) {\n            int weight = getWeight(invokers.get(i), invocation);\n            totalWeight += weight;\n            weights[i] = totalWeight;\n            if (sameWeight && totalWeight != weight * (i + 1)) {\n                sameWeight = false;\n            }\n        }\n        if (totalWeight > 0 && !sameWeight) {\n            // 随机生成一个[0,totalWeight)区间内的数字\n            int offset = ThreadLocalRandom.current().nextInt(totalWeight);\n            // 判断会落在哪个服务提供者的区间\n            for (int i = 0; i < length; i++) {\n                if (offset < weights[i]) {\n                    return invokers.get(i);\n                }\n            }\n  \n        return invokers.get(ThreadLocalRandom.current().nextInt(length));\n    }\n\n}\n```\n\n#### LeastActiveLoadBalance\n\nLeastActiveLoadBalance直译过来就是**最小活跃数负载均衡**。这个名字起得有点不直观，不仔细看官方对活跃数的定义，你压根不知道这玩意是干嘛的。我这么说吧！初始状态下所有服务提供者的活跃数均为0（每个服务提供者的中特定方法都对应一个活跃数，我在后面的源码中会提到），每收到一个请求后，对应的服务提供者的活跃数+1，当这个请求处理完之后，活跃数-1。因此，**Dubbo就认为谁的活跃数越少，谁的处理速度就越快，性能也越好，这样的话，我就优先把请求给活跃数少的服务提供者处理**。\n\n**如果有多个服务提供者的活跃数相等怎么办**？很简单，那就再走一遍RandomLoadBalance。\n\n```java\npublic class LeastActiveLoadBalance extends AbstractLoadBalance {\n\n    public static final String NAME = \"leastactive\";\n\n    @Override\n    protected <T> Invoker<T> doSelect(List<Invoker<T>> invokers, URL url, Invocation invocation) {\n        int length = invokers.size();\n        int leastActive = -1;\n        int leastCount = 0;\n        int[] leastIndexes = new int[length];\n        int[] weights = new int[length];\n        int totalWeight = 0;\n        int firstWeight = 0;\n        boolean sameWeight = true;\n        // 这个for循环的主要作用是遍历invokers列表，找出活跃数最小的Invoker\n        // 如果有多个Invoker具有相同的最小活跃数，还会记录下这些Invoker在invokers集合中的下标，并累加它们的权重，比较它们的权重值是否相等\n        for (int i = 0; i < length; i++) {\n            Invoker<T> invoker = invokers.get(i);\n            // 获取invoker对应的活跃(active)数\n            int active = RpcStatus.getStatus(invoker.getUrl(), invocation.getMethodName()).getActive();\n            int afterWarmup = getWeight(invoker, invocation);\n            weights[i] = afterWarmup;\n            if (leastActive == -1 || active < leastActive) {\n                leastActive = active;\n                leastCount = 1;\n                leastIndexes[0] = i;\n                totalWeight = afterWarmup;\n                firstWeight = afterWarmup;\n                sameWeight = true;\n            } else if (active == leastActive) {\n                leastIndexes[leastCount++] = i;\n                totalWeight += afterWarmup;\n                if (sameWeight && afterWarmup != firstWeight) {\n                    sameWeight = false;\n                }\n            }\n        }\n       // 如果只有一个Invoker具有最小的活跃数，此时直接返回该Invoker即可\n        if (leastCount == 1) {\n            return invokers.get(leastIndexes[0]);\n        }\n        // 如果有多个Invoker具有相同的最小活跃数，但它们之间的权重不同\n        // 这里的处理方式就和RandomLoadBalance一致了\n        if (!sameWeight && totalWeight > 0) {\n            int offsetWeight = ThreadLocalRandom.current().nextInt(totalWeight);\n            for (int i = 0; i < leastCount; i++) {\n                int leastIndex = leastIndexes[i];\n                offsetWeight -= weights[leastIndex];\n                if (offsetWeight < 0) {\n                    return invokers.get(leastIndex);\n                }\n            }\n        }\n        return invokers.get(leastIndexes[ThreadLocalRandom.current().nextInt(leastCount)]);\n    }\n}\n```\n\n活跃数是通过RpcStatus中的一个ConcurrentMap保存的，根据URL以及服务提供者被调用的方法的名称，我们便可以获取到对应的活跃数。也就是说服务提供者中的每一个方法的活跃数都是互相独立的。\n\n```java\npublic class RpcStatus {\n    \n    private static final ConcurrentMap<String, ConcurrentMap<String, RpcStatus>> METHOD_STATISTICS =\n            new ConcurrentHashMap<String, ConcurrentMap<String, RpcStatus>>();\n\n   public static RpcStatus getStatus(URL url, String methodName) {\n        String uri = url.toIdentityString();\n        ConcurrentMap<String, RpcStatus> map = METHOD_STATISTICS.computeIfAbsent(uri, k -> new ConcurrentHashMap<>());\n        return map.computeIfAbsent(methodName, k -> new RpcStatus());\n    }\n    public int getActive() {\n        return active.get();\n    }\n\n}\n```\n\n#### ConsistentHashLoadBalance\n\nConsistentHashLoadBalance小伙伴们应该也不会陌生，在分库分表、各种集群中就经常使用这个负载均衡策略。ConsistentHashLoadBalance即**一致性Hash负载均衡策略**。ConsistentHashLoadBalance中没有权重的概念，具体是哪个服务提供者处理请求是由你的请求的参数决定的，也就是说相同参数的请求总是发到同一个服务提供者。\n\n![img](https://oss.javaguide.cn/java-guide-blog/consistent-hash-data-incline.jpg)\n\n另外，Dubbo为了避免数据倾斜问题（节点不够分散，大量请求落到同一节点），还引入了虚拟节点的概念。通过虚拟节点可以让节点更加分散，有效均衡各个节点的请求量。\n\n![img](https://oss.javaguide.cn/java-guide-blog/consistent-hash-invoker.jpg)\n\n> 官方有详细的[源码分析](https://dubbo.apache.org/zh/docs/v2.7/dev/source/loadbalance/#23-consistenthashloadbalance)。这里还有一个相关的[PR#5440](https://github.com/apache/dubbo/pull/5440)来修复老版本中ConsistentHashLoadBalance存在的一些Bug。感兴趣的小伙伴，可以多花点时间研究一下。\n\n#### RoundRobinLoadBalance\n\n加权轮询负载均衡。\n\n轮询就是把请求依次分配给每个服务提供者。加权轮询就是在轮询的基础上，让更多的请求落到权重更大的服务提供者上。比如假如有两个提供相同服务的服务器S1,S2，S1的权重为7，S2的权重为3。如果我们有10次请求，那么7次会被S1处理，3次被S2处理。但是，如果是RandomLoadBalance的话，很可能存在10次请求有9次都被S1处理的情况（概率性问题）。Dubbo中的RoundRobinLoadBalance的代码实现被修改重建了好几次，Dubbo-2.6.5版本的RoundRobinLoadBalance为平滑加权轮询算法。\n\n## Dubbo序列化协议\n\n### Dubbo支持哪些序列化方式呢？\n\n![img](https://oss.javaguide.cn/github/javaguide/csdn/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM0MzM3Mjcy,size_16,color_FFFFFF,t_70-20230309234143460.png)\n\nDubbo支持多种序列化方式：JDK自带的序列化、hessian2、JSON、Kryo、FST、Protostuff，ProtoBuf等等。\n\nDubbo默认使用的序列化方式是hessian2。\n\n### 谈谈你对这些序列化协议了解？\n\n一般我们不会直接使用JDK自带的序列化方式。主要原因有两个：\n\n1. **不支持跨语言调用**：如果调用的是其他语言开发的服务的时候就不支持了。\n2. **性能差**：相比于其他序列化框架性能更低，主要原因是序列化之后的字节数组体积较大，导致传输成本加大。\n\nJSON序列化由于性能问题，我们一般也不会考虑使用。\n\n像Protostuff，ProtoBuf、hessian2这些都是跨语言的序列化方式，如果有跨语言需求的话可以考虑使用。\n\nKryo和FST这两种序列化方式是Dubbo后来才引入的，性能非常好。不过，这两者都是专门针对Java语言的。Dubbo官网的[一篇文章](https://dubbo.apache.org/zh/docs/v2.7/user/references/protocol/rest/)中提到说推荐使用Kryo作为生产环境的序列化方式。\n\n![img](https://my-blog-to-use.oss-cn-beijing.aliyuncs.com/2020-8/569e541a-22b2-4846-aa07-0ad479f07440.png)\n\nDubbo官方文档中还有一个关于这些[序列化协议的性能对比图](https://dubbo.apache.org/zh/docs/v2.7/user/serialization/#m-zhdocsv27userserialization)可供参考。\n\n![img](https://oscimg.oschina.net/oscnet/up-00c3ce1e5d222e477ed84310239daa2f6b0.png)\n\n> [原文链接](https://javaguide.cn/distributed-system/rpc/dubbo.html)\n\n\n## 相关文章\n\n- [dubbo官方文档](http://dubbo.apache.org/zh-cn/docs/user/quick-start.html)\n- [长文图解七种负载均衡策略](https://mp.weixin.qq.com/s/jveqCUAiqKdKRoea9wESbw)\n- [如果Dubbo还没精通原理，就从这里开始吧](https://mp.weixin.qq.com/s/NEfWwnRVOI73_TQurmxk9A)","categories":["技术栈"]},{"title":"JWT-Json Web Token","slug":"JWT-Json Web Token","url":"/blog/posts/08a7e2371990/","content":"\n# JWT基础概念详解\n\n## 什么是JWT?\n\nJWT（JSON Web Token）是目前最流行的跨域认证解决方案，是一种基于Token的认证授权机制。从JWT的全称可以看出，JWT本身也是Token，一种规范化之后的JSON结构的Token。JWT自身包含了身份验证所需要的所有信息，因此，我们的服务器不需要存储Session信息。这显然增加了系统的可用性和伸缩性，大大减轻了服务端的压力。可以看出，**JWT更符合设计RESTful API时的「Stateless（无状态）」原则**。并且，使用JWT认证可以有效避免CSRF攻击，因为JWT一般是存在在local Storage中，使用JWT进行身份验证的过程中是不会涉及到Cookie的。下面是[RFC7519](https://tools.ietf.org/html/rfc7519)对JWT做的较为正式的定义。\n\n> JSON Web Token (JWT) is a compact, URL-safe means of representing claims to be transferred between two parties. The claims in a JWT are encoded as a JSON object that is used as the payload of a JSON Web Signature (JWS) structure or as the plaintext of a JSON Web Encryption (JWE) structure, enabling the claims to be digitally signed or integrity protected with a Message Authentication Code (MAC) and/or encrypted. ——[JSON Web Token (JWT)](https://tools.ietf.org/html/rfc7519)\n\n## JWT由哪些部分组成？\n\n![此图片来源于：https://supertokens.com/blog/oauth-vs-jwt](https://oss.javaguide.cn/javaguide/system-design/jwt/jwt-composition.png)\n\nJWT本质上就是一组字串，通过（`.`）切分成三个为Base64编码的部分：\n\n- **Header**：描述JWT的元数据，定义了生成签名的算法以及Token的类型。\n- **Payload**：用来存放实际需要传递的数据\n- **Signature（签名）**：服务器通过Payload、Header和一个密钥(Secret)使用Header里面指定的签名算法（默认是 HMAC SHA256）生成。\n\nJWT通常是这样的：`xxxxx.yyyyy.zzzzz`。示例：\n\n\n```text\neyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.\neyJzdWIiOiIxMjM0NTY3ODkwIiwibmFtZSI6IkpvaG4gRG9lIiwiaWF0IjoxNTE2MjM5MDIyfQ.\nSflKxwRJSMeKKF2QT4fwpMeJf36POk6yJV_adQssw5c\n```\n\n你可以在[jwt.io](https://jwt.io/)这个网站上对其JWT进行解码，解码之后得到的就是Header、Payload、Signature这三部分。Header和Payload都是JSON格式的数据，Signature由Payload、Header和Secret(密钥)通过特定的计算公式和加密算法得到。\n\n![img](https://oss.javaguide.cn/javaguide/system-design/jwt/jwt.io.png)\n\n### Header\n\nHeader通常由两部分组成：\n\n- typ（Type）：令牌类型，也就是JWT。\n- alg（Algorithm）：签名算法，比如HS256。\n\n示例：\n\n```json\n{\n  \"alg\": \"HS256\",\n  \"typ\": \"JWT\"\n}\n```\n\nJSON形式的Header被转换成Base64编码，成为JWT的第一部分。\n\n### Payload\n\nPayload也是JSON格式数据，其中包含了Claims(声明，包含JWT的相关信息)。Claims分为三种类型：\n\n- **Registered Claims（注册声明）**：预定义的一些声明，建议使用，但不是强制性的。\n- **Public Claims（公有声明）**：JWT签发方可以自定义的声明，但是为了避免冲突，应该在[IANA JSON Web Token Registry](https://www.iana.org/assignments/jwt/jwt.xhtml)中定义它们。\n- **PrivateClaims（私有声明）**：JWT签发方因为项目需要而自定义的声明，更符合实际项目场景使用。\n\n下面是一些常见的注册声明：\n\n- iss（issuer）：JWT签发方。\n- iat（issuedattime）：JWT签发时间。\n- sub（subject）：JWT主题。\n- aud（audience）：JWT接收方。\n- exp（expirationtime）：JWT的过期时间。\n- nbf（notbeforetime）：JWT生效时间，早于该定义的时间的JWT不能被接受处理。\n- jti（JWTID）：JWT唯一标识。\n\n示例：\n\n\n```json\n{\n  \"uid\": \"ff1212f5-d8d1-4496-bf41-d2dda73de19a\",\n  \"sub\": \"1234567890\",\n  \"name\": \"John Doe\",\n  \"exp\": 15323232,\n  \"iat\": 1516239022,\n  \"scope\": [\"admin\",\"user\"]\n}\n```\n\nPayload部分默认是不加密的，**一定不要将隐私信息存放在Payload当中**！！！JSON形式的Payload被转换成Base64编码，成为JWT的第二部分。\n\n### Signature\n\nSignature部分是对前两部分的签名，作用是防止JWT（主要是payload）被篡改。这个签名的生成需要用到：\n\n- Header+Payload。\n- 存放在服务端的密钥(一定不要泄露出去)。\n- 签名算法。\n\n签名的计算公式如下：\n\n```text\nHMACSHA256(\n  base64UrlEncode(header) + \".\" +\n  base64UrlEncode(payload),\n  secret)\n```\n\n算出签名以后，把Header、Payload、Signature三个部分拼成一个字符串，每个部分之间用\"点\"（`.`）分隔，这个字符串就是JWT。\n\n## 如何基于JWT进行身份验证？\n\n在基于JWT进行身份验证的的应用程序中，服务器通过Payload、Header和Secret(密钥)创建JWT并将JWT发送给客户端。客户端接收到JWT之后，会将其保存在Cookie或者localStorage里面，以后客户端发出的所有请求都会携带这个令牌。\n\n![JWT身份验证示意图](https://oss.javaguide.cn/github/javaguide/system-design/jwt/jwt-authentication%20process.png)\n\n简化后的步骤如下：\n\n1. 用户向服务器发送用户名、密码以及验证码用于登陆系统。\n2. 如果用户用户名、密码以及验证码校验正确的话，服务端会返回已经签名的Token，也就是JWT。\n3. 用户以后每次向后端发请求都在Header中带上这个JWT。\n4. 服务端检查JWT并从中获取用户相关信息。\n\n两点建议：\n\n1. 建议将JWT存放在localStorage中，放在Cookie中会有CSRF风险。\n2. 请求服务端并携带JWT的常见做法是将其放在HTTP Header的`Authorization`字段中（`Authorization:BearerToken`）。\n\n> **[spring-security-jwt-guide](https://github.com/Snailclimb/spring-security-jwt-guide)** 就是一个基于JWT来做身份认证的简单案例，感兴趣的可以看看。\n\n## 如何防止JWT被篡改？\n\n有了签名之后，即使JWT被泄露或者截获，黑客也没办法同时篡改Signature、Header、Payload。这是为什么呢？因为服务端拿到JWT之后，会解析出其中包含的Header、Payload以及Signature。服务端会根据Header、Payload、密钥再次生成一个Signature。拿新生成的Signature和JWT中的Signature作对比，如果一样就说明Header和Payload没有被修改。不过，如果服务端的秘钥也被泄露的话，黑客就可以同时篡改Signature、Header、Payload了。黑客直接修改了Header和Payload之后，再重新生成一个Signature就可以了。\n\n**密钥一定保管好，一定不要泄露出去。JWT安全的核心在于签名，签名安全的核心在密钥**。\n\n## 如何加强JWT的安全性？\n\n1. 使用安全系数高的加密算法。\n2. 使用成熟的开源库，没必要造轮子。\n3. JWT存放在localStorage中而不是Cookie中，避免CSRF风险。\n4. 一定不要将隐私信息存放在Payload当中。\n5. 密钥一定保管好，一定不要泄露出去。JWT安全的核心在于签名，签名安全的核心在密钥。\n6. Payload要加入`exp`（JWT的过期时间），永久有效的JWT不合理。并且，JWT的过期时间不易过长。\n7. ......\n\n> [原文链接](https://javaguide.cn/system-design/security/jwt-intro.html)\n\n# JWT身份认证优缺点分析\n\n## JWT的优势\n\n相比于Session认证的方式来说，使用JWT进行身份认证主要有下面4个优势。\n\n### 无状态\n\nJWT自身包含了身份验证所需要的所有信息，因此，我们的服务器不需要存储Session信息。这显然增加了系统的可用性和伸缩性，大大减轻了服务端的压力。不过，也正是由于JWT的无状态，也导致了它最大的缺点：不可控！就比如说，我们想要在JWT有效期内废弃一个JWT或者更改它的权限的话，并不会立即生效，通常需要等到有效期过后才可以。再比如说，当用户Log out的话，JWT也还有效。除非，我们在后端增加额外的处理逻辑比如将失效的JWT存储起来，后端先验证JWT是否有效再进行处理。具体的解决办法，我们会在后面的内容中详细介绍到，这里只是简单提一下。\n\n### 有效避免了CSRF攻击\n\n**CSRF（CrossSiteRequestForgery）一般被翻译为跨站请求伪造**，属于网络攻击领域范围。相比于SQL脚本注入、XSS等安全攻击方式，CSRF的知名度并没有它们高。但是，它的确是我们开发系统时必须要考虑的安全隐患。就连业内技术标杆Google的产品Gmail也曾在2007年的时候爆出过CSRF漏洞，这给Gmail的用户造成了很大的损失。\n\n那么究竟什么是跨站请求伪造呢？简单来说就是用你的身份去做一些不好的事情（发送一些对你不友好的请求比如恶意转账）。举个简单的例子：小壮登录了某网上银行，他来到了网上银行的帖子区，看到一个帖子下面有一个链接写着“科学理财，年盈利率过万”，小壮好奇的点开了这个链接，结果发现自己的账户少了10000元。这是这么回事呢？原来黑客在链接中藏了一个请求，这个请求直接利用小壮的身份给银行发送了一个转账请求，也就是通过你的Cookie向银行发出请求。\n\n```html\n<a src=\"http://www.mybank.com/Transfer?bankId=11&money=10000\">科学理财，年盈利率过万</a>\n```\n\nCSRF攻击需要依赖Cookie，Session认证中Cookie中的SessionID是由浏览器发送到服务端的，只要发出请求，Cookie就会被携带。借助这个特性，即使黑客无法获取你的SessionID，只要让你误点攻击链接，就可以达到攻击效果。另外，并不是必须点击链接才可以达到攻击效果，很多时候，只要你打开了某个页面，CSRF攻击就会发生。\n\n\n```html\n<img src=\"http://www.mybank.com/Transfer?bankId=11&money=10000\"/>\n```\n\n**那为什么JWT不会存在这种问题呢？**\n\n一般情况下我们使用JWT的话，在我们登录成功获得JWT之后，一般会选择存放在localStorage中。前端的每一个请求后续都会附带上这个JWT，整个过程压根不会涉及到Cookie。因此，即使你点击了非法链接发送了请求到服务端，这个非法请求也是不会携带JWT的，所以这个请求将是非法的。总结来说就一句话：**使用JWT进行身份验证不需要依赖Cookie，因此可以避免CSRF攻击**。不过，这样也会存在XSS攻击的风险。为了避免XSS攻击，你可以选择将JWT存储在标记为httpOnly的Cookie中。但是，这样又导致了你必须自己提供CSRF保护，因此，实际项目中我们通常也不会这么做。常见的避免XSS攻击的方式是过滤掉请求中存在XSS攻击风险的可疑字符串。在Spring项目中，我们一般是通过创建XSS过滤器来实现的。\n\n\n```java\n@Component\n@Order(Ordered.HIGHEST_PRECEDENCE)\npublic class XSSFilter implements Filter {\n\n    @Override\n    public void doFilter(ServletRequest request, ServletResponse response,\n      FilterChain chain) throws IOException, ServletException {\n        XSSRequestWrapper wrappedRequest =\n          new XSSRequestWrapper((HttpServletRequest) request);\n        chain.doFilter(wrappedRequest, response);\n    }\n\n    // other methods\n}\n```\n\n### 适合移动端应用\n\n使用Session进行身份认证的话，需要保存一份信息在服务器端，而且这种方式会依赖到Cookie（需要Cookie保存SessionId），所以不适合移动端。但是，使用JWT进行身份认证就不会存在这种问题，因为只要JWT可以被客户端存储就能够使用，而且JWT还可以跨语言使用。\n\n### 单点登录友好\n\n使用Session进行身份认证的话，实现单点登录，需要我们把用户的Session信息保存在一台电脑上，并且还会遇到常见的Cookie跨域的问题。但是，使用JWT进行认证的话，JWT被保存在客户端，不会存在这些问题。\n\n## JWT身份认证常见问题及解决办法\n\n### 注销登录等场景下JWT还有效\n\n与之类似的具体相关场景有：\n\n- 退出登录;\n- 修改密码;\n- 服务端修改了某个用户具有的权限或者角色；\n- 用户的帐户被封禁/删除；\n- 用户被服务端强制注销；\n- 用户被踢下线；\n- ......\n\n这个问题不存在于Session认证方式中，因为在Session认证方式中，遇到这种情况的话服务端删除对应的Session记录即可。但是，使用JWT认证的方式就不好解决了。我们也说过了，JWT一旦派发出去，如果后端不增加其他逻辑的话，它在失效之前都是有效的。那我们如何解决这个问题呢？查阅了很多资料，我简单总结了下面4种方案：\n\n**1、将JWT存入内存数据库**\n\n将JWT存入DB中，Redis内存数据库在这里是不错的选择。如果需要让某个JWT失效就直接从Redis中删除这个JWT即可。但是，这样会导致每次使用JWT发送请求都要先从DB中查询JWT是否存在的步骤，而且违背了JWT的无状态原则。\n\n**2、黑名单机制**\n\n和上面的方式类似，使用内存数据库比如Redis维护一个黑名单，如果想让某个JWT失效的话就直接将这个JWT加入到黑名单即可。然后，每次使用JWT进行请求的话都会先判断这个JWT是否存在于黑名单中。\n\n前两种方案的核心在于将有效的JWT存储起来或者将指定的JWT拉入黑名单。虽然这两种方案都违背了JWT的无状态原则，但是一般实际项目中我们通常还是会使用这两种方案。\n\n**3、修改密钥(Secret)**:\n\n我们为每个用户都创建一个专属密钥，如果我们想让某个JWT失效，我们直接修改对应用户的密钥即可。但是，这样相比于前两种引入内存数据库带来了危害更大：\n\n- 如果服务是分布式的，则每次发出新的JWT时都必须在多台机器同步密钥。为此，你需要将密钥存储在数据库或其他外部服务中，这样和Session认证就没太大区别了。\n- 如果用户同时在两个浏览器打开系统，或者在手机端也打开了系统，如果它从一个地方将账号退出，那么其他地方都要重新进行登录，这是不可取的。\n\n**4、保持令牌的有效期限短并经常轮换**\n\n很简单的一种方式。但是，会导致用户登录状态不会被持久记录，而且需要用户经常登录。另外，对于修改密码后JWT还有效问题的解决还是比较容易的。说一种我觉得比较好的方式：**使用用户的密码的哈希值对JWT进行签名。因此，如果密码更改，则任何先前的令牌将自动无法验证**。\n\n### JWT的续签问题\n\nJWT有效期一般都建议设置的不太长，那么JWT过期后如何认证，如何实现动态刷新JWT，避免用户经常需要重新登录？我们先来看看在Session认证中一般的做法：假如Session的有效期30分钟，如果30分钟内用户有访问，就把Session有效期延长30分钟。JWT认证的话，我们应该如何解决续签问题呢？查阅了很多资料，我简单总结了下面4种方案：\n\n**1、类似于Session认证中的做法**\n\n这种方案满足于大部分场景。假设服务端给的JWT有效期设置为30分钟，服务端每次进行校验时，如果发现JWT的有效期马上快过期了，服务端就重新生成JWT给客户端。客户端每次请求都检查新旧JWT，如果不一致，则更新本地的JWT。这种做法的问题是仅仅在快过期的时候请求才会更新JWT,对客户端不是很友好。\n\n**2、每次请求都返回新JWT**\n\n这种方案的的思路很简单，但是，开销会比较大，尤其是在服务端要存储维护JWT的情况下。\n\n**3、JWT有效期设置到半夜**\n\n这种方案是一种折衷的方案，保证了大部分用户白天可以正常登录，适用于对安全性要求不高的系统。\n\n**4、用户登录返回两个JWT**\n\n第一个是accessJWT，它的过期时间JWT本身的过期时间比如半个小时，另外一个是refreshJWT它的过期时间更长一点比如为1天。客户端登录后，将accessJWT和refreshJWT保存在本地，每次访问将accessJWT传给服务端。服务端校验accessJWT的有效性，如果过期的话，就将refreshJWT传给服务端。如果有效，服务端就生成新的accessJWT给客户端。否则，客户端就重新登录即可。这种方案的不足是：\n\n- 需要客户端来配合；\n- 用户注销的时候需要同时保证两个JWT都无效；\n- 重新请求获取JWT的过程中会有短暂JWT不可用的情况（可以通过在客户端设置定时器，当accessJWT快过期的时候，提前去通过refreshJWT获取新的accessJWT）;\n- 存在安全问题，只要拿到了未过期的refreshJWT就一直可以获取到accessJWT。\n\n## 总结\n\nJWT其中一个很重要的优势是无状态，但实际上，我们想要在实际项目中合理使用JWT的话，也还是需要保存JWT信息。JWT也不是银弹，也有很多缺陷，具体是选择JWT还是Session方案还是要看项目的具体需求。万万不可尬吹JWT，而看不起其他身份认证方案。另外，不用JWT直接使用普通的Token(随机生成，不包含具体的信息)结合Redis来做身份认证也是可以的。\n\n> [原文链接](https://javaguide.cn/system-design/security/advantages&disadvantages-of-jwt.html)\n\n\n# README\n\n> 此段内容原本是在[自己的jwt项目](https://github.com/xmxe/jwt)中README.md中的内容\n\n## JWT简介\n\n### 什么是JWT\n\n在介绍JWT之前，我们先来回顾一下利用token进行用户身份验证的流程：\n1. 客户端使用用户名和密码请求登录\n2. 服务端收到请求，验证用户名和密码\n3. 验证成功后，服务端会签发一个token，再把这个token返回给客户端\n4. 客户端收到token后可以把它存储起来，比如放到cookie中\n5. 客户端每次向服务端请求资源时需要携带服务端签发的token，可以在cookie或者header中携带\n6. 服务端收到请求，然后去验证客户端请求里面带着的token，如果验证成功，就向客户端返回请求数据\n\n这种基于token的认证方式相比传统的session认证方式更节约服务器资源，并且对移动端和分布式更加友好。其优点如下：\n- 支持跨域访问：cookie是无法跨域的，而token由于没有用到cookie(前提是将token放到请求头中)，所以跨域后不会存在信息丢失问题\n- 无状态：token机制在服务端不需要存储session信息，因为token自身包含了所有登录用户的信息，所以可以减轻服务端压力\n- 更适用CDN：可以通过内容分发网络请求服务端的所有资料\n更适用于移动端：当客户端是非浏览器平台时，cookie是不被支持的，此时采用token认证方式会简单很多\n- 无需考虑CSRF：由于不再依赖cookie，所以采用token认证方式不会发生CSRF，所以也就无需考虑CSRF的防御\n\n而JWT就是上述流程当中token的一种具体实现方式，其全称是JSON Web Token，[官网地址](https://jwt.io/)。通俗地说，**JWT的本质就是一个字符串**，它是将用户信息保存到一个Json字符串中，然后进行编码后得到一个JWT token，并且这个JWT token带有签名信息，接收后可以校验是否被篡改，所以可以用于在各方之间安全地将信息作为Json对象传输。JWT的认证流程如下：\n\n1. 首先，前端通过Web表单将自己的用户名和密码发送到后端的接口，这个过程一般是一个POST请求。建议的方式是通过SSL加密的传输(HTTPS)，从而避免敏感信息被嗅探\n2. 后端核对用户名和密码成功后，将包含用户信息的数据作为JWT的Payload，将其与JWT Header分别进行Base64编码拼接后签名，形成一个JWT Token，形成的JWT Token就是一个如同lll.zzz.xxx的字符串\n3. 后端将JWT Token字符串作为登录成功的结果返回给前端。前端可以将返回的结果保存在浏览器中，退出登录时删除保存的JWT Token即可\n4. 前端在每次请求时将JWT Token放入HTTP请求头中的Authorization属性中(解决XSS和XSRF问题)\n5. 后端检查前端传过来的JWT Token，验证其有效性，比如检查签名是否正确、是否过期、token的接收方是否是自己等等\n6. 验证通过后，后端解析出JWT Token中包含的用户信息，进行其他逻辑操作(一般是根据用户信息得到权限等)，返回结果\n![](https://img-blog.csdnimg.cn/img_convert/900b3e81f832b2f08c2e8aabb540536a.png)\n\n### 为什么要用JWT\n\n1. 传统Session认证的弊端\n我们知道HTTP本身是一种无状态的协议，这就意味着如果用户向我们的应用提供了用户名和密码来进行用户认证，认证通过后HTTP协议不会记录下认证后的状态，那么下一次请求时，用户还要再一次进行认证，因为根据HTTP协议，我们并不知道是哪个用户发出的请求，所以为了让我们的应用能识别是哪个用户发出的请求，我们只能在用户首次登录成功后，在服务器存储一份用户登录的信息，这份登录信息会在响应时传递给浏览器，告诉其保存为cookie，以便下次请求时发送给我们的应用，这样我们的应用就能识别请求来自哪个用户了，这是传统的基于session认证的过程\n然而，传统的session认证有如下的问题：\n    - 每个用户的登录信息都会保存到服务器的session中，随着用户的增多，服务器开销会明显增大\n    - 由于session是存在与服务器的物理内存中，所以在分布式系统中，这种方式将会失效。虽然可以将session统一保存到Redis中，但是这样做无疑增加了系统的复杂性，对于不需要redis的应用也会白白多引入一个缓存中间件\n    - 对于非浏览器的客户端、手机移动端等不适用，因为session依赖于cookie，而移动端经常没有cookie\n    - 因为session认证本质基于cookie，所以如果cookie被截获，用户很容易收到跨站请求伪造攻击。并且如果浏览器禁用了cookie，这种方式也会失效\n    - 前后端分离系统中更加不适用，后端部署复杂，前端发送的请求往往经过多个中间件到达后端，cookie中关于session的信息会转发多次\n    - 由于基于Cookie，而cookie无法跨域，所以session的认证也无法跨域，对单点登录不适用\n\n2. JWT认证的优势\n   对比传统的session认证方式，JWT的优势是：\n    - 简洁：JWT Token数据量小，传输速度也很快\n    - 因为JWT Token是以JSON加密形式保存在客户端的，所以JWT是跨语言的，原则上任何web形式都支持\n    - 不需要在服务端保存会话信息，也就是说不依赖于cookie和session，所以没有了传统session认证的弊端，特别适用于分布式微服务\n    - 单点登录友好：使用Session进行身份认证的话，由于cookie无法跨域，难以实现单点登录。但是，使用token进行认证的话，token可以被保存在客户端的任意位置的内存中，不一定是cookie，所以不依赖cookie，不会存在这些问题\n    - 适合移动端应用：使用Session进行身份认证的话，需要保存一份信息在服务器端，而且这种方式会依赖到Cookie（需要Cookie保存SessionId），所以不适合移动端\n\n    因为这些优势，目前无论单体应用还是分布式应用，都更加推荐用JWT token的方式进行用户认证\n\n### JWT结构\n\nJWT由3部分组成：标头(Header)、有效载荷(Payload)和签名(Signature)。在传输的时候，会将JWT的3部分分别进行Base64编码后用.进行连接形成最终传输的字符串\n\n```\nJWT String = Base64(Header).Base64(Payload).HMACSHA256(base64UrlEncode(header)+\".\"+base64UrlEncode(payload),secret)\n```\n\n**Header**\nJWT头是一个描述JWT元数据的JSON对象，alg属性表示签名使用的算法，默认为HMAC SHA256（写为HS256）,typ属性表示令牌的类型，JWT令牌统一写为JWT。最后，使用Base64 URL算法将上述JSON对象转换为字符串保存\n```json\n{\n\"alg\": \"HS256\",\n\"typ\": \"JWT\"\n}\n```\n\n**Payload**\n有效载荷部分，是JWT的主体内容部分，也是一个JSON对象，包含需要传递的数据。JWT指定七个默认字段供选择\n```\niss：发行人\nexp：到期时间\nsub：主题\naud：用户\nnbf：在此之前不可用\niat：发布时间\njti：JWT ID用于标识该JWT\n```\n这些预定义的字段并不要求强制使用。除以上默认字段外，我们还可以自定义私有字段，一般会把包含用户信息的数据放到payload中，如下例：\n```json\n{\n\"sub\": \"1234567890\",\n\"name\": \"Helen\",\n\"admin\": true\n}\n```\n请注意，默认情况下JWT是未加密的，因为只是采用base64算法，拿到JWT字符串后可以转换回原本的JSON数据，任何人都可以解读其内容，因此不要构建隐私信息字段，比如用户的密码一定不能保存到JWT中，以防止信息泄露。JWT只是适合在网络中传输一些非敏感的信息\n\n**Signature**\n签名哈希部分是对上面两部分数据签名，需要使用base64编码后的header和payload数据，通过指定的算法生成哈希，以确保数据不会被篡改。首先，需要指定一个密钥（secret）。该密码仅仅为保存在服务器中，并且不能向用户公开。然后，使用header中指定的签名算法（默认情况下为HMAC SHA256）根据以下公式生成签名\n```\nHMACSHA256(base64UrlEncode(header)+\".\"+base64UrlEncode(payload),secret)\n```\n在计算出签名哈希后，JWT头，有效载荷和签名哈希的三个部分组合成一个字符串，每个部分用.分隔，就构成整个JWT对象\n\n> 注意JWT每部分的作用，在服务端接收到客户端发送过来的JWT token之后：\n > - header和payload可以直接利用base64解码出原文，从header中获取哈希签名的算法，从payload中获取有效数据\n > - signature由于使用了不可逆的加密算法，无法解码出原文，它的作用是校验token有没有被篡改。服务端获取header中的加密算法之后，利用该算法加上secretKey对header、payload进行加密，比对加密后的数据和客户端发送过来的是否一致。注意secretKey只能保存在服务端，而且对于不同的加密算法其含义有所不同，一般对于MD5类型的摘要加密算法，secretKey实际上代表的是盐值\n\n## 相关文章\n\n- [还分不清Cookie、Session、Token、JWT？](https://mp.weixin.qq.com/s/skZL7RR3SftrB4SNZx59ZA)\n- [JWT实现登录认证+Token自动续期方案](https://mp.weixin.qq.com/s/i73E4zbTh_JCuRCqH_NoVQ)","categories":["技术栈"]},{"title":"常见的限流算法","slug":"常见的限流算法","url":"/blog/posts/f55fdb8ec84c/","content":"\n## 常见限流算法有哪些？\n\n简单介绍4种非常好理解并且容易实现的限流算法！\n\n> 图片来源于InfoQ的一篇文章[《分布式服务限流实战，已经为你排好坑了》](https://www.infoq.cn/article/Qg2tX8fyw5Vt-f3HH673)。\n> - [5种限流算法，7种限流方式，挡住突发流量？](https://mp.weixin.qq.com/s/xNvBdI99fKOsMFdoNC4K3w)\n> - [常用的限流算法有哪些？](https://mp.weixin.qq.com/s/gsBl3J6iUEChODowLU9vjw)\n> - [新来个技术总监，把限流实现的那叫一个优雅，佩服](https://mp.weixin.qq.com/s/lSrFOBZHSlneNUh_tnfxjg)\n> - [十分钟搞懂Java限流及常见方案](https://mp.weixin.qq.com/s/i93_jSf43FcN0aQ9hY8WnQ)\n> - [四种分布式限流算法实现！](https://mp.weixin.qq.com/s/BRQfsbhFjSpl_MH7E-eZYQ)\n\n### 固定窗口计数器算法\n\n固定窗口其实就是时间窗口。固定窗口计数器算法规定了我们单位时间处理的请求数量。假如我们规定系统中某个接口1分钟只能访问33次的话，使用固定窗口计数器算法的实现思路如下：\n\n- 给定一个变量counter来记录当前接口处理的请求数量，初始值为0（代表接口当前1分钟内还未处理请求）。\n- 1分钟之内每处理一个请求之后就将counter+1，当counter=33之后（也就是说在这1分钟内接口已经被访问33次的话），后续的请求就会被全部拒绝。\n- 等到1分钟结束后，将counter重置0，重新开始计数。\n\n**这种限流算法无法保证限流速率，因而无法保证突然激增的流量**。\n\n就比如说我们限制某个接口1分钟只能访问1000次，该接口的QPS为500，前55s这个接口1个请求没有接收，后1s突然接收了1000个请求。然后，在当前场景下，这1000个请求在1s内是没办法被处理的，系统直接就被瞬时的大量请求给击垮了。\n\n![固定窗口计数器算法](https://static001.infoq.cn/resource/image/8d/15/8ded7a2b90e1482093f92fff555b3615.png)\n\n### 滑动窗口计数器算法\n\n滑动窗口计数器算法算的上是固定窗口计数器算法的升级版。滑动窗口计数器算法相比于固定窗口计数器算法的优化在于：它把时间以一定比例分片。例如我们的接口限流每分钟处理60个请求，我们可以把1分钟分为60个窗口。每隔1秒移动一次，每个窗口一秒只能处理不大于60(请求数)/60（窗口数）的请求，如果当前窗口的请求计数总和超过了限制的数量的话就不再处理其他请求。很显然，**当滑动窗口的格子划分的越多，滑动窗口的滚动就越平滑，限流的统计就会越精确**。\n\n![滑动窗口计数器算法](https://static001.infoq.cn/resource/image/ae/15/ae4d3cd14efb8dc7046d691c90264715.png)\n\n### 漏桶算法\n\n我们可以把发请求的动作比作成注水到桶中，我们处理请求的过程可以比喻为漏桶漏水。我们往桶中以任意速率流入水，以一定速率流出水。当水超过桶流量则丢弃，因为桶容量是不变的，保证了整体的速率。如果想要实现这个算法的话也很简单，准备一个队列用来保存请求，然后我们定期从队列中拿请求来执行就好了（和消息队列削峰/限流的思想是一样的）。\n\n![漏桶算法](https://static001.infoq.cn/resource/image/75/03/75938d1010138ce66e38c6ed0392f103.png)\n\n### 令牌桶算法\n\n令牌桶算法也比较简单。和漏桶算法算法一样，我们的主角还是桶（这限流算法和桶过不去啊）。不过现在桶里装的是令牌了，请求在被处理之前需要拿到一个令牌，请求处理完毕之后将这个令牌丢弃（删除）。我们根据限流大小，按照一定的速率往桶里添加令牌。如果桶装满了，就不能继续往里面继续添加令牌了。\n\n![令牌桶算法](https://static001.infoq.cn/resource/image/ec/93/eca0e5eaa35dac938c673fecf2ec9a93.png)\n\n## 单机限流怎么做？\n\n单机限流针对的是单体架构应用。单机限流可以直接使用Google Guava自带的限流工具类RateLimiter。RateLimiter基于令牌桶算法，可以应对突发流量。\n\n> [Guava地址](https://github.com/google/guava)\n\n除了最基本的令牌桶算法(平滑突发限流)实现之外，Guava的RateLimiter还提供了**平滑预热限流**的算法实现。平滑突发限流就是按照指定的速率放令牌到桶里，而平滑预热限流会有一段预热时间，预热时间之内，速率会逐渐提升到配置的速率。\n\n### 实战\n\n\n```xml\n<dependency>\n    <groupId>com.google.guava</groupId>\n    <artifactId>guava</artifactId>\n    <version>31.0.1-jre</version>\n</dependency>\n```\n\n下面是一个简单的Guava平滑突发限流的Demo。\n\n```java\nimport com.google.common.util.concurrent.RateLimiter;\npublic class RateLimiterDemo {\n    public static void main(String[] args) {\n        // 1s放5个令牌到桶里也就是0.2s放1个令牌到桶里\n        RateLimiter rateLimiter = RateLimiter.create(5);\n        for (int i = 0; i < 10; i++) {\n            double sleepingTime = rateLimiter.acquire(1);\n            System.out.printf(\"get 1 tokens: %ss%n\", sleepingTime);\n        }\n    }\n}\n```\n\n输出：\n\n\n```bash\nget 1 tokens: 0.0s\nget 1 tokens: 0.188413s\nget 1 tokens: 0.197811s\nget 1 tokens: 0.198316s\nget 1 tokens: 0.19864s\nget 1 tokens: 0.199363s\nget 1 tokens: 0.193997s\nget 1 tokens: 0.199623s\nget 1 tokens: 0.199357s\nget 1 tokens: 0.195676s\n```\n\n下面是一个简单的Guava平滑预热限流的Demo。\n\n\n```java\nimport com.google.common.util.concurrent.RateLimiter;\nimport java.util.concurrent.TimeUnit;\npublic class RateLimiterDemo {\n    public static void main(String[] args) {\n        // 1s放5个令牌到桶里也就是0.2s放1个令牌到桶里\n        // 预热时间为3s,也就说刚开始的3s内发牌速率会逐渐提升到0.2s放1个令牌到桶里\n        RateLimiter rateLimiter = RateLimiter.create(5, 3, TimeUnit.SECONDS);\n        for (int i = 0; i < 20; i++) {\n            double sleepingTime = rateLimiter.acquire(1);\n            System.out.printf(\"get 1 tokens: %sds%n\", sleepingTime);\n        }\n    }\n}\n```\n\n输出：\n\n```bash\nget 1 tokens: 0.0s\nget 1 tokens: 0.561919s\nget 1 tokens: 0.516931s\nget 1 tokens: 0.463798s\nget 1 tokens: 0.41286s\nget 1 tokens: 0.356172s\nget 1 tokens: 0.300489s\nget 1 tokens: 0.252545s\nget 1 tokens: 0.203996s\nget 1 tokens: 0.198359s\n```\n\n另外，**Bucket4j**是一个非常不错的基于令牌/漏桶算法的限流库。\n\n> [Bucket4j地址](https://github.com/vladimir-bukhtoyarov/bucket4j)\n\n相对于，Guava的限流工具类来说，Bucket4j提供的限流功能更加全面。不仅支持单机限流和分布式限流，还可以集成监控，搭配Prometheus和Grafana使用。不过，毕竟Guava也只是一个功能全面的工具类库，其提供的开箱即用的限流功能在很多单机场景下还是比较实用的。Spring Cloud Gateway中自带的单机限流的早期版本就是基于Bucket4j实现的。后来，替换成了**Resilience4j**。Resilience4j是一个轻量级的容错组件，其灵感来自于Hystrix。自[Netflix宣布不再积极开发Hystrix](https://github.com/Netflix/Hystrix/commit/a7df971cbaddd8c5e976b3cc5f14013fe6ad00e6)之后，Spring官方和Netflix都更推荐使用Resilience4j来做限流熔断。\n\n> [Resilience4j地址](https://github.com/resilience4j/resilience4j)\n\n一般情况下，为了保证系统的高可用，项目的限流和熔断都是要一起做的。Resilience4j不仅提供限流，还提供了熔断、负载保护、自动重试等保障系统高可用开箱即用的功能。并且，Resilience4j的生态也更好，很多网关都使用Resilience4j来做限流熔断的。因此，在绝大部分场景下Resilience4j或许会是更好的选择。如果是一些比较简单的限流场景的话，Guava或者Bucket4j也是不错的选择。\n\n## 分布式限流怎么做？\n\n分布式限流针对的分布式/微服务应用架构应用，在这种架构下，单机限流就不适用了，因为会存在多种服务，并且一种服务也可能会被部署多份。\n\n分布式限流常见的方案：\n\n- **借助中间件架限流**：可以借助Sentinel或者使用Redis来自己实现对应的限流逻辑。\n- **网关层限流**：比较常用的一种方案，直接在网关层把限流给安排上了。不过，通常网关层限流通常也需要借助到中间件/框架。就比如Spring Cloud Gateway的分布式限流实现RedisRateLimiter就是基于Redis+Lua来实现的，再比如SpringCloudGateway还可以整合Sentinel来做限流。\n\n如果你要基于Redis来手动实现限流逻辑的话，建议配合Lua脚本来做。**为什么建议Redis+Lua的方式**？主要有两点原因：\n\n- **减少了网络开销**：我们可以利用Lua脚本来批量执行多条Redis命令，这些Redis命令会被提交到Redis服务器一次性执行完成，大幅减小了网络开销。\n- **原子性**：一段Lua脚本可以视作一条命令执行，一段Lua脚本执行过程中不会有其他脚本或Redis命令同时执行，保证了操作不会被其他指令插入或打扰。\n\n我这里就不放具体的限流脚本代码了，网上也有很多现成的优秀的限流脚本供你参考，就比如Apache网关项目ShenYu的RateLimiter限流插件就基于Redis+Lua实现了令牌桶算法/并发令牌桶算法、漏桶算法、滑动窗口算法。\n\n> [ShenYu地址](https://github.com/apache/incubator-shenyu)\n\n![ShenYu限流脚本](https://oss.javaguide.cn/github/javaguide/csdn/e1e2a75f489e4854990dabe3b6cec522.jpg)\n\n## 实现代码\n\n> 相关代码已上传到[GitHub](https://github.com/xmxe/demo/tree/master/study-demo/src/main/java/com/xmxe/algorithm/limit)\n\n","tags":["代码实战"],"categories":["Java"]},{"title":"IO&NIO","slug":"IO&NIO","url":"/blog/posts/c98fd7c2aa89/","content":"\n## IO\n\n### IO基础知识总结\n\n#### IO流简介\n\nIO即**Input/Output**，输入和输出。数据输入到计算机内存的过程即输入，反之输出到外部存储（比如数据库，文件，远程主机）的过程即输出。数据传输过程类似于水流，因此称为IO流。IO流在Java中分为输入流和输出流，而根据数据的处理方式又分为字节流和字符流。\n\nJava IO流的40多个类都是从如下4个抽象类基类中派生出来的。\n\n- **InputStream/Reader**：所有的输入流的基类，前者是字节输入流，后者是字符输入流。\n- **OutputStream/Writer**：所有输出流的基类，前者是字节输出流，后者是字符输出流。\n\n#### 字节流\n\n##### InputStream（字节输入流）\n\nInputStream用于从源头（通常是文件）读取数据（字节信息）到内存中，java.io.InputStream抽象类是所有字节输入流的父类。InputStream常用方法：\n\n```java\n// 返回输入流中下一个字节的数据。返回的值介于0到255之间。如果未读取任何字节，则代码返回-1，表示文件结束。\nread()\n// 从输入流中读取一些字节存储到数组b中。如果数组b的长度为零，则不读取。如果没有可用字节读取，返回-1。如果有可用字节读取，则最多读取的字节数最多等于b.length，返回读取的字节数。这个方法等价于read(b,0,b.length)。\nread(byte b[])\n// 在read(byte b[])方法的基础上增加了off参数（偏移量）和len参数（要读取的最大字节数）\nread(byte b[], int off,int len)\n// 忽略输入流中的n个字节,返回实际忽略的字节数。\nskip(long n)\n// 返回输入流中可以读取的字节数。\navailable()\n// 关闭输入流释放相关的系统资源。\nclose()\n\n// 从Java9开始，InputStream新增加了多个实用的方法：\n// 读取输入流中的所有字节，返回字节数组。\nreadAllBytes()\n// 阻塞直到读取len个字节。\nreadNBytes(byte[]b,intoff,intlen)\n// 将所有字节从一个输入流传递到一个输出流。\ntransferTo(OutputStreamout)\n```\n\n**FileInputStream**是一个比较常用的字节输入流对象，可直接指定文件路径，可以直接读取单字节数据，也可以读取至字节数组中。\n\nFileInputStream代码示例：\n\n```java\ntry (InputStream fis = new FileInputStream(\"input.txt\")) {\n    System.out.println(\"Number of remaining bytes:\"\n            + fis.available());\n    int content;\n    long skip = fis.skip(2);\n    System.out.println(\"The actual number of bytes skipped:\" + skip);\n    System.out.print(\"The content read from file:\");\n    while ((content = fis.read()) != -1) {\n        System.out.print((char) content);\n    }\n} catch (IOException e) {\n    e.printStackTrace();\n}\n```\n\ninput.txt文件内容：\n\n![img](https://guide-blog-images.oss-cn-shenzhen.aliyuncs.com/github/javaguide/java/image-20220419155214614.png)\n\n输出：\n\n```text\nNumber of remaining bytes:11\nThe actual number of bytes skipped:2\nThe content read from file:JavaGuide\n```\n\n不过，一般我们是不会直接单独使用FileInputStream，通常会配合BufferedInputStream（字节缓冲输入流，后文会讲到）来使用。像下面这段代码在我们的项目中就比较常见，我们通过readAllBytes()读取输入流所有字节并将其直接赋值给一个String对象。\n\n\n```java\n// 新建一个BufferedInputStream对象\nBufferedInputStream bufferedInputStream = new BufferedInputStream(new FileInputStream(\"input.txt\"));\n// 读取文件的内容并复制到String对象中\nString result = new String(bufferedInputStream.readAllBytes());\nSystem.out.println(result);\n```\n**DataInputStream**用于读取指定类型数据，不能单独使用，必须结合FileInputStream。\n\n```java\nFileInputStream fileInputStream = new FileInputStream(\"input.txt\");\n//必须将fileInputStream作为构造参数才能使用\nDataInputStream dataInputStream = new DataInputStream(fileInputStream);\n//可以读取任意具体的类型数据\ndataInputStream.readBoolean();\ndataInputStream.readInt();\ndataInputStream.readUTF();\n```\n\n**ObjectInputStream**用于从输入流中读取Java对象（反序列化），ObjectOutputStream用于将对象写入到输出流(序列化)。\n\n\n```java\nObjectInputStream input = new ObjectInputStream(new FileInputStream(\"object.data\"));\nMyClass object = (MyClass) input.readObject();\ninput.close();\n```\n\n另外，用于序列化和反序列化的类必须实现Serializable接口，对象中如果有属性不想被序列化，使用transient修饰。\n\n##### OutputStream（字节输出流）\n\nOutputStream用于将数据（字节信息）写入到目的地（通常是文件），java.io.OutputStream抽象类是所有字节输出流的父类。OutputStream常用方法：\n\n```java\n// 将特定字节写入输出流。\nwrite(int b)\n// 将数组b写入到输出流，等价于write(b, 0, b.length)。\nwrite(byte b[ ])\n// 在write(byte b[ ])方法的基础上增加了off参数（偏移量）和len参数（要读取的最大字节数）。\nwrite(byte[] b, int off, int len)\n// 刷新此输出流并强制写出所有缓冲的输出字节。\nflush()\n// 关闭输出流释放相关的系统资源。\nclose()\n```\n\n**FileOutputStream**是最常用的字节输出流对象，可直接指定文件路径，可以直接输出单字节数据，也可以输出指定的字节数组。FileOutputStream代码示例：\n\n```java\ntry (FileOutputStream output = new FileOutputStream(\"output.txt\")) {\n    byte[] array = \"JavaGuide\".getBytes();\n    output.write(array);\n} catch (IOException e) {\n    e.printStackTrace();\n}\n```\n\n运行结果：\n\n![img](https://guide-blog-images.oss-cn-shenzhen.aliyuncs.com/github/javaguide/java/image-20220419155514392.png)\n\n类似于FileInputStream,FileOutputStream通常也会配合BufferedOutputStream（字节缓冲输出流，后文会讲到）来使用。\n\n\n```java\nFileOutputStream fileOutputStream = new FileOutputStream(\"output.txt\");\nBufferedOutputStream bos = new BufferedOutputStream(fileOutputStream)\n```\n\n**DataOutputStream**用于写入指定类型数据，不能单独使用，必须结合FileOutputStream\n\n\n```java\n// 输出流\nFileOutputStream fileOutputStream = new FileOutputStream(\"out.txt\");\nDataOutputStream dataOutputStream = new DataOutputStream(fileOutputStream);\n// 输出任意数据类型\ndataOutputStream.writeBoolean(true);\ndataOutputStream.writeByte(1);\n```\n\n**ObjectInputStream**用于从输入流中读取Java对象（ObjectInputStream,反序列化），ObjectOutputStream将对象写入到输出流(ObjectOutputStream，序列化)。\n\n```java\nObjectOutputStream output = new ObjectOutputStream(new FileOutputStream(\"file.txt\")\nPerson person = new Person(\"Guide哥\",\"JavaGuide作者\");\noutput.writeObject(person);\n```\n\n#### 字符流\n\n不管是文件读写还是网络发送接收，信息的最小存储单元都是字节。**那为什么I/O流操作要分为字节流操作和字符流操作呢**？个人认为主要有两点原因：\n\n- 字符流是由Java虚拟机将字节转换得到的，这个过程还算是比较耗时。\n- 如果我们不知道编码类型就很容易出现乱码问题。\n\n乱码问题这个很容易就可以复现，我们只需要将上面提到的FileInputStream代码示例中的input.txt文件内容改为中文即可，原代码不需要改动。\n\n![img](https://guide-blog-images.oss-cn-shenzhen.aliyuncs.com/github/javaguide/java/image-20220419154632551.png)\n\n输出：\n\n\n```java\nNumber of remaining bytes:9\nThe actual number of bytes skipped:2\nThe content read from file:§å®¶å¥½\n```\n\n可以很明显地看到读取出来的内容已经变成了乱码。因此，I/O流就干脆提供了一个直接操作字符的接口，方便我们平时对字符进行流操作。如果音频文件、图片等媒体文件用字节流比较好，如果涉及到字符的话使用字符流比较好。字符流默认采用的是Unicode编码，我们可以通过构造方法自定义编码。顺便分享一下之前遇到的笔试题：常用字符编码所占字节数？utf8:英文占1字节，中文占3字节，unicode：任何字符都占2个字节，gbk：英文占1字节，中文占2字节。\n\n##### Reader（字符输入流）\n\nReader用于从源头（通常是文件）读取数据（字符信息）到内存中，java.io.Reader抽象类是所有字符输入流的父类。\n\nReader用于读取文本，InputStream用于读取原始字节。Reader常用方法：\n\n```java\n// 从输入流读取一个字符。\nread()\n// 从输入流中读取一些字符，并将它们存储到字符数组cbuf中，等价于read(cbuf, 0, cbuf.length)。\nread(char[] cbuf)\n// 在read(char[] cbuf)方法的基础上增加了off参数（偏移量）和len参数（要读取的最大字符数）\nread(char[] cbuf, int off, int len)\n// 忽略输入流中的n个字符,返回实际忽略的字符数。\nskip(long n)\n// 关闭输入流并释放相关的系统资源。\nclose()\n```\n\n**InputStreamReader**是字节流转换为字符流的桥梁，其子类FileReader是基于该基础上的封装，可以直接操作字符文件。\n\n```java\n// 字节流转换为字符流的桥梁\npublic class InputStreamReader extends Reader {\n}\n// 用于读取字符文件\npublic class FileReader extends InputStreamReader {\n}\n```\n\n**FileReader**代码示例：\n\n\n```java\ntry (FileReader fileReader = new FileReader(\"input.txt\");) {\n    int content;\n    long skip = fileReader.skip(3);\n    System.out.println(\"The actual number of bytes skipped:\" + skip);\n    System.out.print(\"The content read from file:\");\n    while ((content = fileReader.read()) != -1) {\n        System.out.print((char) content);\n    }\n} catch (IOException e) {\n    e.printStackTrace();\n}\n```\n\ninput.txt文件内容：\n\n![img](https://guide-blog-images.oss-cn-shenzhen.aliyuncs.com/github/javaguide/java/image-20220419154632551.png)\n\n输出：\n\n```text\nThe actual number of bytes skipped:3\nThe content read from file:我是Guide。\n```\n\n##### Writer（字符输出流）\n\nWriter用于将数据（字符信息）写入到目的地（通常是文件），java.io.Writer抽象类是所有字符输出流的父类。Writer常用方法：\n\n```java\n// 写入单个字符。\nwrite(int c)\n// 写入字符数组cbuf，等价于write(cbuf, 0, cbuf.length)。\nwrite(char[] cbuf)\n// 在write(char[] cbuf)方法的基础上增加了off参数（偏移量）和len参数（要读取的最大字符数）。\nwrite(char[] cbuf, int off, int len)\n// 写入字符串，等价于write(str, 0, str.length())。\nwrite(String str)\n// 在write(String str)方法的基础上增加了off参数（偏移量）和len参数（要读取的最大字符数）。\nwrite(String str, int off, int len)\n// 将指定的字符序列附加到指定的Writer对象并返回该Writer对象。\nappend(CharSequence csq)\n// 将指定的字符附加到指定的Writer对象并返回该Writer对象。\nappend(char c)\n// 刷新此输出流并强制写出所有缓冲的输出字符。\nflush()\n// 关闭输出流释放相关的系统资源。\nclose()\n```\n**OutputStreamWriter**是字符流转换为字节流的桥梁，其子类FileWriter是基于该基础上的封装，可以直接将字符写入到文件。\n\n\n```java\n// 字符流转换为字节流的桥梁\npublic class OutputStreamWriter extends Writer {\n}\n// 用于写入字符到文件\npublic class FileWriter extends OutputStreamWriter {\n}\n```\n\n**FileWriter**代码示例：\n\n\n```java\ntry (Writer output = new FileWriter(\"output.txt\")) {\n    output.write(\"你好，我是Guide。\");\n} catch (IOException e) {\n    e.printStackTrace();\n}\n```\n\n输出结果：\n\n![img](https://guide-blog-images.oss-cn-shenzhen.aliyuncs.com/github/javaguide/java/image-20220419155802288.png)\n\n#### 字节缓冲流\n\nIO操作是很消耗性能的，缓冲流将数据加载至缓冲区，一次性读取/写入多个字节，从而避免频繁的IO操作，提高流的传输效率。字节缓冲流这里采用了装饰器模式来增强InputStream和OutputStream子类对象的功能。举个例子，我们可以通过BufferedInputStream（字节缓冲输入流）来增强FileInputStream的功能。\n\n```java\n// 新建一个BufferedInputStream对象\nBufferedInputStream bufferedInputStream = new BufferedInputStream(new FileInputStream(\"input.txt\"));\n```\n\n字节流和字节缓冲流的性能差别主要体现在我们使用两者的时候都是调用write(int b)和read()这两个一次只读取一个字节的方法的时候。由于字节缓冲流内部有缓冲区（字节数组），因此，字节缓冲流会先将读取到的字节存放在缓存区，大幅减少IO次数，提高读取效率。\n\n我使用write(int b)和read()方法，分别通过字节流和字节缓冲流复制一个524.9 mb的PDF文件耗时对比如下：\n\n```text\n使用缓冲流复制PDF文件总耗时:15428毫秒\n使用普通字节流复制PDF文件总耗时:2555062毫秒\n```\n\n两者耗时差别非常大，缓冲流耗费的时间是字节流的1/165。测试代码如下:\n\n\n```java\n@Test\nvoid copy_pdf_to_another_pdf_buffer_stream() {\n    // 记录开始时间\n    long start = System.currentTimeMillis();\n    try (BufferedInputStream bis = new BufferedInputStream(new FileInputStream(\"深入理解计算机操作系统.pdf\"));\n         BufferedOutputStream bos = new BufferedOutputStream(new FileOutputStream(\"深入理解计算机操作系统-副本.pdf\"))) {\n        int content;\n        while ((content = bis.read()) != -1) {\n            bos.write(content);\n        }\n    } catch (IOException e) {\n        e.printStackTrace();\n    }\n    // 记录结束时间\n    long end = System.currentTimeMillis();\n    System.out.println(\"使用缓冲流复制PDF文件总耗时:\" + (end - start) + \"毫秒\");\n}\n\n@Test\nvoid copy_pdf_to_another_pdf_stream() {\n    // 记录开始时间\n    long start = System.currentTimeMillis();\n    try (FileInputStream fis = new FileInputStream(\"深入理解计算机操作系统.pdf\");\n         FileOutputStream fos = new FileOutputStream(\"深入理解计算机操作系统-副本.pdf\")) {\n        int content;\n        while ((content = fis.read()) != -1) {\n            fos.write(content);\n        }\n    } catch (IOException e) {\n        e.printStackTrace();\n    }\n    // 记录结束时间\n    long end = System.currentTimeMillis();\n    System.out.println(\"使用普通流复制PDF文件总耗时:\" + (end - start) + \"毫秒\");\n}\n```\n\n如果是调用read(byte b[])和write(byte b[], int off, int len)这两个写入一个字节数组的方法的话，只要字节数组的大小合适，两者的性能差距其实不大，基本可以忽略。这次我们使用read(byte b[])和write(byte b[], int off, int len)方法，分别通过字节流和字节缓冲流复制一个524.9mb的PDF文件耗时对比如下：\n\n```text\n使用缓冲流复制PDF文件总耗时:695毫秒\n使用普通字节流复制PDF文件总耗时:989毫秒\n```\n\n两者耗时差别不是很大，缓冲流的性能要略微好一点点。测试代码如下：\n\n```java\n@Test\nvoid copy_pdf_to_another_pdf_with_byte_array_buffer_stream() {\n    // 记录开始时间\n    long start = System.currentTimeMillis();\n    try (BufferedInputStream bis = new BufferedInputStream(new FileInputStream(\"深入理解计算机操作系统.pdf\"));\n         BufferedOutputStream bos = new BufferedOutputStream(new FileOutputStream(\"深入理解计算机操作系统-副本.pdf\"))) {\n        int len;\n        byte[] bytes = new byte[4 * 1024];\n        while ((len = bis.read(bytes)) != -1) {\n            bos.write(bytes, 0, len);\n        }\n    } catch (IOException e) {\n        e.printStackTrace();\n    }\n    // 记录结束时间\n    long end = System.currentTimeMillis();\n    System.out.println(\"使用缓冲流复制PDF文件总耗时:\" + (end - start) + \"毫秒\");\n}\n\n@Test\nvoid copy_pdf_to_another_pdf_with_byte_array_stream() {\n    // 记录开始时间\n    long start = System.currentTimeMillis();\n    try (FileInputStream fis = new FileInputStream(\"深入理解计算机操作系统.pdf\");\n         FileOutputStream fos = new FileOutputStream(\"深入理解计算机操作系统-副本.pdf\")) {\n        int len;\n        byte[] bytes = new byte[4 * 1024];\n        while ((len = fis.read(bytes)) != -1) {\n            fos.write(bytes, 0, len);\n        }\n    } catch (IOException e) {\n        e.printStackTrace();\n    }\n    // 记录结束时间\n    long end = System.currentTimeMillis();\n    System.out.println(\"使用普通流复制PDF文件总耗时:\" + (end - start) + \"毫秒\");\n}\n```\n\n##### BufferedInputStream（字节缓冲输入流）\n\nBufferedInputStream从源头（通常是文件）读取数据（字节信息）到内存的过程中不会一个字节一个字节的读取，而是会先将读取到的字节存放在缓存区，并从内部缓冲区中单独读取字节。这样大幅减少了IO次数，提高了读取效率。BufferedInputStream内部维护了一个缓冲区，这个缓冲区实际就是一个字节数组，通过阅读BufferedInputStream源码即可得到这个结论。\n\n```java\npublic class BufferedInputStream extends FilterInputStream {\n    // 内部缓冲区数组\n    protected volatile byte buf[];\n    // 缓冲区的默认大小\n    private static int DEFAULT_BUFFER_SIZE = 8192;\n    // 使用默认的缓冲区大小\n    public BufferedInputStream(InputStream in) {\n        this(in, DEFAULT_BUFFER_SIZE);\n    }\n    // 自定义缓冲区大小\n    public BufferedInputStream(InputStream in, int size) {\n        super(in);\n        if (size <= 0) {\n            throw new IllegalArgumentException(\"Buffer size <= 0\");\n        }\n        buf = new byte[size];\n    }\n}\n```\n\n缓冲区的大小默认为**8192**字节，当然了，你也可以通过BufferedInputStream(InputStream in, int size)这个构造方法来指定缓冲区的大小。\n\n##### BufferedOutputStream（字节缓冲输出流）\n\nBufferedOutputStream将数据（字节信息）写入到目的地（通常是文件）的过程中不会一个字节一个字节的写入，而是会先将要写入的字节存放在缓存区，并从内部缓冲区中单独写入字节。这样大幅减少了IO次数，提高了读取效率\n\n```java\ntry (BufferedOutputStream bos = new BufferedOutputStream(new FileOutputStream(\"output.txt\"))) {\n    byte[] array = \"JavaGuide\".getBytes();\n    bos.write(array);\n} catch (IOException e) {\n    e.printStackTrace();\n}\n```\n\n类似于BufferedInputStream，BufferedOutputStream内部也维护了一个缓冲区，并且，这个缓存区的大小也是**8192**字节。\n\n#### 字符缓冲流\n\nBufferedReader（字符缓冲输入流）和BufferedWriter（字符缓冲输出流）类似于BufferedInputStream（字节缓冲输入流）和BufferedOutputStream（字节缓冲输入流），内部都维护了一个字节数组作为缓冲区。不过，前者主要是用来操作字符信息。\n\n#### 打印流\n\n下面这段代码大家经常使用吧？\n\n```java\nSystem.out.print(\"Hello！\");\nSystem.out.println(\"Hello！\");\n```\n\nSystem.out实际是用于获取一个PrintStream对象，print方法实际调用的是PrintStream对象的write方法。PrintStream属于字节打印流，与之对应的是PrintWriter（字符打印流）。PrintStream是OutputStream的子类，PrintWriter是Writer的子类。\n\n```java\npublic class PrintStream extends FilterOutputStream\n    implements Appendable, Closeable {\n}\npublic class PrintWriter extends Writer {\n}\n```\n\n#### 随机访问流\n\n这里要介绍的随机访问流指的是支持随意跳转到文件的任意位置进行读写的RandomAccessFile。\n\nRandomAccessFile的构造方法如下，我们可以指定mode（读写模式）。\n\n```java\n// openAndDelete参数默认为false表示打开文件并且这个文件不会被删除\npublic RandomAccessFile(File file, String mode)\n    throws FileNotFoundException {\n    this(file, mode, false);\n}\n// 私有方法\nprivate RandomAccessFile(File file, String mode, boolean openAndDelete)  throws FileNotFoundException{\n  // 省略大部分代码\n}\n```\n\n读写模式主要有下面四种：\n\n- r:只读模式。\n- rw:读写模式\n- rws:相对于rw，rws同步更新对“文件的内容”或“元数据”的修改到外部存储设备。\n- rwd:相对于rw，rwd同步更新对“文件的内容”的修改到外部存储设备。\n\n文件内容指的是文件中实际保存的数据，元数据则是用来描述文件属性比如文件的大小信息、创建和修改时间。RandomAccessFile中有一个文件指针用来表示下一个将要被写入或者读取的字节所处的位置。我们可以通过RandomAccessFile的seek(long pos)方法来设置文件指针的偏移量（距文件开头pos个字节处）。如果想要获取文件指针当前的位置的话，可以使用getFilePointer()方法。\n\nRandomAccessFile代码示例：\n\n```java\nRandomAccessFile randomAccessFile = new RandomAccessFile(new File(\"input.txt\"), \"rw\");\nSystem.out.println(\"读取之前的偏移量：\" + randomAccessFile.getFilePointer() + \",当前读取到的字符\" + (char) randomAccessFile.read() + \"，读取之后的偏移量：\" + randomAccessFile.getFilePointer());\n// 指针当前偏移量为6\nrandomAccessFile.seek(6);\nSystem.out.println(\"读取之前的偏移量：\" + randomAccessFile.getFilePointer() + \",当前读取到的字符\" + (char) randomAccessFile.read() + \"，读取之后的偏移量：\" + randomAccessFile.getFilePointer());\n// 从偏移量7的位置开始往后写入字节数据\nrandomAccessFile.write(new byte[]{'H', 'I', 'J', 'K'});\n// 指针当前偏移量为0，回到起始位置\nrandomAccessFile.seek(0);\nSystem.out.println(\"读取之前的偏移量：\" + randomAccessFile.getFilePointer() + \",当前读取到的字符\" + (char) randomAccessFile.read() + \"，读取之后的偏移量：\" + randomAccessFile.getFilePointer());\n```\n\ninput.txt文件内容：\n\n![img](https://guide-blog-images.oss-cn-shenzhen.aliyuncs.com/github/javaguide/java/image-20220421162050158.png)\n\n输出：\n\n```text\n读取之前的偏移量：0,当前读取到的字符A，读取之后的偏移量：1\n读取之前的偏移量：6,当前读取到的字符G，读取之后的偏移量：7\n读取之前的偏移量：0,当前读取到的字符A，读取之后的偏移量：1\n```\n\ninput.txt文件内容变为ABCDEFGHIJK。\n\nRandomAccessFile的write方法在写入对象的时候如果对应的位置已经有数据的话，会将其覆盖掉。\n\n\n```java\nRandomAccessFile randomAccessFile = new RandomAccessFile(new File(\"input.txt\"), \"rw\");\nrandomAccessFile.write(new byte[]{'H', 'I', 'J', 'K'});\n```\n\n假设运行上面这段程序之前input.txt文件内容变为ABCD，运行之后则变为HIJK。\n\nRandomAccessFile比较常见的一个应用就是实现大文件的**断点续传**。何谓断点续传？简单来说就是上传文件中途暂停或失败（比如遇到网络问题）之后，不需要重新上传，只需要上传那些未成功上传的文件分片即可。分片（先将文件切分成多个文件分片）上传是断点续传的基础。RandomAccessFile可以帮助我们合并文件分片，示例代码如下：\n\n![img](https://guide-blog-images.oss-cn-shenzhen.aliyuncs.com/github/javaguide/java/io/20210609164749122.png)\n\n> [原文链接](https://javaguide.cn/java/io/io-basis.html)\n> [高效快捷读写文件之RandomAccessFile类解说](https://mp.weixin.qq.com/s/CYXVyNsNm36SCu_jrAp5qQ)\n\n### IO设计模式总结\n\n#### 装饰器模式\n\n**装饰器（Decorator）模式**可以在不改变原有对象的情况下拓展其功能。\n\n装饰器模式通过组合替代继承来扩展原始类的功能，在一些继承关系比较复杂的场景（IO这一场景各种类的继承关系就比较复杂）更加实用。对于字节流来说，FilterInputStream（对应输入流）和FilterOutputStream（对应输出流）是装饰器模式的核心，分别用于增强InputStream和OutputStream子类对象的功能。我们常见的BufferedInputStream(字节缓冲输入流)、DataInputStream等等都是FilterInputStream的子类，BufferedOutputStream（字节缓冲输出流）、DataOutputStream等等都是FilterOutputStream的子类。举个例子，我们可以通过BufferedInputStream（字节缓冲输入流）来增强FileInputStream的功能。\n\nBufferedInputStream构造函数如下：\n\n```java\npublic BufferedInputStream(InputStream in) {\n    this(in, DEFAULT_BUFFER_SIZE);\n}\n\npublic BufferedInputStream(InputStream in, int size) {\n    super(in);\n    if (size <= 0) {\n        throw new IllegalArgumentException(\"Buffer size <= 0\");\n    }\n    buf = new byte[size];\n}\n```\n\n可以看出，BufferedInputStream的构造函数其中的一个参数就是InputStream。\n\nBufferedInputStream代码示例：\n\n\n```java\ntry (BufferedInputStream bis = new BufferedInputStream(new FileInputStream(\"input.txt\"))) {\n    int content;\n    long skip = bis.skip(2);\n    while ((content = bis.read()) != -1) {\n        System.out.print((char) content);\n    }\n} catch (IOException e) {\n    e.printStackTrace();\n}\n```\n\n这个时候，你可以会想了：**为啥我们直接不弄一个BufferedFileInputStream（字符缓冲文件输入流）呢**？\n\n```java\nBufferedFileInputStream bfis = new BufferedFileInputStream(\"input.txt\");\n```\n\n如果InputStream的子类比较少的话，这样做是没问题的。不过，InputStream的子类实在太多，继承关系也太复杂了。如果我们为每一个子类都定制一个对应的缓冲输入流，那岂不是太麻烦了。如果你对IO流比较熟悉的话，你会发现ZipInputStream和ZipOutputStream还可以分别增强BufferedInputStream和BufferedOutputStream的能力。\n\n```java\nBufferedInputStream bis = new BufferedInputStream(new FileInputStream(fileName));\nZipInputStream zis = new ZipInputStream(bis);\n\nBufferedOutputStream bos = new BufferedOutputStream(new FileOutputStream(fileName));\nZipOutputStream zipOut = new ZipOutputStream(bos);\n```\n\nZipInputStream和ZipOutputStream分别继承自InflaterInputStream和DeflaterOutputStream。\n\n```java\npublic\nclass InflaterInputStream extends FilterInputStream {\n}\n\npublic\nclass DeflaterOutputStream extends FilterOutputStream {\n}\n```\n\n这也是装饰器模式很重要的一个特征，那就是可以对原始类嵌套使用多个装饰器。为了实现这一效果，装饰器类需要跟原始类继承相同的抽象类或者实现相同的接口。上面介绍到的这些IO相关的装饰类和原始类共同的父类是InputStream和OutputStream。对于字符流来说，BufferedReader可以用来增加Reader（字符输入流）子类的功能，BufferedWriter可以用来增加Writer（字符输出流）子类的功能。\n\n```java\nBufferedWriter bw = new BufferedWriter(new OutputStreamWriter(new FileOutputStream(fileName),\"UTF-8\"));\n```\n\nIO流中的装饰器模式应用的例子实在是太多了，不需要特意记忆，完全没必要哈！搞清了装饰器模式的核心之后，你在使用的时候自然就会知道哪些地方运用到了装饰器模式。\n\n#### 适配器模式\n\n**适配器（Adapter Pattern）模式**主要用于接口互不兼容的类的协调工作，你可以将其联想到我们日常经常使用的电源适配器。\n\n适配器模式中存在被适配的对象或者类称为**适配者（Adaptee）**，作用于适配者的对象或者类称为适配器(Adapter)。适配器分为对象适配器和类适配器。类适配器使用继承关系来实现，对象适配器使用组合关系来实现。\n\nIO流中的字符流和字节流的接口不同，它们之间可以协调工作就是基于适配器模式来做的，更准确点来说是对象适配器。通过适配器，我们可以将字节流对象适配成一个字符流对象，这样我们可以直接通过字节流对象来读取或者写入字符数据。\n\nInputStreamReader和OutputStreamWriter就是两个适配器(Adapter)，同时，它们两个也是字节流和字符流之间的桥梁。InputStreamReader使用StreamDecoder（流解码器）对字节进行解码，实现字节流到字符流的转换，OutputStreamWriter使用StreamEncoder（流编码器）对字符进行编码，实现字符流到字节流的转换。\n\nInputStream和OutputStream的子类是被适配者，InputStreamReader和OutputStreamWriter是适配器。\n\n```java\n// InputStreamReader是适配器，FileInputStream是被适配的类\nInputStreamReader isr = new InputStreamReader(new FileInputStream(fileName), \"UTF-8\");\n// BufferedReader增强InputStreamReader的功能（装饰器模式）\nBufferedReader bufferedReader = new BufferedReader(isr);\n```\n\njava.io.InputStreamReader部分源码：\n\n\n```java\npublic class InputStreamReader extends Reader {\n\t//用于解码的对象\n\tprivate final StreamDecoder sd;\n    public InputStreamReader(InputStream in) {\n        super(in);\n        try {\n            // 获取StreamDecoder对象\n            sd = StreamDecoder.forInputStreamReader(in, this, (String)null);\n        } catch (UnsupportedEncodingException e) {\n            throw new Error(e);\n        }\n    }\n    // 使用StreamDecoder对象做具体的读取工作\n\tpublic int read() throws IOException {\n        return sd.read();\n    }\n}\n```\n\njava.io.OutputStreamWriter部分源码：\n\n```java\npublic class OutputStreamWriter extends Writer {\n    // 用于编码的对象\n    private final StreamEncoder se;\n    public OutputStreamWriter(OutputStream out) {\n        super(out);\n        try {\n           // 获取StreamEncoder对象\n            se = StreamEncoder.forOutputStreamWriter(out, this, (String)null);\n        } catch (UnsupportedEncodingException e) {\n            throw new Error(e);\n        }\n    }\n    // 使用StreamEncoder对象做具体的写入工作\n    public void write(int c) throws IOException {\n        se.write(c);\n    }\n}\n```\n\n**适配器模式和装饰器模式有什么区别呢**？\n\n**装饰器模式**更侧重于动态地增强原始类的功能，装饰器类需要跟原始类继承相同的抽象类或者实现相同的接口。并且，装饰器模式支持对原始类嵌套使用多个装饰器。\n\n**适配器模式**更侧重于让接口不兼容而不能交互的类可以一起工作，当我们调用适配器对应的方法时，适配器内部会调用适配者类或者和适配类相关的类的方法，这个过程透明的。就比如说StreamDecoder（流解码器）和StreamEncoder（流编码器）就是分别基于InputStream和OutputStream来获取FileChannel对象并调用对应的read方法和write方法进行字节数据的读取和写入。\n\n```java\nStreamDecoder(InputStream in, Object lock, CharsetDecoder dec) {\n    // 省略大部分代码\n    // 根据InputStream对象获取FileChannel对象\n    ch = getChannel((FileInputStream)in);\n}\n```\n\n适配器和适配者两者不需要继承相同的抽象类或者实现相同的接口。另外，FutureTask类使用了适配器模式，Executors的内部类RunnableAdapter实现属于适配器，用于将Runnable适配成Callable。FutureTask参数包含Runnable的一个构造方法：\n\n\n```java\npublic FutureTask(Runnable runnable, V result) {\n    // 调用Executors类的callable方法\n    this.callable = Executors.callable(runnable, result);\n    this.state = NEW;\n}\n```\n\nExecutors中对应的方法和适配器：\n\n\n```java\n// 实际调用的是Executors的内部类RunnableAdapter的构造方法\npublic static <T> Callable<T> callable(Runnable task, T result) {\n    if (task == null)\n        throw new NullPointerException();\n    return new RunnableAdapter<T>(task, result);\n}\n// 适配器\nstatic final class RunnableAdapter<T> implements Callable<T> {\n    final Runnable task;\n    final T result;\n    RunnableAdapter(Runnable task, T result) {\n        this.task = task;\n        this.result = result;\n    }\n    public T call() {\n        task.run();\n        return result;\n    }\n}\n```\n\n#### 工厂模式\n\n工厂模式用于创建对象，NIO中大量用到了工厂模式，比如Files类的newInputStream方法用于创建InputStream对象（静态工厂）、Paths类的get方法创建Path对象（静态工厂）、ZipFileSystem类（sun.nio包下的类，属于java.nio相关的一些内部实现）的getPath的方法创建Path对象（简单工厂）。\n\n```java\nInputStream is Files.newInputStream(Paths.get(generatorLogoPath))\n```\n\n#### 观察者模式\n\nNIO中的文件目录监听服务使用到了观察者模式。NIO中的文件目录监听服务基于WatchService接口和Watchable接口。WatchService属于观察者，Watchable属于被观察者。Watchable接口定义了一个用于将对象注册到WatchService（监控服务）并绑定监听事件的方法register。\n\n\n```java\npublic interface Path\n    extends Comparable<Path>, Iterable<Path>, Watchable{\n}\n\npublic interface Watchable {\n    WatchKey register(WatchService watcher,\n                      WatchEvent.Kind<?>[] events,\n                      WatchEvent.Modifier... modifiers)\n        throws IOException;\n}\n```\n\nWatchService用于监听文件目录的变化，同一个WatchService对象能够监听多个文件目录。\n\n```java\n// 创建WatchService对象\nWatchService watchService = FileSystems.getDefault().newWatchService();\n\n// 初始化一个被监控文件夹的Path类:\nPath path = Paths.get(\"workingDirectory\");\n// 将这个path对象注册到WatchService（监控服务）中去\nWatchKey watchKey = path.register(\nwatchService, StandardWatchEventKinds...);\n```\n\nPath类register方法的第二个参数events（需要监听的事件）为可变长参数，也就是说我们可以同时监听多种事件。\n\n\n```java\nWatchKey register(WatchService watcher,\n                  WatchEvent.Kind<?>... events)\n    throws IOException;\n```\n\n常用的监听事件有3种：\n\n- **StandardWatchEventKinds.ENTRY_CREATE**:文件创建。\n- **StandardWatchEventKinds.ENTRY_DELETE**:文件删除。\n- **StandardWatchEventKinds.ENTRY_MODIFY**:文件修改。\n\nregister方法返回WatchKey对象，通过WatchKey对象可以获取事件的具体信息比如文件目录下是创建、删除还是修改了文件、创建、删除或者修改的文件的具体名称是什么。\n\n\n```java\nWatchKey key;\nwhile ((key = watchService.take()) != null) {\n    for (WatchEvent<?> event : key.pollEvents()) {\n      // 可以调用WatchEvent对象的方法做一些事情比如输出事件的具体上下文信息\n    }\n    key.reset();\n}\n```\n\nWatchService内部是通过一个daemon thread（守护线程）采用定期轮询的方式来检测文件的变化，简化后的源码如下所示。\n\n\n```java\nclass PollingWatchService\n    extends AbstractWatchService\n{\n    // 定义一个daemon thread（守护线程）轮询检测文件变化\n    private final ScheduledExecutorService scheduledExecutor;\n\n    PollingWatchService() {\n        scheduledExecutor = Executors\n            .newSingleThreadScheduledExecutor(new ThreadFactory() {\n                 @Override\n                 public Thread newThread(Runnable r) {\n                     Thread t = new Thread(r);\n                     t.setDaemon(true);\n                     return t;\n                 }});\n    }\n\n  void enable(Set<? extends WatchEvent.Kind<?>> events, long period) {\n    synchronized (this) {\n      // 更新监听事件\n      this.events = events;\n\n        // 开启定期轮询\n      Runnable thunk = new Runnable() { public void run() { poll(); }};\n      this.poller = scheduledExecutor\n        .scheduleAtFixedRate(thunk, period, period, TimeUnit.SECONDS);\n    }\n  }\n}\n```\n> [原文链接](https://javaguide.cn/java/io/io-design-patterns.html)\n\n### IO模型详解\n\n#### BIO(Blocking I/O)\n\n**BIO属于同步阻塞IO模型**。\n\n同步阻塞IO模型中，应用程序发起read调用后，会一直阻塞，直到内核把数据拷贝到用户空间。\n\n![图源：《深入拆解Tomcat&Jetty》](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/6a9e704af49b4380bb686f0c96d33b81~tplv-k3u1fbpfcp-watermark.image)\n\n在客户端连接数量不高的情况下，是没问题的。但是，当面对十万甚至百万级连接的时候，传统的BIO模型是无能为力的。因此，我们需要一种更高效的I/O处理模型来应对更高的并发量。\n\n#### NIO(Non-blocking/New I/O)\n\nJava中的NIO于Java1.4中引入，对应`java.nio`包，提供了`Channel`,`Selector`，`Buffer`等抽象。NIO中的N可以理解为Non-blocking，不单纯是New。它是支持面向缓冲的，基于通道的I/O操作方法。对于高负载、高并发的（网络）应用，应使用NIO。Java中的NIO可以看作是**I/O多路复用模型**。也有很多人认为，Java中的NIO属于同步非阻塞IO模型。\n\n我们先来看看**同步非阻塞IO模型**。\n\n![图源：《深入拆解Tomcat&Jetty》](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/bb174e22dbe04bb79fe3fc126aed0c61~tplv-k3u1fbpfcp-watermark.image)\n\n同步非阻塞IO模型中，应用程序会一直发起read调用，等待数据从内核空间拷贝到用户空间的这段时间里，线程依然是阻塞的，直到在内核把数据拷贝到用户空间。相比于同步阻塞IO模型，同步非阻塞IO模型确实有了很大改进。通过轮询操作，避免了一直阻塞。但是，这种IO模型同样存在问题：应用程序不断进行I/O系统调用轮询数据是否已经准备好的过程是十分消耗CPU资源的。这个时候，**I/O多路复用模型**就上场了。\n\n![img](https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/88ff862764024c3b8567367df11df6ab~tplv-k3u1fbpfcp-watermark.image)\n\nIO多路复用模型中，线程首先发起select调用，询问内核数据是否准备就绪，等内核把数据准备好了，用户线程再发起read调用。read调用的过程（数据从内核空间->用户空间）还是阻塞的。目前支持IO多路复用的系统调用，有select，epoll等等。select系统调用，目前几乎在所有的操作系统上都有支持。\n\n- **select调用**：内核提供的系统调用，它支持一次查询多个系统调用的可用状态。几乎所有的操作系统都支持。\n- **epoll调用**：linux2.6内核，属于select调用的增强版本，优化了IO的执行效率。\n\n**IO多路复用模型，通过减少无效的系统调用，减少了对CPU资源的消耗。**\n\nJava中的NIO，有一个非常重要的**选择器(Selector)**的概念，也可以被称为**多路复用器**。通过它，只需要一个线程便可以管理多个客户端连接。当客户端数据到了之后，才会为其服务。\n\n![img](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/0f483f2437ce4ecdb180134270a00144~tplv-k3u1fbpfcp-watermark.image)\n\n#### AIO(Asynchronous I/O)\n\nAIO也就是NIO2。Java7中引入了NIO的改进版NIO2,它是异步IO模型。\n\n异步IO是基于事件和回调机制实现的，也就是应用操作之后会直接返回，不会堵塞在那里，当后台处理完成，操作系统会通知相应的线程进行后续的操作。\n\n![img](https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/3077e72a1af049559e81d18205b56fd7~tplv-k3u1fbpfcp-watermark.image)\n\n目前来说AIO的应用还不是很广泛。Netty之前也尝试使用过AIO，不过又放弃了。这是因为，Netty使用了AIO之后，在Linux系统上的性能并没有多少提升。最后，来一张图，简单总结一下Java中的BIO、NIO、AIO。\n\n![img](https://images.xiaozhuanlan.com/photo/2020/33b193457c928ae02217480f994814b6.png)\n\n> [原文链接](https://javaguide.cn/java/io/io-model.html)\n\n\n### 相关文章\n\n- [如果有人再问你Java IO，把这篇文章砸他头上](https://mp.weixin.qq.com/s?__biz=MzkzODE3OTI0Ng==&mid=2247491115&idx=1&sn=14f1712cc787befd6c78d64612a00f95&source=41#wechat_redirect)\n- [Java I/O体系从原理到应用，这一篇全说清楚了](https://mp.weixin.qq.com/s/asMxiq9yIPa6aaZenfDBmQ)\n- [理解Java的IO(同步非同步阻塞非阻塞)](https://mp.weixin.qq.com/s/p5qM2UJ1uIWyongfVpRbCg)\n- [用三张图看透I/O，学习区分同异步阻塞](https://mp.weixin.qq.com/s/VsCINjx0sPhhBC62k223jw)\n- [1.6万字长文带你读懂Java IO](https://mp.weixin.qq.com/s/5iTAaEZUWkqa5ESZ9lPL6Q)\n- [看一遍就能理解的IO模型详解](https://mp.weixin.qq.com/s/77G2NxfjZlT-icfqrHCizQ)\n- [读取文件时，程序经历了什么？](https://mp.weixin.qq.com/s/_pW0v7a7tKbGZpi3lRQVcw)\n- [常见的IO模型有哪些？Java中BIO、NIO、AIO的区别？](https://mp.weixin.qq.com/s/5pRJ6qP-lRFDTd38BkdMzw)\n- [NIO和IO到底有什么区别？别说你不会！](https://mp.weixin.qq.com/s/Lda80PTxJnd5oZh5VWSi4Q)\n- [万字长文：助你攻破JAVA NIO技术壁垒](https://mp.weixin.qq.com/s/H5sNv_a992MKUKXxK2QQEA)\n- [JAVA语言异步非阻塞设计模式](https://mp.weixin.qq.com/s/6PsSQXamf6w0w-B-ZgGNsg)\n- [一篇带你彻底读懂IO流技术！](https://mp.weixin.qq.com/s/pS2SXhmF7O3Gk8WRkCFZaw)\n- [文件读写操作与常用技巧分享](https://mp.weixin.qq.com/s/LJS6joxDCt71M9tEJ6GxFA)\n- [IO流为什么必须手动关闭，不能像其他的对象坐等GC回收?](https://mp.weixin.qq.com/s/emjHDexsHoIFmZGhwwrKfA)\n- [面试官：BIO、NIO、AIO之间有什么区别？](https://mp.weixin.qq.com/s/GAQxxEmYbE8Sy3oS9ePvhA)\n\n\n## NIO\n\n> [NIO demo](https://github.com/xmxe/demo/tree/master/study-demo/src/main/java/com/xmxe/jdkfeature/nio)\n> [浅析Java NIO](https://mp.weixin.qq.com/s/XEOYvRhtcOK2J7kdRIZcXQ)\n\nNIO主要有三大核心部分：Channel(通道)，Buffer(缓冲区),Selector(选择区)。\nIO是面向流的，NIO是面向缓冲区的。传统IO基于字节流和字符流进行操作，而NIO基于Channel和Buffer进行操作，数据总是从通道读取到缓冲区中，或者从缓冲区写入到通道中。\n\n\n### Buffer\n\nNIO中的关键Buffer实现有：ByteBuffer,CharBuffer,DoubleBuffer,FloatBuffer,IntBuffer,LongBuffer,ShortBuffer。分别对应基本数据类型:byte,char,double,float,int,long,short。NIO中还有MappedByteBuffer,HeapByteBuffer,DirectByteBuffer等缓冲区,本质上是一块可以写入数据，然后可以从中读取数据的内存。这块内存被包装成NIO Buffer对象，并提供了一组方法，用来方便的访问该块内存。这块内存被包装成NIO。\n\nBuffer对象，并提供了一组方法，用来方便的访问该块内存，Buffer：顾名思义是一块缓冲区，实际上是一个容器，一个连续数组。Channel提供从文件、网络读取数据的渠道，但是读写的数据都必须经过Buffer。\n向Buffer中写数据：从Channel到Buffer使用fileChannel.read(buf).或者通过Buffer的put()方法 buf.put()\n从Buffer中读取数据：从Buffer到Channel使用channel.write(buf)) .或者使用get()方法 buf.get()\n\n```java\n/**\n* JAVA处理大文件，一般用BufferedReader,BufferedInputStream这类带缓冲的IO类，不过如果文件超大的话，更快的方式是采用MappedByteBuffer。MappedByteBuffer是NIO引入的文件内存映射方案，读写性能极高。NIO最主要的就是实现了对异步操作的支持。其中一种通过把一个套接字通道(SocketChannel)注册到一个选择器(Selector)中,不时调用后者的选择(select)方法就能返回满足的选择键(SelectionKey),键中包含了SOCKET事件信息。这就是select模型。\n*\n* FileChannel提供了map方法来把文件影射为内存映像文件：MappedByteBuffer map(int mode,long position,long size); 可以把文件的从position开始的size大小的区域映射为内存映像文件，mode指出了可访问该内存映像文件的方式:READ_ONLY（只读）:试图修改得到的缓冲区将导致抛出 ReadOnlyBufferException.(MapMode.READ_ONLY);READ_WRITE（读/写）:对得到的缓冲区的更改最终将传播到文件；该更改对映射到同一文件的其他程序不一定是可见的(MapMode.READ_WRITE);PRIVATE（专用）:对得到的缓冲区的更改不会传播到文件，并且该更改对映射到同一文件的其他程序也不是可见的；相反，会创建缓冲区已修改部分的专用副本。 (MapMode.PRIVATE)\n\n* MappedByteBuffer是ByteBuffer的子类，其扩充了三个方法：force()：缓冲区是READ_WRITE模式下，此方法对缓冲区内容的修改强行写入文件；load()：将缓冲区的内容载入内存，并返回该缓冲区的引用；isLoaded()：如果缓冲区的内容在物理内存中，则返回真，否则返回假\n*/\npublic void mappedByteBuffer(){\n    File file = new File(\"D://data.txt\");\n    long len = file.length();\n    byte[] ds = new byte[(int) len];\n    try (RandomAccessFile randomAccessFile = new RandomAccessFile(file, \"r\");){\n        MappedByteBuffer mappedByteBuffer = randomAccessFile.getChannel().map(FileChannel.MapMode.READ_ONLY, 0, len);\n        for (int offset = 0; offset < len; offset++) {\n            byte b = mappedByteBuffer.get();\n            ds[offset] = b;\n        }\n        Scanner scan = new Scanner(new ByteArrayInputStream(ds)).useDelimiter(\" \");\n        while (scan.hasNext()) {\n        \tSystem.out.print(scan.next() + \" \");\n        }\n    } catch (Exception e) {\n       e.printStackTrace();\n    }\n        \n/**\n* map过程\n* FileChannel提供了map方法把文件映射到虚拟内存，通常情况可以映射整个文件，如果文件比较大，可以进行分段映射。FileChannel中的几个变量：MapMode mode：内存映像文件访问的方式，共三种：MapMode.READ_ONLY：只读，试图修改得到的缓冲区将导致抛出异常。MapMode.READ_WRITE：读/写，对得到的缓冲区的更改最终将写入文件；但该更改对映射到同一文件的其他程序不一定是可见的。MapMode.PRIVATE：私用，可读可写,但是修改的内容不会写入文件，只是buffer自身的改变，这种能力称之为”copy on write”。\n*\n* position：文件映射时的起始位置。\n* allocationGranularity：Memory allocation size for mapping buffers，通过native函数initIDs初始化。\n*/\n}\n```\n\n- capacity(缓冲区数组的总长度 即可以容纳的最大数据量；在缓冲区创建时被设定并且不能改变)。capacity作为一个内存块，Buffer有一个固定的大小值，也叫“capacity”.你只能往里写capacity个byte、long，char等类型。一旦Buffer满了，需要将其清空（通过读数据或者清除数据）才能继续写数据往里写数据。\n\n- position(下一个要操作的数据元素的位置,下一个要被读或写的元素的索引，每次读写缓冲区数据时都会改变改值，为下次读写作准备),当你写数据到Buffer中时，position表示当前的位置。初始的position值为0.当一个byte、long等数据写到Buffer后，position会向前移动到下一个可插入数据的Buffer单元。position最大可为capacity – 1.当读取数据时，也是从某个特定位置读。当将Buffer从写模式切换到读模式，position会被重置为0. 当从Buffer的position处读取数据时，position向前移动到下一个可读的位置。\n\n- limit(缓冲区数组中不可操作的下一个元素的位置：limit<=capacity表示缓冲区的当前终点，不能对缓冲区超过极限的位置进行读写操作。且极限是可以修改的)。limit在写模式下，Buffer的limit表示你最多能往Buffer里写多少数据。 写模式下，limit等于Buffer的capacity。当切换Buffer到读模式时，limit表示你最多能读到多少数据。因此，当切换Buffer到读模式时，limit会被设置成写模式下的position值。换句话说，你能读到之前写入的所有数据（limit被设置成已写数据的数量，这个值在写模式下就是position）\n\n- mark(标记，调用mark()来设置mark=position，再调用reset()可以让position恢复到标记的位置)\nmark <= position <= limit <= capacity。position和limit的含义取决于Buffer处在读模式还是写模式。不管Buffer处在什么模式，capacity的含义总是一样的。\n\n#### Buffer常用方法\n```java\nallocate(int capacity)//从堆空间中分配一个容量大小为capacity的byte数组作为缓冲区的byte数据存储器\nallocateDirect(int capacity)//是不使用JVM堆栈而是通过操作系统来创建内存块用作缓冲区，它与当前操作系统能够更好的耦合，因此能进一步提高I/O操作速度。但是分配直接缓冲区的系统开销很大，因此只有在缓冲区较大并长期存在，或者需要经常重用时，才使用这种缓冲区\nwrap(byte[] array)//这个缓冲区的数据会存放在byte数组中，bytes数组或buff缓冲区任何一方中数据的改动都会影响另一方。其实ByteBuffer底层本来就有一个bytes数组负责来保存buffer缓冲区中的数据，通过allocate方法系统会帮你构造一个byte数组\nwrap(byte[] array, int offset, int length) //在上一个方法的基础上可以指定偏移量和长度，这个offset也就是包装后byteBuffer的position，而length呢就是limit-position的大小，从而我们可以得到limit的位置为length+position(offset)\nlimit(), limit(10)等//其中读取和设置这4个属性的方法的命名和jQuery中的val(),val(10)类似，一个负责get，一个负责set\nreset()//把position设置成mark的值，相当于之前做过一个标记，现在要退回到之前标记的地方\nclear()//position = 0;limit = capacity;mark = -1;有点初始化的味道，但是并不影响底层byte数组的内容\nflip()//limit = position;position = 0;mark = -1;翻转，也就是让flip之后的position到limit这块区域变成之前的0到position这块，翻转就是将一个处于存数据状态的缓冲区变为一个处于准备取数据的状态 一般在从Buffer读出数据前调用。\nrewind()//把position设为0，mark设为-1，不改变limit的值 一般在把数据重写入Buffer前调用 或者重新读取\nremaining()//return limit - position;返回limit和position之间相对位置差\nhasRemaining()//return position < limit返回是否还有未读内容\ncompact()//把从position到limit中的内容移到0到limit-position的区域内，position和limit的取值也分别变成limit-position、capacity。如果先将positon设置到limit，再compact，那么相当于clear()\nget()//相对读，从position位置读取一个byte，并将position+1，为下次读写作准备\nget(int index)//绝对读，读取byteBuffer底层的bytes中下标为index的byte，不改变position\nget(byte[] dst, int offset, int length)//从position位置开始相对读，读length个byte，并写入dst下标从offset到offset+length的区域\nput(byte b)//相对写，向position的位置写入一个byte，并将postion+1，为下次读写作准备\nput(int index, byte b)//绝对写，向byteBuffer底层的bytes中下标为index的位置插入byte b，不改变position\nput(ByteBuffer src)//用相对写，把src中可读的部分（也就是position到limit）写入此byteBuffer\nput(byte[] src, int offset, int length)//从src数组中的offset到offset+length区域读取数据并使用相对写写入此byteBuffer\nByteOrder order()//检索此缓冲区的字节顺序。\nByteBuffer order(ByteOrder bo)//修改缓冲区的字节顺序。\nByteBuffer putInt(int value)//编写int值的相对put方法（可选操作） 。以当前字节顺序将包含给定int值的四个字节写入当前位置的缓冲区，然后将位置递增四。\nbyte[] array()//返回支持此缓冲区的字节数组（可选操作） 。对此缓冲区内容的修改将导致返回的数组的内容被修改，反之亦然。在调用此方法之前调用hasArray方法，以确保此缓冲区具有可访问的后台阵列。\nBuffer.mark()\n//通过调用Buffer.mark()方法，可以标记Buffer中的一个特定position。之后可以通过调用Buffer.reset()方法恢复到这个position\n//可以使用equals()和compareTo()方法两个Buffer。\nequals()\n//当满足下列条件时，表示两个Buffer相等：有相同的类型（byte、char、int等）。Buffer中剩余的byte、char等的个数相等。Buffer中所有剩余的byte、char等都相同。如你所见，equals只是比较Buffer的一部分，不是每一个在它里面的元素都比较。实际上，它只比较Buffer中的剩余元素。\ncompareTo()方法\n//compareTo()方法比较两个Buffer的剩余元素(byte、char等)， 如果满足下列条件，则认为一个Buffer“小于”另一个Buffer：第一个不相等的元素小于另一个Buffer中对应的元素 。所有元素都相等，但第一个Buffer比另一个先耗尽(第一个Buffer的元素个数比另一个少)。\n```\n\n\n### Channel\n\nNIO中的Channel的主要实现有：FileChannel(从文件中读写数据) 、DatagramChannel(通过UDP读写网络中的数据)、SocketChannel(通过TCP读写网络中的数据)、ServerSocketChannel(可以监听新进来的TCP连接，像Web服务器那样。对每一个新进来的连接都会创建一个SocketChannel)\n\n```java\n    /**\n    * NIO读取文件 RandomAccessFile进行操作，也可以通FileInputStream.getChannel()获取channel进行操作\n    */\n    public void newIORead() {\n        RandomAccessFile aFile = null;\n        try {\n            aFile = new RandomAccessFile(\"src/nio.txt\", \"rw\");\n            FileChannel fileChannel = aFile.getChannel();\n            // buffer分配空间 根据Buffer实现类表明分配时的单位 下面代表分配1024个字节 CharBuffer就代表1024个字符\n            ByteBuffer buf = ByteBuffer.allocate(1024);\n            // 通道必须结合Buffer使用，不能直接向通道中读/写数据，\n            // read()表示读channel数据写入到buffer，write()表示读取buffer数据写入到channel。\n            int bytesRead = fileChannel.read(buf);\n            while (bytesRead != -1) {\n                // 在读模式下，可以读取之前写入到buffer的所有数据,调用flip()方法,position设回0，并将limit设成之前的position的值\n                // 数据就是从position到limit的数据\n                // capacity(缓冲区数组的总长度),position(下一个要操作的数据元素的位置),limit(缓冲区数组中不可操作的下一个元素的位置：limit<=capacity),mark(用于记录当前position的前一个位置或者默认是-1)\n                buf.flip();\n                // hasRemaining()用于判断当前位置(position)和限制(limit)之间是否有任何元素\n                while (buf.hasRemaining()) {\n                    System.out.print((char) buf.get());\n                }\n                // clear()方法：position将被设回0，limit设置成capacity，换句话说，Buffer被清空了，\n                // 其实Buffer中的数据并未被清除，只是这些标记告诉我们可以从哪里开始往Buffer里写数据。\n                // 如果Buffer中有一些未读的数据，调用clear()方法，数据将“被遗忘”，意味着不再有任何标记会告诉你哪些数据被读过，哪些还没有。\n                // 如果Buffer中仍有未读的数据，且后续还需要这些数据，但是此时想要先写些数据，那么使用compact()方法。\n                // compact()方法将所有未读的数据拷贝到Buffer起始处。然后将position设到最后一个未读元素正后面。limit属性依然像clear()方法一样，设置成capacity。\n                // 现在Buffer准备好写数据了，但是不会覆盖未读的数据。\n                buf.compact();\n                bytesRead = fileChannel.read(buf);\n            }\n        } catch (Exception e) {\n            e.printStackTrace();\n        } finally {\n            try {\n                if (aFile != null) {\n                    aFile.close();\n                }\n            } catch (Exception e) {\n                e.printStackTrace();\n            }\n        }\n    }\n\n    /**\n     * NIO写数据 通过FileChannel写入数据\n     */\n    public void FileChannelOnWrite() {\n        try {\n            RandomAccessFile accessFile = new RandomAccessFile(\"D://file1.txt\", \"rw\");\n            FileChannel fc = accessFile.getChannel();\n            byte[] bytes = new String(\"write to file1.txt\").getBytes();\n            // 获得ByteBuffer的实例 类似allocate(int capacity)方法\n            ByteBuffer byteBuffer = ByteBuffer.wrap(bytes);\n            // 读取缓冲区数据写入到通道。\n            fc.write(byteBuffer);\n            // 清空缓存区 使得缓存区可以继续写入数据\n            byteBuffer.clear();\n            // 缓存区写入内容\n            byteBuffer.put(new String(\",a good boy\").getBytes());\n            // 写模式转化读模式\n            byteBuffer.flip();\n            // 读取缓冲区数据写入到通道。\n            fc.write(byteBuffer);\n            fc.close();\n            accessFile.close();\n        } catch (Exception e) {\n            e.printStackTrace();\n        }\n    }\n\n    /**\n     * FileChannel的transferFrom()方法可以将数据从源通道传输到FileChannel中\n     * （译者注：这个方法在JDK文档中的解释为将字节从给定的可读取字节通道传输到此通道的文件中）。\n     */\n    public void testTransferFrom() {\n        try {\n            RandomAccessFile fromFile = new RandomAccessFile(\"D://file1.txt\", \"rw\");\n            FileChannel fromChannel = fromFile.getChannel();\n            RandomAccessFile toFile = new RandomAccessFile(\"D://file2.txt\", \"rw\");\n            FileChannel toChannel = toFile.getChannel();\n\n            long position = 0;\n            long count = fromChannel.size();\n            toChannel.transferFrom(fromChannel, position, count);\n\n            fromFile.close();\n            toFile.close();\n        } catch (Exception e) {\n            e.printStackTrace();\n        }\n    }\n\n    /**\n     * transferTo()方法将数据从FileChannel传输到其他的channel中\n     */\n    public static void testTransferTo() {\n        try {\n            RandomAccessFile fromFile = new RandomAccessFile(\"D://file1.txt\", \"rw\");\n            FileChannel fromChannel = fromFile.getChannel();\n            RandomAccessFile toFile = new RandomAccessFile(\"D://file3.txt\", \"rw\");\n            FileChannel toChannel = toFile.getChannel();\n\n            long position = 0;\n            long count = fromChannel.size();\n            fromChannel.transferTo(position, count, toChannel);\n            fromFile.close();\n            toFile.close();\n        } catch (Exception e) {\n            e.printStackTrace();\n        }\n    }\n\n    //-----------------------nio socket-------------------------------------\n\n    /**\n     * NIO新建socket client\n     */\n    public void newIOSocketClient() {\n        ByteBuffer buffer = ByteBuffer.allocate(1024);\n        SocketChannel socketChannel = null;\n        try {\n            // 通过 ServerSocketChannel.open()方法来创建一个新的ServerSocketChannel对象，\n            // 该对象关联了一个未绑定ServerSocket的通道.通过调用该对象上的socket()方法可以获取与之关联的ServerSocket。\n            socketChannel = SocketChannel.open();\n            socketChannel.configureBlocking(false);\n            socketChannel.connect(new InetSocketAddress(\"10.10.195.115\", 8080));\n            // 为了确定连接是否建立，可以调用finishConnect()的方法。\n            if (socketChannel.finishConnect()) {\n                int i = 0;\n                while (true) {\n                    TimeUnit.SECONDS.sleep(1);\n                    String info = \"I'm \" + i++ + \"-th information from client\";\n                    buffer.clear();\n                    buffer.put(info.getBytes());\n                    buffer.flip();\n                    while (buffer.hasRemaining()) {\n                        System.out.println(buffer);\n                        socketChannel.write(buffer);\n                    }\n                }\n            }\n        } catch (Exception e) {\n            e.printStackTrace();\n        } finally {\n            try {\n                if (socketChannel != null) {\n                    socketChannel.close();\n                }\n            } catch (Exception e) {\n                e.printStackTrace();\n            }\n        }\n    }\n\n    /**\n     * NIO socket server\n     * \n     * @throws Exception\n     */\n    public void NIOSocketServer() throws Exception {\n        ServerSocketChannel socketChannel = ServerSocketChannel.open();\n        // 非阻塞模式\n        socketChannel.configureBlocking(false);\n        // socketChannel.socket().bind(new InetSocketAddress(9999));jdk 1.7之前\n        socketChannel.bind(new InetSocketAddress(9999));\n        ByteBuffer byteBuffer = ByteBuffer.allocate(1024);\n        while (true) {\n            // 通过 ServerSocketChannel.accept() 方法监听新进来的连接。\n            // 在阻塞模式下当 accept()方法返回的时候,它返回一个包含新进来的连接的SocketChannel，否则accept()方法会一直阻塞到有新连接到达。\n            // 在非阻塞模式下，在没有新连接的情况下，accept()会立即返回null，该模式下通常不会仅仅只监听一个连接,因此需在while循环中调用\n            // accept()方法.\n            SocketChannel channel = socketChannel.accept();\n            if (channel != null) {\n                InetSocketAddress remoteAddress = (InetSocketAddress) channel.getRemoteAddress();\n                System.out.println(remoteAddress.getAddress());\n                System.out.println(remoteAddress.getPort());\n                channel.read(byteBuffer);\n                byteBuffer.flip();\n                while (byteBuffer.hasRemaining()) {\n                    System.out.print((char) byteBuffer.get());\n                }\n            }\n        }\n    }\n\n    /**\n     * UDP发送方\n     * \n     * @throws Exception\n     */\n    public void DatagramChannelsend() throws Exception {\n        // 通过DatagramChannel的open()方法来创建。需要注意DatagramChannel的open()方法只是打开获得通道，\n        // 但此时尚未连接。尽管DatagramChannel无需建立连接（远端连接），但仍然可以通过isConnect()检测当前的channel是否声明了远端连接地址。\n        DatagramChannel channel = DatagramChannel.open();\n        ByteBuffer byteBuffer = ByteBuffer.wrap(new String(\"i 'm client\").getBytes());\n        // 通过send()方法将ByteBuffer中的内容发送到指定的SocketAddress对象所描述的地址。在阻塞模式下，调用线程会被阻塞至有数据包被加入传输队列。\n        // 非阻塞模式下，如果发送内容为空则返回0，否则返回发送的字节数。发送数据报是一个全有或全无(all-or-nothing)的行为。\n        // 如果传输队列没有足够空间来承载整个数据报，那么什么内容都不会被发送。\n        // 请注意send()方法返回的非零值并不表示数据报到达了目的地，仅代表数据报被成功加到本地网络层的传输队列。\n        // 此外，传输过程中的协议可能将数据报分解成碎片，被分解的数据报在目的地会被重新组合起来，接收者将看不到碎片。\n        // 但是，如果有一个碎片不能按时到达，那么整个数据报将被丢弃。分解有助于发送大数据报，但也会会造成较高的丢包率。\n        int bytesSent = channel.send(byteBuffer, new InetSocketAddress(\"127.0.0.1\", 9999));\n        System.out.println(\"bytesSent=\"+bytesSent);\n    }\n\n    /**\n     * UDP接收方\n     */\n    public void receiveData() throws IOException {\n        DatagramChannel channel = DatagramChannel.open();\n        channel.socket().bind(new InetSocketAddress(9999));\n        ByteBuffer byteBuffer = ByteBuffer.allocate(1024);\n        byteBuffer.clear();\n        // 通过receive()方法接受DatagramChannel中数据。从该方法将传入的数据报的数据将被复制到ByteBuffer中，\n        // 同时返回一个SocketAddress对象以指出数据来源。在阻塞模式下，receive()将会阻塞至有数据包到来，\n        // 非阻塞模式下，如果没有可接受的包则返回null。如果包内的数据大小超过缓冲区容量时，多出的数据会被悄悄抛弃\n        SocketAddress address = channel.receive(byteBuffer);// receive data\n        System.out.println(address);\n        byteBuffer.flip();\n        while (byteBuffer.hasRemaining()) {\n            System.out.print((char) byteBuffer.get());\n        }\n    }\n```\n\n#### Channel常用方法 \n\n```java\nint read(ByteBuffer dst) //从Channel到中读取数据到ByteBuffer \nlong read(ByteBuffer[] dsts) //将Channel到中的数据“分散”到ByteBuffer[] \nint write(ByteBuffer src) //将ByteBuffer到中的数据写入到Channel \nlong write(ByteBuffer[] srcs) //将ByteBuffer[]到中的数据“聚集”到Channel \nlong position() //返回此通道的文件位置 \nFileChannel position(long p) //设置此通道的文件位置 \nlong size() //返回此通道的文件的当前大小 \nFileChannel truncate(long s) //将此通道的文件截取为给定大小 \nvoid force(boolean metaData) //强制将所有对此通道的文件更新写入到存储设备中\n```\n\n### Selector\n\nSelector(选择区)用于监听多个通道的事件（比如：连接打开，数据到达）因此，单个线程可以监听多个数据通道。\n\nSelector的创建\n```java\nSelector selector = Selector.open();\n```\n为了将Channel和Selector配合使用，必须将Channel注册到Selector上，通过SelectableChannel.register()方法来实现，沿用nio创建socket\n```java\nServerSocketChannel channel = ServerSocketChannel.open(); \nchannel.socket().bind(new InetSocketAddress(PORT)); \nchannel.configureBlocking(false);\nSelectionKey key = channel.register(selector, SelectionKey.OP_ACCEPT);\n```\n与Selector一起使用时，Channel必须处于非阻塞模式下。这意味着不能将FileChannel与Selector一起使用，因为FileChannel不能切换到非阻塞模式。而套接字通道都可以。\n注意register()方法的第二个参数。这是一个“interest集合”，意思是在通过Selector监听Channel时对什么事件感兴趣。可以监听四种不同类型的事件：\n\n1. Connect连接 \n2. Accept接收 \n3. Read读\n4. Write写 \n\n这四种事件用SelectionKey的四个常量来表示：1.SelectionKey.OP_CONNECT 2. SelectionKey.OP_ACCEPT 3. SelectionKey.OP_READ 4.SelectionKey.OP_WRITE\n\n**SelectionKey**\n一个SelectionKey键表示了一个特定的通道对象和一个特定的选择器对象之间的注册关系。\n当向Selector注册Channel时，register()方法会返回一个SelectionKey对象。这个对象包含了一些你感兴趣的属性\ninterest集合：就像向Selector注册通道一节中所描述的，interest集合是你所选择的感兴趣的事件集合。可以通过SelectionKey读写interest集合。\nready集合：通道已经准备就绪的操作的集合。在一次选择(Selection)之后，你会首先访问这个ready set。可以这样访问ready集合： int readySet = selectionKey.readyOps();\n可以用像检测interest集合那样的方法，来检测channel中什么事件或操作已经就绪。但是，也可以使用以下四个方法，它们都会返回一个布尔类型：\n\n```java\nselectionKey.isAcceptable(); \nselectionKey.isConnectable();\nselectionKey.isReadable(); \nselectionKey.isWritable();\n```\n从SelectionKey访问Channel和Selector很简单:\n```java\nChannel channel = selectionKey.channel(); \nSelector selector = selectionKey.selector();\n```\n可以将一个对象或者更多信息附着到SelectionKey上，这样就能方便的识别某个给定的通道。例如，可以附加与通道一起使用的Buffer，或是包含聚集数据的某个对象。使用方法：\n```java\nselectionKey.attach(theObject); \nObject attachedObj = selectionKey.attachment();\n```\n还可以在用register()方法向Selector注册Channel的时候附加对象。如： \n```java\nSelectionKey key = channel.register(selector, SelectionKey.OP_READ, theObject);\n```\n\n**SelectionKey常用方法**\n```java\nkey.attachment(); //返回SelectionKey的attachment，attachment可以在注册channel的时候指定。\nkey.channel(); //返回该SelectionKey对应的channel。\nkey.selector(); //返回该SelectionKey对应的Selector。\nkey.interestOps(); //返回代表需要Selector监控的IO操作的bit mask\nkey.readyOps(); //返回一个bit mask，代表在相应channel上可以进行的IO操作。\n\n```\n通过Selector选择通道一旦向Selector注册了一或多个通道，就可以调用几个重载的select()方法。这些方法返回你所感兴趣的事件（如连接、接受、读或写）已经准备就绪的那些通道。\n\n#### select()方法\n\n```java\nint select() //阻塞到至少有一个通道在你注册的事件上就绪了\nint select(long timeout) //和select()一样，除了最长会阻塞timeout毫秒(参数)。\nint selectNow() //不会阻塞，不管什么通道就绪都立刻返回（译者注：此方法执行非阻塞的选择操作。如果自从前一次选择操作后，没有通道变成可选择的，则此方法直接返回零。\n//select()方法返回的int值表示有多少通道已经就绪。亦即自上次调用select()方法后有多少通道变成就绪状态。如果调用select()方法，有一个通道变成就绪状态，返回了1，若再次调用select()方法，如果另一个通道就绪了，它会再次返回1。如果对第一个就绪的channel没有做任何操作，现在就有两个就绪的通道，但在每次select()方法调用之间，只有一个通道就绪了。一旦调用了select()方法，并且返回值表明有一个或更多个通道就绪了，然后可以通过调用selector的selectedKeys()方法，Set selectedKeys = selector.selectedKeys();当向Selector注册Channel时，Channel.register()方法会返回一个SelectionKey对象。这个对象代表了注册到该Selector的通道。注意每次迭代末尾的keyIterator.remove()调用。Selector不会自己从已选择键集中移除SelectionKey实例。必须在处理完通道时自己移除。下次该通道变成就绪时，Selector会再次将其放入已选择键集中。\nSelectionKey.channel()//方法返回的通道需要转型成你要处理的类型，如ServerSocketChannel或SocketChannel等。。\n\n```\n\n```java\n    static class ServerSelector {\n        private static final int BUF_SIZE = 1024;\n        private static final int PORT = 8080;\n        private static final int TIMEOUT = 3000;\n\n        // public static void main(String[] args){\n        //     selector();\n        // }\n\n        /**\n         * 接收就绪处理\n         * @param key\n         * @throws Exception\n         */\n        public void handleAccept(SelectionKey key) throws Exception {\n            ServerSocketChannel serverSocketChannel = (ServerSocketChannel) key.channel();\n            SocketChannel socketChannel = serverSocketChannel.accept();\n            socketChannel.configureBlocking(false);\n            socketChannel.register(key.selector(), SelectionKey.OP_READ, ByteBuffer.allocateDirect(BUF_SIZE));\n        }\n\n        /**\n         * 读就绪处理\n         * @param key\n         * @throws Exception\n         */\n        public void handleRead(SelectionKey key) throws Exception {\n            SocketChannel socketChannel = (SocketChannel) key.channel();\n            ByteBuffer buf = (ByteBuffer) key.attachment();\n            long bytesRead = socketChannel.read(buf);\n            while (bytesRead > 0) {\n                buf.flip();\n                while (buf.hasRemaining()) {\n                    System.out.print((char) buf.get());\n                }\n                System.out.println();\n                buf.clear();\n                bytesRead = socketChannel.read(buf);\n            }\n            if (bytesRead == -1) {\n                socketChannel.close();\n            }\n        }\n\n        /**\n         * 写就绪处理\n         * @param key\n         * @throws Exception\n         */\n        public void handleWrite(SelectionKey key) throws Exception {\n            ByteBuffer buf = (ByteBuffer) key.attachment();\n            buf.flip();\n            SocketChannel socketChannel = (SocketChannel) key.channel();\n            while (buf.hasRemaining()) {\n                socketChannel.write(buf);\n            }\n            buf.compact();\n        }\n\n        /**\n         * 注册选择器\n         */\n        public void selector() {\n            Selector selector = null;\n            ServerSocketChannel serverSocketChannel = null;\n            try {\n                // 打开选择器\n                selector = Selector.open();\n                // 打开ServerSocketChannel通道\n                serverSocketChannel = ServerSocketChannel.open();\n                serverSocketChannel.socket().bind(new InetSocketAddress(PORT));\n                // 设置非阻塞模式\n                serverSocketChannel.configureBlocking(false);\n                // 注册 监听接收\n                serverSocketChannel.register(selector, SelectionKey.OP_ACCEPT);\n                while (true) {\n                    if (selector.select(TIMEOUT) == 0) {\n                        System.out.println(\"未检测到有通道就绪\");\n                        continue;\n                    }\n                    Iterator<SelectionKey> iter = selector.selectedKeys().iterator();\n                    while (iter.hasNext()) {\n                        SelectionKey key = iter.next();\n                        if (key.isAcceptable()) {\n                            // 接收就绪\n                            handleAccept(key);\n                        }\n                        if (key.isReadable()) {\n                            // 读就绪\n                            handleRead(key);\n                        }\n                        if (key.isWritable() && key.isValid()) {\n                            // 写就绪\n                            handleWrite(key);\n                        }\n                        if (key.isConnectable()) {\n                            // 连接就绪\n                            System.out.println(\"isConnectable = true\");\n                        }\n                        iter.remove();\n                    }\n                }\n            } catch (Exception e) {\n                e.printStackTrace();\n            } finally {\n                try {\n                    if (selector != null) {\n                        selector.close();\n                    }\n                    if (serverSocketChannel != null) {\n                        serverSocketChannel.close();\n                    }\n                } catch (Exception e) {\n                    e.printStackTrace();\n                }\n            }\n        }\n    }\n\n    static class ClientSelector {\n\n        /*标识数字*/\n        private static int flag = 0;\n        /*缓冲区大小*/\n        private static int BLOCK = 4096;\n        /*接受数据缓冲区*/\n        private static ByteBuffer sendbuffer = ByteBuffer.allocate(BLOCK);\n        /*发送数据缓冲区*/\n        private static ByteBuffer receivebuffer = ByteBuffer.allocate(BLOCK);\n        /*服务器端地址*/\n        private final static InetSocketAddress SERVER_ADDRESS = new InetSocketAddress(\n                \"localhost\", 8888);\n    \n        // public static void main(String[] args) {\n        //     selector();\n        // }\n        public void selector() throws Exception{\n            // 打开socket通道\n            SocketChannel socketChannel = SocketChannel.open();\n            // 设置为非阻塞方式\n            socketChannel.configureBlocking(false);\n            // 打开选择器\n            Selector selector = Selector.open();\n            // 注册连接服务端socket动作\n            socketChannel.register(selector, SelectionKey.OP_CONNECT);\n            // 连接\n            socketChannel.connect(SERVER_ADDRESS);\n    \n            int count = 0;\n            while (true) {\n                //选择一组键，其相应的通道已为 I/O 操作准备就绪。\n                //此方法执行处于阻塞模式的选择操作。\n                int selctorCount = selector.select();\n                if(selctorCount <= 0) continue;\n                //返回此选择器的已选择键集。\n                Set<SelectionKey> selectionKeys = selector.selectedKeys();\n                //System.out.println(selectionKeys.size());\n                Iterator<SelectionKey> iterator = selectionKeys.iterator();\n                while (iterator.hasNext()) {\n                    SelectionKey selectionKey = iterator.next();\n                    if (selectionKey.isConnectable()) {\n                        System.out.println(\"client connect\");\n                        SocketChannel client = (SocketChannel) selectionKey.channel();\n                        // 判断此通道上是否正在进行连接操作。\n                        // 完成套接字通道的连接过程。\n                        if (client.isConnectionPending()) {\n                            client.finishConnect();\n                            System.out.println(\"完成连接!\");\n                            sendbuffer.clear();\n                            sendbuffer.put(\"Hello,Server\".getBytes());\n                            sendbuffer.flip();\n                            client.write(sendbuffer);\n                        }\n                        client.register(selector, SelectionKey.OP_READ);\n                    } else if (selectionKey.isReadable()) {\n                        SocketChannel client = (SocketChannel) selectionKey.channel();\n                        //将缓冲区清空以备下次读取\n                        receivebuffer.clear();\n                        //读取服务器发送来的数据到缓冲区中\n                        count = client.read(receivebuffer);\n                        if(count > 0){\n                            String receiveText = new String( receivebuffer.array(),0,count);\n                            System.out.println(\"客户端接受服务器端数据--:\"+receiveText);\n                            client.register(selector, SelectionKey.OP_WRITE);\n                        }\n    \n                    } else if (selectionKey.isWritable()) {\n                        sendbuffer.clear();\n                        SocketChannel client = (SocketChannel) selectionKey.channel();\n                        String sendText = \"message from client--\" + (flag++);\n                        sendbuffer.put(sendText.getBytes());\n                        //将缓冲区各标志复位,因为向里面put了数据标志被改变要想从中读取数据发向服务器,就要复位\n                        sendbuffer.flip();\n                        client.write(sendbuffer);\n                        System.out.println(\"客户端向服务器端发送数据--：\"+sendText);\n                        client.register(selector, SelectionKey.OP_READ);\n                    }\n                }\n                selectionKeys.clear();\n            }\n        }\n    }\n\n```\n\n> [NIO之Selector选择器](https://www.cnblogs.com/snailclimb/p/9086334.html)\n\n### Netty\n\n> [Netty demo](https://github.com/xmxe/demo/tree/master/study-demo/src/main/java/com/xmxe/jdkfeature/nio/netty)\n> [让Netty“榨干”你的CPU](https://mp.weixin.qq.com/s/uMak8HmIyl78hcEUtovAvg)\n> [Netty简易实战，傻瓜都能看懂！](//https://mp.weixin.qq.com/s/W-KZFn40FnwIksP1zQP4RQ)\n> [Java NIO？看这一篇就够了！](https://blog.csdn.net/forezp/article/details/88414741)\n> [NIO系列教程](https://ifeve.com/overview/)\n> [Netty底层的IO模型是什么？](https://mp.weixin.qq.com/s/jpWIhws9b2ASD5k6A1pBvg)\n","categories":["Java"]},{"title":"计算机网络","slug":"计算机网络","url":"/blog/posts/32a593b381de/","content":"\n\n## TCP与UDP\n\n### TCP与UDP的区别\n\n1. **是否面向连接**：UDP在传送数据之前不需要先建立连接。而TCP提供面向连接的服务，在传送数据之前必须先建立连接，数据传送结束后要释放连接。\n2. **是否是可靠传输**：远地主机在收到UDP报文后，不需要给出任何确认，并且不保证数据不丢失，不保证是否顺序到达。TCP提供可靠的传输服务，TCP在传递数据之前，会有三次握手来建立连接，而且在数据传递时，有确认、窗口、重传、拥塞控制机制。通过TCP连接传输的数据，无差错、不丢失、不重复、并且按序到达。\n3. **是否有状态**：这个和上面的“是否可靠传输”相对应。TCP传输是有状态的，这个有状态说的是TCP会去记录自己发送消息的状态比如消息是否发送了、是否被接收了等等。为此，TCP需要维持复杂的连接状态表。而UDP是无状态服务，简单来说就是不管发出去之后的事情了\n4. **传输效率**：由于使用TCP进行传输的时候多了连接、确认、重传等机制，所以TCP的传输效率要比UDP低很多。\n5. **传输形式**：TCP是面向字节流的，UDP是面向报文的。\n6. **首部开销**：TCP首部开销（20～60字节）比UDP首部开销（8字节）要大。\n7. **是否提供广播或多播服务**：TCP只支持点对点通信，UDP支持一对一、一对多、多对一、多对多；\n8. ......\n\n\n| 区别                   | TCP            | UDP        |\n| ---------------------- | -------------- | ---------- |\n| 是否面向连接           | 是             | 否         |\n| 是否可靠               | 是             | 否         |\n| 是否有状态             | 是             | 否         |\n| 传输效率               | 较慢           | 较快       |\n| 传输形式               | 字节流         | 数据报文段 |\n| 首部开销               | 20 ～ 60 bytes | 8 bytes    |\n| 是否提供广播或多播服务 | 否             | 是         |\n\n### 什么时候选择TCP,什么时候选UDP?\n\n- **UDP一般用于即时通信**，比如：语音、视频、直播等等。这些场景对传输数据的准确性要求不是特别高，比如你看视频即使少个一两帧，实际给人的感觉区别也不大。\n- **TCP用于对传输准确性要求特别高的场景**，比如文件传输、发送和接收邮件、远程登录等等。\n\n### HTTP基于TCP还是UDP？\n\n**HTTP协议是基于TCP协议的**，所以发送HTTP请求之前首先要建立TCP连接也就是要经历3次握手。\n\n🐛修正（参见[issue#1915](https://github.com/Snailclimb/JavaGuide/issues/1915)）：HTTP3.0之前是基于TCP协议的，而HTTP3.0将弃用TCP，改用基于UDP的QUIC协议。此变化主要为了解决HTTP/2中存在的队头阻塞问题。由于HTTP/2在单个TCP连接上使用了多路复用，受到TCP拥塞控制的影响，少量的丢包就可能导致整个TCP连接上的所有流被阻塞。\n\n### 使用TCP的协议有哪些?使用UDP的协议有哪些?\n\n**运行于TCP协议之上的协议**：\n\n1. **HTTP协议**：超文本传输协议(HTTP，HyperTextTransferProtocol)主要是为Web浏览器与Web服务器之间的通信而设计的。当我们使用浏览器浏览网页的时候，我们网页就是通过HTTP请求进行加载的。\n2. **HTTPS协议**：更安全的超文本传输协议(HTTPS,HypertextTransferProtocolSecure)，身披SSL外衣的HTTP协议\n3. **FTP协议**：文件传输协议FTP（FileTransferProtocol），提供文件传输服务，基于TCP实现可靠的传输。使用FTP传输文件的好处是可以屏蔽操作系统和文件存储方式。\n4. **SMTP协议**：简单邮件传输协议（SMTP，SimpleMailTransferProtocol）的缩写，基于TCP协议，用来发送电子邮件。注意⚠️：接受邮件的协议不是SMTP而是POP3协议。\n5. **POP3/IMAP协议**：POP3和IMAP两者都是负责邮件接收的协议。\n6. **Telnet协议**：远程登陆协议，通过一个终端登陆到其他服务器。被一种称为SSH的非常安全的协议所取代。\n7. **SSH协议**:SSH（SecureShell）是目前较可靠，专为远程登录会话和其他网络服务提供安全性的协议。利用SSH协议可以有效防止远程管理过程中的信息泄露问题。SSH建立在可靠的传输协议TCP之上。\n8. ......\n\n**运行于UDP协议之上的协议**：\n\n1. **DHCP协议**：动态主机配置协议，动态配置IP地址\n2. **DNS**：域名系统（DNS，DomainNameSystem）将人类可读的域名(例如，www.baidu.com)转换为机器可读的IP地址(例如，220.181.38.148)。我们可以将其理解为专为互联网设计的电话薄。实际上DNS同时支持UDP和TCP协议。\n\n\n## TCP三次握手与四次挥手\n\n### 建立连接-TCP三次握手\n\n![TCP三次握手图解](https://oss.javaguide.cn/github/javaguide/cs-basics/network/tcp-shakes-hands-three-times.png)\n\n建立一个TCP连接需要“三次握手”，缺一不可：\n\n- **一次握手**:客户端发送带有SYN（SEQ=x）标志的数据包->服务端，然后客户端进入**SYN_SEND**状态，等待服务器的确认；\n- **二次握手**:服务端发送带有SYN+ACK(SEQ=y,ACK=x+1)标志的数据包–>客户端,然后服务端进入**SYN_RECV**状态\n- **三次握手**:客户端发送带有ACK(ACK=y+1)标志的数据包–>服务端，然后客户端和服务器端都进入**ESTABLISHED**状态，完成TCP三次握手。\n\n当建立了3次握手之后，客户端和服务端就可以传输数据啦！\n\n#### 为什么要三次握手?\n\n三次握手的目的是建立可靠的通信信道，说到通讯，简单来说就是数据的发送与接收，而三次握手最主要的目的就是双方确认自己与对方的发送与接收是正常的。\n\n1. **第一次握手**：Client什么都不能确认；Server确认了对方发送正常，自己接收正常\n2. **第二次握手**：Client确认了：自己发送、接收正常，对方发送、接收正常；Server确认了：对方发送正常，自己接收正常\n3. **第三次握手**：Client确认了：自己发送、接收正常，对方发送、接收正常；Server确认了：自己发送、接收正常，对方发送、接收正常\n\n三次握手就能确认双方收发功能都正常，缺一不可。\n\n> 更详细的解答可以看这个：[TCP为什么是三次握手，而不是两次或四次？-车小胖的回答-知乎](https://www.zhihu.com/question/24853633/answer/115173386)。\n\n#### 第2次握手传回了ACK，为什么还要传回SYN？\n\n服务端传回发送端所发送的ACK是为了告诉客户端：“我接收到的信息确实就是你所发送的信号了”，这表明从客户端到服务端的通信是正常的。回传SYN则是为了建立并确认从服务端到客户端的通信。\n\n> SYN同步序列编号(SynchronizeSequenceNumbers)是TCP/IP建立连接时使用的握手信号。在客户机和服务器之间建立正常的TCP网络连接时，客户机首先发出一个SYN消息，服务器使用SYN-ACK应答表示接收到了这个消息，最后客户机再以ACK(Acknowledgement)消息响应。这样在客户机和服务器之间才能建立起可靠的TCP连接，数据才可以在客户机和服务器之间传递。\n\n### 断开连接-TCP四次挥手\n\n![TCP四次挥手图解](https://oss.javaguide.cn/github/javaguide/cs-basics/network/tcp-waves-four-times.png)\n\n断开一个TCP连接则需要“四次挥手”，缺一不可：\n\n1. **第一次挥手**：客户端发送一个FIN（SEQ=X）标志的数据包->服务端，用来关闭客户端到服务器的数据传送。然后，客户端进入**FIN-WAIT-1**状态。\n2. **第二次挥手**：服务器收到这个FIN（SEQ=X）标志的数据包，它发送一个ACK（SEQ=X+1）标志的数据包->客户端。然后，此时服务端进入**CLOSE-WAIT**状态，客户端进入**FIN-WAIT-2**状态。\n3. **第三次挥手**：服务端关闭与客户端的连接并发送一个FIN(SEQ=y)标志的数据包->客户端请求关闭连接，然后，服务端进入**LAST-ACK**状态。\n4. **第四次挥手**：客户端发送ACK(SEQ=y+1)标志的数据包->服务端并且进入**TIME-WAIT**状态，服务端在收到ACK(SEQ=y+1)标志的数据包后进入**CLOSE**状态。此时，如果客户端等待2MSL后依然没有收到回复，就证明服务端已正常关闭，随后，客户端也可以关闭连接了。\n\n只要四次挥手没有结束，客户端和服务端就可以继续传输数据！\n\n#### 为什么要四次挥手？\n\nTCP是全双工通信，可以双向传输数据。任何一方都可以在数据传送结束后发出连接释放的通知，待对方确认后进入半关闭状态。当另一方也没有数据再发送的时候，则发出连接释放通知，对方确认后就完全关闭了TCP连接。\n\n举个例子：A和B打电话，通话即将结束后。\n\n1. **第一次挥手**：A说“我没啥要说的了”\n2. **第二次挥手**：B回答“我知道了”，但是B可能还会有要说的话，A不能要求B跟着自己的节奏结束通话\n3. **第三次挥手**：于是B可能又巴拉巴拉说了一通，最后B说“我说完了”\n4. **第四次挥手**：A回答“知道了”，这样通话才算结束。\n\n#### 为什么不能把服务器发送的ACK和FIN合并起来，变成三次挥手？\n\n因为服务器收到客户端断开连接的请求时，可能还有一些数据没有发完，这时先回复ACK，表示接收到了断开连接的请求。等到数据发完之后再发FIN，断开服务器到客户端的数据传送。\n\n#### 如果第二次挥手时服务器的ACK没有送达客户端，会怎样？\n\n客户端没有收到ACK确认，会重新发送FIN请求。\n\n#### 为什么第四次挥手客户端需要等待2*MSL（报文段最长寿命）时间后才进入CLOSED状态？\n\n第四次挥手时，客户端发送给服务器的ACK有可能丢失，如果服务端因为某些原因而没有收到ACK的话，服务端就会重发FIN，如果客户端在2*MSL的时间内收到了FIN，就会重新发送ACK并再次等待2MSL，防止Server没有收到ACK而不断重发FIN。\n\n> **MSL(MaximumSegmentLifetime)**:一个片段在网络中最大的存活时间，2MSL就是一个发送和一个回复所需的最大时间。如果直到2MSL，Client都没有再次收到FIN，那么Client推断ACK已经被成功接收，则结束TCP连接。\n\n### 总结\n\n#### TCP协议三次握手\n\n**第一次握手**：客户端先向服务端发送一个请求连接的报文段，这个报文段SYN位设置为1,序列号Seq(Sequence Number)设置为某一值，假设为X，发送出去之后客户端进入SYN_SEND状态，等待服务器的确认\n**第二次握手**：服务器收到SYN报文段。服务器收到客户端的SYN报文段，需要对这个SYN报文段进行确认，设置Acknowledgment Number为x+1(Sequence Number+1)；同时，自己自己还要发送SYN请求信息，将SYN位置为1，Sequence Number为y。服务器端将上述所有信息放到一个报文段（即SYN+ACK报文段）中，一并发送给客户端，此时服务器进入SYN_RECV状态\n**第三次握手**：客户端收到服务器的SYN+ACK报文段。然后将Acknowledgment Number设置为y+1，向服务器发送ACK报文段，这个报文段发送完毕以后，客户端和服务器端都进入ESTABLISHED状态，完成TCP三次握手。\n\n#### 为什么要三次握手而不是两次?\n\n为了防止已失效的连接请求报文段突然又传送到了服务端，因而产生错误。如下：\n\n> “已失效的连接请求报文段”的产生在这样一种情况下：client发出的第一个连接请求报文段并没有丢失，而是在某个网络结点长时间的滞留了，以致延误到连接释放以后的某个时间才到达server。本来这是一个早已失效的报文段。但server收到此失效的连接请求报文段后，就误认为是client再次发出的一个新的连接请求。于是就向client发出确认报文段，同意建立连接。\n>\n> 假设不采用“三次握手”，那么只要server发出确认，新的连接就建立了。由于现在client并没有发出建立连接的请求，因此不会理睬server的确认，也不会向server发送数据。但server却以为新的运输连接已经建立，并一直等待client发来数据。这样，server的很多资源就白白浪费掉了。\n>\n> 采用“三次握手”的办法可以防止上述现象发生。例如刚才那种情况，client不会向server的确认发出确认。server由于收不到确认，就知道client并没有要求建立连接。”，防止了服务器端的一直等待而浪费资源。\n> 完成了三次握手，客户端和服务器端就可以开始传送数据\n\n#### 第三次握手失败了怎么办?\n\n在tcp三次握手中,第二次握手完成后connect,就成功返回了.如果第三次握手的ack包丢了,此时客户端已认为连接是成功的,如果没有应用层的心跳包,客户端会一直维护这个连接,请问如何避免这种情况？\n第二次握手服务器收到SYN包,然后发出SYN+ACK数据包确认收到并且请求建立连接，服务器进入SYN_RECV状态。而这个时候第三次握手时客户端发送ACK给服务器失败了，服务器没办法进入ESTABLISH状态，这个时候肯定不能传输数据的，不论客户端主动发送数据与否，服务器都会有定时器发送第二步SYN+ACK数据包，如果客户端再次发送ACK成功，建立连接。\n如果一直不成功，服务器肯定会有超时（大概64s）设置，超时之后会给客户端发「RTS报文」(连接重置)，进入CLOSED状态，防止SYN洪泛攻击，这个时候客户端应该也会关闭连接\n\n> SYN洪泛攻击:\n> SYN攻击利用的是TCP的三次握手机制，攻击端利用伪造的IP地址向被攻击端发出请求，而被攻击端发出的响应报文将永远发送不到目的地，那么「被攻击端在等待关闭这个连接的过程中消耗了资源」，如果有成千上万的这种连接，主机资源将被耗尽，从而达到攻击的目的。\n\n#### TCP协议四次分手\n\n**第一次分手**：主机1，设置序列号Seq(Sequence Number)和确认包ACK(Acknowledgment Number)，假设seq为x+2,ACK=y+1,再将FIN标志位设置为1,向主机2发送FIN报文段；之后主机1进入FIN_WAIT_1状态；这表示主机1没有数据要发送给主机2了；\n**第二次分手**：主机2收到了主机1发送的FIN报文段，向主机1回一个ACK报文段(其值为接收到的FIN报文的seq值+1)；主机1进入FIN_WAIT_2状态,等待主机二的断开请求包FIN；\n**第三次分手**：主机2向主机1发送FIN报文段，意思是我可以断开连接了,请求关闭连接，同时主机2进入CLOSE_WAIT状态；\n**第四次分手**：主机1收到主机2发送的FIN报文段，向主机2发送ACK报文段,值为刚刚接收到的FIN包Seq值+1，然后主机1进入TIME_WAIT状态；主机2收到主机1的ACK报文段以后，就关闭连接；此时，主机1等待2MSL后依然没有收到回复，则证明Server端已正常关闭，那好，主机1也可以关闭连接了。\n\n#### 为什么要四次挥手\n\nTCP是全双工模式，这就意味着，当主机1发出FIN报文段时，只是表示主机1已经没有数据要发送了，主机1告诉主机2，它的数据已经全部发送完毕了；但是，这个时候主机1还是可以接受来自主机2的数据；当主机2返回ACK报文段时，表示它已经知道主机1没有数据发送了，但是主机2还是可以发送数据到主机1的；当主机2也发送了FIN报文段时，这个时候就表示主机2也没有数据要发送了，就会告诉主机1，我也没有数据要发送了，之后彼此就会愉快的中断这次TCP连接。如果要正确的理解四次分手的原理，就需要了解四次分手过程中的状态变化。\n\n#### 四次挥手状态解释\n\nFIN_WAIT_1:这个状态要好好解释一下，其实FIN_WAIT_1和FIN_WAIT_2状态的真正含义都是表示等待对方的FIN报文。而这两种状态的区别是：FIN_WAIT_1状态实际上是当SOCKET在ESTABLISHED状态时，它想主动关闭连接，向对方发送了FIN报文，此时该SOCKET即进入到FIN_WAIT_1状态。「也就是,发出FIN包之后进入FIN_WAIT_1状态」\n\n而当对方回应ACK报文后，则进入到FIN_WAIT_2状态，当然在实际的正常情况下，无论对方何种情况下，都应该马上回应ACK报文，所以FIN_WAIT_1状态一般是比较难见到的，而FIN_WAIT_2状态还有时常常可以用netstat看到。「也就是,发出ACK报文之后进入FIN_WAIT_2状态」最新面试题整理好了，大家可以在Java面试库小程序在线刷题。\n\n主动方FIN_WAIT_2：上面已经详细解释了这种状态，实际上FIN_WAIT_2状态下的SOCKET，表示半连接，也即有一方要求close连接，但另外还告诉对方，我暂时还有点数据需要传送给你(ACK信息)，稍后再关闭连接。\n\n主动方CLOSE_WAIT：这种状态的含义其实是表示在等待关闭。怎么理解呢？当对方close一个SOCKET后发送FIN报文给自己，你系统毫无疑问地会回应一个ACK报文给对方，此时则进入到CLOSE_WAIT状态。接下来呢，实际上你真正需要考虑的事情是察看你是否还有数据发送给对方，如果没有的话，那么你也就可以close这个SOCKET，发送FIN报文给对方，也即关闭连接。所以你在CLOSE_WAIT状态下，需要完成的事情是等待你去关闭连接。\n\n被动方LAST_ACK:这个状态还是比较容易好理解的，它是被动关闭一方在发送FIN报文后，最后等待对方的ACK报文。当收到ACK报文后，也即可以进入到CLOSED可用状态了。「也就是接收到了对方的FIN包,自己发出了ACK以及FIN包之后的状态」\n\n被动方TIME_WAIT:表示收到了对方的FIN报文，并发送出了ACK报文，就等2MSL后即可回到CLOSED可用状态了。如果FIN_WAIT1状态下，收到了对方同时带FIN标志和ACK标志的报文时，可以直接进入到TIME_WAIT状态，而无须经过FIN_WAIT_2状态。想成为架构师，这份架构师图谱建议看看，少走弯路。\n\n主动方CLOSED:表示连接中断。\n\n## TCP如何保证传输的可靠性？\n\n1. **基于数据块传输**：应用数据被分割成TCP认为最适合发送的数据块，再传输给网络层，数据块被称为报文段或段。\n2. **对失序数据包重新排序以及去重**：TCP为了保证不发生丢包，就给每个包一个序列号，有了序列号能够将接收到的数据根据序列号排序，并且去掉重复序列号的数据就可以实现数据包去重。\n3. **校验和**:TCP将保持它首部和数据的检验和。这是一个端到端的检验和，目的是检测数据在传输过程中的任何变化。如果收到段的检验和有差错，TCP将丢弃这个报文段和不确认收到此报文段。\n4. **超时重传**:当发送方发送数据之后，它启动一个定时器，等待目的端确认收到这个报文段。接收端实体对已成功收到的包发回一个相应的确认信息（ACK）。如果发送端实体在合理的往返时延（RTT）内未收到确认消息，那么对应的数据包就被假设为[已丢失](https://zh.wikipedia.org/wiki/丢包)并进行重传。\n5. **流量控制**:TCP连接的每一方都有固定大小的缓冲空间，TCP的接收端只允许发送端发送接收端缓冲区能接纳的数据。当接收方来不及处理发送方的数据，能提示发送方降低发送的速率，防止包丢失。TCP使用的流量控制协议是可变大小的滑动窗口协议（TCP利用滑动窗口实现流量控制）。\n6. **拥塞控制**:当网络拥塞时，减少数据的发送。\n\n## TCP如何实现流量控制？\n\nTCP利用滑动窗口实现流量控制。流量控制是为了控制发送方发送速率，保证接收方来得及接收。接收方发送的确认报文中的窗口字段可以用来控制发送方窗口大小，从而影响发送方的发送速率。将窗口字段设置为0，则发送方不能发送数据。\n\n为什么需要流量控制?这是因为双方在通信的时候，发送方的速率与接收方的速率是不一定相等，如果发送方的发送速率太快，会导致接收方处理不过来。如果接收方处理不过来的话，就只能把处理不过来的数据存在接收缓冲区(ReceivingBuffers)里（失序的数据包也会被存放在缓存区里）。如果缓存区满了发送方还在狂发数据的话，接收方只能把收到的数据包丢掉。出现丢包问题的同时又疯狂浪费着珍贵的网络资源。因此，我们需要控制发送方的发送速率，让接收方与发送方处于一种动态平衡才好。这里需要注意的是（常见误区）：\n\n- 发送端不等同于客户端\n- 接收端不等同于服务端\n\nTCP为全双工(Full-Duplex,FDX)通信，双方可以进行双向通信，客户端和服务端既可能是发送端又可能是服务端。因此，两端各有一个发送缓冲区与接收缓冲区，两端都各自维护一个发送窗口和一个接收窗口。接收窗口大小取决于应用、系统、硬件的限制（TCP传输速率不能大于应用的数据处理速率）。通信双方的发送窗口和接收窗口的要求相同\n\n**TCP发送窗口可以划分成四个部分**：\n\n1. 已经发送并且确认的TCP段（已经发送并确认）；\n2. 已经发送但是没有确认的TCP段（已经发送未确认）；\n3. 未发送但是接收方准备接收的TCP段（可以发送）；\n4. 未发送并且接收方也并未准备接受的TCP段（不可发送）。\n\n**TCP发送窗口结构图示**：\n\n![TCP发送窗口结构](https://oss.javaguide.cn/github/javaguide/cs-basics/network/tcp-send-window.png)\n\n- **SND.WND**：发送窗口。\n- **SND.UNA**：SendUnacknowledged指针，指向发送窗口的第一个字节。\n- **SND.NXT**：SendNext指针，指向可用窗口的第一个字节。\n\n**可用窗口大小=SND.UNA+SND.WND-SND.NXT**。\n\n**TCP接收窗口可以划分成三个部分**：\n\n1. 已经接收并且已经确认的TCP段（已经接收并确认）；\n2. 等待接收且允许发送方发送TCP段（可以接收未确认）；\n3. 不可接收且不允许发送方发送TCP段（不可接收）。\n\n**TCP接收窗口结构图示**：\n\n![TCP接收窗口结构](https://oss.javaguide.cn/github/javaguide/cs-basics/network/tcp-receive-window.png)\n\n接收窗口的大小是根据接收端处理数据的速度动态调整的。如果接收端读取数据快，接收窗口可能会扩大。否则，它可能会缩小。另外，这里的滑动窗口大小只是为了演示使用，实际窗口大小通常会远远大于这个值。\n\n## TCP的拥塞控制是怎么实现的？\n\n在某段时间，若对网络中某一资源的需求超过了该资源所能提供的可用部分，网络的性能就要变坏。这种情况就叫拥塞。拥塞控制就是为了防止过多的数据注入到网络中，这样就可以使网络中的路由器或链路不致过载。拥塞控制所要做的都有一个前提，就是网络能够承受现有的网络负荷。拥塞控制是一个全局性的过程，涉及到所有的主机，所有的路由器，以及与降低网络传输性能有关的所有因素。相反，流量控制往往是点对点通信量的控制，是个端到端的问题。流量控制所要做到的就是抑制发送端发送数据的速率，以便使接收端来得及接收。\n\n![TCP的拥塞控制](https://oss.javaguide.cn/github/javaguide/cs-basics/network/tcp-congestion-control.png)\n\n为了进行拥塞控制，TCP发送方要维持一个拥塞窗口(cwnd) 的状态变量。拥塞控制窗口的大小取决于网络的拥塞程度，并且动态变化。发送方让自己的发送窗口取为拥塞窗口和接收方的接受窗口中较小的一个。\n\nTCP的拥塞控制采用了四种算法，即慢开始、拥塞避免、快重传和快恢复。在网络层也可以使路由器采用适当的分组丢弃策略（如主动队列管理AQM），以减少网络拥塞的发生。\n\n- **慢开始**： 慢开始算法的思路是当主机开始发送数据时，如果立即把大量数据字节注入到网络，那么可能会引起网络阻塞，因为现在还不知道网络的符合情况。经验表明，较好的方法是先探测一下，即由小到大逐渐增大发送窗口，也就是由小到大逐渐增大拥塞窗口数值。cwnd初始值为1，每经过一个传播轮次，cwnd加倍。\n- **拥塞避免**： 拥塞避免算法的思路是让拥塞窗口cwnd缓慢增大，即每经过一个往返时间RTT就把发送方的cwnd加1.\n- **快重传与快恢复**： 在TCP/IP中，快速重传和恢复（fastretransmitandrecovery，FRR）是一种拥塞控制算法，它能快速恢复丢失的数据包。没有FRR，如果数据包丢失了，TCP将会使用定时器来要求传输暂停。在暂停的这段时间内，没有新的或复制的数据包被发送。有了FRR，如果接收机接收到一个不按顺序的数据段，它会立即给发送机发送一个重复确认。如果发送机接收到三个重复确认，它会假定确认件指出的数据段丢失了，并立即重传这些丢失的数据段。有了FRR，就不会因为重传时要求的暂停被耽误。　当有单独的数据包丢失时，快速重传和恢复（FRR）能最有效地工作。当有多个数据信息包在某一段很短的时间内丢失时，它则不能很有效地工作。\n\n## ARQ协议了解吗?\n\n**自动重传请求**（AutomaticRepeat-reQuest，ARQ）是OSI模型中数据链路层和传输层的错误纠正协议之一。它通过使用确认和超时这两个机制，在不可靠服务的基础上实现可靠的信息传输。如果发送方在发送后一段时间之内没有收到确认信息（Acknowledgements，就是我们常说的ACK），它通常会重新发送，直到收到确认或者重试超过一定的次数。\n\nARQ包括停止等待ARQ协议和连续ARQ协议。\n\n### 停止等待ARQ协议\n\n停止等待协议是为了实现可靠传输的，它的基本原理就是每发完一个分组就停止发送，等待对方确认（回复ACK）。如果过了一段时间（超时时间后），还是没有收到ACK确认，说明没有发送成功，需要重新发送，直到收到确认后再发下一个分组；\n\n在停止等待协议中，若接收方收到重复分组，就丢弃该分组，但同时还要发送确认。\n\n**1)无差错情况:**\n\n发送方发送分组,接收方在规定时间内收到,并且回复确认.发送方再次发送。\n\n**2)出现差错情况（超时重传）:**\n\n停止等待协议中超时重传是指只要超过一段时间仍然没有收到确认，就重传前面发送过的分组（认为刚才发送过的分组丢失了）。因此每发送完一个分组需要设置一个超时计时器，其重传时间应比数据在分组传输的平均往返时间更长一些。这种自动重传方式常称为自动重传请求ARQ。另外在停止等待协议中若收到重复分组，就丢弃该分组，但同时还要发送确认。\n\n**3)确认丢失和确认迟到**\n\n- **确认丢失**：确认消息在传输过程丢失。当A发送M1消息，B收到后，B向A发送了一个M1确认消息，但却在传输过程中丢失。而A并不知道，在超时计时过后，A重传M1消息，B再次收到该消息后采取以下两点措施：1.丢弃这个重复的M1消息，不向上层交付。2.向A发送确认消息。（不会认为已经发送过了，就不再发送。A能重传，就证明B的确认消息丢失）。\n- **确认迟到**：确认消息在传输过程中迟到。A发送M1消息，B收到并发送确认。在超时时间内没有收到确认消息，A重传M1消息，B仍然收到并继续发送确认消息（B收到了2份M1）。此时A收到了B第二次发送的确认消息。接着发送其他数据。过了一会，A收到了B第一次发送的对M1的确认消息（A也收到了2份确认消息）。处理如下：1.A收到重复的确认后，直接丢弃。2.B收到重复的M1后，也直接丢弃重复的M1。\n\n### 连续ARQ协议\n\n连续ARQ协议可提高信道利用率。发送方维持一个发送窗口，凡位于发送窗口内的分组可以连续发送出去，而不需要等待对方确认。接收方一般采用累计确认，对按序到达的最后一个分组发送确认，表明到这个分组为止的所有分组都已经正确收到了。\n\n**优点**：信道利用率高，容易实现，即使确认丢失，也不必重传。\n\n**缺点**：不能向发送方反映出接收方已经正确收到的所有分组的信息。比如：发送方发送了5条消息，中间第三条丢失（3号），这时接收方只能对前两个发送确认。发送方无法知道后三个分组的下落，而只好把后三个全部重传一次。这也叫Go-Back-N（回退N），表示需要退回来重传已经发送过的N个消息\n\n## Http、Https\n\n### HTTP协议\n\n#### HTTP协议介绍\n\nHTTP协议，全称超文本传输协议（HypertextTransferProtocol）。顾名思义，HTTP协议就是用来规范超文本的传输，超文本，也就是网络上的包括文本在内的各式各样的消息，具体来说，主要是来规范浏览器和服务器端的行为的。并且，HTTP是一个无状态（stateless）协议，也就是说服务器不维护任何有关客户端过去所发请求的消息。这其实是一种懒政，有状态协议会更加复杂，需要维护状态（历史信息），而且如果客户或服务器失效，会产生状态的不一致，解决这种不一致的代价更高。\n\n#### HTTP协议通信过程\n\nHTTP是应用层协议，它以TCP（传输层）作为底层协议，默认端口为80.通信过程主要如下：\n\n1. 服务器在80端口等待客户的请求。\n2. 浏览器发起到服务器的TCP连接（创建套接字Socket）。\n3. 服务器接收来自浏览器的TCP连接。\n4. 浏览器（HTTP客户端）与Web服务器（HTTP服务器）交换HTTP消息。\n5. 关闭TCP连接。\n\n#### HTTP协议优点\n\n扩展性强、速度快、跨平台支持性好。\n\n### HTTPS协议\n\n#### HTTPS协议介绍\n\nHTTPS协议（HyperTextTransferProtocolSecure），是HTTP的加强安全版本。HTTPS是基于HTTP的，也是用TCP作为底层协议，并额外使用SSL/TLS协议用作加密和安全认证。默认端口号是443.\n\nHTTPS协议中，SSL通道通常使用基于密钥的加密算法，密钥长度通常是40比特或128比特。\n\n#### HTTPS协议优点\n\n保密性好、信任度高。\n\n### HTTPS的核心—SSL/TLS协议\n\nHTTPS之所以能达到较高的安全性要求，就是结合了SSL/TLS和TCP协议，对通信数据进行加密，解决了HTTP数据透明的问题。接下来重点介绍一下SSL/TLS的工作原理。\n\n#### SSL和TLS的区别？\n\n**SSL和TLS没有太大的区别**,SSL指安全套接字协议（SecureSocketsLayer），首次发布与1996年。SSL的首次发布其实已经是他的3.0版本，SSL1.0从未面世，SSL2.0则具有较大的缺陷（DROWN缺陷——DecryptingRSAwithObsoleteandWeakenedeNcryption）。很快，在1999年，SSL3.0进一步升级，新版本被命名为TLS1.0。因此，TLS是基于SSL之上的，但由于习惯叫法，通常把HTTPS中的核心加密协议混称为SSL/TLS。\n\n#### SSL/TLS的工作原理\n\n##### 非对称加密\n\nSSL/TLS的核心要素是非对称加密。非对称加密采用两个密钥——一个公钥，一个私钥。在通信时，私钥仅由解密者保存，公钥由任何一个想与解密者通信的发送者（加密者）所知。\n\n> 可以设想一个场景，在某个自助邮局，每个通信信道都是一个邮箱，每一个邮箱所有者都在旁边立了一个牌子，上面挂着一把钥匙：这是我的公钥，发送者请将信件放入我的邮箱，并用公钥锁好。\n>\n> 但是公钥只能加锁，并不能解锁。解锁只能由邮箱的所有者——因为只有他保存着私钥。\n>\n> 这样，通信信息就不会被其他人截获了，这依赖于私钥的保密性。\n\n非对称加密的公钥和私钥需要采用一种复杂的数学机制生成（密码学认为，为了较高的安全性，尽量不要自己创造加密方案）。公私钥对的生成算法依赖于单向陷门函数。\n\n> 单向函数：已知单向函数f，给定任意一个输入x，易计算输出y=f(x)；而给定一个输出y，假设存在f(x)=y，很难根据f来计算出x。\n>\n> 单向陷门函数：一个较弱的单向函数。已知单向陷门函数f，陷门h，给定任意一个输入x，易计算出输出y=f(x;h)；而给定一个输出y，假设存在f(x;h)=y，很难根据f来计算出x，但可以根据f和h来推导出x。\n\n上图就是一个单向函数（不是单项陷门函数），假设有一个绝世秘籍，任何知道了这个秘籍的人都可以把苹果汁榨成苹果，那么这个秘籍就是“陷门”了吧。在这里，函数f的计算方法相当于公钥，陷门h相当于私钥。公钥f是公开的，任何人对已有输入，都可以用f加密，而要想根据加密信息还原出原信息，必须要有私钥才行。\n\n##### 对称加密\n\n使用SSL/TLS进行通信的双方需要使用非对称加密方案来通信，但是非对称加密设计了较为复杂的数学算法，在实际通信过程中，计算的代价较高，效率太低，因此，SSL/TLS实际对消息的加密使用的是对称加密。\n\n> 对称加密：通信双方共享唯一密钥k，加解密算法已知，加密方利用密钥k加密，解密方利用密钥k解密，保密性依赖于密钥k的保密性。\n\n对称加密的密钥生成代价比公私钥对的生成代价低得多，那么有的人会问了，为什么SSL/TLS还需要使用非对称加密呢？因为对称加密的保密性完全依赖于密钥的保密性。在双方通信之前，需要商量一个用于对称加密的密钥。我们知道网络通信的信道是不安全的，传输报文对任何人是可见的，密钥的交换肯定不能直接在网络信道中传输。因此，使用非对称加密，对对称加密的密钥进行加密，保护该密钥不在网络信道中被窃听。这样，通信双方只需要一次非对称加密，交换对称加密的密钥，在之后的信息通信中，使用绝对安全的密钥，对信息进行对称加密，即可保证传输消息的保密性。\n\n##### 公钥传输的信赖性\n\nSSL/TLS介绍到这里，了解信息安全的朋友又会想到一个安全隐患，设想一个下面的场景：\n\n> 客户端C和服务器S想要使用SSL/TLS通信，由上述SSL/TLS通信原理，C需要先知道S的公钥，而S公钥的唯一获取途径，就是把S公钥在网络信道中传输。要注意网络信道通信中有几个前提：\n>\n> 1. 任何人都可以捕获通信包\n> 2. 通信包的保密性由发送者设计\n> 3. 保密算法设计方案默认为公开，而（解密）密钥默认是安全的\n>\n> 因此，假设S公钥不做加密，在信道中传输，那么很有可能存在一个攻击者A，发送给C一个诈包，假装是S公钥，其实是诱饵服务器AS的公钥。当C收获了AS的公钥（却以为是S的公钥），C后续就会使用AS公钥对数据进行加密，并在公开信道传输，那么A将捕获这些加密包，用AS的私钥解密，就截获了C本要给S发送的内容，而C和S二人全然不知。\n>\n> 同样的，S公钥即使做加密，也难以避免这种信任性问题，C被AS拐跑了！\n\n为了公钥传输的信赖性问题，第三方机构应运而生——证书颁发机构（CA，CertificateAuthority）。CA默认是受信任的第三方。CA会给各个服务器颁发证书，证书存储在服务器上，并附有CA的电子签名（见下节）。\n\n当客户端（浏览器）向服务器发送HTTPS请求时，一定要先获取目标服务器的证书，并根据证书上的信息，检验证书的合法性。一旦客户端检测到证书非法，就会发生错误。客户端获取了服务器的证书后，由于证书的信任性是由第三方信赖机构认证的，而证书上又包含着服务器的公钥信息，客户端就可以放心的信任证书上的公钥就是目标服务器的公钥。\n\n##### 数字签名\n\n好，到这一小节，已经是SSL/TLS的尾声了。上一小节提到了数字签名，数字签名要解决的问题，是防止证书被伪造。第三方信赖机构CA之所以能被信赖，就是靠数字签名技术。\n\n数字签名，是CA在给服务器颁发证书时，使用散列+加密的组合技术，在证书上盖个章，以此来提供验伪的功能。具体行为如下：\n\n> CA知道服务器的公钥，对证书采用散列技术生成一个摘要。CA使用CA私钥对该摘要进行加密，并附在证书下方，发送给服务器。\n>\n> 现在服务器将该证书发送给客户端，客户端需要验证该证书的身份。客户端找到第三方机构CA，获知CA的公钥，并用CA公钥对证书的签名进行解密，获得了CA生成的摘要。\n>\n> 客户端对证书数据（包含服务器的公钥）做相同的散列处理，得到摘要，并将该摘要与之前从签名中解码出的摘要做对比，如果相同，则身份验证成功；否则验证失败。\n\n总结来说，带有证书的公钥传输机制如下：\n\n1. 设有服务器S，客户端C，和第三方信赖机构CA。\n2. S信任CA，CA是知道S公钥的，CA向S颁发证书。并附上CA私钥对消息摘要的加密签名。\n3. S获得CA颁发的证书，将该证书传递给C。\n4. C获得S的证书，信任CA并知晓CA公钥，使用CA公钥对S证书上的签名解密，同时对消息进行散列处理，得到摘要。比较摘要，验证S证书的真实性。\n5. 如果C验证S证书是真实的，则信任S的公钥（在S证书中）。\n\n对于数字签名，我这里讲的比较简单，如果你没有搞清楚的话，强烈推荐你看看[数字签名及数字证书原理](https://www.bilibili.com/video/BV18N411X7ty/)这个视频，这是我看过最清晰的讲解。\n\n![img](https://oss.javaguide.cn/github/javaguide/image-20220321121814946.png)\n\n#### 总结https\n\n##### https加密过程\n\n服务器A、客户端B、授权机构CA\nA向CA申请证书，A把网址、签名等摘要发给CA，CA用自己的私钥加密A的摘要信息（公钥）生成证书，返回证书给A。客户端B访问服务器A，A返回证书(包含A的公钥)，B使用CA的公钥验证CA的签名，判断证书合法性。之后B使用A的公钥加密B的（随机数/密钥）发送给A，A使用自己的私钥解密得到B的（随机数/密钥），通过密钥对称加密要传输的数据\n**使用数字签名确保消息不可否认，使用数字证书对用户身份进行认证**\n\n那为啥要用第三方权威机构（Certificate Authority，简称 CA）私钥对摘要加密呢？\n\n因为摘要算法是公开的，中间人可以替换掉证书明文，再根据证书上的摘要算法计算出摘要后把证书上的摘要也给替换掉！这样client拿到证书后计算摘要发现一样，误以为此证书是合法就中招了。\n\n**为啥要先生成摘要再加密呢，不能直接加密？**\n\n因为使用非对称加密是非常耗时的。如果把整个证书内容都加密生成签名的话，客户端验验签也需要把签名解密，证书明文较长，客户端验签就需要很长的时间，而用摘要的话，会把内容很长的明文压缩成小得多的定长字符串，客户端验签的话就会快得多。\n\n\n> - **端口号**：HTTP默认是80，HTTPS默认是443。\n> - **URL前缀**：HTTP的URL前缀是`http://`，HTTPS的URL前缀是`https://`。\n> - **安全性和资源消耗**：HTTP协议运行在TCP之上，所有传输的内容都是明文，客户端和服务器端都无法验证对方的身份。HTTPS是运行在SSL/TLS之上的HTTP协议，SSL/TLS运行在TCP之上。所有传输的内容都经过加密，加密采用对称加密，但对称加密的密钥用服务器方的证书进行了非对称加密。所以说，HTTP安全性没有HTTPS高，但是HTTPS比HTTP耗费更多服务器资源。\n\n##### https相关文章\n\n- [看完这篇HTTPS，和面试官扯皮就没问题了](https://mp.weixin.qq.com/s/tOHSImjJKiSoGHxJ4pbNeQ)\n- [终于有人把HTTPS原理讲清楚了！](https://mp.weixin.qq.com/s/2_iPy3-qh_VuzBCLtOFGxQ)\n- [硬核！30张图解HTTP常见的面试题](https://mp.weixin.qq.com/s/JNXsBvX32kBQN3UlzfAKHg)\n- [漫画：什么是HTTPS协议？](https://mp.weixin.qq.com/s/7rn4ruT2KemKRJRUkCwyfA)\n- [一个故事讲完https](https://mp.weixin.qq.com/s/3M0CqFQP2q37XMpDHbjF9g)\n- [用了HTTPS就一定安全吗？](https://mp.weixin.qq.com/s/URdOsAiUn_65Zr_eG6CyxQ)\n- [18张图彻底弄懂HTTPS的原理！](https://mp.weixin.qq.com/s/B7bg0ggzGg4-k-M95-evaA)\n- [HTTP3到底是个什么鬼](https://mp.weixin.qq.com/s/-sD504t5Ebm0Ubwalos8DA)\n- [几幅图，拿下HTTPS关于加解密、加签验签的那些事](https://mp.weixin.qq.com/s/sYuvL9ucjyaUIhwkHllSsg)\n- [Web登录其实没那么简单](https://mp.weixin.qq.com/s/LhzWXV-xW9elL1TE2RKhDw)\n- [摘要、数字签名、数字证书区别](https://zhuanlan.zhihu.com/p/32754315)\n- [https原理](https://mp.weixin.qq.com/s/e4s6TuMJYMbBAv5bpqR79A)\n\n## RPC\n\nHTTP接口和RPC接口都是生产上常用的接口，顾名思义，HTTP接口使用基于HTTP协议的URL传参调用，而RPC接口则基于远程过程调用。RPC（即`Remote Procedure Call`，远程过程调用）和HTTP（`HyperText Transfer Protocol`，超文本传输协议），两者前者是一种方法，后者则是一种协议。两者都常用于实现服务，在这个层面最本质的区别是RPC服务主要工作在TCP协议之上（也可以在HTTP协议），而HTTP服务工作在HTTP协议之上。由于HTTP协议基于TCP协议，所以RPC服务天然比HTTP更轻量，效率更胜一筹。两者都是基于网络实现的，从这一点上，都是基于`Client/Server`架构。\n\n### RPC（Remote Procedure Call）服务\n\nRPC服务基本架构包含了四个核心的组件，分别是Client、Server、Clent Stub以及Server Stub。\n\n![图片](https://mmbiz.qpic.cn/sz_mmbiz_png/knmrNHnmCLFUiaP9RbCU7coo2ia2sDNQZ1m4Yf9QPEUQp7bCLJ8QN2xtJPUibyRtiaria4niblrxRjTbhhvGmO3c02bA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)\n\n- **Client （客户端）**：服务调用方。\n- **Server（服务端）**：服务提供方。\n- **Client Stub（客户端存根）**：存放服务端的地址消息，负责将客户端的请求参数打包成网络消息，然后通过网络发送给服务提供方。\n- **Server Stub（服务端存根）**：接收客户端发送的消息，再将客户端请求参数打包成网络消息，然后通过网络远程发送给服务方。\nRPC效率优势明显，在实际开发中，客户端和服务端在技术方案中约定客户端的调用参数和服务端的返回参数之后就可以各自开发，任何客户端只要按照接口定义的规范发送入参都可以调用该RPC服务，服务端也能按接口定义的规范出参返回计算结果。这样既实现了客户端和服务端之间的解耦，也使得RPC接口可以在多个项目中重复利用。\nRPC调用分为同步方式和异步方式。同步调用即客户端等待调用完成并返回结果；异步调用即客户端不等待调用执行完成返回结果，变成单向调用或者通过回调函数等待接收到返回结果的通知。\n\n### 流行的RPC框架\n\n目前流行的RPC框架有很多，下面介绍常见的三种。\n\n- **gRPC**：gRPC是Google公布的开源项目，基于HTTP2.0协议，并支持常见的众多编程语言。HTTP 2.0协议是基于二进制的HTTP协议的升级版本，gRPC底层使用了Netty框架。\n- **Thrift**：Thrift是Facebook的一个开源项目，主要是一个跨语言的服务开发框架。它有一个代码生成器来对它所定义的IDL文件自动生成服务代码框架。Thrift对于底层的RPC通讯都是透明的，用户只需要对其进行二次开发即可，省去了一系列重复的前置基础开发工作。\n- **Dubbo**：Dubbo是阿里集团开源的一个极为出名的RPC框架，在很多互联网公司和企业应用中广泛使用。协议和序列化框架都可以插拔是及其鲜明的特色。\n\n### RPC接口和HTTP接口的区别与联系\n\nRPC接口即相当于调用本地接口一样调用远程服务的接口；HTTP接口是基于http协议的post接口和get接口（等等，2.0版本协议子支持更多）。\n\n#### 传输协议\n\n- RPC：可以基于TCP协议，也可以基于HTTP协议。\n- HTTP：基于HTTP协议。\n\n#### 传输效率\n\n- RPC：使用自定义的TCP协议，可以让请求报文体积更小，或者使用HTTP2.0协议，也可以很好地减少报文体积，提高传输效率。\n- HTTP：如果时基于HTTP1.1的协议，请求中会包含很多无用的内容；如果是基于HTTP2.0，那么简单地封装一下还是可以作为一个RPC使用的，这时标准RPC框架更多是服务治理。\n\n#### 性能消耗\n\n- RPC：可以基于thrift实现高效的二进制传输\n- HTTP：大部分是通过json实现的，字节大小和序列化耗时都比thrift要更消耗性能\n\n#### 负载均衡\n\n- RPC：基本都自带了负载均衡策略\n- HTTP：需要配置Nginx，HAProxy实现\n\n#### 服务治理（下游服务新增，重启，下线时如何不影响上游调用者）\n\n- RPC：能做到自动通知，不影响上游\n- HTTP：需要事先通知，修改Nginx/HAProxy配置\n\n> RPC主要用于公司内部服务调用，性能消耗低，传输效率高，服务治理方便。HTTP主要用于对外的异构环境，浏览器调用，APP接口调用，第三方接口调用等等。\n\n### RPC和HTTP都可以用于实现远程过程调用，如何选择？\n\n- 从速度上看，RPC比HTTP更快，虽然底层都是TCP，但是http协议的信息往往比较臃肿，不过可以采用gzip压缩\n- 从难度上看，RPC实现较为复杂，http相对简单\n- 从灵活性上看，HTTP更胜一筹，因为它不关心实现细节，跨平台，跨语言\n\n**两者有不同的使用场景：**\n\n- 如果对效率要求更高，并且开发过程使用统一的技术栈，那么RPC还是不错的\n- 如果需要更加灵活，跨语言、跨平台，显然HTTP更合适\n\n再插一句，最近新兴的微服务概念更加强调独立、自治、灵活，而RPC限制较多。因此微服务框架中，一般都会采用HTTP的Restful服务，像在公司内部使用hsf协议，对接外部系统使用微服务。\n\n> [RPC核心，万变不离其宗](https://mp.weixin.qq.com/s/eyXZGlRGaqQ6EPNvGFAgBg)\n\n## 相关文章\n\n- [面试官：TCP为什么要三次握手与四次分手？大部分人答不上来！](https://mp.weixin.qq.com/s/EDsDatrzmY_w09b5s-HrVw)\n- [一文搞定UDP和TCP高频面试题！](https://zhuanlan.zhihu.com/p/108822858?utm_source=wechat_timeline&utm_medium=social&utm_oi=1040923520439672832&from=timeline)\n- [近两万字TCP硬核知识，教你吊打面试官！](https://mp.weixin.qq.com/s/0VtugnDC5V30EZrctpGwKA)\n- [万字长文带你解析23个问题TCP疑难杂症！](https://mp.weixin.qq.com/s/P103z3rEmKmSgqenjUi9lQ)\n- [15张图，了解一下TCP/IP必知也必会的10个问题](https://mp.weixin.qq.com/s/qf8L52VtGTzWcF0NB5Filg)\n- [网络篇：朋友面试之TCP/IP，回去等通知吧](https://mp.weixin.qq.com/s/ACQz-OZN-oitoMzdvI3RfA)\n- [三次握手和四次挥手到底是个什么鬼东西？](https://mp.weixin.qq.com/s/rT1O1wYoNEcznSGQ3s9FQA)\n- [带你了解TCP/IP，UDP，Socket之间关系](https://zhuanlan.zhihu.com/p/345263331)\n- [TCP、HTTP、Socket，傻傻分不清？](https://mp.weixin.qq.com/s/IDcnAAW-qHDqqH5jq8GcRg)\n- [理解TCP和UDP协议到底有啥区别？](https://mp.weixin.qq.com/s/rbl4WUA8lksfh4s9U9QW3g)\n- [HTTP协议一知半解？这篇帮你全面了解](https://mp.weixin.qq.com/s/Rh5BMzZxIYKsHzt-hliKMw)\n- [大白话告诉你TCP为什么需要三次握手四次挥手](https://mp.weixin.qq.com/s/tlWDBDu7UBeXaz1LUxA3Jg)\n- [能将三次握手理解到这个深度，面试官拍案叫绝！](https://mp.weixin.qq.com/s/upBMDQsc9NfpdAFGZ34O4A)\n- [字节一面：“为什么网络要分层？每一层的职责、包含哪些协议？”](https://mp.weixin.qq.com/s/Q_A9NGd-2_HKDJKRoMqz1w)\n- [TCP就没什么缺陷吗？](https://mp.weixin.qq.com/s/EFSYnqOZRAXfHReHTqS5vw)\n- [一台服务器最大并发tcp连接数多少？65535？](https://mp.weixin.qq.com/s/b_0Q1cAk_z9mYOK32sOz7w)\n- [36张图，一次性补全网络基础知识](https://mp.weixin.qq.com/s/cvX942T2L66fOh3intIsGQ)\n- [你应该知道的12道经典计算机网络面试题](https://mp.weixin.qq.com/s?__biz=MzkzODE3OTI0Ng==&amp;mid=2247491160&amp;idx=1&amp;sn=1ad5d5e6f9dc4f9a44f06114fe97e05c&amp;source=41#wechat_redirect)\n- [在浏览器输入URL回车后，会发生什么？](https://mp.weixin.qq.com/s/EzLPyNdqTMQvEZyFDhbOyw)\n- [说一下从url输入到返回请求的过程](https://mp.weixin.qq.com/s/j4zrIHnizAe_36qpjLwwiQ)\n- [如果女朋友突然问你DNS是个啥...](https://mp.weixin.qq.com/s/ZGYYHLAUHaaRzKwgKvTVTg)\n- [万字长文爆肝DNS协议！](https://mp.weixin.qq.com/s/1a9-mgpakjSNjurI_WJbZw)\n- [Java网络编程总结(精华版)](https://mp.weixin.qq.com/s/J2GUUQNsH_kWsVEszyWC7A)","tags":["计算机"]},{"title":"Zookeeper","slug":"ZooKeeper","url":"/blog/posts/dc0180d6f359/","content":"\n> [zookeeper demo](https://github.com/xmxe/demo/tree/master/study-demo/src/main/java/com/xmxe/zookeeper)\n\n## Zookeeper基本原理\n\n### ZooKeeper重要概念解读\n\n\n#### Data model（数据模型）\n\nZooKeeper数据模型采用层次化的多叉树形结构，每个节点上都可以存储数据，这些数据可以是数字、字符串或者是二级制序列。并且。每个节点还可以拥有N个子节点，最上层是根节点以“/”来代表。每个数据节点在ZooKeeper中被称为znode，它是ZooKeeper中数据的最小单元。并且，每个znode都一个唯一的路径标识。强调一句：**ZooKeeper主要是用来协调服务的，而不是用来存储业务数据的，所以不要放比较大的数据在znode上，ZooKeeper给出的上限是每个结点的数据大小最大是1M**。ZooKeeper节点路径标识方式和Unix文件系统路径非常相似，都是由一系列使用斜杠\"/\"进行分割的路径表示，开发人员可以向这个节点中写入数据，也可以在节点下面创建子节点。\n\n#### znode（数据节点）\n\n介绍了ZooKeeper树形数据模型之后，我们知道每个数据节点在ZooKeeper中被称为znode，它是ZooKeeper中数据的最小单元。你要存放的数据就放在上面，是你使用ZooKeeper过程中经常需要接触到的一个概念。\n\n##### znode4种类型\n\n我们通常是将znode分为4大类：\n\n- **持久（PERSISTENT）节点**：一旦创建就一直存在即使ZooKeeper集群宕机，直到将其删除。\n- **临时（EPHEMERAL）节点**：临时节点的生命周期是与客户端会话（session）绑定的，会话消失则节点消失。并且，临时节点只能做叶子节点，不能创建子节点。\n- **持久顺序（PERSISTENT_SEQUENTIAL）节点**：除了具有持久（PERSISTENT）节点的特性之外，子节点的名称还具有顺序性。比如`/node1/app0000000001`、`/node1/app0000000002`。\n- **临时顺序（EPHEMERAL_SEQUENTIAL）节点**：除了具备临时（EPHEMERAL）节点的特性之外，子节点的名称还具有顺序性。\n\n##### znode数据结构\n\n每个znode由2部分组成:\n\n- **stat**：状态信息\n- **data**：节点存放的数据的具体内容\n\n如下所示，我通过get命令来获取根目录下的dubbo节点的内容。（get命令在下面会介绍到）。\n\n```bash\n[zk: 127.0.0.1:2181(CONNECTED) 6] get /dubbo\n# 该数据节点关联的数据内容为空\nnull\n# 下面是该数据节点的一些状态信息，其实就是Stat对象的格式化输出\ncZxid = 0x2\nctime = Tue Nov 27 11:05:34 CST 2018\nmZxid = 0x2\nmtime = Tue Nov 27 11:05:34 CST 2018\npZxid = 0x3\ncversion = 1\ndataVersion = 0\naclVersion = 0\nephemeralOwner = 0x0\ndataLength = 0\nnumChildren = 1\n```\n\nStat类中包含了一个数据节点的所有状态信息的字段，包括事务ID-cZxid、节点创建时间-ctime和子节点个数-numChildren等等。\n\n| znode 状态信息 | 解释                                                         |\n| -------------- | ------------------------------------------------------------ |\n| cZxid          | create ZXID，即该数据节点被创建时的事务id                   |\n| ctime          | create time，即该节点的创建时间                              |\n| mZxid          | modified ZXID，即该节点最终一次更新时的事务id               |\n| mtime          | modified time，即该节点最后一次的更新时间                    |\n| pZxid          | 该节点的子节点列表最后一次修改时的事务id，只有子节点列表变更才会更新pZxid，子节点内容变更不会更新 |\n| cversion       | 子节点版本号，当前节点的子节点每次变化时值增加1             |\n| dataVersion    | 数据节点内容版本号，节点创建时为0，每更新一次节点内容(不管内容有无变化)该版本号的值增加1 |\n| aclVersion     | 节点的ACL版本号，表示该节点ACL信息变更次数               |\n| ephemeralOwner | 创建该临时节点的会话的sessionId；如果当前节点为持久节点，则ephemeralOwner=0 |\n| dataLength     | 数据节点内容长度                                             |\n| numChildren    | 当前节点的子节点个数                                         |\n\n#### 版本（version）\n\n在前面我们已经提到，对应于每个znode，ZooKeeper都会为其维护一个叫作Stat的数据结构，Stat中记录了这个znode的三个相关的版本：\n\n- **dataVersion**：当前znode节点的版本号\n- **cversion**：当前znode子节点的版本\n- **aclVersion**：当前znode的ACL的版本。\n\n#### ACL（权限控制）\n\nZooKeeper采用ACL（AccessControlLists）策略来进行权限控制，类似于UNIX文件系统的权限控制。对于znode操作的权限，ZooKeeper提供了以下5种：\n\n- **CREATE**:能创建子节点\n- **READ**：能获取节点数据和列出其子节点\n- **WRITE**:能设置/更新节点数据\n- **DELETE**:能删除子节点\n- **ADMIN**:能设置节点ACL的权限\n\n其中尤其需要注意的是，CREATE和DELETE这两种权限都是针对子节点的权限控制。对于身份认证，提供了以下几种方式：\n\n- **world**：默认方式，所有用户都可无条件访问。\n- **auth**:不使用任何id，代表任何已认证的用户。\n- **digest**:用户名:密码认证方式：*username:password*。\n- **ip**:对指定ip进行限制。\n\n#### Watcher（事件监听器）\n\nWatcher（事件监听器），是ZooKeeper中的一个很重要的特性。ZooKeeper允许用户在指定节点上注册一些Watcher，并且在一些特定事件触发的时候，ZooKeeper服务端会将事件通知到感兴趣的客户端上去，该机制是ZooKeeper实现分布式协调服务的重要特性。\n\n\n#### 会话（Session）\n\nSession可以看作是ZooKeeper服务器与客户端的之间的一个TCP长连接，通过这个连接，客户端能够通过心跳检测与服务器保持有效的会话，也能够向ZooKeeper服务器发送请求并接受响应，同时还能够通过该连接接收来自服务器的Watcher事件通知。Session有一个属性叫做：sessionTimeout，sessionTimeout代表会话的超时时间。当由于服务器压力太大、网络故障或是客户端主动断开连接等各种原因导致客户端连接断开时，只要在sessionTimeout规定的时间内能够重新连接上集群中任意一台服务器，那么之前创建的会话仍然有效。另外，在为客户端创建会话之前，服务端首先会为每个客户端都分配一个sessionID。由于sessionID是ZooKeeper会话的一个重要标识，许多与会话相关的运行机制都是基于这个sessionID的，因此，无论是哪台服务器为客户端分配的sessionID，都务必保证全局唯一。\n\n### ZooKeeper集群\n\n为了保证高可用，最好是以集群形态来部署ZooKeeper，这样只要集群中大部分机器是可用的（能够容忍一定的机器故障），那么ZooKeeper本身仍然是可用的。通常3台服务器就可以构成一个ZooKeeper集群了。ZooKeeper官方提供的架构图就是一个ZooKeeper集群整体对外提供服务。每一个Server代表一个安装ZooKeeper服务的服务器。组成ZooKeeper服务的服务器都会在内存中维护当前的服务器状态，并且每台服务器之间都互相保持着通信。集群间通过ZAB协议（ZooKeeperAtomicBroadcast）来保持数据的一致性。**最典型集群模式：Master/Slave模式（主备模式）**。在这种模式中，通常Master服务器作为主服务器提供写服务，其他的Slave服务器从服务器通过异步复制的方式获取Master服务器最新的数据提供读服务。\n\n#### ZooKeeper集群角色\n\n但是，在ZooKeeper中没有选择传统的Master/Slave概念，而是引入了Leader、Follower和Observer三种角色。ZooKeeper集群中的所有机器通过一个Leader选举过程来选定一台称为“Leader”的机器，Leader既可以为客户端提供写服务又能提供读服务。除了Leader外，Follower和Observer都只能提供读服务。Follower和Observer唯一的区别在于Observer机器不参与Leader的选举过程，也不参与写操作的“过半写成功”策略，因此Observer机器可以在不影响写性能的情况下提升集群的读性能。\n\n| 角色     | 说明                                                         |\n| -------- | ------------------------------------------------------------ |\n| Leader   | 为客户端提供读和写的服务，负责投票的发起和决议，更新系统状态。 |\n| Follower | 为客户端提供读服务，如果是写服务则转发给Leader。参与选举过程中的投票。 |\n| Observer | 为客户端提供读服务，如果是写服务则转发给Leader。不参与选举过程中的投票，也不参与“过半写成功”策略。在不影响写性能的情况下提升集群的读性能。此角色于ZooKeeper3.3系列新增的角色。 |\n\n#### ZooKeeper集群Leader选举过程\n\n当Leader服务器出现网络中断、崩溃退出与重启等异常情况时，就会进入Leader选举过程，这个过程会选举产生新的Leader服务器。\n\n这个过程大致是这样的：\n\n1. **Leaderelection（选举阶段）**：节点在一开始都处于选举阶段，只要有一个节点得到超半数节点的票数，它就可以当选准leader。\n2. **Discovery（发现阶段）**：在这个阶段，followers跟准leader进行通信，同步followers最近接收的事务提议。\n3. **Synchronization（同步阶段）**:同步阶段主要是利用leader前一阶段获得的最新提议历史，同步集群中所有的副本。同步完成之后准leader才会成为真正的leader。\n4. **Broadcast（广播阶段）**:到了这个阶段，ZooKeeper集群才能正式对外提供事务服务，并且leader可以进行消息广播。同时如果有新的节点加入，还需要对新节点进行同步。\n\n#### ZooKeeper集群中的服务器状态\n\n- **LOOKING**：寻找Leader。\n- **LEADING**：Leader状态，对应的节点为Leader。\n- **FOLLOWING**：Follower状态，对应的节点为Follower。\n- **OBSERVING**：Observer状态，对应节点为Observer，该节点不参与Leader选举。\n\n#### ZooKeeper集群为啥最好奇数台？\n\nZooKeeper集群在宕掉几个ZooKeeper服务器之后，如果剩下的ZooKeeper服务器个数大于宕掉的个数的话整个ZooKeeper才依然可用。假如我们的集群中有n台ZooKeeper服务器，那么也就是剩下的服务数必须大于n/2。先说一下结论，2n和2n-1的容忍度是一样的，都是n-1，大家可以先自己仔细想一想，这应该是一个很简单的数学问题了。比如假如我们有3台，那么最大允许宕掉1台ZooKeeper服务器，如果我们有4台的的时候也同样只允许宕掉1台。假如我们有5台，那么最大允许宕掉2台ZooKeeper服务器，如果我们有6台的的时候也同样只允许宕掉2台。综上，何必增加那一个不必要的ZooKeeper呢？\n\n#### ZooKeeper选举的过半机制防止脑裂\n\n**何为集群脑裂？**\n\n对于一个集群，通常多台机器会部署在不同机房，来提高这个集群的可用性。保证可用性的同时，会发生一种机房间网络线路故障，导致机房间网络不通，而集群被割裂成几个小集群。这时候子集群各自选主导致“脑裂”的情况。举例说明：比如现在有一个由6台服务器所组成的一个集群，部署在了2个机房，每个机房3台。正常情况下只有1个leader，但是当两个机房中间网络断开的时候，每个机房的3台服务器都会认为另一个机房的3台服务器下线，而选出自己的leader并对外提供服务。若没有过半机制，当网络恢复的时候会发现有2个leader。仿佛是1个大脑（leader）分散成了2个大脑，这就发生了脑裂现象。脑裂期间2个大脑都可能对外提供了服务，这将会带来数据一致性等问题。\n\n**过半机制是如何防止脑裂现象产生的？**\n\nZooKeeper的过半机制导致不可能产生2个leader，因为少于等于一半是不可能产生leader的，这就使得不论机房的机器如何分配都不可能发生脑裂。\n\n### ZAB协议和Paxos算法\n\nPaxos算法应该可以说是ZooKeeper的灵魂了。但是，ZooKeeper并没有完全采用Paxos算法，而是使用ZAB协议作为其保证数据一致性的核心算法。另外，在ZooKeeper的官方文档中也指出，ZAB协议并不像Paxos算法那样，是一种通用的分布式一致性算法，它是一种特别为Zookeeper设计的崩溃可恢复的原子消息广播算法。\n\n#### ZAB协议介绍\n\nZAB（ZooKeeperAtomicBroadcast原子广播）协议是为分布式协调服务ZooKeeper专门设计的一种支持崩溃恢复的原子广播协议。在ZooKeeper中，主要依赖ZAB协议来实现分布式数据一致性，基于该协议，ZooKeeper实现了一种主备模式的系统架构来保持集群中各个副本之间的数据一致性。\n\n#### ZAB协议两种基本的模式：崩溃恢复和消息广播\n\nZAB协议包括两种基本的模式，分别是\n\n- **崩溃恢复**：当整个服务框架在启动过程中，或是当Leader服务器出现网络中断、崩溃退出与重启等异常情况时，ZAB协议就会进入恢复模式并选举产生新的Leader服务器。当选举产生了新的Leader服务器，同时集群中已经有过半的机器与该Leader服务器完成了状态同步之后，ZAB协议就会退出恢复模式。其中，所谓的状态同步是指数据同步，用来保证集群中存在过半的机器能够和Leader服务器的数据状态保持一致。\n- **消息广播**：当集群中已经有过半的Follower服务器完成了和Leader服务器的状态同步，那么整个服务框架就可以进入消息广播模式了。当一台同样遵守ZAB协议的服务器启动后加入到集群中时，如果此时集群中已经存在一个Leader服务器在负责进行消息广播，那么新加入的服务器就会自觉地进入数据恢复模式：找到Leader所在的服务器，并与其进行数据同步，然后一起参与到消息广播流程中去。\n\n> 关于ZAB协议&Paxos算法需要讲和理解的东西太多了，具体可以看下面这两篇文章：\n> [图解Paxos一致性协议](http://codemacro.com/2014/10/15/explain-poxos/)\n> [ZookeeperZAB协议分析](https://dbaplus.cn/news-141-1875-1.html)\n\n\n### Zookeeper基本原理\n\n1. ZooKeeper分为服务器端（Server）和客户端（Client），客户端可以连接到整个ZooKeeper服务的任意服务器上（除非leaderServes参数被显式设置，leader不允许接受客户端连接）。\n2. 客户端使用并维护一个TCP连接，通过这个连接发送请求、接受响应、获取观察的事件以及发送心跳。如果这个TCP连接中断，客户端将自动尝试连接到另外的ZooKeeper服务器。客户端第一次连接到ZooKeeper服务时，接受这个连接的ZooKeeper服务器会为这个客户端建立一个会话。当这个客户端连接到另外的服务器时，这个会话会被新的服务器重新建立。\n3. 上图中每一个Server代表一个安装Zookeeper服务的机器，即是整个提供Zookeeper服务的集群（或者是由伪集群组成）；\n4. 组成ZooKeeper服务的服务器必须彼此了解。它们维护一个内存中的状态图像，以及持久存储中的事务日志和快照，只要大多数服务器可用，ZooKeeper服务就可用；\n5. ZooKeeper启动时，将从实例中选举一个leader，Leader负责处理数据更新等操作，一个更新操作成功的标志是当且仅当大多数Server在内存中成功修改数据。每个Server在内存中存储了一份数据。\n6. Zookeeper是可以集群复制的，集群间通过Zab协议（Zookeeper Atomic Broadcast）来保持数据的一致性；\n7. Zab协议包含两个阶段：**leader election阶段**和**Atomic Brodcast阶段**。\n    - 集群中将选举出一个leader，其他的机器则称为follower，所有的写操作都被传送给leader，并通过brodcast将所有的更新告诉给follower。\n    - 当leader崩溃或者leader失去大多数的follower时，需要重新选举出一个新的leader，让所有的服务器都恢复到一个正确的状态。\n    - 当leader被选举出来，且大多数服务器完成了和leader的状态同步后，leadder election的过程就结束了，就将会进入到Atomic brodcast的过程。\n    - Atomic Brodcast同步leader和follower之间的信息，保证leader和follower具有形同的系统状态。\n\n### Zookeeper角色\n\n启动Zookeeper服务器集群环境后，多个Zookeeper服务器在工作前会选举出一个Leader。选举出leader前，所有server不区分角色，都需要平等参与投票（obServer除外，不参与投票）；选主过程完成后，存在以下几种角色\n\n- 领导者（leader）:领导者负责进行投票的发起和决议，更新系统状态。\n- 学习者（Learner）或跟随者（Follower）:Follower用于接收客户请求并向客户端返回结果，在选主过程中参与投票。Follower可以接收client请求，如果是写请求将转发给leader来更新系统状态。\n- 观察者（ObServer）:ObServer可以接收客户端连接，将写请求转发给leader节点。但Observer不参加投票过程，只同步leader的状态，ObServer的目的是为了扩展系统，提高德取谏度。\n\n#### 为什么需要server?\n\n- ZooKeeper需保证高可用和强一致性;\n- 为了支持更多的客户端，需要增加更多的Server;\n- Follower增多会导致投票阶段延迟增大，影响性能\n\n#### 在Zookeeper中ObServer起到什么作用？\n- ObServer不参与投票过程，只同步leader的状态\n- Observers接受客户端的连接，并将写请求转发给leader节点\n- 加入更多ObServer节点，提高伸缩性，同时还不影响吞吐率\n\n#### 为什么在Zookeeper中Server数目一般为奇数？\n我们知道在Zookeeper中Leader选举算法采用了Zab协议。Zab核心思想是当多数Server写成功，则任务数据写成功。\n\n①如果有3个Server，则最多允许1个Server挂掉。\n\n②如果有4个Server，则同样最多允许1个Server挂掉。既然3个或者4个Server，同样最多允许1个Server挂掉，那么它们的可靠性是一样的，所以选择奇数个ZooKeeper Server即可，这里选择3个Server。\n\n### ZooKeeper的写数据流程\n\n1. Client向ZooKeeper的Server1上写数据，发送一个写请求。\n2. 如果Server1不是Leader，那么Server1会把接受到的请求进一步转发给Leader，因为每个ZooKeeper的Server里面有一个是Leader。这个Leader会将写请求广播给各个Server，比如Server1和Server2，各个Server写成功后就会通知Leader。\n3. 当Leader收到大多数Server数据写成功了，那么就说明数据写成功了。如果这里三个节点的话，只要有两个节点数据写成功了，那么就认为数据写成功了。写成功之后，Leader会告诉Server1数据写成功了。\n4. Server1会进一步通知Client数据写成功了，这时就认为整个写操作成功。\n\n\n### 总结\n\n1. ZooKeeper本身就是一个分布式程序（只要半数以上节点存活，ZooKeeper就能正常服务）。\n2. 为了保证高可用，最好是以集群形态来部署ZooKeeper，这样只要集群中大部分机器是可用的（能够容忍一定的机器故障），那么ZooKeeper本身仍然是可用的。\n3. ZooKeeper将数据保存在内存中，这也就保证了高吞吐量和低延迟（但是内存限制了能够存储的容量不太大，此限制也是保持znode中存储的数据量较小的进一步原因）。\n4. ZooKeeper是高性能的。在“读”多于“写”的应用程序中尤其地明显，因为“写”会导致所有的服务器间同步状态。（“读”多于“写”是协调服务的典型场景。）\n5. ZooKeeper有临时节点的概念。当创建临时节点的客户端会话一直保持活动，瞬时节点就一直存在。而当会话终结时，瞬时节点被删除。持久节点是指一旦这个znode被创建了，除非主动进行znode的移除操作，否则这个znode将一直保存在ZooKeeper上。\n6. ZooKeeper底层其实只提供了两个功能：①管理（存储、读取）用户程序提交的数据；②为用户程序提供数据节点监听服务。\n\n## Zookeeper相关概念进阶\n\n### 一致性问题\n\n设计一个分布式系统必定会遇到一个问题，因为分区容忍性（partition tolerance）的存在，就必定要求我们需要在系统可用性（availability）和数据一致性（consistency）中做出权衡。这就是著名的CAP定理。理解起来其实很简单，比如说把一个班级作为整个系统，而学生是系统中的一个个独立的子系统。这个时候班里的小红小明偷偷谈恋爱被班里的大嘴巴小花发现了，小花欣喜若狂告诉了周围的人，然后小红小明谈恋爱的消息在班级里传播起来了。当在消息的传播（散布）过程中，你抓到一个同学问他们的情况，如果回答你不知道，那么说明整个班级系统出现了数据不一致的问题（因为小花已经知道这个消息了）。而如果他直接不回答你，因为整个班级有消息在进行传播（为了保证一致性，需要所有人都知道才可提供服务），这个时候就出现了系统的可用性问题。而上述前者就是Eureka的处理方式，它保证了AP（可用性），后者就是我们今天所要讲的ZooKeeper的处理方式，它保证了CP（数据一致性）。\n\n### 一致性协议和算法\n\n而为了解决数据一致性问题，在科学家和程序员的不断探索中，就出现了很多的一致性协议和算法。比如2PC（两阶段提交），3PC（三阶段提交），Paxos算法等等。这时候请你思考一个问题，同学之间如果采用传纸条的方式去传播消息，那么就会出现一个问题——我咋知道我的小纸条有没有传到我想要传递的那个人手中呢？万一被哪个小家伙给劫持篡改了呢？这个时候就引申出一个概念——拜占庭将军问题。它意指在不可靠信道上试图通过消息传递的方式达到一致性是不可能的，所以所有的一致性算法的必要前提就是安全可靠的消息通道。而为什么要去解决数据一致性的问题？你想想，如果一个秒杀系统将服务拆分成了下订单和加积分服务，这两个服务部署在不同的机器上了，万一在消息的传播过程中积分系统宕机了，总不能你这边下了订单却没加积分吧？你总得保证两边的数据需要一致吧？\n\n#### 2PC（两阶段提交）\n\n两阶段提交是一种保证分布式系统数据一致性的协议，现在很多数据库都是采用的两阶段提交协议来完成分布式事务的处理。在介绍2PC之前，我们先来想想分布式事务到底有什么问题呢？还拿秒杀系统的下订单和加积分两个系统来举例吧，我们此时下完订单会发个消息给积分系统告诉它下面该增加积分了。如果我们仅仅是发送一个消息也不收回复，那么我们的订单系统怎么能知道积分系统的收到消息的情况呢？如果我们增加一个收回复的过程，那么当积分系统收到消息后返回给订单系统一个Response，但在中间出现了网络波动，那个回复消息没有发送成功，订单系统是不是以为积分系统消息接收失败了？它是不是会回滚事务？但此时积分系统是成功收到消息的，它就会去处理消息然后给用户增加积分，这个时候就会出现积分加了但是订单没下成功。所以我们所需要解决的是在分布式系统中，整个调用链中，我们所有服务的数据处理要么都成功要么都失败，即所有服务的原子性问题。\n\n在两阶段提交中，主要涉及到两个角色，分别是协调者和参与者。\n\n第一阶段：当要执行一个分布式事务的时候，事务发起者首先向协调者发起事务请求，然后协调者会给所有参与者发送prepare请求（其中包括事务内容）告诉参与者你们需要执行事务了，如果能执行我发的事务内容那么就先执行但不提交，执行后请给我回复。然后参与者收到prepare消息后，他们会开始执行事务（但不提交），并将Undo和Redo信息记入事务日志中，之后参与者就向协调者反馈是否准备好了。\n\n第二阶段：第二阶段主要是协调者根据参与者反馈的情况来决定接下来是否可以进行事务的提交操作，即提交事务或者回滚事务。比如这个时候所有的参与者都返回了准备好了的消息，这个时候就进行事务的提交，协调者此时会给所有的参与者发送Commit请求，当参与者收到Commit请求的时候会执行前面执行的事务的提交操作，提交完毕之后将给协调者发送提交成功的响应。而如果在第一阶段并不是所有参与者都返回了准备好了的消息，那么此时协调者将会给所有参与者发送回滚事务的rollback请求，参与者收到之后将会回滚它在第一阶段所做的事务处理，然后再将处理情况返回给协调者，最终协调者收到响应后便给事务发起者返回处理失败的结果。\n\n![2PC流程](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/1a7210167f1d4d4fb97afcec19902a59~tplv-k3u1fbpfcp-zoom-1.image)\n\n个人觉得2PC实现得还是比较鸡肋的，因为事实上它只解决了各个事务的原子性问题，随之也带来了很多的问题。\n\n- **单点故障问题**，如果协调者挂了那么整个系统都处于不可用的状态了。\n- **阻塞问题**，即当协调者发送prepare请求，参与者收到之后如果能处理那么它将会进行事务的处理但并不提交，这个时候会一直占用着资源不释放，如果此时协调者挂了，那么这些资源都不会再释放了，这会极大影响性能。\n- **数据不一致问题**，比如当第二阶段，协调者只发送了一部分的commit请求就挂了，那么也就意味着，收到消息的参与者会进行事务的提交，而后面没收到的则不会进行事务提交，那么这时候就会产生数据不一致性问题。\n\n#### 3PC（三阶段提交）\n\n因为2PC存在的一系列问题，比如单点，容错机制缺陷等等，从而产生了**3PC（三阶段提交）**。那么这三阶段又分别是什么呢？\n\n1. **CanCommit阶段**：协调者向所有参与者发送CanCommit请求，参与者收到请求后会根据自身情况查看是否能执行事务，如果可以则返回YES响应并进入预备状态，否则返回NO。\n2. **PreCommit阶段**：协调者根据参与者返回的响应来决定是否可以进行下面的PreCommit操作。如果上面参与者返回的都是YES，那么协调者将向所有参与者发送PreCommit预提交请求，参与者收到预提交请求后，会进行事务的执行操作，并将Undo和Redo信息写入事务日志中，最后如果参与者顺利执行了事务则给协调者返回成功的响应。如果在第一阶段协调者收到了任何一个NO的信息，或者在一定时间内并没有收到全部的参与者的响应，那么就会中断事务，它会向所有参与者发送中断请求（abort），参与者收到中断请求之后会立即中断事务，或者在一定时间内没有收到协调者的请求，它也会中断事务。\n3. **DoCommit阶段**：这个阶段其实和2PC的第二阶段差不多，如果协调者收到了所有参与者在PreCommit阶段的YES响应，那么协调者将会给所有参与者发送DoCommit请求，参与者收到DoCommit请求后则会进行事务的提交工作，完成后则会给协调者返回响应，协调者收到所有参与者返回的事务提交成功的响应之后则完成事务。若协调者在PreCommit阶段收到了任何一个NO或者在一定时间内没有收到所有参与者的响应，那么就会进行中断请求的发送，参与者收到中断请求后则会通过上面记录的回滚日志来进行事务的回滚操作，并向协调者反馈回滚状况，协调者收到参与者返回的消息后，中断事务。\n\n![3PC流程](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/80854635d48c42d896dbaa066abf5c26~tplv-k3u1fbpfcp-zoom-1.image)\n\n> 这里是3PC在成功的环境下的流程图，你可以看到3PC在很多地方进行了超时中断的处理，比如协调者在指定时间内为收到全部的确认消息则进行事务中断的处理，这样能减少同步阻塞的时间。还有需要注意的是，3PC在DoCommit阶段参与者如未收到协调者发送的提交事务的请求，它会在一定时间内进行事务的提交。为什么这么做呢？是因为这个时候我们肯定保证了在第一阶段所有的协调者全部返回了可以执行事务的响应，这个时候我们有理由相信其他系统都能进行事务的执行和提交，所以不管协调者有没有发消息给参与者，进入第三阶段参与者都会进行事务的提交操作。\n\n总之，3PC通过一系列的超时机制很好的缓解了阻塞问题，但是最重要的一致性并没有得到根本的解决，比如在PreCommit阶段，当一个参与者收到了请求之后其他参与者和协调者挂了或者出现了网络分区，这个时候收到消息的参与者都会进行事务提交，这就会出现数据不一致性问题。所以，要解决一致性问题还需要靠Paxos算法⭐️⭐️⭐️。\n\n#### Paxos算法\n\nPaxos算法是基于消息传递且具有高度容错特性的一致性算法，是目前公认的解决分布式一致性问题最有效的算法之一，其解决的问题就是在分布式系统中如何就某个值（决议）达成一致。在Paxos中主要有三个角色，分别为Proposer提案者、Acceptor表决者、Learner学习者。Paxos算法和2PC一样，也有两个阶段，分别为Prepare和accept阶段。\n\n##### prepare阶段\n\n- Proposer提案者：负责提出proposal，每个提案者在提出提案时都会首先获取到一个具有全局唯一性的、递增的提案编号N，即在整个集群中是唯一的编号N，然后将该编号赋予其要提出的提案，在第一阶段是只将提案编号发送给所有的表决者。\n- Acceptor表决者：每个表决者在accept某提案后，会将该提案编号N记录在本地，这样每个表决者中保存的已经被accept的提案中会存在一个编号最大的提案，其编号假设为maxN。每个表决者仅会accept编号大于自己本地maxN的提案，在批准提案时表决者会将以前接受过的最大编号的提案作为响应反馈给Proposer。\n\n![paxos第一阶段](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/cd1e5f78875b4ad6b54013738f570943~tplv-k3u1fbpfcp-zoom-1.image)\n\n##### accept阶段\n\n当一个提案被Proposer提出后，如果Proposer收到了超过半数的Acceptor的批准（Proposer本身同意），那么此时Proposer会给所有的Acceptor发送真正的提案（你可以理解为第一阶段为试探），这个时候Proposer就会发送提案的内容和提案编号。表决者收到提案请求后会再次比较本身已经批准过的最大提案编号和该提案编号，如果该提案编号大于等于已经批准过的最大提案编号，那么就accept该提案（此时执行提案内容但不提交），随后将情况返回给Proposer。如果不满足则不回应或者返回NO。\n\n![paxos第二阶段1](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/dad7f51d58b24a72b249278502ec04bd~tplv-k3u1fbpfcp-zoom-1.image)\n\n当Proposer收到超过半数的accept，那么它这个时候会向所有的acceptor发送提案的提交请求。需要注意的是，因为上述仅仅是超过半数的acceptor批准执行了该提案内容，其他没有批准的并没有执行该提案内容，所以这个时候需要向未批准的acceptor发送提案内容和提案编号并让它无条件执行和提交，而对于前面已经批准过该提案的acceptor来说仅仅需要发送该提案的编号，让acceptor执行提交就行了。\n\n![paxos第二阶段2](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/9359bbabb511472e8de04d0826967996~tplv-k3u1fbpfcp-zoom-1.image)\n\n而如果Proposer如果没有收到超过半数的accept那么它将会将递增该Proposal的编号，然后重新进入Prepare阶段。\n\n##### paxos算法的死循环问题\n\n其实就有点类似于两个人吵架，小明说我是对的，小红说我才是对的，两个人据理力争的谁也不让谁🤬🤬。比如说，此时提案者P1提出一个方案M1，完成了Prepare阶段的工作，这个时候acceptor则批准了M1，但是此时提案者P2同时也提出了一个方案M2，它也完成了Prepare阶段的工作。然后P1的方案已经不能在第二阶段被批准了（因为acceptor已经批准了比M1更大的M2），所以P1自增方案变为M3重新进入Prepare阶段，然后acceptor，又批准了新的M3方案，它又不能批准M2了，这个时候M2又自增进入Prepare阶段。。。就这样无休无止的永远提案下去，这就是paxos算法的死循环问题。那么如何解决呢？很简单，人多了容易吵架，我现在就允许一个能提案就行了。\n\n### 引出ZAB\n\n#### Zookeeper架构\n\n作为一个优秀高效且可靠的分布式协调框架，ZooKeeper在解决分布式数据一致性问题时并没有直接使用Paxos，而是专门定制了一致性协议叫做ZAB(ZooKeeperAtomicBroadcast)原子广播协议，该协议能够很好地支持崩溃恢复。\n\n![Zookeeper架构](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/07bf6c1e10f84fc58a2453766ca6bd18~tplv-k3u1fbpfcp-zoom-1.image)\n\n#### ZAB中的三个角色\n\n和介绍Paxos一样，在介绍ZAB协议之前，我们首先来了解一下在ZAB中三个主要的角色，Leader领导者、Follower跟随者、Observer观察者。\n\n- Leader：集群中唯一的写请求处理者，能够发起投票（投票也是为了进行写请求）。\n- Follower：能够接收客户端的请求，如果是读请求则可以自己处理，如果是写请求则要转发给Leader。在选举过程中会参与投票，有选举权和被选举权。\n- Observer：就是没有选举权和被选举权的Follower。\n\n在ZAB协议中对zkServer(即上面我们说的三个角色的总称)还有两种模式的定义，分别是消息广播和崩溃恢复。\n\n#### 消息广播模式\n\n说白了就是ZAB协议是如何处理写请求的，上面我们不是说只有Leader能处理写请求嘛？那么我们的Follower和Observer是不是也需要同步更新数据呢？总不能数据只在Leader中更新了，其他角色都没有得到更新吧？不就是在整个集群中保持数据的一致性嘛？如果是你，你会怎么做呢？废话，第一步肯定需要Leader将写请求广播出去呀，让Leader问问Followers是否同意更新，如果超过半数以上的同意那么就进行Follower和Observer的更新（和Paxos一样）。当然这么说有点虚，画张图理解一下。\n\n![消息广播](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/b64c7f25a5d24766889da14260005e31~tplv-k3u1fbpfcp-zoom-1.image)\n\n嗯。。。看起来很简单，貌似懂了🤥🤥🤥。这两个Queue哪冒出来的？答案是ZAB需要让Follower和Observer保证顺序性。何为顺序性，比如我现在有一个写请求A，此时Leader将请求A广播出去，因为只需要半数同意就行，所以可能这个时候有一个FollowerF1因为网络原因没有收到，而Leader又广播了一个请求B，因为网络原因，F1竟然先收到了请求B然后才收到了请求A，这个时候请求处理的顺序不同就会导致数据的不同，从而产生数据不一致问题。所以在Leader这端，它为每个其他的zkServer准备了一个队列，采用先进先出的方式发送消息。由于协议是通过TCP来进行网络通信的，保证了消息的发送顺序性，接受顺序性也得到了保证。除此之外，在ZAB中还定义了一个全局单调递增的事务IDZXID，它是一个64位long型，其中高32位表示epoch年代，低32位表示事务id。epoch是会根据Leader的变化而变化的，当一个Leader挂了，新的Leader上位的时候，年代（epoch）就变了。而低32位可以简单理解为递增的事务id。定义这个的原因也是为了顺序性，每个proposal在Leader中生成后需要通过其ZXID来进行排序，才能得到处理。\n\n#### 崩溃恢复模式\n\n说到崩溃恢复我们首先要提到ZAB中的Leader选举算法，当系统出现崩溃影响最大应该是Leader的崩溃，因为我们只有一个Leader，所以当Leader出现问题的时候我们势必需要重新选举Leader。Leader选举可以分为两个不同的阶段，第一个是我们提到的Leader宕机需要重新选举，第二则是当Zookeeper启动时需要进行系统的Leader初始化选举。下面我先来介绍一下ZAB是如何进行初始化选举的。假设我们集群中有3台机器，那也就意味着我们需要两台以上同意（超过半数）。比如这个时候我们启动了server1，它会首先投票给自己，投票内容为服务器的myid和ZXID，因为初始化所以ZXID都为0，此时server1发出的投票为(1,0)。但此时server1的投票仅为1，所以不能作为Leader，此时还在选举阶段所以整个集群处于Looking状态。接着server2启动了，它首先也会将投票选给自己(2,0)，并将投票信息广播出去（server1也会，只是它那时没有其他的服务器了），server1在收到server2的投票信息后会将投票信息与自己的作比较。首先它会比较ZXID，ZXID大的优先为Leader，如果相同则比较myid，myid大的优先作为Leader。所以此时server1发现server2更适合做Leader，它就会将自己的投票信息更改为(2,0)然后再广播出去，之后server2收到之后发现和自己的一样无需做更改，并且自己的投票已经超过半数，则确定server2为Leader，server1也会将自己服务器设置为Following变为Follower。整个服务器就从Looking变为了正常状态。当server3启动发现集群没有处于Looking状态时，它会直接以Follower的身份加入集群。还是前面三个server的例子，如果在整个集群运行的过程中server2挂了，那么整个集群会如何重新选举Leader呢？其实和初始化选举差不多。首先毫无疑问的是剩下的两个Follower会将自己的状态从Following变为Looking状态，然后每个server会向初始化投票一样首先给自己投票（这不过这里的zxid可能不是0了，这里为了方便随便取个数字）。假设server1给自己投票为(1,99)，然后广播给其他server，server3首先也会给自己投票(3,95)，然后也广播给其他server。server1和server3此时会收到彼此的投票信息，和一开始选举一样，他们也会比较自己的投票和收到的投票（zxid大的优先，如果相同那么就myid大的优先）。这个时候server1收到了server3的投票发现没自己的合适故不变，server3收到server1的投票结果后发现比自己的合适于是更改投票为(1,99)然后广播出去，最后server1收到了发现自己的投票已经超过半数就把自己设为Leader，server3也随之变为Follower。\n\n> 请注意ZooKeeper为什么要设置奇数个结点？比如这里我们是三个，挂了一个我们还能正常工作，挂了两个我们就不能正常工作了（已经没有超过半数的节点数了，所以无法进行投票等操作了）。而假设我们现在有四个，挂了一个也能工作，但是挂了两个也不能正常工作了，这是和三个一样的，而三个比四个还少一个，带来的效益是一样的，所以Zookeeper推荐奇数个server。\n\n那么说完了ZAB中的Leader选举方式之后我们再来了解一下崩溃恢复是什么玩意？其实主要就是当集群中有机器挂了，我们整个集群如何保证数据一致性？如果只是Follower挂了，而且挂的没超过半数的时候，因为我们一开始讲了在Leader中会维护队列，所以不用担心后面的数据没接收到导致数据不一致性。如果Leader挂了那就麻烦了，我们肯定需要先暂停服务变为Looking状态然后进行Leader的重新选举（上面我讲过了），但这个就要分为两种情况了，分别是确保已经被Leader提交的提案最终能够被所有的Follower提交和跳过那些已经被丢弃的提案。确保已经被Leader提交的提案最终能够被所有的Follower提交是什么意思呢？假设Leader(server2)发送commit请求（忘了请看上面的消息广播模式），他发送给了server3，然后要发给server1的时候突然挂了。这个时候重新选举的时候我们如果把server1作为Leader的话，那么肯定会产生数据不一致性，因为server3肯定会提交刚刚server2发送的commit请求的提案，而server1根本没收到所以会丢弃。\n\n![崩溃恢复](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/4b8365e80bdf441ea237847fb91236b7~tplv-k3u1fbpfcp-zoom-1.image)\n\n那怎么解决呢？聪明的同学肯定会质疑，这个时候server1已经不可能成为Leader了，因为server1和server3进行投票选举的时候会比较ZXID，而此时server3的ZXID肯定比server1的大了。(不理解可以看前面的选举算法)。那么跳过那些已经被丢弃的提案又是什么意思呢？假设Leader(server2)此时同意了提案N1，自身提交了这个事务并且要发送给所有Follower要commit的请求，却在这个时候挂了，此时肯定要重新进行Leader的选举，比如说此时选server1为Leader（这无所谓）。但是过了一会，这个挂掉的Leader又重新恢复了，此时它肯定会作为Follower的身份进入集群中，需要注意的是刚刚server2已经同意提交了提案N1，但其他server并没有收到它的commit信息，所以其他server不可能再提交这个提案N1了，这样就会出现数据不一致性问题了，所以该提案N1最终需要被抛弃掉。\n\n![崩溃恢复](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/99cdca39ad6340ae8b77e8befe94e36e~tplv-k3u1fbpfcp-zoom-1.image)\n\n### Zookeeper的几个理论知识\n\n了解了ZAB协议还不够，它仅仅是Zookeeper内部实现的一种方式，而我们如何通过Zookeeper去做一些典型的应用场景呢？比如说集群管理，分布式锁，Master选举等等。这就涉及到如何使用Zookeeper了，但在使用之前我们还需要掌握几个概念。比如Zookeeper的**数据模型、会话机制、ACL、Watcher机制**等等。\n\n#### 数据模型\n\nzookeeper数据存储结构与标准的Unix文件系统非常相似，都是在根节点下挂很多子节点(树型)。但是zookeeper中没有文件系统中目录与文件的概念，而是使用了znode作为数据节点。znode是zookeeper中的最小数据单元，每个znode上都可以保存数据，同时还可以挂载子节点，形成一个树形化命名空间。\n\n![zk数据模型](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/663240470d524dd4ac6e68bde0b666eb~tplv-k3u1fbpfcp-zoom-1.image)\n\n每个znode都有自己所属的节点类型和节点状态。其中节点类型可以分为**持久节点、持久顺序节点、临时节点和临时顺序节点**。\n\n- **持久节点**：一旦创建就一直存在，直到将其删除。\n- **持久顺序节点**：一个父节点可以为其子节点维护一个创建的先后顺序，这个顺序体现在节点名称上，是节点名称后自动添加一个由10位数字组成的数字串，从0开始计数。\n- **临时节点**：临时节点的生命周期是与客户端会话绑定的，会话消失则节点消失。临时节点只能做叶子节点，不能创建子节点。\n- **临时顺序节点**：父节点可以创建一个维持了顺序的临时节点(和前面的持久顺序性节点一样)。\n\n节点状态中包含了很多节点的属性比如czxid、mzxid等等，在zookeeper中是使用Stat这个类来维护的。下面我列举一些属性解释。\n\n- czxid：CreatedZXID，该数据节点被创建时的事务ID。\n- mzxid：ModifiedZXID，节点最后一次被更新时的事务ID。\n- ctime：CreatedTime，该节点被创建的时间。\n- mtime：ModifiedTime，该节点最后一次被修改的时间。\n- version：节点的版本号。\n- cversion：子节点的版本号。\n- aversion：节点的ACL版本号。\n- ephemeralOwner：创建该节点的会话的sessionID，如果该节点为持久节点，该值为0。\n- dataLength：节点数据内容的长度。\n- numChildre：该节点的子节点个数，如果为临时节点为0。\n- pzxid：该节点子节点列表最后一次被修改时的事务ID，注意是子节点的列表，不是内容。\n\n#### 会话\n\n我想这个对于后端开发的朋友肯定不陌生，不就是session吗？只不过zk客户端和服务端是通过TCP长连接维持的会话机制，其实对于会话来说你可以理解为保持连接状态。在zookeeper中，会话还有对应的事件，比如CONNECTION_LOSS连接丢失事件、SESSION_MOVED会话转移事件、SESSION_EXPIRED会话超时失效事件。\n\n#### ACL\n\nACL为AccessControlLists，它是一种权限控制。在zookeeper中定义了5种权限，它们分别为：\n\n- CREATE：创建子节点的权限。\n- READ：获取节点数据和子节点列表的权限。\n- WRITE：更新节点数据的权限。\n- DELETE：删除子节点的权限。\n- ADMIN：设置节点ACL的权限。\n\n#### Watcher机制\n\nWatcher为事件监听器，是zk非常重要的一个特性，很多功能都依赖于它，它有点类似于订阅的方式，即客户端向服务端注册指定的watcher，当服务端符合了watcher的某些事件或要求则会向客户端发送事件通知，客户端收到通知后找到自己定义的Watcher然后执行相应的回调方法。\n\n![watcher机制](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/ac87b7cff7b44c63997ff0f6a7b6d2eb~tplv-k3u1fbpfcp-zoom-1.image)\n\n### Zookeeper的几个典型应用场景\n\n#### 选主\n\n还记得上面我们的所说的临时节点吗？因为Zookeeper的强一致性，能够很好地在保证在高并发的情况下保证节点创建的全局唯一性(即无法重复创建同样的节点)。利用这个特性，我们可以让多个客户端创建一个指定的节点，创建成功的就是master。但是，如果这个master挂了怎么办？你想想为什么我们要创建临时节点？还记得临时节点的生命周期吗？master挂了是不是代表会话断了？会话断了是不是意味着这个节点没了？还记得watcher吗？我们是不是可以让其他不是master的节点监听节点的状态，比如说我们监听这个临时节点的父节点，如果子节点个数变了就代表master挂了，这个时候我们触发回调函数进行重新选举，或者我们直接监听节点的状态，我们可以通过节点是否已经失去连接来判断master是否挂了等等。\n\n![选主](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/00468757fb8f4f51875f645fbb7b25a2~tplv-k3u1fbpfcp-zoom-1.image)\n\n总的来说，我们可以完全利用临时节点、节点状态和watcher来实现选主的功能，临时节点主要用来选举，节点状态和watcher可以用来判断master的活性和进行重新选举。\n\n#### 分布式锁\n\n分布式锁的实现方式有很多种，比如Redis、数据库、zookeeper等。个人认为zookeeper在实现分布式锁这方面是非常非常简单的。上面我们已经提到过了zk在高并发的情况下保证节点创建的全局唯一性，这玩意一看就知道能干啥了。实现互斥锁呗，又因为能在分布式的情况下，所以能实现分布式锁呗。如何实现呢？这玩意其实跟选主基本一样，我们也可以利用临时节点的创建来实现。首先肯定是如何获取锁，因为创建节点的唯一性，我们可以让多个客户端同时创建一个临时节点，创建成功的就说明获取到了锁。然后没有获取到锁的客户端也像上面选主的非主节点创建一个watcher进行节点状态的监听，如果这个互斥锁被释放了（可能获取锁的客户端宕机了，或者那个客户端主动释放了锁）可以调用回调函数重新获得锁。\n\n>zk中不需要向redis那样考虑锁得不到释放的问题了，因为当客户端挂了，节点也挂了，锁也释放了。是不是很简单？\n\n那能不能使用zookeeper同时实现共享锁和独占锁呢？答案是可以的，不过稍微有点复杂而已。还记得有序的节点吗？这个时候我规定所有创建节点必须有序，当你是读请求（要获取共享锁）的话，如果没有比自己更小的节点，或比自己小的节点都是读请求，则可以获取到读锁，然后就可以开始读了。若比自己小的节点中有写请求，则当前客户端无法获取到读锁，只能等待前面的写请求完成。如果你是写请求（获取独占锁），若没有比自己更小的节点，则表示当前客户端可以直接获取到写锁，对数据进行修改。若发现有比自己更小的节点，无论是读操作还是写操作，当前客户端都无法获取到写锁，等待所有前面的操作完成。这就很好地同时实现了共享锁和独占锁，当然还有优化的地方，比如当一个锁得到释放它会通知所有等待的客户端从而造成羊群效应。此时你可以通过让等待的节点只监听他们前面的节点。具体怎么做呢？其实也很简单，你可以让读请求监听比自己小的最后一个写请求节点，写请求只监听比自己小的最后一个节点，感兴趣的小伙伴可以自己去研究一下。\n\n#### 命名服务\n\n如何给一个对象设置ID，大家可能都会想到UUID，但是UUID最大的问题就在于它太长了。那么在条件允许的情况下，我们能不能使用zookeeper来实现呢？我们之前提到过zookeeper是通过树形结构来存储数据节点的，那也就是说，对于每个节点的全路径，它必定是唯一的，我们可以使用节点的全路径作为命名方式了。而且更重要的是，路径是我们可以自己定义的，这对于我们对有些有语意的对象的ID设置可以更加便于理解。\n\n#### 集群管理和注册中心\n\n看到这里是不是觉得zookeeper实在是太强大了，它能干的事情还很多呢。可能我们会有这样的需求，我们需要了解整个集群中有多少机器在工作，我们想对集群中的每台机器的运行时状态进行数据采集，对集群中机器进行上下线操作等等。而zookeeper天然支持的watcher和临时节点能很好的实现这些需求。我们可以为每条机器创建临时节点，并监控其父节点，如果子节点列表有变动（我们可能创建删除了临时节点），那么我们可以使用在其父节点绑定的watcher进行状态监控和回调。\n\n![集群管理](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/f3d70709f10f4fa6b09125a56a976fda~tplv-k3u1fbpfcp-zoom-1.image)\n\n至于注册中心也很简单，我们同样也是让服务提供者在zookeeper中创建一个临时节点并且将自己的ip、port、调用方式写入节点，当服务消费者需要进行调用的时候会通过注册中心找到相应的服务的地址列表(IP端口什么的)，并缓存到本地(方便以后调用)，当消费者调用服务时，不会再去请求注册中心，而是直接通过负载均衡算法从地址列表中取一个服务提供者的服务器调用服务。当服务提供者的某台服务器宕机或下线时，相应的地址会从服务提供者地址列表中移除。同时，注册中心会将新的服务地址列表发送给服务消费者的机器并缓存在消费者本机（当然你可以让消费者进行节点监听，我记得Eureka会先试错，然后再更新）。\n\n![注册中心](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/469cebf9670740d1a6711fe54db70e05~tplv-k3u1fbpfcp-zoom-1.image)\n\n> [原文链接](https://javaguide.cn/distributed-system/distributed-process-coordination/zookeeper/zookeeper-plus.html)\n\n**ZooKeeper应用场景总结**\n\n**统一命名服务**\n\n1. 在分布式环境下，经常需要对应用/服务进行统一命名，便于识别不同服务。\n    - 类似于域名与ip之间对应关系，ip不容易记住，而域名容易记住。\n    - 通过名称来获取资源或服务的地址，提供者等信息。\n\n2. 按照层次结构组织服务/应用名称。\n    - 可将服务名称以及地址信息写到ZooKeeper上，客户端通过ZooKeeper获取可用服务列表类\n\n**配置管理**\n1. 分布式环境下，配置文件管理和同步是一个常见问题。\n    - 一个集群中，所有节点的配置信息是一致的，比如Hadoop集群。\n    - 对配置文件修改后，希望能够快速同步到各个节点上。\n\n2. 配置管理可交由ZooKeeper实现。\n    - 可将配置信息写入ZooKeeper上的一个Znode。\n    - 各个节点监听这个Znode。\n    - 一旦Znode中的数据被修改，ZooKeeper将通知各个节点。\n\n**集群管理**\n1. 分布式环境中，实时掌握每个节点的状态是必要的。\n    - 可根据节点实时状态做出一些调整。\n\n2. 可交由ZooKeeper实现。\n    - 可将节点信息写入ZooKeeper上的一个Znode。\n    - 监听这个Znode可获取它的实时状态变化。\n\n3. 典型应用\n    - Hbase中Master状态监控与选举。\n\n**分布式通知与协调**\n\n1. 分布式环境中，经常存在一个服务需要知道它所管理的子服务的状态。\n    - NameNode需知道各个Datanode的状态。\n    - JobTracker需知道各个TaskTracker的状态。\n\n2. 心跳检测机制可通过ZooKeeper来实现。\n\n3. 信息推送可由ZooKeeper来实现，ZooKeeper相当于一个发布/订阅系统。\n\n**分布式锁**\n\n处于不同节点上不同的服务，它们可能需要顺序的访问一些资源，这里需要一把分布式的锁。\n分布式锁具有以下特性：\n1. ZooKeeper是强一致的。比如各个节点上运行一个ZooKeeper客户端，它们同时创建相同的Znode，但是只有一个客户端创建成功。\n2. 实现锁的独占性。创建Znode成功的那个客户端才能得到锁，其它客户端只能等待。当前客户端用完这个锁后，会删除这个Znode，其它客户端再尝试创建Znode，获取分布式锁。\n3. 控制锁的时序。各个客户端在某个Znode下创建临时Znode，这个类型必须为CreateMode.EPHEMERAL_SEQUENTIAL，这样该Znode可掌握全局访问时序。\n\n**分布式队列**\n\n分布式队列分为两种：\n1. 当一个队列的成员都聚齐时，这个队列才可用，否则一直等待所有成员到达，这种是同步队列。\n    - 一个job由多个task组成，只有所有任务完成后，job才运行完成。\n    - 可为job创建一个/job目录，然后在该目录下，为每个完成的task创建一个临时的Znode，一旦临时节点数目达到task总数，则表明job运行完成。\n\n2. 队列按照FIFO方式进行入队和出队操作，例如实现生产者和消费者模型。\n\n## 安装部署\n\nzookeeper的安装模式有三种：\n\n- 单机模式（stand-alone）：单机单server\n- 集群模式：多机多server，形成集群\n- 伪集群模式：单机多个server，形成伪集群\n\n```properties\n#服务器之间或客户端与服务器之间维持心跳的时间间隔，也就是每个tickTime时间就会发送一个心跳。单位为毫秒\ntickTime=2000\n#所有跟随者与领导者进行连接并同步的时间，如果在设定的时间内，半数以上的跟随者未能完成同步，领导者便会宣布放弃领导地位，进行另一次的领导选举,#单位为tick值的倍数\ninitLimit=10\n#对于主节点与从节点进行同步操作时的超时时间，单位为tick值的倍数。\nsyncLimit=5\nclientPort=2181\ndataDir=/usr/local/zookeeper/data\ndataLogDir=/usr/local/zookeeper/dataLog\n#server.A=B：C：D：其中A是一个数字，表示这个是第几号服务器；B是这个服务器的ip地址；C表示的是这个服务器与集群中的Leader服务器交换信息的端口；D表示的是万一集群中的Leader服务器挂了，需要一个端口来重新进行选举，选出一个新的Leader，而这个端口就是用来执行选举时服务器相互通信的端口。如果是伪集群的配置方式，由于B都是一样，所以不同的Zookeeper实例通信端口号不能一样，所以要给它们分配不同的端口号。\nserver.1=192.168.236.128:2888:3888\nserver.2=192.168.236.129:2888:3888\nserver.3=192.168.236.130:2888:3888\n```\n\n### 启动命令\n\n```\n./zkServer.sh start\n./zkCli.sh -server 127.0.0.1:2181\n```\n\n| 参数                   | 描述                                                         |\n| ---------------------- | ------------------------------------------------------------ |\n| dataDir                | 用于存放内存数据库快照的文件夹，同时用于集群的myid文件也存在这个文件夹里|\n| dataLogDir             | 用于单独设置transaction log的目录，transaction log分离可以避免和普通log还有快照的竞争。 |\n| tickTime               | 心跳时间，为了确保client—server连接存在的，以毫秒为单位，最小超时时间为两个心跳时间|\n| clientPort             | 客户端监听端口                                               |\n| globalOutstandingLimit | client请求队列的最大长度，防止内存溢出，默认值为1000。       |\n| preAllocSize           | 预分配的Transaction log空间block为proAllocSize KB，默认block为64M，一般不需要更改，除非snapshot过于频繁 |\n| snapCount              | 在snapCount个snapshot后写一次transaction log，默认值是100000|\n| traceFile              | 用于记录请求的log，打开会影响性能，用于debug，最好不要定义。|\n| maxClientCnxns         | 最大并发客户端数，用于防止DOS的，默认值是10，设置为0是不加限|\n| clientPortBindAddress  | 可以设置指定的client ip以及端口，不设置的话等于ANY:clientPort|\n| minSessionTimeout      | 最小的客户端session超时时间，默认值为2个tickTime，单位是毫秒|\n| maxSessionTimeout      | 最大的客户端session超时时间，默认值为20个tickTime，单位是毫秒|\n| electionAlg            | 用于选举的实现的参数，0为以原始的基于UDP的方式协作，1为不进行用户验证的基于UDP的快速选举，2为进行用户验证的基于UDP的快速选举，3为基于TCP的快速选举，默认值为3。|\n| initLimit              | 多少个tickTime内，允许其他server连接并初始化数据，如果zooKeeper管理的数据较大，则应相应增大这个值 |\n| syncLimit              | 多少个tickTime内，允许其他server连接并初始化数据，如果zooKeeper管理的数据较大，则应相应增大这个值 |\n| leaderServes           | leader是否接受客户端连接。默认值为yes。leader负责协调更新。当更新吞吐量远高于读取吞吐量时，可以设置为不接受客户端连接，以便leader可以专注于同步协调工作。|\n| server.x=ip:xxxx:xxxx  | 配置集群里面的主机信息，其中server.x的x要写在myid文件中，决定当前机器的id，server.x=第一个port用于连接leader，第二个用于leader选举。如果 electionAlg为0，则不需要第二个port。hostname也可以填ip。|\n| group.x=nnnnn[:nnnnn]  | 分组信息，表明哪个组有哪些节点，例如group．1＝1：2：3 group．2＝4：5：6group.3=7:8:9。 |\n| weight.x=nnnnn         | 权重信息，表明哪个结点的权重是多少，例如weight．1＝1weight．2＝1weight.3=1 |\n\n### ZooKeeper安装和使用\n\n#### 使用Docker安装zookeeper\n\n**a.使用Docker下载ZooKeeper**\n\n```bash\ndocker pull zookeeper:3.5.8\n```\n\n**b.运行ZooKeeper**\n\n```bash\ndocker run -d --name zookeeper -p 2181:2181 zookeeper:3.5.8\n```\n\n#### 连接ZooKeeper服务\n\n**a.进入ZooKeeper容器中**\n\n先使用`docker ps`查看ZooKeeper的ContainerID，然后使用`docker exec -it ContainerID /bin/bash`命令进入容器中。\n\n**b.先进入bin目录,然后通过`./zkCli.sh -server 127.0.0.1:2181`命令连接ZooKeeper服务**\n\n```bash\nroot@eaf70fc620cb:/apache-zookeeper-3.5.8-bin# cd bin\n```\n\n### 常用命令演示\n\n#### 查看常用命令(help命令)\n\n通过`help`命令查看ZooKeeper常用命令\n\n#### 创建节点(create命令)\n\n通过`create`命令在根目录创建了node1节点，与它关联的字符串是\"node1\"\n\n```bash\n[zk:127.0.0.1:2181(CONNECTED) 34] create /node1 “node1”\n```\n\n通过`create`命令在根目录创建了node1节点，与它关联的内容是数字123\n\n```bash\n[zk: 127.0.0.1:2181(CONNECTED) 1] create /node1/node1.1 123\nCreated /node1/node1.1\n```\n\n#### 更新节点数据内容(set命令)\n\n```bash\n[zk: 127.0.0.1:2181(CONNECTED) 11] set /node1 \"set node1\"\n```\n\n#### 获取节点的数据(get命令)\n\n`get`命令可以获取指定节点的数据内容和节点的状态,可以看出我们通过`set`命令已经将节点数据内容改为\"set node1\"。\n\n```bash\n[zk: zookeeper(CONNECTED) 12] get -s /node1\nset node1\ncZxid = 0x47\nctime = Sun Jan 20 10:22:59 CST 2019\nmZxid = 0x4b\nmtime = Sun Jan 20 10:41:10 CST 2019\npZxid = 0x4a\ncversion = 1\ndataVersion = 1\naclVersion = 0\nephemeralOwner = 0x0\ndataLength = 9\nnumChildren = 1\n```\n\n#### 查看某个目录下的子节点(ls命令)\n\n通过`ls`命令查看根目录下的节点\n\n```bash\n[zk: 127.0.0.1:2181(CONNECTED) 37] ls /\n[dubbo, ZooKeeper, node1]\n```\n\n通过`ls`命令查看node1目录下的节点\n\n```bash\n[zk: 127.0.0.1:2181(CONNECTED) 5] ls /node1\n[node1.1]\n```\n\nZooKeeper中的ls命令和linux命令中的ls类似，这个命令将列出绝对路径path下的所有子节点信息（列出1级，并不递归）\n\n#### 查看节点状态(stat命令)\n\n通过`stat`命令查看节点状态\n\n```bash\n[zk: 127.0.0.1:2181(CONNECTED) 10] stat /node1\ncZxid = 0x47\nctime = Sun Jan 20 10:22:59 CST 2019\nmZxid = 0x47\nmtime = Sun Jan 20 10:22:59 CST 2019\npZxid = 0x4a\ncversion = 1\ndataVersion = 0\naclVersion = 0\nephemeralOwner = 0x0\ndataLength = 11\nnumChildren = 1\n```\n\n\n#### 查看节点信息和状态(ls2命令)\n\n`ls2`命令更像是`ls`命令和`stat`命令的结合。`ls2`命令返回的信息包括2部分：\n\n1. 子节点列表\n2. 当前节点的stat信息。\n\n```bash\n[zk: 127.0.0.1:2181(CONNECTED) 7] ls2 /node1\n[node1.1]\ncZxid = 0x47\nctime = Sun Jan 20 10:22:59 CST 2019\nmZxid = 0x47\nmtime = Sun Jan 20 10:22:59 CST 2019\npZxid = 0x4a\ncversion = 1\ndataVersion = 0\naclVersion = 0\nephemeralOwner = 0x0\ndataLength = 11\nnumChildren = 1\n```\n\n#### 删除节点(delete命令)\n\n这个命令很简单，但是需要注意的一点是如果你要删除某一个节点，那么这个节点必须无子节点才行。\n\n```bash\n[zk: 127.0.0.1:2181(CONNECTED) 3] delete /node1/node1.1\n```\n\n\n### ZooKeeperJava客户端Curator简单使用\n\nCurator是Netflix公司开源的一套ZooKeeper Java客户端框架，相比于Zookeeper自带的客户端zookeeper来说，Curator的封装更加完善，各种API都可以比较方便地使用。Curator4.0+版本对ZooKeeper3.5.x支持比较好。开始之前，请先将下面的依赖添加进你的项目。\n\n```xml\n<dependency>\n    <groupId>org.apache.curator</groupId>\n    <artifactId>curator-framework</artifactId>\n    <version>4.2.0</version>\n</dependency>\n<dependency>\n    <groupId>org.apache.curator</groupId>\n    <artifactId>curator-recipes</artifactId>\n    <version>4.2.0</version>\n</dependency>\n```\n\n#### 连接ZooKeeper客户端\n\n通过CuratorFrameworkFactory创建CuratorFramework对象，然后再调用CuratorFramework对象的`start()`方法即可！\n\n```java\nprivate static final int BASE_SLEEP_TIME = 1000;\nprivate static final int MAX_RETRIES = 3;\n\n// Retry strategy. Retry 3 times, and will increase the sleep time between retries.\nRetryPolicy retryPolicy = new ExponentialBackoffRetry(BASE_SLEEP_TIME, MAX_RETRIES);\nCuratorFramework zkClient = CuratorFrameworkFactory.builder()\n    // the server to connect to (can be a server list)\n    .connectString(\"127.0.0.1:2181\")\n    .retryPolicy(retryPolicy)\n    .build();\nzkClient.start();\n```\n\n对于一些基本参数的说明：\n\n- baseSleepTimeMs：重试之间等待的初始时间\n- maxRetries：最大重试次数\n- connectString：要连接的服务器列表\n- retryPolicy：重试策略\n\n#### 数据节点的增删改查\n\n##### 创建节点\n\n我们在[ZooKeeper常见概念解读](https://javaguide.cn/distributed-system/distributed-process-coordination/zookeeper/zookeeper-intro.html)中介绍到，我们通常是将znode分为4大类：\n\n- **持久（PERSISTENT）节点**：一旦创建就一直存在即使ZooKeeper集群宕机，直到将其删除。\n- **临时（EPHEMERAL）节点**：临时节点的生命周期是与客户端会话（session）绑定的，会话消失则节点消失。并且，临时节点只能做叶子节点，不能创建子节点。\n- **持久顺序（PERSISTENT_SEQUENTIAL）节点**：除了具有持久（PERSISTENT）节点的特性之外，子节点的名称还具有顺序性。比如`/node1/app0000000001`、`/node1/app0000000002`。\n- **临时顺序（EPHEMERAL_SEQUENTIAL）节点**：除了具备临时（EPHEMERAL）节点的特性之外，子节点的名称还具有顺序性。\n\n你在使用的ZooKeeper的时候，会发现`CreateMode`类中实际有7种znode类型，但是用的最多的还是上面介绍的4种。\n\n**a.创建持久化节点**\n\n你可以通过下面两种方式创建持久化的节点。\n\n```java\n//注意:下面的代码会报错，下文说了具体原因\nzkClient.create().forPath(\"/node1/00001\");\nzkClient.create().withMode(CreateMode.PERSISTENT).forPath(\"/node1/00002\");\n```\n\n但是，你运行上面的代码会报错，这是因为的父节点`node1`还未创建。你可以先创建父节点`node1`，然后再执行上面的代码就不会报错了。\n\n```java\nzkClient.create().forPath(\"/node1\");\n```\n\n更推荐的方式是通过下面这行代码，**creatingParentsIfNeeded方法可以保证父节点不存在的时候自动创建父节点，这是非常有用的**。\n\n```java\nzkClient.create().creatingParentsIfNeeded().withMode(CreateMode.PERSISTENT).forPath(\"/node1/00001\");\n```\n\n**b.创建临时节点**\n\n```java\nzkClient.create().creatingParentsIfNeeded().withMode(CreateMode.EPHEMERAL).forPath(\"/node1/00001\");\n```\n\n**c.创建节点并指定数据内容**\n\n```java\nzkClient.create().creatingParentsIfNeeded().withMode(CreateMode.EPHEMERAL).forPath(\"/node1/00001\",\"java\".getBytes());\nzkClient.getData().forPath(\"/node1/00001\");//获取节点的数据内容，获取到的是byte数组\n```\n\n**d.检测节点是否创建成功**\n\n```java\nzkClient.checkExists().forPath(\"/node1/00001\");//不为null的话，说明节点创建成功\n```\n\n##### 删除节点\n\n**a.删除一个子节点**\n\n```java\nzkClient.delete().forPath(\"/node1/00001\");\n```\n\n**b.删除一个节点以及其下的所有子节点**\n\n```java\nzkClient.delete().deletingChildrenIfNeeded().forPath(\"/node1\");\n```\n\n##### 获取/更新节点数据内容\n\n```java\nzkClient.create().creatingParentsIfNeeded().withMode(CreateMode.EPHEMERAL).forPath(\"/node1/00001\",\"java\".getBytes());\nzkClient.getData().forPath(\"/node1/00001\");//获取节点的数据内容\nzkClient.setData().forPath(\"/node1/00001\",\"c++\".getBytes());//更新节点数据内容\n```\n\n##### 获取某个节点的所有子节点路径\n\n```java\nList<String> childrenPaths = zkClient.getChildren().forPath(\"/node1\");\n```\n\n\n## 相关文章\n\n- [ZooKeeper基本原理及安装部署](https://zhuanlan.zhihu.com/p/30024403?utm_source=wechat_timeline&utm_medium=social&utm_oi=1040923520439672832&from=timeline)\n- [不耍流氓，有答案的Zookeeper面试题](https://mp.weixin.qq.com/s/QUIEoZLhkF3ozBj_GS9UMA)\n- [ZooKeeper不仅仅是注册中心，你还知道有哪些](https://mp.weixin.qq.com/s?__biz=MzkzODE3OTI0Ng==&mid=2247491198&idx=1&sn=7bfd06e81d7fd361b1db2631ed81e9f0&source=41#wechat_redirect)\n- [什么是Zookeeper?](https://zhuanlan.zhihu.com/p/62526102?utm_source=wechat_timeline&utm_medium=social&utm_oi=1040923520439672832&from=timeline)\n- [不懂ZooKeeper？没关系，这一篇给你讲的明明白白](https://mp.weixin.qq.com/s/X-10DjJQE3sIBVmoOVV77g)\n- [ZooKeeper的十二连问，你顶得了嘛](https://mp.weixin.qq.com/s/Nx8QrO8bwVRatP6jzb6HZg)\n- [ZooKeeper到底解决了什么问题？](https://mp.weixin.qq.com/s/kaXKnzbaq0NNMPKrfowQHg)\n- [大白话说清楚Zookeeper的选举机制！](https://mp.weixin.qq.com/s/Co2XO6EiFu-oEsFookpxuw)\n- [Zookeeper的选举流程是怎样的？](https://mp.weixin.qq.com/s/RdTDJSMYBqkZGnGsmwoDvw)\n- [Zookeeper怎么保证分布式事务的最终一致性？](https://mp.weixin.qq.com/s/n0UKU7BxPT1r4OCPPKHGaA)\n- [Zookeeper夺命连环9问](https://mp.weixin.qq.com/s/KyDmcyi6bALQg-W6F-VuUg)\n- [不懂Zookeeper？没关系呀，你看这篇就够了！](https://mp.weixin.qq.com/s/u9Gn0XSIB35k3BxlW_E23g)\n- [Zookeeper的5个核心知识点！](https://mp.weixin.qq.com/s/ByfASCD2V-JcBtVCXTXPew)\n- [Zookeeper典型应用场景介绍](https://mp.weixin.qq.com/s/we1KRq_WtRveHES2Xw6C9A)\n- [zkcli.sh命令](https://www.cnblogs.com/chengxuyuanzhilu/p/6698059.html)","tags":["安装"],"categories":["技术栈"]},{"title":"Spring相关","slug":"Spring相关","url":"/blog/posts/e47eee220e74/","content":"\n> 我写的关于Spring扩展插件的一个[示例](https://github.com/xmxe/springboot/tree/3.x/springboot-lifecycle)，里面有很多Spring扩展的测试。\n> 其他相关文章:[Spring中的Bean对象](https://xmxe.gitee.io/blog/posts/b435885d7cf1/)\n\n## Spring循环依赖\n\n1. 如果是原型bean的循环依赖，Spring无法解决\n2. 如果是构造参数注入的循环依赖，Spring无法解决\n\n容器为了缓存这些单例的Bean需要一个数据结构来存储，比如Map {k:name; v:bean}。\n而我们创建一个Bean就可以往Map中存入一个Bean。这时候我们仅需要一个Map就可以满足创建+缓存的需求。\n但是创建Bean过程中可能会遇到循环依赖问题，比如A对象依赖了一个B对象，而B对象内部又依赖了一个A:\n\n```java\npublic class A {\n    B b;\n}\npublic class B {\n    A a;\n}\n```\n假设A和B我都定义为单例的对象，并且需要在项目启动过程中自动注入，如下：\n```java\n@Component\npublic class A {\n  @Autowired\n  B b;\n}\n@Component\npublic class B {\n  @Autowired\n  A a;\n}\n```\n\n> 一般来说，如果我们的代码中出现了循环依赖，则说明我们的代码在设计的过程中可能存在问题，我们应该尽量避免循环依赖的发生。不过一旦发生了循环依赖，Spring默认也帮我们处理好了，当然这并不能说明循环依赖这种代码就没问题。实际上在目前最新版的Spring中，循环依赖是要额外开启的，如果不额外配置，发生了循环依赖就直接报错了\n\n### 一级缓存\n\n1. 实例化A对象。\n2. 填充A的属性阶段时需要去填充B对象，而此时B对象还没有创建，所以这里为了完成A的填充就必须要先去创建B对象；\n3. 实例化B对象。\n4. 执行到B对象的填充属性阶段，又会需要去获取A对象，而此时Map中没有A，因为A还没有创建完成，导致又需要去创建A对象。\n这样，就会循环往复，一直创建下去，只到堆栈溢出。\n\n**为什么不能在实例化A之后就放入Map**？因为此时A尚未创建完整，所有属性都是默认值，并不是一个完整的对象，在执行业务时可能会抛出未知的异常。所以必须要在A创建完成之后才能放入Map。\n\n### 二级缓存\n\n此时我们引入二级缓存用另外一个Map2 {k:name;v:earlybean}来存储尚未已经开始创建但是尚未完整创建的对象。\n\n1. 实例化A对象之后，将A对象放入Map2中。\n2. 在填充A的属性阶段需要去填充B对象，而此时B对象还没有创建，所以这里为了完成A的填充就必须要先去创建B对象。\n3. 创建B对象的过程中，实例化B对象之后，将B对象放入Map2中。\n4. 执行到B对象填充属性阶段，又会需要去获取A对象，而此时Map中没有A，因为A还没有创建完成，但是我们继续从Map2中拿到尚未创建完毕的A的引用赋值给a字段。这样B对象其实就已经创建完整了，尽管B.a对象是一个还未创建完成的对象。\n5. 此时将B放入Map并且从Map2中删除。\n6. 这时候B创建完成，A继续执行b的属性填充可以拿到B对象，这样A也完成了创建。B.a也完整了\n7. 此时将A对象放入Map并从Map2中删除。\n\n**二级缓存已然解决了循环依赖问题，为什么还需要三级缓存**？\n\n从上面的流程中我们可以看到使用两级缓存可以完美解决循环依赖的问题，但是Spring中还有另外一个问题需要解决，这就是初始化过程中的AOP实现。AOP是Spring的重要功能，实现方式就是使用代理模式动态增强类的功能。动态单例目前有两种技术可以实现，一种是JDK自带的基于接口的动态Proxy技术，一种是CGlib基于字节码动态生成的Proxy技术，这两种技术都是需要原始对象创建完毕，之后基于原始对象生成代理对象的。那么我们发现，在二级缓存的设计下，我们需要在放入缓存Map之前将代理对象生成好。\n将流程改为：\n\n1. 实例化Bean对象，为Bean对象在内存中分配空间，各属性赋值为默认值\n2. 如果有动态代理，生成Bean对象的代理Proxy对象\n3. 初始化Proxy对象，为Bean对象填充属性\n4. 将Proxy放入缓存\n这样虽然也可以解决，AOP的问题，但是我们知道Spring中AOP的实现是通过后置处理器BeanPostProcessor机制来实现的，而后置处理器是在填充属性结束后才执行的。流程如下：\n1. 实例化对象\n2. 对象填充属性\n3. BeanPostProcessor doBefore\n4. init-method\n5. BeanPostProcessor doAfter --AOP是在这个阶段实现的\n所以要实现上面的方案，势必需要将BeanPostProcessor阶段提前或者侵入到填充属性的流程中，那么从程序设计上来说，这样做肯定是不美的\n\n> 面试官会问：为什么要使用三级缓存呢？二级缓存能解决循环依赖吗？\n> 答：如果要使用二级缓存解决循环依赖，意味着所有Bean在实例化后就要完成AOP代理，这样违背了Spring设计的原则，Spring在设计之初就是通过AnnotationAwareAspectJAutoProxyCreator这个后置处理器来在Bean生命周期的最后一步来完成AOP代理，而不是在实例化后就立马进行AOP代理\n\n### 三级缓存\n\nSpring引入了第三级缓存来解决这个问题，Map3 {k:name v:ObjectFactory}，这个缓存的value就不是Bean对象了，而是一个接口对象由一段lamda表达式实现。在这段lamda表达式中去完成一些BeanPostProcessor的执行。\n1. 实例化A对象之后，将A的ObjectFactory对象放入Map3中。\n2. 在填充A的属性阶段需要去填充B对象，而此时B对象还没有创建，所以这里为了完成A的填充就必须要先去创建B对象。\n3. 创建B对象的过程中，实例化B的ObjectFactory对象之后，将B对象放入Map2中。\n4. 执行到B对象填充属性阶段，又会需要去获取A对象，而此时Map1中没有A，因为A还没有创建完成，但是我们继续从Map2中也拿不到，到Map3中获取了A的ObjectFactory对象，通过ObjectFactory对象获取A的早期对象，并将这个早期对象放入Map2中，同时删除Map3中的A，将尚未创建完毕的A的引用赋值给a字段。这样B对象其实就已经创建完整了，尽管B.a对象是一个还未创建完成的对象。\n5. 此时将B放入Map并且从Map3中删除。\n6. 这时候B创建完成，A继续执行b的属性填充可以拿到B对象，这样A也完成了创建。\n7. 此时将A对象放入Map并从Map2中删除。\n\n### 带图解析\n\n![](https://mmbiz.qpic.cn/sz_mmbiz_png/GvtDGKK4uYl7Vdb6FD5gpKdIVy6ibibEbgSFMjDAgsG3lvnhB8NBlP5fts8ZiaT2fKk4pOmVGhDk5Hjwh0T38xvDg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)\n\n我们在这里引入了一个缓存池。当我们需要创建AService的实例的时候，会首先通过Java反射创建出来一个原始的AService，这个原始AService可以简单理解为刚刚new出来（实际是刚刚通过反射创建出来）还没设置任何属性的AService，此时，我们把这个AService先存入到一个缓存池中。\n\n接下来我们就需要给AService的属性设置值了，同时还要处理AService的依赖，这时我们发现AService依赖BService，那么就去创建BService对象，结果创建BService的时候，发现BService依赖AService，那么此时就先从缓存池中取出来AService先用着，然后继续BService创建的后续流程，直到BService创建完成后，将之赋值给AService，此时AService和BService就都创建完成了。\n\n可能有小伙伴会说，BService从缓存池中拿到的AService是一个半成品，并不是真正的最终的AService，但是小伙伴们要知道，Java是引用传递（也可以认为是值传递，只不过这个值是内存地址），BService当时拿到的是AService的引用，说白了就是一块内存地址而已，根据这个地址找到的就是AService，所以，后续如果AService创建完成后，BService所拿到的AService就是完整的AService了。\n\n那么上面提到的这个缓存池，在Spring容器中有一个专门的名字，就叫做**earlySingletonObjects**，这是Spring三级缓存中的**二级缓存**，这里保存的是刚刚通过反射创建出来的Bean，这些Bean还没有经历过完整生命周期，Bean的属性可能都还没有设置，Bean需要的依赖都还没有注入进来。另外两级缓存分别是：\n\n**singletonObjects**：这是**一级缓存**，一级缓存中保存的是所有经历了完整生命周期的Bean，即一个Bean从创建、到属性赋值、到各种处理器的执行等等，都经历过了，就存到singletonObjects中，当我们需要获取一个Bean的时候，首先会去一级缓存中查找，当一级缓存中没有的时候，才会考虑去二级缓存。\n**singletonFactories**：这是**三级缓存**。在一级缓存和二级缓存中，缓存的key是beanName，缓存的value则是一个Bean对象，但是在三级缓存中，缓存的value是一个Lambda表达式，通过这个Lambda表达式可以创建出来目标对象的一个代理对象。\n有的小伙伴可能会觉得奇怪，按照上文的介绍，一级缓存和二级缓存就足以解决循环依赖了，为什么还冒出来一个三级缓存？那就得考虑AOP的情况了！\n\n**AOP的创建流程**\n\n正常来说是我们首先通过反射获取到一个Bean的实例，然后就是给这个Bean填充属性，属性填充完毕之后，接下来就是执行各种BeanPostProcessor了，如果这个Bean中有需要代理的方法，那么系统就会自动配置对应的后置处理器，举一个简单例子，假设我有如下一个Service：\n```java\n@Service\npublic class UserService {\n    @Async\n    public void hello() {\n        System.out.println(\"hello>>>\"+Thread.currentThread().getName());\n    }\n}\n```\n那么系统就会自动提供一个名为AsyncAnnotationBeanPostProcessor的处理器，在这个处理器中，系统会生成一个代理的UserService对象，并用这个对象代替原本的UserService。那么小伙伴们要搞清楚的是，原本的UserService和新生成的代理的UserService是两个不同的对象，占两块不同的内存地址！！！\n\n我们再来回顾下面这张图：\n![](https://mmbiz.qpic.cn/sz_mmbiz_png/GvtDGKK4uYl7Vdb6FD5gpKdIVy6ibibEbgSFMjDAgsG3lvnhB8NBlP5fts8ZiaT2fKk4pOmVGhDk5Hjwh0T38xvDg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)\n\n如果AService最终是要生成一个代理对象的话，那么AService存到缓存池的其实还是原本的AService，因为此时还没到处理AOP那一步（要先给各个属性赋值，然后才是AOP处理），这就导致BService从缓存池里拿到的AService是原本的AService，等到BService创建完毕之后，AService的属性赋值才完成，接下来在AService后续的创建流程中，AService会变成了一个代理对象了，不是缓存池里的AService了，最终就导致BService所依赖的AService和最终创建出来的AService不是同一个。\n\n为了解决这个问题，Spring引入了三级缓存`singletonFactories`。`singletonFactories`的工作机制是这样的（假设AService最终是一个代理对象）：当我们创建一个AService的时候，通过反射刚把原始的AService创建出来之后，先去判断当前一级缓存中是否存在当前Bean，如果不存在，则首先向三级缓存中添加一条记录，记录的key就是当前Bean的beanName，value则是一个Lambda表达式ObjectFactory，通过执行这个Lambda可以给当前AService生成代理对象。然后如果二级缓存中存在当前AService Bean，则移除掉。现在继续去给AService各个属性赋值，结果发现AService需要BService，然后就去创建BService，创建BService的时候，发现BService又需要用到AService，于是就先去一级缓存中查找是否有AService，如果有，就使用，如果没有，则去二级缓存中查找是否有AService，如果有，就使用，如果没有，则去三级缓存中找出来那个ObjectFactory，然后执行这里的getObject方法，这个方法在执行的过程中，会去判断是否需要生成一个代理对象，如果需要就生成代理对象返回，如果不需要生成代理对象，则将原始对象返回即可。最后，把拿到手的对象存入到二级缓存中以备下次使用，同时删除掉三级缓存中对应的数据。这样AService所依赖的BService就创建好了。接下来继续去完善AService，去执行各种后置的处理器，此时，有的后置处理器想给AService生成代理对象，发现AService已经是代理对象了，就不用生成了，直接用已有的代理对象去代替AService即可。至此，AService和BService都搞定。本质上，singletonFactories是把AOP的过程提前了。\n\n总的来说，Spring解决循环依赖把握住两个关键点：\n\n- 提前暴露：刚刚创建好的对象还没有进行任何赋值的时候，将之暴露出来放到缓存中，供其他Bean提前引用（二级缓存）。\n- 提前AOP：A依赖B的时候，去检查是否发生了循环依赖（检查的方式就是将正在创建的A标记出来，然后B需要A，B去创建A的时候，发现A正在创建，就说明发生了循环依赖），如果发生了循环依赖，就提前进行AOP处理，处理完成后再使用（三级缓存）。\n\n原本AOP这个过程是属性赋完值之后，再由各种后置处理器去处理AOP的（AbstractAutoProxyCreator），但是如果发生了循环依赖，就先AOP，然后属性赋值，最后等到后置处理器执行的时候，就不再做AOP的处理了。不过需要注意，三级缓存并不能解决所有的循环依赖\n\n> [如何通过三级缓存解决Spring循环依赖※](https://mp.weixin.qq.com/s/ig22T20Ie3jmTLhuPVPmdA)\n> [Spring能解决所有循环依赖吗？](https://mp.weixin.qq.com/s/Un8pyET2XDXpDY4FnRbwXw)\n\n### 相关文章\n\n- [Spring三级缓存解决循环依赖](https://mp.weixin.qq.com/s/ns9JEpvMt7U-nsMZzEUIUQ)\n- [终于有人把Spring循环依赖讲清楚了！](https://mp.weixin.qq.com/s/L1PJ-cikoS8sOORszEYnfw)\n- [烂大街的Spring循环依赖该如何回答？](https://mp.weixin.qq.com/s/5VHU2qRQMPL0IOZuEOPmQA)\n- [spring：我是如何解决循环依赖的？](https://mp.weixin.qq.com/s/7S9wVOVJyoHiC_RnhZrJTw)\n- [Spring为何需要三级缓存解决循环依赖，而不是二级缓存？](https://mp.weixin.qq.com/s/BaRlMNo0HlPP9Vz4x9bUaA)\n- [图解Spring循环依赖，写得太好了！](https://mp.weixin.qq.com/s/JuS6aewMXSp22zjwWCKt6g)\n- [Spring是如何解决循环依赖的](https://mp.weixin.qq.com/s/e7e-Pct5CcMrHBCsdjsLSQ)\n- [Spring面试题之循环依赖的理解](https://mp.weixin.qq.com/s/amgsB3MMvcA2pw9N2wECDw)\n- [Spring的循环依赖，到底是什么样的](https://mp.weixin.qq.com/s/qWWjWpIbghj5v6-KCd8xxA)\n- [面试官:SpringBoot循环依赖，如何解决？](https://mp.weixin.qq.com/s/YSXfAn8n313TMFIKM92LPw)\n- [透过源码，捋清楚循环依赖到底是如何解决的！](https://mp.weixin.qq.com/s/YIokfCvLKLhcsEpO734Qtg)\n\n## Spring相关注解\n\n### @RequestBody\n\n主要用来接收前端传递给后端的json字符串中的数据的(请求体中的数据的),GET方式无请求体，所以使用@RequestBody接收数据时，前端不能使用GET方式提交数据，而是用POST方式进行提交,在方法里面标记，可以作为一个对象接收,也可以作为字符串接收，关键在于Spring中对json的解析配置.\n```js\n// jquery使用下面的请求时可以使用@RequestBody注解接收参数\nvar data =  {\"id\" : $(\"#id\").val(),\"userId\" : $(\"#userId\").val()}\n$.ajax({\n\turl : \"/api/updateFeedback\",\n\tasync : false,\n\ttype : \"POST\",\n\tcontentType : 'application/json',\n\tdataType : 'json',\n\tdata :JSON.stringify(data),\n\tsuccess : function(data) {}\n});\n```\n> [@RequestBody接收数组、List参数、@Deprecated标记废弃方法](https://mp.weixin.qq.com/s/iyubfxmV_8KU8v4cg1A7tg)\n\n### @PostConstruct和@PreDestroy\n\n@PostConstruct该注解被用来修饰一个非静态的void()方法,被@PostConstruct修饰的方法会当bean创建完成的时候，会后置执行@PostConstruct修饰的方法。PostConstruct在构造函数之后执行，bean的init()方法之前执行,相当于init-method,使用在方法上，当Bean初始化时执行,Constructor(构造方法) -> @Autowired(依赖注入) -> @PostConstruct(注释的方法)\n\n@PreDestroy类似于destory-method在servlet destory()方法之后执行\n\n\n### @Configuration和@Component的区别\n一句话概括就是@Configuration中所有带@Bean注解的方法都会被动态代理，因此调用该方法返回的都是同一个实例。\n理解：调用@Configuration类中的@Bean注解的方法，返回的是同一个实例；而调用@Component类中的@Bean注解的方法，返回的是一个新的实例。\n\n> [终于搞懂了@Configuration和@Component的区别](https://mp.weixin.qq.com/s/-_h5Hz6MOBb8TK3qm9gBog)\n> [@Configuration和@Component有何区别？](https://mp.weixin.qq.com/s/D84pWlXs7wbHFYvCE5TAVQ)\n\n### 条件注解\n> [Spring Boot中条件注解底层如何实现的？](https://mp.weixin.qq.com/s/XhNTfz6nw-rfP2avh0owAQ)\n\n## Spring设计模式\n\n### 工厂设计模式\n\nSpring使用工厂模式可以通过BeanFactory或ApplicationContext创建bean对象。\n\n**两者对比**：\n\n- BeanFactory：延迟注入(使用到某个bean的时候才会注入),相比于ApplicationContext来说会占用更少的内存，程序启动速度更快。\n- ApplicationContext：容器启动的时候，不管你用没用到，一次性创建所有bean。BeanFactory仅提供了最基本的依赖注入支持，ApplicationContext扩展了BeanFactory,除了有BeanFactory的功能还有额外更多功能，所以一般开发人员使用ApplicationContext会更多。\n\nApplicationContext的三个实现类：\n\n1. ClassPathXmlApplication：把上下文文件当成类路径资源。\n2. FileSystemXmlApplication：从文件系统中的XML文件载入上下文定义信息。\n3. XmlWebApplicationContext：从Web系统中的XML文件载入上下文定义信息。\n\nExample:\n\n```java\nimport org.springframework.context.ApplicationContext;\nimport org.springframework.context.support.FileSystemXmlApplicationContext;\n\npublic class App {\n\tpublic static void main(String[] args) {\n\t\tApplicationContext context = new FileSystemXmlApplicationContext(\n\t\t\t\t\"C:/work/IOC Containers/springframework.applicationcontext/src/main/resources/bean-factory-config.xml\");\n\n\t\tHelloApplicationContext obj = (HelloApplicationContext) context.getBean(\"helloApplicationContext\");\n\t\tobj.getMsg();\n\t}\n}\n```\n\n### 单例设计模式\n\n在我们的系统中，有一些对象其实我们只需要一个，比如说：线程池、缓存、对话框、注册表、日志对象、充当打印机、显卡等设备驱动程序的对象。事实上，这一类对象只能有一个实例，如果制造出多个实例就可能会导致一些问题的产生，比如：程序的行为异常、资源使用过量、或者不一致性的结果。\n\n**使用单例模式的好处**:\n\n- 对于频繁使用的对象，可以省略创建对象所花费的时间，这对于那些重量级对象而言，是非常可观的一笔系统开销；\n- 由于new操作的次数减少，因而对系统内存的使用频率也会降低，这将减轻GC压力，缩短GC停顿时间。\n\n**Spring中bean的默认作用域就是singleton(单例)的**。除了singleton作用域，Spring中bean还有下面几种作用域：\n\n- **prototype**：每次获取都会创建一个新的bean实例。也就是说，连续getBean()两次，得到的是不同的Bean实例。\n- **request（仅Web应用可用）**:每一次HTTP请求都会产生一个新的bean（请求bean），该bean仅在当前HTTPrequest内有效。\n- **session（仅Web应用可用）**:每一次来自新session的HTTP请求都会产生一个新的bean（会话bean），该bean仅在当前HTTPsession内有效。\n- **application/global-session（仅Web应用可用）**：每个Web应用在启动时创建一个Bean（应用Bean），该bean仅在当前应用启动时间内有效。\n- **websocket（仅Web应用可用）**：每一次WebSocket会话产生一个新的bean。\n\n> [详解Spring的6种内置作用域及其应用场景](https://mp.weixin.qq.com/s/FB-eiiYIaGUY1Y20qBxanw)\n\nSpring通过ConcurrentHashMap实现单例注册表的特殊方式实现单例模式。Spring实现单例的核心代码如下：\n\n```java\n// 通过ConcurrentHashMap（线程安全）实现单例注册表\nprivate final Map<String, Object> singletonObjects = new ConcurrentHashMap<String, Object>(64);\n\npublic Object getSingleton(String beanName, ObjectFactory<?> singletonFactory) {\n        Assert.notNull(beanName, \"'beanName' must not be null\");\n        synchronized (this.singletonObjects) {\n            // 检查缓存中是否存在实例\n            Object singletonObject = this.singletonObjects.get(beanName);\n            if (singletonObject == null) {\n                // ...省略了很多代码\n                try {\n                    singletonObject = singletonFactory.getObject();\n                }\n                // ...省略了很多代码\n                // 如果实例对象在不存在，我们注册到单例注册表中。\n                addSingleton(beanName, singletonObject);\n            }\n            return (singletonObject != NULL_OBJECT ? singletonObject : null);\n        }\n    }\n    // 将对象添加到单例注册表\n    protected void addSingleton(String beanName, Object singletonObject) {\n            synchronized (this.singletonObjects) {\n                this.singletonObjects.put(beanName, (singletonObject != null ? singletonObject : NULL_OBJECT));\n\n            }\n        }\n}\n```\n\n**单例Bean存在线程安全问题吗？**\n\n大部分时候我们并没有在项目中使用多线程，所以很少有人会关注这个问题。单例Bean存在线程问题，主要是因为当多个线程操作同一个对象的时候是存在资源竞争的。常见的有两种解决办法：\n\n1. 在Bean中尽量避免定义可变的成员变量。\n2. 在类中定义一个ThreadLocal成员变量，将需要的可变成员变量保存在ThreadLocal中（推荐的一种方式）。\n\n不过，大部分Bean实际都是无状态（没有实例变量）的（比如Dao、Service），这种情况下，Bean是线程安全的。\n\n### 代理设计模式\n\n**代理模式在AOP中的应用**\n\n**AOP(Aspect-Oriented Programming，面向切面编程)**,能够将那些与业务无关，却为业务模块所共同调用的逻辑或责任（例如事务处理、日志管理、权限控制等）封装起来，便于减少系统的重复代码，降低模块间的耦合度，并有利于未来的可拓展性和可维护性。**Spring AOP就是基于动态代理的**，如果要代理的对象，实现了某个接口，那么Spring AOP会使用**JDKProxy**去创建代理对象，而对于没有实现接口的对象，就无法使用JDKProxy去进行代理了，这时候Spring AOP会使用**Cglib**生成一个被代理对象的子类来作为代理，如下图所示：\n\n![SpringAOPProcess](https://my-blog-to-use.oss-cn-beijing.aliyuncs.com/2019-6/SpringAOPProcess.jpg)\n\n当然，你也可以使用AspectJ,SpringAOP已经集成了AspectJ，AspectJ应该算的上是Java生态系统中最完整的AOP框架了。使用AOP之后我们可以把一些通用功能抽象出来，在需要用到的地方直接使用即可，这样大大简化了代码量。我们需要增加新功能时也方便，这样也提高了系统扩展性。日志功能、事务管理等等场景都用到了AOP。\n\n#### Spring AOP和AspectJ AOP有什么区别?\n\n**Spring AOP属于运行时增强，而AspectJ是编译时增强**。Spring AOP基于代理(Proxying)，而AspectJ基于字节码操作(Bytecode Manipulation)。Spring AOP已经集成了AspectJ，AspectJ应该算的上是Java生态系统中最完整的AOP框架了。AspectJ相比于Spring AOP功能更加强大，但是Spring AOP相对来说更简单，如果我们的切面比较少，那么两者性能差异不大。但是，当切面太多的话，最好选择AspectJ，它比Spring AOP快很多。\n\n### 模板方法模式\n\n模板方法模式是一种行为设计模式，它定义一个操作中的算法的骨架，而将一些步骤延迟到子类中。模板方法使得子类可以不改变一个算法的结构即可重定义该算法的某些特定步骤的实现方式。\n\n```java\npublic abstract class Template {\n    //这是我们的模板方法\n    public final void TemplateMethod(){\n        PrimitiveOperation1();\n        PrimitiveOperation2();\n        PrimitiveOperation3();\n    }\n\n    protected void  PrimitiveOperation1(){\n        //当前类实现\n    }\n\n    //被子类实现的方法\n    protected abstract void PrimitiveOperation2();\n    protected abstract void PrimitiveOperation3();\n\n}\npublic class TemplateImpl extends Template {\n\n    @Override\n    public void PrimitiveOperation2() {\n        //当前类实现\n    }\n\n    @Override\n    public void PrimitiveOperation3() {\n        //当前类实现\n    }\n}\n```\n\nSpring中JdbcTemplate、HibernateTemplate等以Template结尾的对数据库操作的类，它们就使用到了模板模式。一般情况下，我们都是使用继承的方式来实现模板模式，但是Spring并没有使用这种方式，而是使用Callback模式与模板方法模式配合，既达到了代码复用的效果，同时增加了灵活性。\n\n### 观察者模式\n\n观察者模式是一种对象行为型模式。它表示的是一种对象与对象之间具有依赖关系，当一个对象发生改变的时候，这个对象所依赖的对象也会做出反应。Spring事件驱动模型就是观察者模式很经典的一个应用。Spring事件驱动模型非常有用，在很多场景都可以解耦我们的代码。比如我们每次添加商品的时候都需要重新更新商品索引，这个时候就可以利用观察者模式来解决这个问题。\n\n#### Spring事件驱动模型中的三种角色\n\n##### 事件角色\n\n`ApplicationEvent`(`org.springframework.context`包下)充当事件的角色,这是一个抽象类，它继承了`java.util.EventObject`并实现了`java.io.Serializable`接口。\n\nSpring中默认存在以下事件，他们都是对`ApplicationContextEvent`的实现(继承自`ApplicationContextEvent`)：\n\n- ContextStartedEvent：ApplicationContext启动后触发的事件;\n- ContextStoppedEvent：ApplicationContext停止后触发的事件;\n- ContextRefreshedEvent：ApplicationContext初始化或刷新完成后触发的事件;\n- ContextClosedEvent：ApplicationContext关闭后触发的事件。\n\n![ApplicationEvent-Subclass](https://my-blog-to-use.oss-cn-beijing.aliyuncs.com/2019-6/ApplicationEvent-Subclass.png)\n\n##### 事件监听者角色\n\n`ApplicationListener`充当了事件监听者角色，它是一个接口，里面只定义了一个`onApplicationEvent()`方法来处理`ApplicationEvent`。`ApplicationListener`接口类源码如下，可以看出接口定义看出接口中的事件只要实现了`ApplicationEvent`就可以了。所以，在Spring中我们只要实现`ApplicationListener`接口的`onApplicationEvent()`方法即可完成监听事件\n\n```java\npackage org.springframework.context;\nimport java.util.EventListener;\n@FunctionalInterface\npublic interface ApplicationListener<E extends ApplicationEvent> extends EventListener {\n    void onApplicationEvent(E var1);\n}\n```\n\n##### 事件发布者角色\n\n`ApplicationEventPublisher`充当了事件的发布者，它也是一个接口。\n\n```java\n@FunctionalInterface\npublic interface ApplicationEventPublisher {\n    default void publishEvent(ApplicationEvent event) {\n        this.publishEvent((Object)event);\n    }\n\n    void publishEvent(Object var1);\n}\n```\n\n`ApplicationEventPublisher`接口的`publishEvent()`这个方法在`AbstractApplicationContext`类中被实现，阅读这个方法的实现，你会发现实际上事件真正是通过`ApplicationEventMulticaster`来广播出去的。具体内容过多，就不在这里分析了，后面可能会单独写一篇文章提到。\n\n#### Spring的事件流程总结\n\n1. 定义一个事件:实现一个继承自ApplicationEvent，并且写相应的构造函数；\n2. 定义一个事件监听者：实现ApplicationListener接口，重写onApplicationEvent()方法；\n3. 使用事件发布者发布消息:可以通过ApplicationEventPublisher的publishEvent()方法发布消息。\n\nExample:\n\n\n```java\n// 定义一个事件,继承自ApplicationEvent并且写相应的构造函数\npublic class DemoEvent extends ApplicationEvent{\n    private static final long serialVersionUID = 1L;\n\n    private String message;\n\n    public DemoEvent(Object source,String message){\n        super(source);\n        this.message = message;\n    }\n\n    public String getMessage() {\n         return message;\n          }\n\n\n// 定义一个事件监听者,实现ApplicationListener接口，重写onApplicationEvent()方法；\n@Component\npublic class DemoListener implements ApplicationListener<DemoEvent>{\n\n    //使用onApplicationEvent接收消息\n    @Override\n    public void onApplicationEvent(DemoEvent event) {\n        String msg = event.getMessage();\n        System.out.println(\"接收到的信息是：\"+msg);\n    }\n\n}\n// 发布事件，可以通过ApplicationEventPublisher的publishEvent()方法发布消息。\n@Component\npublic class DemoPublisher {\n\n    @Autowired\n    ApplicationContext applicationContext;\n\n    public void publish(String message){\n        //发布事件\n        applicationContext.publishEvent(new DemoEvent(this, message));\n    }\n}\n```\n\n当调用DemoPublisher的publish()方法的时候，比如demoPublisher.publish(\"你好\")，控制台就会打印出:接收到的信息是：你好。\n\n### 适配器模式\n\n适配器模式(Adapter Pattern)将一个接口转换成客户希望的另一个接口，适配器模式使接口不兼容的那些类可以一起工作。\n\n#### Spring AOP中的适配器模式\n\n我们知道Spring AOP的实现是基于代理模式，但是Spring AOP的增强或通知(Advice)使用到了适配器模式，与之相关的接口是AdvisorAdapter。Advice常用的类型有：BeforeAdvice（目标方法调用前,前置通知）、AfterAdvice（目标方法调用后,后置通知）、AfterReturningAdvice(目标方法执行结束后，return之前)等等。每个类型Advice（通知）都有对应的拦截器:MethodBeforeAdviceInterceptor、AfterReturningAdviceInterceptor、ThrowsAdviceInterceptor等等。Spring预定义的通知要通过对应的适配器，适配成MethodInterceptor接口(方法拦截器)类型的对象（如：MethodBeforeAdviceAdapter通过调用getInterceptor方法，将MethodBeforeAdvice适配成MethodBeforeAdviceInterceptor）。\n\n#### Spring MVC中的适配器模式\n\n在SpringMVC中，DispatcherServlet根据请求信息调用HandlerMapping，解析请求对应的Handler。解析到对应的Handler（也就是我们平常说的Controller控制器）后，开始由HandlerAdapter适配器处理。HandlerAdapter作为期望接口，具体的适配器实现类用于对目标类进行适配，Controller作为需要适配的类。\n\n**为什么要在SpringMVC中使用适配器模式？**\n\nSpringMVC中的Controller种类众多，不同类型的Controller通过不同的方法来对请求进行处理。如果不利用适配器模式的话，DispatcherServlet直接获取对应类型的Controller，需要的自行来判断，像下面这段代码一样：\n\n\n```java\nif(mappedHandler.getHandler() instanceof MultiActionController){\n   ((MultiActionController)mappedHandler.getHandler()).xxx\n}else if(mappedHandler.getHandler() instanceof XXX){\n    ...\n}else if(...){\n   ...\n}\n```\n\n假如我们再增加一个Controller类型就要在上面代码中再加入一行判断语句，这种形式就使得程序难以维护，也违反了设计模式中的开闭原则–对扩展开放，对修改关闭。\n\n### 装饰者模式\n\n装饰者模式可以动态地给对象添加一些额外的属性或行为。相比于使用继承，装饰者模式更加灵活。简单点儿说就是当我们需要修改原有的功能，但我们又不愿直接去修改原有的代码时，设计一个Decorator套在原有代码外面。其实在JDK中就有很多地方用到了装饰者模式，比如InputStream家族，InputStream类下有FileInputStream(读取文件)、BufferedInputStream(增加缓存,使读取文件速度大大提升)等子类都在不修改InputStream代码的情况下扩展了它的功能。\n\n![装饰者模式示意图](https://my-blog-to-use.oss-cn-beijing.aliyuncs.com/2019-6/Decorator.jpg)\n\nSpring中配置DataSource的时候，DataSource可能是不同的数据库和数据源。我们能否根据客户的需求在少修改原有类的代码下动态切换不同的数据源？这个时候就要用到装饰者模式(这一点我自己还没太理解具体原理)。Spring中用到的包装器模式在类名上含有Wrapper或者Decorator。这些类基本上都是动态地给一个对象添加一些额外的职责\n\n### 总结\n\nSpring框架中用到了哪些设计模式？\n\n- **工厂设计模式**:Spring使用工厂模式通过BeanFactory、ApplicationContext创建bean对象。\n- **代理设计模式**:Spring AOP功能的实现。\n- **单例设计模式**:Spring中的Bean默认都是单例的。\n- **模板方法模式**:Spring中jdbcTemplate、hibernateTemplate等以Template结尾的对数据库操作的类，它们就使用到了模板模式。\n- **包装器设计模式**:我们的项目需要连接多个数据库，而且不同的客户在每次访问中根据需要会去访问不同的数据库。这种模式让我们可以根据客户的需求能够动态切换不同的数据源。\n- **观察者模式**:Spring事件驱动模型就是观察者模式很经典的一个应用。\n- **适配器模式**:Spring AOP的增强或通知(Advice)使用到了适配器模式、Spring MVC中也是用到了适配器模式适配Controller。\n- .....\n\n> [原文链接](https://javaguide.cn/system-design/framework/spring/spring-design-patterns-summary.html)\n> [Spring用到了哪些设计模式](https://mp.weixin.qq.com/s/O-gUDExJc5AKwb-t4ZDkNA)\n> [Spring中经典的9种设计模式](https://mp.weixin.qq.com/s/gz2-izPrgW1AGbqqovT0cA)\n\n## Spring MVC\n\n### 组件\n\n**Handle**:Handler是一个Controller的对象和请求方式的组合的一个Object对象\n**HandleExcutionChains**：是HandleMapping返回的一个处理执行链，它是对Handle的二次封装，将拦截器关联到一起。然后，在DispatcherServlert中完成了拦截器链对handler的过滤。**DispatcherServlet**要将一个请求交给哪个特定的Controller，它需要咨询一个Bean——这个Bean为“HandlerMapping”。HandlerMapping是把一个URL指定到一个Controller上，（就像应用系统的web.xml文件使用<servlet-mapping\\>将URL映射到servlet）。\n**DispatcherServlet**：作为前端控制器，整个流程控制的中心，控制其它组件执行，统一调度，降低组件之间的耦合性，提高每个组件的扩展性。\n作用：接收请求，响应结果，相当于转发器，中央处理器。有了dispatcherServlet减少了其它组件之间的耦合度。用户请求到达前端控制器，它就相当于mvc模式中的c，dispatcherServlet是整个流程控制的中心，由它调用其它组件处理用户的请,dispatcherServlet的存在降低了组件之间的耦合性\n**HandlerMapping**：通过扩展处理器映射器实现不同的映射方式，作用:根据请求的url查找Handler,HandlerMapping负责根据用户请求找到Handler即处理器，springmvc提供了不同的映射器实现不同的映射方式，例如：配置文件方式，实现接口方式，注解方式等\n**HandlAdapter**：通过扩展处理器适配器，支持更多类型的处理器。\n作用：按照特定规则（HandlerAdapter要求的规则）去执行Handler，通过HandlerAdapter对处理器进行执行，这是适配器模式的应用，通过扩展适配器可以对更多类型的处理器进行执行。\n**ViewResolver**：通过扩展视图解析器，支持更多类型的视图解析，例如：jsp、freemarker、pdf、excel等。作用：进行视图解析，根据逻辑视图名解析成真正的视图（view）\n作用：View Resolver负责将处理结果生成View视图，View Resolver首先根据逻辑视图名解析成物理视图名即具体的页面地址，再生成View视图对象，最后对View进行渲染将处理结果通过页面展示给用户。springmvc框架提供了很多的View视图类型，包括：jstlView、freemarkerView、pdfView等。一般情况下需要通过页面标签或页面模版技术将模型数据通过页面展示给用户，需要由工程师根据业务需求开发具体的页面。\n\n### DispatcherServlet的工作流程\n\n![](/images/DispatcherServlet的工作流程.png)\n\n1. 向服务器发送HTTP请求，请求被前端控制器DispatcherServlet捕获。\n2. DispatcherServlet根据-servlet.xml中的配置对请求的URL进行解析，得到请求资源标识符（URI）。然后根据该URI，调用HandlerMapping获得该Handler配置的所有相关的对象（包括Handler对象以及Handler对象对应的拦截器），最后以HandlerExecutionChain对象的形式返回。\n3. DispatcherServlet根据获得的Handler，选择一个合适的HandlerAdapter。（附注：如果成功获得HandlerAdapter后，此时将开始执行拦截器的preHandler(…)方法）。\n4. 提取Request中的模型数据，填充Handler入参，开始执行Handler(Controller)。在填充Handler的入参过程中，根据你的配置，Spring将帮你做一些额外的工作：HttpMessageConveter：将请求消息（如Json、xml等数据）转换成一个对象，将对象转换为指定的响应信息。\n数据转换：对请求消息进行数据转换。如String转换成Integer、Double等。数据根式化：对请求消息进行数据格式化。如将字符串转换成格式化数字或格式化日期等。数据验证：验证数据的有效性（长度、格式等），验证结果存储到BindingResult或Error中。\n5. Handler(Controller)执行完成后，向DispatcherServlet返回一个ModelAndView对象；\n6. 根据返回的ModelAndView，选择一个适合的ViewResolver(必须是已经注册到Spring容器中的ViewResolver)返回给DispatcherServlet。\n7. ViewResolver结合Model和View，来渲染视图。\n8. 视图负责将渲染结果返回给客户端。\n\nagain\n1. 用户发送请求至前端控制器DispatcherServlet。\n2. DispatcherServlet收到请求调用HandlerMapping处理器映射器。\n3. 处理器映射器找到具体的处理器（controller或者handle）(可以根据xml配置、注解进行查找)，生成处理器对象及处理器拦截器(如果有则生成)一并返回给DispatcherServlet。\n4. DispatcherServlet调用HandlerAdapter处理器适配器。\n5. HandlerAdapter经过适配调用具体的处理器(Controller，也叫后端控制器)。\n6. Controller执行完成返回ModelAndView。\n7. HandlerAdapter将controller执行结果ModelAndView返回给DispatcherServlet。\n8. DispatcherServlet将ModelAndView传给ViewReslover视图解析器。\nViewReslover解析后返回具体View。\n9. DispatcherServlet根据View进行渲染视图（即将模型数据填充至视图中）。\n10. DispatcherServlet响应用户。\n\n核心架构的具体流程步骤如下：\n1. 首先用户发送请求——>DispatcherServlet，前端控制器收到请求后自己不进行处理，而是委托给其他的解析器进行处理，作为统一访问点，进行全局的流程控制；\n2. DispatcherServlet——>HandlerMapping，HandlerMapping将会把请求映射为HandlerExecutionChain对象（包含一个Handler处理器（页面控制器）对象、多个HandlerInterceptor拦截器）对象，通过这种策略模式，很容易添加新的映射策略；\n3. DispatcherServlet——>HandlerAdapter，HandlerAdapter将会把处理器包装为适配器，从而支持多种类型的处理器，即适配器设计模式的应用，从而很容易支持很多类型的处理器；\n4. HandlerAdapter——>处理器功能处理方法的调用，HandlerAdapter将会根据适配的结果调用真正的处理器的功能处理方法，完成功能处理；并返回一个ModelAndView对象（包含模型数据、逻辑视图名）；\n5. ModelAndView的逻辑视图名——>ViewResolver，ViewResolver将把逻辑视图名解析为具体的View，通过这种策略模式，很容易更换其他视图技术；\n6. View——>渲染，View会根据传进来的Model模型数据进行渲染，此处的Model实际是一个Map数据结构，因此很容易支持其他视图技术；\n7. 返回控制权给DispatcherServlet，由DispatcherServlet返回响应给用户，到此一个流程结束。\n下边两个组件通常情况下需要开发：\nHandler：处理器，即后端控制器用controller表示。\nView：视图，即展示给用户的界面，视图中通常需要标签语言展示模型数据。\n\n### Spring MVC相关文章\n\n- [SpringMVC执行过程解析](https://mp.weixin.qq.com/s?__biz=Mzg2MDYzODI5Nw==&amp;mid=2247494012&amp;idx=2&amp;sn=1c5052c5b4a5547a21449412f619327b&amp;source=41#wechat_redirect)\n- [Spring MVC请求处理过程不是两张流程图就能讲清楚的](https://mp.weixin.qq.com/s/klCAv0TM2tvgy4xqoC1hQw)\n- [Spring MVC初始化流程分析](https://mp.weixin.qq.com/s/IeMOfnXhOX5RCf4i5Xsdzw)\n- [Spring MVC源码分析之DispatcherServlet](https://mp.weixin.qq.com/s/F0QZ-Ukgtn3oC6a4loM9Vg)\n- [Spring MVC九大组件之HandlerMapping深入分析](https://mp.weixin.qq.com/s/0x7_OXPDFX5BqF0jGxN2Vg)\n- [Spring MVC九大组件之HandlerAdapter深入分析](https://mp.weixin.qq.com/s/NCnawbIaLUQJNZiDi2yeCw)\n- [Spring MVC九大组件之ViewResolver深入分析](https://mp.weixin.qq.com/s/rn-6QyuYIsM_P5b4B1OIrg)\n- [编写Spring MVC控制器的14个技巧！涨知识了！](https://mp.weixin.qq.com/s/685jUKqg6I6r0RbmirsRxg)\n- [SpringMVC异常处理体系深入分析](https://mp.weixin.qq.com/s/ZKBQSCMPV7T9yNcMQ5_pQQ)\n- [使用Spring MVC的14个顶级技巧！](https://mp.weixin.qq.com/s/DAWdH_0VWZp3oaws_leJhQ)\n- [Spring5里边的新玩法！这种URL请求让我涨见识了](https://mp.weixin.qq.com/s/uzI-ilR-cv5iX3HWFUOmXA)\n\n\n## [Spring Boot](https://github.com/xmxe/springboot)\n\n**配置文件默认的查找路径如下**：\n```\nfile:./config/\nfile:./\nclasspath:/config/\nclasspath:/\n```\n配置⽂件名可以通过`spring.config.name`修改，最简单的⽅法是放置⼀个配置⽂件到jar包同层⽬录下，或是同层⽬录下的config⼦⽬录下，启动jar包即可加载配置⽂件实现配置项的覆盖。spring boot指定外部的配置⽂件,可以通过修改启动参数的值来指定加载⽬录或是加载⽂件:`spring.config.location`\n\n```shell\n$ java -jar myproject.jar --spring.config.location=classpath:/default.properties,classpath:/override.properties.\n```\n这样不会去默认位置加载配置⽂件，⽽是加载类路径下default.properties和override.properties的⽂件，override.properties中的同名配置会覆盖default.properties,如果指定的路径是以/结尾则是⽬录配置，会去⽬录下找配置⽂件。\n\n> [如何不重新编译让Spring Boot配置文件生效](https://mp.weixin.qq.com/s/pNAU_w6RQIjzxfaadjV_pA)\n\n**特定配置**\n在开发、测试、发布过程中，这三个场景⽐较固定，通常会定义三份不同的配置application-{profile}.yml，在使⽤时通过profile参数来切换。applicaiton-dev.yml，applicaiton-test.yml，applicaiton-prd.yml启动时，通过指定spring.profiles.active参数来切换配置⽂件\n\nspringboot项⽬启动的时候可以直接使⽤java -jar xxxjar这样。\n1. -DpropName=propValue的形式携带，要放在-jar参数前⾯,`java -Dxxx=test -DprocessType=1 -jar xxx.jar`,取值:System.getProperty(\"propName\")\n2. 参数直接跟在命令后⾯,`java -jar xxx jar processType=1 processType2=2`,取值:参数就是jar包⾥主启动类中main⽅法的args参数，按顺序来\n3. springboot的⽅式，--key=value⽅式,`java -jar xxx.jar --xxx=test`,取值:spring的@value(\"$(xxx)”)\n\n区别：\n\n1. -D参数为jvm参数，项⽬启动完后可通过`System.getProperty(\"nacos.standalone\")`进⾏读取,也可以通过这个⽅式`Integer.getInteger(\"nacos.http.timeout\",5000);`获取jvm参数\n2. --参数，是通过main的args传⼊进去的，args参数最后会放⼊env环境变量⾥，所以配置bean（@ConfigurationProperties被注解修饰的）的配置值也被覆盖。\n3. spring boot修改配置参数时命令行优先级最高,其次环境变量,最后是配置文件，使用命令行时--优先级最高,其次是-D(VM options)\n\n**@Value失效的情况**\n1. 使用static或final修饰\n2. 类没有注册为bean\n3. 构造方法调用该注解修饰的字段也会失效\n\n```java\n@ConfigurationProperties(prefix=\"user\")\n@PropertySource(value = {\"demo/props/demo.properties\"})\n// @PropertySource(value = {\"classpath:user.yml\"}, factory = PropertySourceFactory.class)\n// @Profile:指定组件在哪个环境的情况下才能被注册到容器中，不指定，任何环境下都能注册这个组件\n\n@Autowired\nEnvironment environmen\nenvironmen.getProperty(\"propName\")\n```\nSpringboot中默认的静态资源路径有4个，分别是：\n```\nclasspath:/METAINF/resources/，classpath:/resources/，classpath:/static/，classpath:/public/\n优先级顺序为:META-INF/resources>resources>static>public\n```\n\n\n## XML配置\n\n### <mvc:annotation-driven />\nSpring 3.0.x中使用了<mvc:annotation-driven />后，默认会帮我们注册默认处理请求，参数和返回值的类，其中最主要的两个类：DefaultAnnotationHandlerMapping和AnnotationMethodHandlerAdapter，分别为HandlerMapping的实现类和HandlerAdapter的实现类。从3.1.x版本开始对应实现类改为了RequestMappingHandlerMapping和RequestMappingHandlerAdapter。\n\n### <context:component-scan />\n当配置了<mvc:annotation-driven />后，Spring就知道了我们启用注解驱动。然后Spring通过<context:component-scan />标签的配置，会自动为我们将扫描到的@Component,@Controller,@Service,@Repository等注解标记的组件注册到工厂中，来处理我们的请求.\n<context:component-scan />标签是告诉Spring来扫描指定包下的类，并注册被@Component，@Controller，@Service，@Repository等注解标记的组件。而<mvc:annotation-driven />是告知Spring，我们启用注解驱动\n\n### <mvc:default-servlet-handler />\n当在web.xml中将前端控制器的映射请求设置为\"/\"时所有的请求包括静态资源的请求都提交到DispatcherServlet进行处理，所以访问静态资源会404，SpringM VC在全局配置文件中提供了一个<mvc:default-servlet-handler/>标签。在WEB容器启动的时候会在上下文中定义一个DefaultServletHttpRequestHandler，它会对DispatcherServlet的请求进行处理，如果该请求已经作了映射，那么会接着交给后台对应的处理程序，如果没有作映射，就交给WEB应用服务器默认的Servlet处理，从而找到对应的静态资源，只有再找不到资源时才会报错。\n一般WEB应用服务器默认的Servlet都是default。如果默认Servlet用不同名称自定义配置，或者在缺省Servlet名称未知的情况下使用了不同的Servlet容器，则必须显式提供默认Servlet的名称，如下：\n```xml\n<mvc:default-servlet-handler default-servlet-name=\"myCustomDefaultServlet\"/>\n```\n相当于在web.xml里这样配置\n\n```xml\n<servlet-mapping>\n\t<servlet-name>default</servlet-name>\n\t<url-pattern>*.js</url-pattern>\n</servlet-mapping>\n```\n\n### <context:annotation-config />\n< context:annotation-config>是用于激活那些已经在spring容器里注册过的bean上面的注解，也就是显示的向Spring注册AutowiredAnnotationBeanPostProcessor，CommonAnnotationBeanPostProcessor，PersistenceAnnotationBeanPostProcessor，RequiredAnnotationBeanPostProcessor这四个Processor，注册这4个BeanPostProcessor的作用，就是为了你的系统能够识别相应的注解。BeanPostProcessor就是处理注解的处理器。\n一般来说，这些注解我们还是比较常用，尤其是@Autowired的注解，比如我们要使用@Autowired注解，那么就必须事先在Spring容器中声明AutowiredAnnotationBeanPostProcessor Bean。传统声明方式如下\n```xml\n<bean class=\"org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor\"/>\n```\n在自动注入的时候更是经常使用，所以如果总是需要按照传统的方式一条一条配置显得有些繁琐和没有必要，于是spring给我们提供<context:annotation-config />的简化配置方式，自动帮你完成声明。\n\n### <context:component-scan base-package=”XX.XX”/>\n该配置项其实也包含了自动注入上述processor的功能，因此**当使用<context:component-scan />后，就可以将<context:annotation-config />移除了**。\n<context:annotation-config />：仅能够在已经在已经注册过的bean上面起作用。对于没有在spring容器中注册的bean，它并不能执行任何操作。\n<context:component-scan base-package=\"XX.XX\"/>除了具有上面的功能之外，还具有自动将带有@component,@service,@Repository等注解的对象注册到spring容器中的功能。\n如果同时使用这两个配置会不会出现重复注入的情况呢？\n<context:annotation-config />和<context:component-scan />同时存在的时候，前者会被忽略。如@Autowire，@Resource等注解只会被注入一次！\n\n### <context:spring-configured />\n在没有注入ioc容器的类里面进行依赖注入,例如当一个类没有被Spring注册为bean,却想要在这个类里面使用@Autowired注解需要在XML里配置此标签\n\n### 注册拦截器\n```xml\n<!-- 自定义拦截器，拦截所有请求，验证是否登录 -->\n<mvc:interceptors>\n\t<mvc:interceptor>\n\t<mvc:mapping path=\"/**\"/>\n\t<bean class=\"com.example.interceptor.CommonInterceptor\"></bean>\n\t</mvc:interceptor>\n</mvc:interceptors>\n```\n\n### 注册自定义参数解析器\n```xml\n<!--注册自定义参数解析器-->\n <mvc:annotation-driven>\n        <mvc:argument-resolvers>\n            <bean class=\"com.liyao.pre.UserIdArgumentResolver\"/>\n        </mvc:argument-resolvers>\n  </mvc:annotation-driven>\n```\n\n### 属性文件读取\n```xml\n<!-- 属性文件读取-->\n<context:property-placeholder location=\"classpath:jdbc.properties\" />\n<!--或者-->\n<bean id=\"propertyPlaceholderConfigurer\" class=\"org.springframework,beans.factory.config.PropertyPlaceholderConfigurer\">\n    <property name=\"locations\">\n        <list>\n            <value>jdbc.properties<value/>\n        </list>\n    </property>\n</bean>\n```\n### 有关classpath和classpath\\*\nSpring可以通过指定classpath\\*:或classpath:前缀加路径的方式从classpath下加载文件。**classpath\\*:可以从多个jar文件中加载相同的文件。classpath:只能加载找到的第一个文件。**而使用classpath加载一般的优先级为：当前classes > jar包中的classes\n\n### 自定义消息转换器\n```xml\n<!--自定义消息转换器-->\n<mvc:annotation-driven >\n\t<mvc:message-converters register-defaults=\"true\">\n        <!--字符串转换器-->\n        <bean class=\"org.springframework.http.converter.StringHttpMessageConverter\" >\n            <property name = \"supportedMediaTypes\">\n                <list>\n                    <value>application/json;charset=utf-8</value>\n                    <value>text/html;charset=utf-8</value>\n                </list>\n            </property>\n        </bean>\n        <!--json转换器-->\n        <bean class=\"org.springframework.http.converter.json.MappingJackson2HttpMessageConverter\" />\n        <!--自己定义的消息转换器-->\n        <bean class =\"com.dzf.converter.MyMessageConverter\"> \n            <property name = \"supportedMediaTypes\">\n                <list>\n                    <value>application/json;charset=utf-8</value>\n                    <value>application/x-result;charset=utf-8</value>\n                    <value>text/html;charset=utf-8</value>\n                </list>\n            </property>\n        </bean>\n        </mvc:message-converters>\n    </mvc:annotation-driven>\n```\n\n### 扫描\n```xml\n<context:include-filter> <!--只扫描某个目录-->\n<context:exclude-filter> <!--不扫描某个目录-->\n\n```\n### Spring会自动发现基础的JTA实现\n```xml\n<tx:jta-transaction-manager /> <!--Spring会自动发现基础的JTA实现-->\n\n<!-- 自动为spring容器中那些配置@aspectJ切面的bean创建代理，织入切面。当然，spring\n在内部依旧采用AnnotationAwareAspectJAutoProxyCreator进行自动代理的创建工作，但具体实现的细节已经被<aop:aspectj-autoproxy />隐藏起来了-->\n<aop:aspectj-autoproxy />\n```\n\n### <aop:aspectj-autoproxy />\n```xml\n<!-- 有一个proxy-target-class属性，默认为false，表示使用jdk动态代理织入增强，当配为<aop:aspectj-autoproxy  poxy-target-class=\"true\"/>时，表示使用CGLib动态代理技术织入增强。不过即使proxy-target-class设置为false，如果目标类没有声明接口，则spring将自动使用CGLib动态代理。-->\n<aop:aspectj-autoproxy />\n\n```\n\n### 扫描@Scheduled\n```xml\n<task:annotation-driven /> <!--扫描@Scheduled-->\n```\n\n### org.springframework.web.filter.HiddenHttpMethodFilter\n浏览器form表单只支持GET与POST请求，而DELETE、PUT等method并不支持，spring3.0添加了一个过滤器，可以将这些请求转换为标准的http方法，使得支持GET、POST、PUT与DELETE请求，该过滤器为HiddenHttpMethodFilter,需要注意的是，由于doFilterInternal方法只对method为post的表单进行过滤，所以在页面中必须如下设置：\n```xml\n<form action=\"...\" method=\"post\">\n   <input type=\"hidden\" name=\"_method\" value=\"put\" />\n        ......\n</form>\n  而不是使用：\n<form action=\"...\" method=\"put\">\n        ......\n</form>\n```\nHiddenHttpMethodFilter必须作用于dispatcher前\n\n### org.springframework.web.context.ContextLoaderListener\nContextLoaderListener的作用就是启动Web容器时，自动装配ApplicationContext.xml的配置信息。\nContextLoaderListener继承自ContextLoader，实现的是ServletContextListener接口。在web.xml配置这个监听器，启动容器时，就会默认执行它实现的方法。ContextLoaderListener可以指定在Web应用程序启动时载入Ioc容器，正是通过ContextLoader来实现的，ContextLoader来完成实际的WebApplicationContext，也就是Ioc容器的初始化工作。如果没有显式声明，则系统默认在WEB-INF/applicationContext.xml。\n\n### org.springframework.web.util.IntrospectorCleanupListener\nJDK中的java.beans.Introspector类的用途是发现Java类是否符合JavaBean规范,如果有的框架或程序用到了Introspector类,那么就会启用一个系统级别的缓存,此缓存会存放一些曾加载并分析过的JavaBean的引用。当Web服务器关闭时,由于此缓存中存放着这些JavaBean的引用,所以垃圾回收器无法回收Web容器中的JavaBean对象,最后导致内存变大。而org.springframework.web.util.IntrospectorCleanupListener就是专门用来处理Introspector内存泄漏问题的辅助类。IntrospectorCleanupListener会在Web服务器停止时清理Introspector缓存,使那些Javabean能被垃圾回收器正确回收。Spring自身不会出现这种问题，因为Spring在加载并分析完一个类之后会马上刷新JavaBeans Introspector缓存,这就保证Spring中不会出现这种内存泄漏的问题。但有些程序和框架在使用了JavaBeans Introspector之后,没有进行清理工作(如Quartz,Struts),最后导致内存泄漏\n\n\n## 其他\n\n- [Java面试中常问的Spring问题，你都会吗？](https://zhuanlan.zhihu.com/p/42092555)\n- [如果我是面试官，我会问你Spring这些问题](https://mp.weixin.qq.com/s/SqsAO3dBF3d5iQ5TDqmSRQ)\n- [推荐收藏：Spring面试63问！](https://mp.weixin.qq.com/s/txmK9ui20aXTewNOG17ZxQ)\n- [spring中那些让你爱不释手的代码技巧](https://mp.weixin.qq.com/s/Aet8wzzzxGGfAAJAnBcLng)\n- [《轻松读懂spring》之IOC的主干流程（上）](https://mp.weixin.qq.com/s/SZn9WRZjOuGXo2sX6TU0Uw)\n- [我们到底为什么要用IoC和AOP](https://mp.weixin.qq.com/s/LjMV4TAng0kldVoubk8T-Q)\n- [@Conditional的强大之处](https://mp.weixin.qq.com/s/rONWZ1YcnGc7QDW4-XOKlA)\n- [SpringBatch批处理框架，真心强啊！！](https://mp.weixin.qq.com/s/M14kvrWMT_4MRZDZ1TpTYA)\n- [手写Spring框架](https://mp.weixin.qq.com/s/YfS9xtaXWnt42xkk-kk4WA)\n- [Spring的Bean明明设置了Scope为Prototype，为什么还是只能获取到单例对象？](https://mp.weixin.qq.com/s/_j_0fZKTX6YUhgytWXRKEw)\n- [聊聊Spring核心](https://mp.weixin.qq.com/s/xqs0Q8zRKsOp-LdsW-uKeQ)\n- [揭秘Spring依赖注入和SpEL表达式](https://mp.weixin.qq.com/s/uBLKOXiwOsaa5za_grlMZw)","categories":["Spring"]},{"title":"MySQL相关知识点","slug":"MySQL相关知识点","url":"/blog/posts/c12d32c74ecf/","content":"\n## MySQL应用知识点\n\n### 递归function,父子查询\n\n创建函数时注意分隔符，mysql遇到分号就执行，在创建函数的时候容易报错（use near ' 'at line）,可使用**delimiter //** 定义分隔符\n\ndelimiter // (创建函数报错时添加这个)\n\n**getParentList**\n\n```sql\nCREATE FUNCTION `getParentList`(rootId varchar(100))\nRETURNS varchar(1000)\nBEGIN\nDECLARE fid varchar(100) default '';\nDECLARE str varchar(1000) default rootId;\nWHILE rootId is not null do\n  SET fid =(SELECT parentid FROM treeNodes WHERE id = rootId);\n  IF fid is not null THEN\n\tSET str = concat(str, ',', fid);\n\tSET rootId = fid;\n  ELSE\n\tSET rootId = fid;\n  END IF;\nEND WHILE;\nreturn str;\nEND;-- (这样只有当//出现之后，MySQL解释器才会执行这段语句)\n\ndelimiter ; -- (改回默认的MySQL delimiter：“;”）\n```\n使用：\n\n```sql\nselect getParentList('001001');\nselect * from tbl where FIND_IN_SET(id,getParentList('001001'))\n```\n\n**getChildList**\n\n```sql\n-- 在root用户下创建 %代表任何ip都可以连接\nCREATE DEFINER=`root`@`%` FUNCTION `getChildList`(rootId varchar(100)) RETURNS varchar(2000) CHARSET utf8\nBEGIN\nDECLARE str varchar(2000);\nDECLARE cid varchar(1000);-- 注意设置的长度,过短的话当树很高的时候容易导致查询数据不全\nSET str = '$';\nSET cid = rootId;\nWHILE cid is not null DO\n  SET str = concat(str, ',', cid);\n  SELECT group_concat(id) INTO cid FROM fke_supervision_template where FIND_IN_SET(parent_id, cid) > 0;\nEND WHILE;\n RETURN str;\nEND\n```\n使用：\n```sql\nselect getChildList('001001001');\nselect * from tbl where FIND_IN_SET(id,getChildList('001001001'))\n```\n\n**FIND_IN_SET(str,strlist)**,str-要查询的字符串,strlist-字段名参数以”,”分隔如(1,2,6,8)。查询字段(strlist)中包含(str)的结果，返回结果为null或结果。`SELECT FIND_IN_SET('b','a,b,c,d');`因为b在strlist集合中放在2的位置,下标从1开始\n\n```sql\nselect * from treenodes where FIND_IN_SET(id,'1,2,3,4,5');\n```\n使用find_in_set函数一次返回多条记录，id是一个表的字段，然后每条记录分别是id等于1，2，3，4，5的时候，有点类似in（集合）\n```sql\nselect * from treenodes where id in(1,2,3,4,5);\n```\n**当使用find_in_set配合递归函数完成递归查询时，有时会查询很慢，此时sql可以这样优化**\n```sql\nselect * from (select ... from t1 join t2 on ...)temp,(select getChildList('10001') as cids) where find_in_set(id,cids);\n```\n> [MySQL中FIND_IN_SET函数执行非常慢的某种写法](https://blog.csdn.net/wokelv/article/details/78915502)\n\n**MySQL8.0使用CTE完成递归查询**\n\n```sql\nWITH RECURSIVE cte AS (\nSELECT  ID,  PID, NAME, LEVEL, Type  FROM tmp_zjs  WHERE  ID = '102' UNION ALL \nSELECT sou.ID, sou.PID, sou.NAME, sou.LEVEL, sou.Type  FROM \ncte c INNER JOIN tmp_zjs sou ON c.ID = sou.PID  )\n\nSELECT * FROM cte\n```\n> [MySQL使用递归CTE遍历分层数据](https://www.begtut.com/mysql/mysql-recursive-cte.html)\n> [MySQL CTE(公共表表达式)](https://www.yiibai.com/mysql/cte.html)\n> [一种避免递归查询所有子部门的树数据表设计与实现](https://mp.weixin.qq.com/s/ymAjaaKKwgbpkHOZ9tgbJw)\n\n\n### 触发器\n\n```sql\nCREATE TRIGGER trigger_name\ntrigger_time\ntrigger_event ON tbl_name\nFOR EACH ROW\ntrigger_stmt\n-- 其中：\n-- trigger_name：标识触发器名称，用户自行指定；\n--trigger_time：标识触发时机，取值为BEFORE或AFTER；\n-- trigger_event：标识触发事件，取值为INSERT、UPDATE或DELETE；\n-- tbl_name：标识建立触发器的表名，即在哪张表上建立触发器；\n-- trigger_stmt：触发器程序体，可以是一句SQL语句，或者用BEGIN和END包含的多条语句。\n```\n\n### 存储过程(游标)\n\n```sql\nCREATE DEFINER=`root`@`%` PROCEDURE `NewProc`(IN a VARCHAR(50),OUT b VARCHAR(50))\nBEGIN\ndeclare my_id varchar(32); -- 自定义变量1\ndeclare my_name varchar(50); -- 自定义变量2\nDECLARE done INT DEFAULT FALSE; -- 自定义控制游标循环变量,默认false\nDECLARE My_Cursor CURSOR FOR ( SELECT id, area_name FROM fke_area where id < 20 order by id desc); -- 定义游标并输入结果集\nDECLARE CONTINUE HANDLER FOR NOT FOUND SET done = TRUE; -- 绑定控制变量到游标,游标循环结束自动转true\n\nOPEN My_Cursor; -- 打开游标\n\nmyLoop: LOOP -- 开始循环体,myLoop为自定义循环名,结束循环时用到\n\tFETCH My_Cursor into my_id, my_name; -- 将游标当前读取行的数据顺序赋予自定义变量\n\tIF done THEN -- 判断是否继续循环\n\t\tLEAVE myLoop; -- 结束循环 (break) iterate myLoop(continue)\n\tEND IF;\n  -- 自己要做的事情,在 sql 中直接使用自定义变量即可\n\tIF (my_id < 7 ) THEN\n\t\tset b = my_name;\n\tELSEIF(a='a') THEN -- 不用\"==\"做判断,用'='\n\t\tset b = 'q';\n\tELSE\n\t\tset b = 'qq';\n\tEND IF;\n  -- COMMIT; -- 提交事务\nEND LOOP myLoop; -- 结束自定义循环体\nCLOSE My_Cursor; -- 关闭游标\nEND\n```\n调用\n```sql\nset @a = 'a';set @b = 'b';call NewProc(@a,@b);select @b;\n```\n\ndemo\n\n```sql\nCREATE DEFINER=`root`@`%` PROCEDURE `zxjc_city_month_data`(IN `table_name` varchar(50),IN `start_time` VARCHAR(50),IN `end_time` varchar(50))\nBEGIN\n\tDECLARE ydfh_table VARCHAR(255);\n\tDECLARE fdcl_table VARCHAR(255);\n\tDECLARE dates date;\n\tDECLARE datee date;\n\tdelete from zxjc_city_month_data where time = table_name;\n\tset dates = DATE_FORMAT(start_time,'%Y-%m-%d');\n\tset datee = DATE_FORMAT(end_time,'%Y-%m-%d');\n\tset ydfh_table = concat('zxjc_dp_sspwrgrid_his_',table_name);\n\tset fdcl_table = concat('zxjc_dp_ssgenerator_his_',table_name);\n\tset @str = concat('insert into zxjc_city_month_data(ydfh,fdcl,grl,ydl,rdb,city,time) SELECT round(ydfh.ydfh,2) as ydfh,round(fdcl.fdcl,2) as fdcl,round(grl.grl,2) as grl,round(( ydfh.ydfh * 1000 )/( 24 * 30 ),2) as ydl,round( grl.grl / ( grl.fdl * 3600 )* 100, 2 ) as rdb,dm.dq as city,\\'',table_name , '\\' as time\tFROM bi_xzqh_dm dm LEFT JOIN ( SELECT sum( VALUE ) ydfh, id FROM ' , ydfh_table , ' WHERE meas_type = \\'93872001\\' AND create_time BETWEEN \\'',start_time,'\\' AND \\'',end_time,'\\' GROUP BY id ) ydfh ON ydfh.id = concat( \\'0101\\', dm.xzqh_dm ) LEFT JOIN ( SELECT city, sum( VALUE ) fdcl FROM ' , fdcl_table , ' WHERE create_time BETWEEN \\'' ,start_time, '\\' AND \\'' ,end_time,'\\' GROUP BY city ) fdcl ON fdcl.city = dm.dq LEFT JOIN ( SELECT city,round(ifnull(sum(\nCASE WHEN (oms_rfdl=0 || oms_rfdl is null) and tmr_rfdl!=0 and tmr_rfdl is not null THEN tmr_rfdl\n     WHEN (tmr_rfdl = 0||tmr_rfdl is NULL) and dfdc_rfdl!=0 and dfdc_rfdl is not null THEN dfdc_rfdl\n     WHEN (dfdc_rfdl = 0||dfdc_rfdl is NULL) and qsh_rfdl!=0 and qsh_rfdl is not null THEN qsh_rfdl\n     WHEN (qsh_rfdl = 0||qsh_rfdl is NULL) and gl_rfdl!=0 and gl_rfdl is not null THEN gl_rfdl\nELSE oms_rfdl END ),0),2) as fdl, ROUND(IFNULL(sum(grl),0),2) as grl FROM zxjc_dp_fdjdata_day WHERE date_time BETWEEN \\'',dates,'\\' AND \\'',datee,'\\' GROUP BY city ) grl ON grl.city = dm.dq');\n\n-- \t\t\tselect @str;\n\tPREPARE cm FROM @str;\n\tEXECUTE cm;\nEND\n```\n调用\n```sql\ncall zxjc_city_month_data('202308', '2023-08-01 00:00:00', '2023-08-31 23:59:59');\n```\n如果调用输出out的话\n```sql\nCREATE FUNCTION myfunc(OUT out_param1 INT(11), OUT out_param2 VARCHAR(20)))\nBEGIN\nSET out_param1= 10;\nSET out_param2='hello';\nEND;\nCALL myfunc(@a,@b);\nSELECT CONCAT('out_param1=',@a,' out_param2=',@b);\n-- 或者\nCREATE PROCEDURE `get_total_orders`(OUT total INT)\nBEGIN\n\tSELECT COUNT into total from orders;\nEND;\n-- set @total = 1;\nCALL get_total_orders(@total);\nSELECT @total as total;\n```\n\n#### 存储过程和函数的区别\n\n一、含义不同\n\n1、存储过程：存储过程是SQL 语句和可选控制流语句的预编译集合，以一个名称存储并作为一个单元处理。\n2、函数：是由一个或多个 SQL 语句组成的子程序，可用于封装代码以便重新使用。 函数限制比较多，如不能用临时表，只能用表变量等\n\n二、使用条件不同\n\n1、存储过程：可以在单个存储过程中执行一系列 SQL 语句。而且可以从自己的存储过程内引用其它存储过程，这可以简化一系列复杂语句。\n2、函数：自定义函数诸多限制，有许多语句不能使用，许多功能不能实现。函数可以直接引用返回值，用表变量返回记录集。但是，用户定义函数不能用于执行一组修改全局数据库状态的操作。\n\n三、执行方式不同\n\n1、存储过程：存储过程可以返回参数，如记录集，函数只能返回值或者表对象。存储过程的参数有in，out，inout三种，存储过程声明时不需要返回类型。\n2、函数：函数参数只有in，而函数需要描述返回类型，且函数中必须包含一个有效的return语句。\n\n\n### MySQL事件(如定时执行存储过程)\n\n1. 确保事件调度器已启用。您可以通过运行以下命令来检查和启用事件调度器（如果未启用）：\n\n```sql\n-- 查看启动器状态 ON为打开 OFF为关闭\nSHOW VARIABLES LIKE 'event_scheduler';\n-- 打开\nSET GLOBAL EVENT_SCHEDULER = ON;\n-- 或者my.ini配置文件下加入\n[mysqld]\nevent_scheduler = ON\n-- 然后重启MySQL\n```\n\n2. 创建一个存储过程\n\n3. 创建事件\n\n```sql\nCREATE EVENT event_name ON SCHEDULE\n-- AT '2023-07-19 10:00:00' -- 某一时间执行一次\n-- AT CURRENT_TIMESTAMP\nEVERY 1 DAY -- 设置执行间隔，例如每天\nSTARTS '2023-07-19 10:00:00' -- 设置事件的开始时间\nENDS '2023-07-21 10:00:00' -- 设置事件的结束时间（可选）\nON COMPLETION PRESERVE -- 使事件在完成后保持有效（可选）\nENABLE -- 启用事件（必需）\nCOMMENT '这是一个定时执行存储过程的事件'\nDO\n    CALL my_stored_procedure(); -- 调用存储过程\n-- DO CALL procedure_name('input1', @output1);\n```\n\n4. 查看事件\n\n```sql\nshow events;\n-- 条件筛选\nshow events like '%event_name%';\n```\n\n5. 修改事件\n\n```sql\n--开启事件\nalter event event_name enable;\n--关闭事件\nalter event event_name disable;\n-- 修改执行计划\nALTER EVENT event_name ON SCHEDULE schedule;\n-- 修改事件主体\nALTER EVENT event_name DO event_body;\n-- 重命名事件\nALTER EVENT event_name RENAME TO new_event_name;\n-- 移动事件到其他数据库\n-- 可修改事件名称\nALTER EVENT demo.event_name RENAME TO test.event_name;\n-- 删除事件\ndrop event event_name;\n```\n\n### 关键字、常用操作\n\n#### exists\n\n表A（小表），表B（大表）\n```sql\nselect * from A where c in (select c from B) -- ，用到了A表上c列的索引；\nselect * from A where exists(select c from B where c=A.c) -- 效率高，用到了B表上c列的索引。\n-- 相反的\nselect * from B where c in (select c from A) -- 效率高，用到了B表上c列的索引\nselect * from B where exists(select c from A where c=B.c) -- 效率低，用到了A表上c列的索引。\n```\n> [MySQL多表联合查询有何讲究？](https://mp.weixin.qq.com/s/O121Y0sywAtt4jk9vy4b0w)\n\n#### 逃离符escape\n\n如果要查%或者\\_，怎么办呢？使用escape，转义字符后面的%或\\_就不作为通配符了，注意前面没有转义字符的%和\\_仍然起通配符作用\n\n```sql\nselect username from user where username like '%xiao/_%' escape '/';\nselect username from user where username like '%xiao/%%' escape '/';\n```\n\n#### 分组排序\n\n```sql\nselect a.id,a.class,a.source\nfrom student a left join student b on a.class=b.class and a.source<=b.source\ngroup by a.class,a.source\norder by a.class,a.source\n```\n> [阿里二面：group by怎么优化？](https://mp.weixin.qq.com/s/igGKepCsf-SUKChco4P6Lw)\n> [MySQL中的distinct和group by哪个效率更高？](https://mp.weixin.qq.com/s/zc7_PZxwaQAwwqNsRG2_KQ)\n\n\n#### 有就更新,没有就插入\n\n```sql\nINSERT INTO table (name, gender)  VALUES ('Jerry', 'boy')  ON DUPLICATE KEY UPDATE name='Jerry'，gender='girl';\n-- 或\nREPLACE INTO table(name, gender) VALUES ('Jerry', 'girl');\n-- 或\nREPLACE INTO table(name, gender) SELECT 'Jerry', 'girl';\n-- 或\nREPLACE INTO test SET name='Jerry', gender='girl';\n```\n\n#### 复制表\n\n```sql\n-- 不保留数据\ncreate table 表名 as select 列名 from 表名 where 1=2;\ncreate table 表名(列名) as select 列名 from 表名 where 1=2;\n-- 保留数据\ninsert into 表名 ( ) select a.* from ()a\n```\n\n#### 重复数据\n```sql\n-- 显示重复\nselect * from tablename group by id having count(*)>1\n-- 不显示重复\nselect * from tablename group by id having count(*)=1\n```\n> [避免MySQL插入重复数据的4种方式](https://mp.weixin.qq.com/s/_hgA89R6GWwJgxC9KBb63A)\n\n#### 分页\n\nlimit(start,size) start:从第几条记录开始 size:读取几条记录\n\n\n#### 定时备份\n\n```shell\n#db用户名\ndbuser=root\n#db密码\ndbpasswd=\"123456\"\n#ip地址\ndbip=127.0.0.1\n#备份的数据库名字前缀\npre_name=\"test\"\n\n#备份操作的日志文件\nbakfile=/data/sqlbak/log.txt\n#备份数据的目录\nbakdatadir=/data/sqlbak/bakdata\n#备份数据的目录-持久化备份\npersist_path=/data/sqlbak/persistdata\n\n#备份保存时间,单位:天\ndateoutday=15\n\n#备份单个数据库\nbak_single()\n{\n  #db name\n  sdb=$1\n  #日期字符串\n  datestr=$2\n\n  echo \"bak_single ${sdb}_${datestr}.sql.gz BEGIN...\" >> ${bakfile}\n  #备份\n  mysqldump -u${dbuser} -h ${dbip} -p${dbpasswd} --single-transaction  --no-create-db -R -C -B ${sdb} > ${sdb}_${datestr}.sql\n  #注释掉sql脚本中 USE DATABASE 语句\n  sed -i \"s/USE/-- USE/\" ${sdb}_${datestr}.sql\n  #压缩\n  gzip ${sdb}_${datestr}.sql\n  #记录日志\n  echo \"bak_single ${sdb}_${datestr}.sql.gz  COMPLETE...\" >> ${bakfile}\n}\n\n#备份所有数据库\nbak_all()\n{\n  #日期字符串\n  all_datestr=$(date \"+%Y_%m_%d_%H_%M_%S\")\n\n  date_dir=$(date \"+%Y%m%d\")\n\n  echo \"bak_all begin ${all_datestr} ====================\" >> ${bakfile}\n  #所有的数据库\n  alldb=`mysql -u${dbuser} -h ${dbip} -p${dbpasswd} -e \"show databases\"`\n  for dbname in ${alldb}; do\n    {\n    #只备份指定前缀的数据库\n    if [[ ${dbname} =~ ${pre_name} ]]; then\n      bak_single ${dbname} ${all_datestr} \n      fi\n    }&\n    done\n           \n    #等待所有数据库备份完成\n    wait\n    #压缩\n    tar czvf dbbak_${all_datestr}.tar *_${all_datestr}.sql.gz\n    #删除\n    rm *_${all_datestr}.sql.gz\n    #是否存在当天日期命名的目录\n    if [ ! -d ${bakdatadir}/${date_dir} ]; then\n      mkdir -p ${bakdatadir}/${date_dir}\n      fi\n      #移动到当天日期命名的目录中\n      mv dbbak_${all_datestr}.tar ${bakdatadir}/${date_dir}\n      echo \"bak_all finish dbbak_${all_datestr}.tar ===============\" >> ${bakfile}\n}\n\n#备份所有数据库-不会定时删除\nbak_all_persist()\n{\n  #日期字符串\n  all_datestr=$(date \"+%Y_%m_%d_%H_%M_%S\")\n\n  echo \"bak_all_persist begin ${all_datestr} ====================\" >> ${bakfile}\n  #所有的数据库\n  alldb=`mysql -u${dbuser} -h ${dbip} -p${dbpasswd} -e \"show databases\"`\n    for dbname in ${alldb}; do\n      {\n        #只备份指定前缀的数据库\n        if [[ ${dbname} == ${pre_name} ]]; then\n          bak_single ${dbname} ${all_datestr}\n          fi\n      }&\n      done\n           \n      #等待所有数据库备份完成\n      wait\n\n      #压缩\n      tar czvf dbpersistbak_${all_datestr}.tar *_${all_datestr}.sql.gz\n      #删除\n      rm *_${all_datestr}.sql.gz\n      #是否存在当天日期命名的目录\n      if [ ! -d ${persist_path} ]; then\n        mkdir -p ${persist_path}\n        fi\n        #移动持久化备份的目录中\n        mv dbpersistbak_${all_datestr}.tar ${persist_path}\n        #\n        echo \"bak_all_persist finish dbbak_${all_datestr}.tar ====================\" >> ${bakfile}\n}\n\n#检查过期备份\ncheck_date_out()\n{\n  #当前目录\n  curpath=`pwd`\n  #当前日期\n  curdate=$(date \"+%Y%m%d\")\n  #最早的保存日期\n  lastdate=`date -d \"${curdate} - ${dateoutday} day\" +%Y%m%d`\n  #进入备份目录\n  cd ${bakdatadir}\n  #目录列表\n  pathlst=`ls`\n  #检查目录是否过期，删除已过期的目录\n  for tmpdate in ${pathlst[*]}; do\n    if [[ ${tmpdate} -le ${lastdate} ]]; then\n      rm -rf ${tmpdate}\n      echo \"check_date_out, curdate:${curdate} delete ${tmpdate} \" >> ${bakfile}\n      fi\n      done\n      #回到当前目录\n      cd ${curpath}\n}\n\ncase \"$1\" in\n   s)\n      bak_single $2 $3\n      ;;\n   a)\n      bak_all\n      ;;\n   p)\n      bak_all_persist\n      ;;\n  chk) \n      check_date_out\n      ;;\n   *)\n    echo \"Please use correct command...\"\n      ;;\nesac\n\n```\n**函数功能**\n备份是以数据库为单位进行备份的，先备份单个数据库，然后再把所有的备份数据库打包一起\n\nbak_single函数表示备份单个数据，传入参数是需要备份的数据库名字和日期字符串，备份文件名由这两个参数构成，也就是说，备份的文件名由数据库名+日期组成，比如test1_2021_08_16_10_05_30.sql表示test1数据库的备份文件，备份的时间2021_08_16_10_05_30\n\nbak_all函数表示备份所有的数据库，不需要传入参数，先执行SQL语句show databases查询出所有的数据库，然后过滤出我们需要备份的数据库，脚本中需要备份的数据库名都是以test开头的，具体的过滤规则可以按照各自的需求自行修改\n\nbak_all函数for循环体中{和}以及它们后面的&表示启动一个新进程并执行大括号中间的命令，也即每个数据库启动一个进程进行备份，for循环结束之后的wait命令表示等待for循环中所有进程结束，也就是等待所有数据库备份全部完成之后，才会执行wait后面的命令,全部备份完成之后，会创建一个以当前日期命名的目录，并打包所有备份的数据库的SQL脚本，放到此目录中，同时删除原始的备份文件\n\n这里采用的是分库备份，分库备份的好处是：如果所有库都备份成一个备份文件时，恢复其中一个库的数据是比较麻烦的，所以采用分库备份，利于恢复单库数据\n\ncheck_date_out函数是检查备份目录是否过期，如果过期的话，直接删除过期的目录，脚本开头的变量dateoutday指定了备份保留的天数\n\nbak_all_persist函数是持久备份，备份过程和bak_all函数一样，只不过这里是备份到另一个持久数据的目录中，脚本开头的变量persist_path指定了持久备份目录，目录里面的备份文件不会自动删除，需要手工去删除\n\n持久化目录的主要应用场景：有时线上数据库表有数据校正或者表格结构有变动，为了防止误操作，再执行操作之前，先调用bak_all_persist函数备份下数据库，这样，即使出现误操作，还能恢复数据\n\n备份参数说明\n--single-transaction\n此选项会将隔离级别设置为可重复读(REPEATABLE READ),让整个数据在dump过程中保证数据的一致性，且不会锁表，这个选项对导出InnoDB的数据表很有用\n\n--no-create-db\n正常导出的SQL脚本中会有类似CREATE DATABASE语句，加了--no-create-db选项之后就没有此语句了\n\n-C\n服务器传给客户端的过程中先压缩再传递\n\n-R\n导出存储过程以及自定义函数\n\n在mysqldump导出数据库SQL脚本之后，sed -i \"s/USE/-- USE/\" ${sdb}_${datestr}.sql命令的作用是注释掉SQL脚本中的USE DATABASE XXX语句，这个也比较实用的，有时候线上数据会导入到内网，重现线上的一些BUG，但是内网可能已经有一个同名的数据库了，如果注释了这行语句，就可以导入到其他数据库，否则，需要先手工处理SQL脚本，然后再导入\n\n**如何使用**\n假如备份MySQL脚本的名字是bak.sh,下面是脚本的使用方法\n\n备份对单个数据库\n```shell\n# 备份logindb数据库\n./bak.sh logindb \"2021_08_16_10_05_30\"\n# 执行上述命令后，会在当前目录下生成名为logindb_2021_08_16_10_05_30.sql.gz的文件\n```\n备份所有数据库\n```./bak.sh a```\n检查备份保留时间\n```./bak.sh chk```\n持久化备份\n```./bak.sh p```\n添加定时任务\n要实现自动备份功能，还需要添加定时任务，间隔指定时间调用备份脚本，执行ctrontab -e命令，输入以下语句\n\n```shell\n*/10 * * * * /data/sqlbak/bak.sh a\n*/15 * * * * /data/sqlbak/bak.sh chk\n```\n上述定时任务是每10分钟备份一次所有数据库，每15分钟检查一次过期的备份，当然，具体的备份策略根据具体的场景不同，可以根据实际情况调整\n\n> [一个自动备份MySQL的脚本](https://zhuanlan.zhihu.com/p/427491577)\n\n#### 查看数据大小\n\n```sql\n-- 进入information_schema数据库（存放了其他的数据库的信息）\nuse information_schema;\n\nmysql> use information_schema\nReading table information for completion of table and column names\nYou can turn off this feature to get a quicker startup with -A\n\n-- 查询所有数据库数据大小\nselect concat(round(sum(data_length/1024/1024),2),'MB') as data from tables;\n\n-- 查看指定数据库数据的大小\nselect concat(round(sum(data_length/1024/1024),2),'MB') as data from tables where table_schema='database_name';\n\n-- 查看指定数据库的某个表的数据大小\nselect concat(round(sum(data_length/1024/1024),2),'MB') as data from tables where table_schema='database_name' and table_name='table_name';\n\n--查询索引+数据大小\nselect concat(round(sum(data_length/1024/1024),2),'MB') as '数据大小' , concat(round(sum(index_length/1024/1024),2),'MB') as '索引大小', round(sum(data_length/1024/1024),2)+round(sum(index_length/1024/1024),2) as 'all' from tables where table_schema='数据库名称';\n```\n\n## MySQL函数\n\n### 字符串函数\n\n> 对于针对字符串位置的操作，第一个位置被标记为1。\n\n**ASCII(str)**:返回字符串str的最左面字符的ASCII代码值。如果str是空字符串，返回0。如果str是NULL，返回NULL。\n```sql\nselect ASCII('2');-- 50\nselect ASCII(2);-- 50\nselect ASCII('dx');-- 100\n```\n\n**ORD(str)**:如果字符串str最左面字符是一个多字节字符，通过以格式((first byte ASCII code) * 256+(second byte ASCII code))[ * 256+third byte ASCII code...]返回字符的ASCII代码值来返回多字节字符代码。如果最左面的字符不是一个多字节字符。返回与ASCII()函数返回的相同值。\n```sql\nselect ORD('2');-- 50\n```\n\n**CONV(N,from_base,to_base)**:在不同的数字基之间变换数字。返回数字N的字符串数字，从from_base基变换为to_base基，如果任何参数是NULL，返回NULL。参数N解释为一个整数，但是可以指定为一个整数或一个字符串。最小基是2且最大的基是36。如果to_base是一个负数，N被认为是一个有符号数，否则，N被当作无符号数。CONV以64位点精度工作。\n```sql\nselect CONV(\"a\",16,2);-- '1010'\nselect CONV(\"6E\",18,8);-- '172'\nselect CONV(-17,10,-18);-- '-H'\nselect CONV(10+\"10\"+'10'+0xa,10,10);-- '40'\n```\n\n**BIN(N)**:返回二进制值N的一个字符串表示，在此N是一个长整数(BIGINT)数字，这等价于`CONV(N,10,2)`。如果N是NULL，返回NULL。\n```sql\nselect BIN(12);-- '1100'\n```\n\n**OCT(N)**:返回八进制值N的一个字符串的表示，在此N是一个长整型数字，这等价于`CONV(N,10,8)`。如果N是NULL，返回NULL。\n```sql\nselect OCT(12);-- '14'\n```\n\n**HEX(N)**:返回十六进制值N一个字符串的表示，在此N是一个长整型(BIGINT)数字，这等价于`CONV(N,10,16)`。如果N是NULL，返回NULL。\n```sql\nselect HEX(255);-- 'FF'\n```\n\n**CHAR(N,...)**:CHAR()将参数解释为整数并且返回由这些整数的ASCII代码字符组成的一个字符串。NULL值被跳过。\n```sql\nselect CHAR(77,121,83,81,'76');-- 'MySQL'\nselect CHAR(77,77.3,'77.3');-- 'MMM'\n```\n\n**CONCAT(str1,str2,...)**:返回来自于参数连结的字符串。如果任何参数是NULL，返回NULL。可以有超过2个的参数。一个数字参数被变换为等价的字符串形式。\n```sql\nselect CONCAT('My', 'S', 'QL');-- 'MySQL'\nselect CONCAT('My', NULL, 'QL');-- NULL\nselect CONCAT(14.3);-- '14.3'\n```\n\n**LENGTH(str)，OCTET_LENGTH(str)，CHAR_LENGTH(str)，CHARACTER_LENGTH(str)**,返回字符串str的长度。\n```sql\nselect LENGTH('text');-- 4\nselect OCTET_LENGTH('text');-- 4\n```\n注意，对于多字节字符，其`CHAR_LENGTH()`仅计算一次。\n\n**LOCATE(substr,str)，POSITION(substr IN str)**,返回子串substr在字符串str第一个出现的位置，如果substr不是在str里面，返回0\n```sql\nselect LOCATE('bar', 'foobarbar');-- 4\nselect LOCATE('xbar', 'foobar');-- 0\n```\n该函数是多字节可靠的。\n\n**LOCATE(substr,str,pos)**:返回子串substr在字符串str第一个出现的位置，从位置pos开始。如果substr不是在str里面，返回0。\n```sql\nselect LOCATE('bar', 'foobarbar',5);-- 7\n```\n这函数是多字节可靠的。\n\n**INSTR(str,substr)**:返回子串substr在字符串str中的第一个出现的位置。这与有2个参数形式的LOCATE()相同，除了参数被颠倒。\n```sql\nselect INSTR('foobarbar', 'bar');-- 4\nselect INSTR('xbar', 'foobar');-- 0\n```\n这函数是多字节可靠的。\n\n**LPAD(str,len,padstr)**:返回字符串str，左面用字符串padstr填补直到str是len个字符长。\n```sql\nselect LPAD('hi',4,'??');-- '??hi'\n```\n\n**RPAD(str,len,padstr)**:返回字符串str，右面用字符串padstr填补直到str是len个字符长。\n```sql\nselect RPAD('hi',5,'?');-- 'hi???'\n```\n\n**LEFT(str,len)**:返回字符串str的最左面len个字符。\n```sql\nselect LEFT('foobarbar', 5);-- 'fooba'\n```\n该函数是多字节可靠的。\n\n**RIGHT(str,len)**:返回字符串str的最右面len个字符。\n```sql\nselect RIGHT('foobarbar', 4);-- 'rbar'\n```\n该函数是多字节可靠的。\n\n**SUBSTRING(str,pos,len)，SUBSTRING(str FROM pos FOR len)，MID(str,pos,len)**\n从字符串str返回一个len个字符的子串，从位置pos开始。使用FROM的变种形式是ANSI SQL92语法。\n```sql\nselect SUBSTRING('Quadratically',5,6);-- 'ratica'\n```\n该函数是多字节可靠的。\n\n**SUBSTRING(str,pos)，SUBSTRING(str FROM pos)**:从字符串str的起始位置pos返回一个子串。\n```sql\nselect SUBSTRING('Quadratically',5);-- 'ratically'\nselect SUBSTRING('foobarbar' FROM 4);-- 'barbar'\n```\n该函数是多字节可靠的。\n\n**SUBSTRING_INDEX(str,delim,count)**:返回从字符串str的第count个出现的分隔符delim之后的子串。如果count是正数，返回最后的分隔符到左边(从左边数)的所有字符。如果count是负数，返回最后的分隔符到右边的所有字符(从右边数)。\n```sql\nselect SUBSTRING_INDEX('www.mysql.com', '.', 2);-- 'www.mysql'\nselect SUBSTRING_INDEX('www.mysql.com', '.', -2);-- 'mysql.com'\n```\n该函数对多字节是可靠的。\n\n**LTRIM(str)**:返回删除了其前置空格字符的字符串str。\n```sql\nselect LTRIM(' barbar');-- 'barbar'\n```\n\n**RTRIM(str)**:返回删除了其拖后空格字符的字符串str。\n```sql\nselect RTRIM('barbar   ');-- 'barbar'\n```\n该函数对多字节是可靠的。\n\n**TRIM([[BOTH | LEADING | TRAILING] [remstr] FROM] str)**:返回字符串str，其所有remstr前缀或后缀被删除了。如果没有修饰符BOTH、LEADING或TRAILING给出，BOTH被假定。如果remstr没被指定，空格被删除。\n```sql\nselect TRIM(' bar   ');-- 'bar'\nselect TRIM(LEADING 'x' FROM 'xxxbarxxx');-- 'barxxx'\nselect TRIM(BOTH 'x' FROM 'xxxbarxxx');-- 'bar'\nselect TRIM(TRAILING 'xyz' FROM 'barxxyz');-- 'barx'\n```\n该函数对多字节是可靠的。\n\n**SOUNDEX(str)**:返回str的一个同音字符串。听起来“大致相同”的2个字符串应该有相同的同音字符串。一个“标准”的同音字符串长是4个字符，但是SOUNDEX()函数返回一个任意长的字符串。你可以在结果上使用SUBSTRING()得到一个“标准”的同音串。所有非数字字母字符在给定的字符串中被忽略。所有在A-Z之外的字符国际字母被当作元音。\n```sql\nselect SOUNDEX('Hello');-- 'H400'\nselect SOUNDEX('Quadratically');-- 'Q36324'\n```\n\n**SPACE(N)**:返回由N个空格字符组成的一个字符串。\n```sql\nselect SPACE(6);-- '     '\n```\n\n**REPLACE(str,from_str,to_str)**:返回字符串str，其字符串from_str的所有出现由字符串to_str代替。\n```sql\nselect REPLACE('www.mysql.com', 'w', 'Ww');-- 'WwWwWw.mysql.com'\n```\n该函数对多字节是可靠的。\n\n**REPEAT(str,count)**:返回由重复countTimes次的字符串str组成的一个字符串。如果count<=0，返回一个空字符串。如果str或count是NULL，返回NULL。\n```sql\nselect REPEAT('MySQL', 3);-- 'MySQLMySQLMySQL'\n```\n\n**REVERSE(str)**:返回颠倒字符顺序的字符串str。\n```sql\nselect REVERSE('abc');-- 'cba'\n```\n该函数对多字节可靠的。\n\n**INSERT(str,pos,len,newstr)**:返回字符串str，在位置pos起始的子串且len个字符长得子串由字符串newstr代替。\n```sql\nselect INSERT('Quadratic', 3, 4, 'What');-- 'QuWhattic'\n```\n该函数对多字节是可靠的。\n\n**ELT(N,str1,str2,str3,...)**:如果N=1，返回str1，如果N=2，返回str2，等等。如果N小于1或大于参数个数，返回NULL。ELT()是FIELD()反运算。\n```sql\nselect ELT(1, 'ej', 'Heja', 'hej', 'foo');-- 'ej'\nselect ELT(4, 'ej', 'Heja', 'hej', 'foo');-- 'foo'\n```\n\n**FIELD(str,str1,str2,str3,...)**:返回str在str1,str2,str3,...清单的索引。如果str没找到，返回0。FIELD()是ELT()反运算。\n```sql\nselect FIELD('ej', 'Hej', 'ej', 'Heja', 'hej', 'foo');-- 2\nselect FIELD('fo', 'Hej', 'ej', 'Heja', 'hej', 'foo');-- 0\n```\n\n**FIND_IN_SET(str,strlist)**:如果字符串str在由N子串组成的表strlist之中，返回一个1到N的值。一个字符串表是被“,”分隔的子串组成的一个字符串。如果第一个参数是一个常数字符串并且第二个参数是一种类型为SET的列，FIND_IN_SET()函数被优化而使用位运算！如果str不是在strlist里面或如果strlist是空字符串，返回0。如果任何一个参数是NULL，返回NULL。如果第一个参数包含一个“,”，该函数将工作不正常。\n```sql\nSELECT FIND_IN_SET('b','a,b,c,d');-- 2\n```\n\n**MAKE_SET(bits,str1,str2,...)**:返回一个集合(包含由“,”字符分隔的子串组成的一个字符串)，由相应的位在bits集合中的的字符串组成。str1对应于位0，str2对应位1，等等。在str1,str2,...中的NULL串不添加到结果中。\n```sql\nSELECT MAKE_SET(1,'a','b','c');-- 'a'\nSELECT MAKE_SET(1 | 4,'hello','nice','world');-- 'hello,world'\nSELECT MAKE_SET(0,'a','b','c');-- ''\n```\n\n**EXPORT_SET(bits,on,off,[ separator,[number_of_bits] ])**:返回一个字符串，在这里对于在“bits”中设定每一位，你得到一个“on”字符串，并且对于每个复位(reset)的位，你得到一个“off”字符串。每个字符串用“separator”分隔(缺省“,”)，并且只有“bits”的“number_of_bits”(缺省64)位被使用。\n```sql\nselect EXPORT_SET(5,'Y','N',',',4);-- Y,N,Y,N\n```\n\n**LCASE(str),LOWER(str)**:返回字符串str，根据当前字符集映射(缺省是ISO-8859-1 Latin1)把所有的字符改变成小写。该函数对多字节是可靠的。\n```sql\nselect LCASE('QUADRATICALLY');-- 'quadratically'\n```\n\n**UCASE(str),UPPER(str)**:返回字符串str，根据当前字符集映射(缺省是ISO-8859-1 Latin1)把所有的字符改变成大写。该函数对多字节是可靠的。\n```sql\nselect UCASE('Hej');-- 'HEJ'\n```\n该函数对多字节是可靠的。\n\n**LOAD_FILE(file_name)**:读入文件并且作为一个字符串返回文件内容。文件必须在服务器上，你必须指定到文件的完整路径名，而且你必须有file权限。文件必须所有内容都是可读的并且小于max_allowed_packet。如果文件不存在或由于上面原因之一不能被读出，函数返回NULL。\n```sql\nUPDATE table_name SET blob_column=LOAD_FILE(\"/tmp/picture\") WHERE id=1;\n```\nMySQL必要时自动变换数字为字符串，并且反过来也如此：\n```sql\nSELECT 1+\"1\";-- 2\nSELECT CONCAT(2,' test');-- '2 test'\n```\n如果你想要明确地变换一个数字到一个字符串，把它作为参数传递到CONCAT()。如果字符串函数提供一个二进制字符串作为参数，结果字符串也是一个二进制字符串。被变换到一个字符串的数字被当作是一个二进制字符串。这仅影响比较\n\n**COMPRESS(string_to_compress)**:压缩一个字符串。这个函数要求MySQL已经用一个诸如zlib的压缩库压缩过。否则，返回值始终是NULL。**UNCOMPRESS()** 可将压缩过的字符串进行解压缩。\n\n**CONCAT_WS(separator ,str1 ,str2 ,...)**:CONCAT_WS()代表CONCAT With Separator，是CONCAT()的特殊形式。第一个参数是其它参数的分隔符，分隔符的位置放在要连接的两个字符串之间。分隔符可以是一个字符串，也可以是其它参数。如果分隔符为NULL，则结果为NULL。函数会忽略任何分隔符参数后的NULL值。\n\n**FORMAT(X ,D)**:将number X设置为格式'#,###,###.##',以四舍五入的方式保留到小数点后D位,而返回结果为一个字符串。\n\n**QUOTE(str)**:引证一个字符串，由此产生一个在SQL语句中可用作完全转义数据值的结果。 返回的字符串由单引号标注，每例都带有单引号(‘'’)、反斜线符号(‘\\’)、ASCII NUL以及前面有反斜线符号的Control-Z。如果自变量的值为NULL,则返回不带单引号的单词“NULL”。\n\n**UNCOMPRESSED_LENGTH(compressed_string)**:返回压缩字符串压缩前的长度。\n\n**UNHEX(str)**:执行从HEX(str)的反向操作。就是说，它将参数中的每一对十六进制数字理解为一个数字，并将其转化为该数字代表的字符。结果字符以二进制字符串的形式返回\n\n\n### 时间函数\n\n这里是一个使用日期函数的例子。下面的查询选择了所有记录，其date_col的值是在最后30天以内：\n```sql\nSELECT something FROM table WHERE TO_DAYS(NOW()) - TO_DAYS(date_col) <= 30;\n```\n\n**DAYOFWEEK(date)**:返回日期date的星期索引(1=星期天，2=星期一,……7=星期六)。这些索引值对应于ODBC标准。\n```sql\nselect DAYOFWEEK('1998-02-03');-- 3\n```\n\n**WEEKDAY(date)**:返回date的星期索引(0=星期一，1=星期二,……6=星期天)。\n```sql\nselect WEEKDAY('1997-10-04 22:23:00');-- 5\nselect WEEKDAY('1997-11-05');-- 2\n```\n\n**DAYOFMONTH(date)**:返回date的月份中日期，在1到31范围内。\n```sql\nselect DAYOFMONTH('1998-02-03');-- 3\n```\n\n**DAYOFYEAR(date)**:返回date在一年中的日数,在1到366范围内。\n```sql\nselect DAYOFYEAR('1998-02-03');-- 34\n```\n\n**MONTH(date)**:返回date的月份，范围1到12。\n```sql\nselect MONTH('1998-02-03');-- 2\n```\n\n**DAYNAME(date)**:返回date的星期名字。\n```sql\nselect DAYNAME(\"1998-02-05\");-- 'Thursday'\n```\n\n**MONTHNAME(date)**:返回date的月份名字。\n```sql\nselect MONTHNAME(\"1998-02-05\");-- 'February'\n```\n\n**QUARTER(date)**:返回date一年中的季度，范围1到4。\n```sql\nselect QUARTER('98-04-01');-- 2\n```\n\n**WEEK(date),WEEK(date,first)**:对于星期天是一周的第一天的地方，有一个单个参数，返回date的周数，范围在0到52。2个参数形式WEEK()允许你指定星期是否开始于星期天或星期一。如果第二个参数是0，星期从星期天开始，如果第二个参数是1，从星期一开始。\n```sql\nselect WEEK('1998-02-20');-- 7\nselect WEEK('1998-02-20',0);-- 7\nselect WEEK('1998-02-20',1);-- 8\n```\n\n**YEAR(date)**:返回date的年份，范围在1000到9999。\n```sql\nselect YEAR('98-02-03');-- 1998\n```\n\n**HOUR(time)**:返回time的小时，范围是0到23。\n```sql\nselect HOUR('10:05:03');-- 10\n```\n\n**MINUTE(time)**:返回time的分钟，范围是0到59。\n```sql\nselect MINUTE('98-02-03 10:05:03');-- 5\n```\n\n**SECOND(time)**:回来time的秒数，范围是0到59。\n```sql\nselect SECOND('10:05:03');-- 3\n```\n\n**PERIOD_ADD(P,N)**:增加N个月到阶段P(以格式YYMM或YYYYMM)。以格式YYYYMM返回值。注意阶段参数P不是日期值。\n```sql\nselect PERIOD_ADD(9801,2);-- 199803\n```\n\n**PERIOD_DIFF(P1,P2)**:返回在时期P1和P2之间月数，P1和P2应该以格式YYMM或YYYYMM。注意，时期参数P1和P2不是日期值。\n```sql\n　　mysql> select PERIOD_DIFF(9802,199703);-- 11\n```\n\n**DATE_ADD(date,INTERVAL expr type),DATE_SUB(date,INTERVAL expr type)，ADDDATE(date,INTERVAL expr type),SUBDATE(date,INTERVAL expr type)**,这些功能执行日期运算。对于MySQL3.22，他们是新的。ADDDATE()和SUBDATE()是DATE_ADD()和DATE_SUB()的同义词。在MySQL3.23中，你可以使用+和-而不是DATE_ADD()和DATE_SUB()。（见例子）date是一个指定开始日期的DATETIME或DATE值，expr是指定加到开始日期或从开始日期减去的间隔值一个表达式，expr是一个字符串；它可以以一个“-”开始表示负间隔。type是一个关键词，指明表达式应该如何被解释。EXTRACT(type FROM date)函数从日期中返回“type”间隔。下表显示了type和expr参数怎样被关联\n\n| type值        | 含义               | 期望的expr格式               |\n| ------------- | ------------------ | ---------------------------- |\n| SECOND        | 秒                 | SECONDS                      |\n| MINUTE        | 分钟               | MINUTES                      |\n| HOUR          | 时间               | HOURS                        |\n| DAY           | 天                 | DAYS                         |\n| MONTH         | 月                 | MONTHS                       |\n| YEAR          | 年                 | YEARS                        |\n| MINUTE_SECOND | 分钟和秒           | \"MINUTES:SECONDS\"            |\n| HOUR_MINUTE   | 小时和分钟         | \"HOURS:MINUTES\"              |\n| DAY_HOUR      | 天和小时           | \"DAYS HOURS\"                 |\n| YEAR_MONTH    | 年和月             | \"YEARS-MONTHS\"               |\n| HOUR_SECOND   | 小时, 分钟         | \"HOURS:MINUTES:SECONDS\"      |\n| DAY_MINUTE    | 天, 小时, 分钟     | \"DAYS HOURS:MINUTES\"         |\n| DAY_SECOND    | 天, 小时, 分钟, 秒 | \"DAYS HOURS:MINUTES:SECONDS\" |\n\nMySQL在expr格式中允许任何标点分隔符。表示显示的是建议的分隔符。如果date参数是一个DATE值并且你的计算仅仅包含YEAR、MONTH和DAY部分(即，没有时间部分)，结果是一个DATE值。否则结果是一个DATETIME值。\n```sql\nSELECT \"1997-12-31 23:59:59\" + INTERVAL 1 SECOND;-- 1998-01-01 00:00:00\nSELECT INTERVAL 1 DAY + \"1997-12-31\";-- 1998-01-01\nSELECT \"1998-01-01\" - INTERVAL 1 SECOND;-- 1997-12-31 23:59:59\nSELECT DATE_ADD(\"1997-12-31 23:59:59\",INTERVAL 1 SECOND);-- 1998-01-01 00:00:00\nSELECT DATE_ADD(\"1997-12-31 23:59:59\",INTERVAL 1 DAY);-- 1998-01-01 23:59:59\nSELECT DATE_ADD(\"1997-12-31 23:59:59\",INTERVAL \"1:1\" MINUTE_SECOND);-- 1998-01-01 00:01:00\nSELECT DATE_SUB(\"1998-01-01 00:00:00\",INTERVAL \"1 1:1:1\" DAY_SECOND);-- 1997-12-30 22:58:59\nSELECT DATE_ADD(\"1998-01-01 00:00:00\", INTERVAL \"-1 10\" DAY_HOUR);-- 1997-12-30 14:00:00\nSELECT DATE_SUB(\"1998-01-02\", INTERVAL 31 DAY);-- 1997-12-02\nSELECT EXTRACT(YEAR FROM \"1999-07-02\");-- 1999\nSELECT EXTRACT(YEAR_MONTH FROM \"1999-07-02 01:02:03\");-- 199907\nSELECT EXTRACT(DAY_MINUTE FROM \"1999-07-02 01:02:03\");-- 20102\n```\n\n如果你指定太短的间隔值(不包括type关键词期望的间隔部分)，MySQL假设你省掉了间隔值的最左面部分。例如，如果你指定一个type是DAY_SECOND，值expr被希望有天、小时、分钟和秒部分。如果你象\"1:10\"这样指定值，MySQL假设日子和小时部分是丢失的并且值代表分钟和秒。换句话说，\"1:10\" DAY_SECOND以它等价于\"1:10\" MINUTE_SECOND的方式解释，这对那MySQL解释TIME值表示经过的时间而非作为一天的时间的方式有二义性。如果你使用确实不正确的日期，结果是NULL。如果你增加MONTH、YEAR_MONTH或YEAR并且结果日期大于新月份的最大值天数，日子在新月用最大的天调整。\n```sql\nselect DATE_ADD('1998-01-30', Interval 1 month);-- 1998-02-28\n```\n注意，从前面的例子中词INTERVAL和type关键词不是区分大小写的。\n　　\n**TO_DAYS(date)**:给出一个日期date，返回一个天数(从0年的天数)。\n```sql\nselect TO_DAYS(950501);-- 728779\nselect TO_DAYS('1997-10-07');-- 729669\n```\nTO_DAYS()不打算用于使用格列高里历(1582)出现前的值。\n\n**FROM_DAYS(N)**:给出一个天数N，返回一个DATE值。\n```sql\nselect FROM_DAYS(729669);-- '1997-10-07'\n```\n\n**DATE_FORMAT(date,format)**:根据format字符串格式化date值。下列修饰符可以被用在format字符串中:\n\n- %M 月名字(January……December)\n- %W 星期名字(Sunday……Saturday)\n- %D 有英语前缀的月份的日期(1st,2nd,3rd,等等。)\n- %Y 年,数字,4位\n- %y 年,数字,2位\n- %a 缩写的星期名字(Sun……Sat)\n- %d 月份中的天数,数字(00……31)\n- %e 月份中的天数,数字(0……31)\n- %m 月,数字(01……12)\n- %c 月,数字(1……12)\n- %b 缩写的月份名字(Jan……Dec)\n- %j 一年中的天数(001……366)\n- %H 小时(00……23)\n- %k 小时(0……23)\n- %h 小时(01……12)\n- %I 小时(01……12)\n- %l 小时(1……12)\n- %i 分钟,数字(00……59)\n- %r 时间,12小时(hh:mm:ss [AP]M)\n- %T 时间,24小时(hh:mm:ss)\n- %S 秒(00……59)\n- %s 秒(00……59)\n- %p AM或PM\n- %w 一个星期中的天数(0=Sunday ……6=Saturday)\n- %U 星期(0……52),这里星期天是星期的第一天\n- %u 星期(0……52),这里星期一是星期的第一天\n- %% 一个文字“%”。\n\n所有的其他字符不做解释被复制到结果中。\n```sql\nselect DATE_FORMAT('1997-10-04 22:23:00', '%W %M %Y');-- 'Saturday October 1997'\nselect DATE_FORMAT('1997-10-04 22:23:00', '%H:%i:%s');-- '22:23:00'\nselect DATE_FORMAT('1997-10-04 22:23:00','%D %y %a %d %m %b %j');-- '4th 97 Sat 04 10 Oct 277'\nselect DATE_FORMAT('1997-10-04 22:23:00','%H %k %I %r %T %S %w');-- '22 22 10 10:23:00 PM 22:23:00 00 6'\n```\n\nMySQL3.23中，在格式修饰符字符前需要%。在MySQL更早的版本中，%是可选的。\n\n**TIME_FORMAT(time,format)**:这像上面的DATE_FORMAT()函数一样使用，但是format字符串只能包含处理小时、分钟和秒的那些格式修饰符。其他修饰符产生一个NULL值或0。\n　　\n**CURDATE(),CURRENT_DATE**:以'YYYY-MM-DD'或YYYYMMDD格式返回今天日期值，取决于函数是在一个字符串还是数字上下文被使用。\n```sql\nselect CURDATE();-- '1997-12-15'\nselect CURDATE() + 0;-- 19971215\n```\n\n**CURTIME(),CURRENT_TIME**:以'HH:MM:SS'或HHMMSS格式返回当前时间值，取决于函数是在一个字符串还是在数字的上下文被使用。\n```sql\nselect CURTIME();-- '23:50:26'\nselect CURTIME() + 0;-- 235026\n```\n\n**NOW(),SYSDATE(),CURRENT_TIMESTAMP**,以'YYYY-MM-DD HH:MM:SS'或YYYYMMDDHHMMSS格式返回当前的日期和时间，取决于函数是在一个字符串还是在数字的上下文被使用。\n```sql\nselect NOW();-- '1997-12-15 23:50:26'\nselect NOW() + 0;-- 19971215235026\n```\n\n**UNIX_TIMESTAMP(),UNIX_TIMESTAMP(date)**:如果没有参数调用，返回一个Unix时间戳记(从'1970-01-01 00:00:00'GMT开始的秒数)。如果UNIX_TIMESTAMP()用一个date参数被调用，它返回从'1970-01-01 00:00:00'GMT开始的秒数值。date可以是一个DATE字符串、一个DATETIME字符串、一个TIMESTAMP或以YYMMDD或YYYYMMDD格式的本地时间的一个数字。\n```sql\nselect UNIX_TIMESTAMP();-- 882226357\nselect UNIX_TIMESTAMP('1997-10-04 22:23:00');-- 875996580\n```\n\n当UNIX_TIMESTAMP被用于一个TIMESTAMP列，函数将直接接受值，没有隐含的“string-to-unix-timestamp”变换。\n\n**FROM_UNIXTIME(unix_timestamp)**:以'YYYY-MM-DD HH:MM:SS'或YYYYMMDDHHMMSS格式返回unix_timestamp参数所表示的值，取决于函数是在一个字符串还是或数字上下文中被使用。\n```sql\nselect FROM_UNIXTIME(875996580);-- '1997-10-04 22:23:00'\nselect FROM_UNIXTIME(875996580) + 0;-- 19971004222300\n```\n\n**FROM_UNIXTIME(unix_timestamp,format)**:返回表示Unix时间标记的一个字符串，根据format字符串格式化。format可以包含与DATE_FORMAT()函数列出的条目同样的修饰符。\n```sql\nselect FROM_UNIXTIME(UNIX_TIMESTAMP(),'%Y %D %M %h:%i:%s %x');-- '1997 23rd December 03:43:30 x'\n```\n\n**SEC_TO_TIME(seconds)**:返回seconds参数，变换成小时、分钟和秒，值以'HH:MM:SS'或HHMMSS格式化，取决于函数是在一个字符串还是在数字上下文中被使用。\n```sql\nselect SEC_TO_TIME(2378);-- '00:39:38'\nselect SEC_TO_TIME(2378) + 0;-- 3938\n```\n\n**TIME_TO_SEC(time)**:返回time参数，转换成秒。\n```sql\nselect TIME_TO_SEC('22:23:00');-- 80580\nselect TIME_TO_SEC('00:39:38');-- 2378\n```\nMysql取系统函数：\n```sql\nSelect curtime();\nSelect curdate();\nSelect sysdate();\nselect now();\n```\n\n**ADDDATE(date ,INTERVAL expr type ),ADDDATE(expr ,days)**:当被第二个参数的INTERVAL格式激活后，ADDDATE()就是DATE_ADD()的同义词。相关函数SUBDATE()则是DATE_SUB()的同义词。对于INTERVAL参数上的信息，请参见关于DATE_ADD()的论述。\n\n**ADDTIME(expr ,expr2)**:ADDTIME()将expr2添加至expr然后返回结果。expr是一个时间或时间日期表达式，而expr2是一个时间表达式。\n\n**CONVERT_TZ(dt ,from_tz ,to_tz)**:将时间日期值dt从from_tz给出的时区转到to_tz给出的时区，然后返回结果值。关于可能指定的时区的详细论述，若自变量无效，则这个函数会返回NULL\n\n**DATE(expr)**:提取日期或时间日期表达式expr中的日期部分。\n\n**DATEDIFF(expr,expr2)**:返回起始时间expr和结束时间expr2之间的天数。Expr和expr2为日期或date-and-time表达式。计算中只用到这些值的日期部分。\n\n**DATE_ADD(date ,INTERVAL expr type ),DATE_SUB(date ,INTERVAL expr type)**:这些函数执行日期运算。date是一个DATETIME或DATE值，用来指定起始时间。expr是一个表达式，用来指定从起始日期添加或减去的时间间隔值。Expr是一个字符串;对于负值的时间间隔，它可以以一个‘-’开头。type为关键词，它指示了表达式被解释的方式。\n\n**DAY(date)**:DAY()和DAYOFMONTH()的意义相同\n\n**EXTRACT(type FROM date)**:函数所使用的时间间隔类型说明符同DATE_ADD()或DATE_SUB()的相同,但它从日期中提取其部分，而不是执行日期运算。\n\n**GET_FORMAT(DATE|TIME|DATETIME, 'EUR'|'USA'|'JIS'|'ISO'|'INTERNAL')**:返回一个格式字符串。这个函数在同DATE_FORMAT()及STR_TO_DATE()函数结合时很有用\n\n**LAST_DAY(date)**:获取一个日期或日期时间值，返回该月最后一天对应的值。若参数无效，则返回NULL。\n\n**LOCALTIME,LOCALTIME()**:LOCALTIME及LOCALTIME()和NOW()具有相同意义。\n\n**LOCALTIMESTAMP, LOCALTIMESTAMP()**:LOCALTIMESTAMP和LOCALTIMESTAMP()和NOW()具有相同意义。\n\n**MAKEDATE(year ,dayofyear)**:给出年份值和一年中的天数值，返回一个日期。dayofyear必须大于0，否则结果为NULL。\n\n**MAKETIME(hour ,minute ,second)**:返回由hour、minute和second参数计算得出的时间值\n\n**CROSECOND(expr)**:从时间或日期时间表达式expr返回微秒值，其数字范围从0到999999。\n\n**STR_TO_DATE(str,format)**:这是DATE_FORMAT()函数的倒转。它获取一个字符串str和一个格式字符串format。若格式字符串包含日期和时间部分，则STR_TO_DATE()返回一个DATETIME值，若该字符串只包含日期部分或时间部分，则返回一个DATE或TIME值。\n\n**SUBDATE(date ,INTERVAL expr type),SUBDATE(expr,days)**:当被第二个参数的INTERVAL型式调用时,SUBDATE()和DATE_SUB()的意义相同。对于有关INTERVAL参数的信息，见有关DATE_ADD()的讨论。\n\n**SUBTIME(expr,expr2)**:SUBTIME()从expr中提取expr2,然后返回结果。expr是一个时间或日期时间表达式，而xpr2是一个时间表达式。\n\n**SYSDATE()**:返回当前日期和时间值，格式为'YYYY-MM-DD HH:MM:SS'或YYYYMMDDHHMMSS，具体格式根据函数是否用在字符串或数字语境而定。\n\n**TIME(expr)**:提取一个时间或日期时间表达式的时间部分，并将其以字符串形式返回。\n\n**TIMEDIFF(expr ,expr2)**:返回起始时间expr和结束时间expr2之间的时间。expr和expr2为时间或date-and-time表达式 , 两个的类型必须一样。\n\n**TIMESTAMP(expr) , TIMESTAMP(expr ,expr2)**:对于一个单参数,该函数将日期或日期时间表达式expr作为日期时间值返回，对于两个参数,它将时间表达式expr2添加到日期或日期时间表达式expr中，将theresult作为日期时间值返回。\n\n**TIMESTAMPADD(interval ,int_expr ,datetime_expr)**:将整型表达式int_expr添加到日期或日期时间表达式datetime_expr中。int_expr的单位被时间间隔参数给定，该参数必须是以下值的其中一个：FRAC_SECOND、SECOND、MINUTE、HOUR、DAY、WEEK、MONTH、QUARTER或YEAR。可使用所显示的关键词指定Interval值，或使用SQL_TSI_前缀。例如,DAY或SQL_TSI_DAY都是正确的\n\n**TIMESTAMPDIFF(interval ,datetime_expr1 ,datetime_expr2)**:返回日期或日期时间表达式datetime_expr1和datetime_expr2之间的整数差。其结果的单位由interval参数给出。interval的法定值同TIMESTAMPADD()函数说明中所列出的相同。\n\n**UTC_DATE()**:返回当前UTC日期值，其格式为'YYYY-MM-DD'或YYYYMMDD，具体格式取决于函数是否用在字符串或数字语境中。\n\n**UTC_TIME()**:返回当前UTC值，其格式为'HH:MM:SS'或HHMMSS，具体格式根据该函数是否用在字符串或数字语境而定。\n\n**UTC_TIMESTAMP()**:返回当前UTC日期及时间值，格式为'YYYY-MM-DD HH:MM:SS'或YYYYMMDDHHMMSS，具体格式根据该函数是否用在字符串或数字语境而定\n\n**WEEKOFYEAR(date)**:将该日期的阳历周以数字形式返回，范围是从1到53。它是一个兼容度函数，相当于WEEK(date ,3)。\n\n**YEARWEEK(date), YEARWEEK(date,start)**:返回一个日期对应的年或周。start参数的工作同start参数对WEEK()的工作相同。结果中的年份可以和该年的第一周和最后一周对应的日期参数有所不同。\n\n### 控制流程函数\n\n**CASE WHEN THEN函数**\n\n语法：\n```sql\nCASE value WHEN [compare-value] THEN result [WHEN [compare-value] THEN result ……] [ELSE result ] END CASE WHEN [condition] THEN result [WHEN[condition] THEN result ……] [ELSE result] END;\n```\n函数用法说明：在第一个方案的返回结果中，value=compare-value。而第二个方案的返回结果是第一种情况的真实结果。如果没有匹配的结果值，则返回结果为ELSE后的结果，如果没有ELSE部分，则返回值为NULL\n\n**IF函数用法**\n\n语法：\n```sql\nIF(expr1,expr2,expr3)\n```\n函数用法说明：如果expr1是TRUE(expr1 <> 0 and expr1 <> NULL) ，则IF()的返回值为expr2;否则返回值则为expr3。IF()的返回值为数字值或字符串值，具体情况视其所在语境而定\n\n**IFNULL函数**\n\n语法：\n```sql\nIFNULL(expr1,expr2)\n```\n函数用法说明：假如expr1不为NULL，则IFNULL()的返回值为expr1;否则其返回值为expr2。IFNULL()的返回值是数字或是字符串，具体情况取决于其所使用的语境\n\n### 数学函数\n\n**ABS(X)**:返回X的绝对值\n\n**ACOS(X)**:返回X反余弦,即,余弦是X的值。若X不在-1到1的范围之内，则返回NULL。\n\n**ASIN(X)**:返回X的反正弦，即，正弦为X的值。若X不在-1到1的范围之内，则返回NULL。\n\n**ATAN(X)**:返回X的反正切，即，正切为X的值。\n\n**ATAN(Y,X),ATAN2(Y,X)**:返回两个变量X及Y的反正切。它类似于Y或X的反正切计算, 除非两个参数的符号均用于确定结果所在象限。\n\n**CEILING(X),CEIL(X)**:返回不小于X的最小整数值。\n\n**COS(X)**:返回X的余弦，其中X在弧度上已知。\n\n**COT(X)**:返回X的余切\n\n**CRC32(expr)**:计算循环冗余码校验值并返回一个32比特无符号值。若参数为NULL，则结果为NULL。该参数应为一个字符串，而且在不是字符串的情况下会被作为字符串处理（若有可能）\n\n**DEGREES(X)**:返回参数X,该参数由弧度被转化为度。\n\n**EXP(X)**:返回e的X乘方后的值(自然对数的底)。\n\n**FLOOR(X)**:返回不大于X的最大整数值。\n\n**FORMAT(X,D)**:将数字X的格式写成'#,###,###.##'格式,即保留小数点后D位，而第D位的保留方式为四舍五入，然后将结果以字符串的形式返回\n\n**LN(X)**:返回X的自然对数,即X相对于基数e的对数\n\n**LOG(X),LOG(B,X)**:若用一个参数调用，这个函数就会返回X的自然对数。\n\n**LOG2(X)**:返回X的基数为2的对数。\n\n**LOG10(X)**:返回X的基数为10的对数。\n\n**MOD(N,M),N%M,N MOD M**:模操作。返回N被M除后的余数。\n\n**PI()**:返回ϖ(pi)的值。默认的显示小数位数是7位,然而MySQL内部会使用完全双精度值。\n\n**POW(X,Y),POWER(X,Y)**:返回X的Y乘方的结果值。\n\n**RADIANS(X)**:返回由度转化为弧度的参数X, (注意ϖ弧度等于180度)。\n\n**RAND(),RAND(N)**:返回一个随机浮点值v，范围在0到1之间(即,其范围为0 ≤ v ≤ 1.0)。若已指定一个整数参数N，则它被用作种子值，用来产生重复序列。\n\n**ROUND(X),ROUND(X,D)**:返回参数X,其值接近于最近似的整数。在有两个参数的情况下，返回X，其值保留到小数点后D位，而第D位的保留方式为四舍五入。若要接保留X值小数点左边的D位，可将D设为负值。\n\n**SIGN(X)**:返回参数作为-1、0或1的符号，该符号取决于X的值为负、零或正。\n\n**SIN(X)**:返回X正弦，其中X在弧度中被给定。\n\n**SQRT(X)**:返回非负数X的二次方根。\n\n**TAN(X)**:返回X的正切，其中X在弧度中被给定。\n\n**TRUNCATE(X,D)**:返回被舍去至小数点后D位的数字X。若D的值为0,则结果不带有小数点或不带有小数部分。可以将D设为负数,若要截去(归零)X小数点左起第D位开始后面所有低位的值\n\n\n### 全文搜索功能函数\n\nMATCH (col1,col2,...)\nAGAINST (expr [IN BOOLEAN MODE | WITH QUERY EXPANSION])\n\n### 加密函数\n\n**AES_ENCRYPT(str,key_str),AES_DECRYPT(crypt_str,key_str)**:这些函数允许使用官方AES进行加密和数据加密(高级加密标准)算法,即以前人们所熟知的“Rijndael”。保密关键字的长度为128比特，不过你可以通过改变源而将其延长到256比特。我们选择了128比特的原因是它的速度要快得多，且对于大多数用途而言这个保密程度已经够用。\n\n**DECODE(crypt_str,pass_str)**:使用pass_str作为密码，解密加密字符串crypt_str，crypt_str应该是由ENCODE()返回的字符串。\n\n**ENCODE(str,pass_str)**:使用pass_str作为密码，解密str。使用DECODE()解密结果。\n\n**DES_DECRYPT(crypt_str [,key_str ])**:使用DES_ENCRYPT()加密一个字符串。若出现错误，这个函数会返回NULL。\n\n**DES_ENCRYPT(str [,(key_num |key_str )])**:用Triple-DES算法给出的关键字加密字符串。若出现错误，这个函数会返回NULL\n\n**ENCRYPT(str [,salt ])**:使用Unix crypt()系统调用加密str。salt参数应为一个至少包含2个字符的字符串。若没有给出salt参数，则使用任意值。\n\n**MD5(str)**:为字符串算出一个MD5 128比特检查和。该值以32位十六进制数字的二进制字符串的形式返回,若参数为NULL则会返回NULL。例如，返回值可被用作散列关键字\n\n**OLD_PASSWORD(str)**:当PASSWORD()的执行变为改善安全性时，OLD_PASSWORD()会被添加到MySQL。OLD_PASSWORD()返回从前的PASSWORD()执行值(4.1之前)，同时允许你为任何4.1之前的需要连接到你的5.1版本MySQL服务器前客户端设置密码，从而不至于将它们切断\n\n**PASSWORD(str)**:从原文密码str计算并返回密码字符串，当参数为NULL时返回NULL。这个函数用于用户授权表的Password列中的加密MySQL密码存储\n\n### 信息函数\n\n**BENCHMARK(count,expr)**:函数重复count次执行表达式expr。它可以被用于计算MySQL处理表达式的速度。结果值通常为0 。另一种用处来自mysql客户端内部,能够报告问询执行的次数\n\n**CHARSET(str)**:返回字符串自变量的字符集。\n\n**COERCIBILITY(str)**:返回字符串自变量的整序可压缩性值。\n\n**COLLATION(str)**:返回惠字符串参数的排序方式。\n\n**CONNECTION_ID()**:返回对于连接的连接ID(线程ID)。每个连接都有各自的唯一ID。\n\n**CURRENT_USER,CURRENT_USER()**:返回当前话路被验证的用户名和主机名组合。这个值符合确定你的存取权限的MySQL账户。在被指定SQL SECURITY DEFINER特征的存储程序内，CURRENT_USER()返回程序的创建者\n\n**DATABASE()**:返回使用utf8字符集的默认(当前)数据库名。在存储程序里，默认数据库是同该程序向关联的数据库，但并不一定与调用语境的默认数据库相同。\n\n**FOUND_ROWS()**:SELECT语句可能包括一个LIMIT子句，用来限制服务器返回客户端的行数。在有些情况下，需要不用再次运行该语句而得知在没有LIMIT时到底该语句返回了多少行。为了知道这个行数,包括在SELECT语句中选择SQL_CALC_FOUND_ROWS，随后调用FOUND_ROWS()\n\n**LAST_INSERT_ID(),LAST_INSERT_ID(expr)**:自动返回最后一个INSERT或UPDATE问询为AUTO_INCREMENT列设置的第一个发生的值。\n\n**ROW_COUNT()**:返回被前面语句升级的、插入的或删除的行数。这个行数和mysql客户端显示的行数及mysql_affected_rows() C API函数返回的值相同。\n\n**SCHEMA()**:这个函数和DATABASE()具有相同的意义\n\n**SESSION_USER()**:SESSION_USER()和USER()具有相同的意义。\n\n**SYSTEM_USER()**:SYSTEM_USER()和USER()具有相同的意义\n\n**USER()**:返回当前MySQL用户名和机主名\n\n**VERSION()**:返回指示MySQL服务器版本的字符串。这个字符串使用utf8字符集。\n\n### 其他函数\n\n**DEFAULT(col_name)**:返回一个表列的默认值。若该列没有默认值则会产生错误。\n\n**GET_LOCK(str,timeout)**:设法使用字符串str给定的名字得到一个锁，超时为timeout秒。若成功得到锁，则返回1，若操作超时则返回0 (例如,由于另一个客户端已提前封锁了这个名字),若发生错误则返回NULL(诸如缺乏记忆或线程mysqladmin kill被断开)。假如你有一个用GET_LOCK()得到的锁，当你执行RELEASE_LOCK()或你的连接断开(正常或非正常)时，这个锁就会解除\n\n**INET_ATON(expr)**:给出一个作为字符串的网络地址的点地址表示，返回一个代表该地址数值的整数。地址可以是4或8比特地址。\n\n**INET_NTOA(expr)**:给定一个数字网络地址(4或8比特),返回作为字符串的该地址的电地址表示\n\n**IS_FREE_LOCK(str)**:检查名为str的锁是否可以使用(换言之没有被封锁)。若锁可以使用，则返回1(没有人在用这个锁),若这个锁正在被使用，则返回0，出现错误则返回NULL(诸如不正确的参数)\n\n**IS_USED_LOCK(str)**:检查名为str的锁是否正在被使用(换言之,被封锁)。若被封锁，则返回使用该锁的客户端的连接标识符。否则返回NULL。\n\n**MASTER_POS_WAIT(log_name ,log_pos [,timeout])**:该函数对于控制主从同步很有用处。它会持续封锁，直到从设备阅读和应用主机记录中所有补充资料到指定的位置。返回值是其为到达指定位置而必须等待的记录事件的数目。若从设备SQL线程没有被启动、从设备主机信息尚未初始化、参数不正确或出现任何错误，则该函数返回NULL。若超时时间被超过，则返回-1。若在MASTER_POS_WAIT()等待期间，从设备SQL线程中止，则该函数返回NULL。若从设备由指定位置通过，则函数会立即返回结果。\n\n**NAME_CONST(name,value)**:返回给定值。当用来产生一个结果集合列时,NAME_CONST()促使该列使用给定名称。\n\n**RELEASE_LOCK(str)**:解开被GET_LOCK()获取的，用字符串str所命名的锁。若锁被解开，则返回1，若改线程尚未创建锁，则返回0(此时锁没有被解开),若命名的锁不存在，则返回NULL。若该锁从未被对GET_LOCK()的调用获取，或锁已经被提前解开，则该锁不存在。\n\n**SLEEP(duration)**:睡眠(暂停)时间为duration参数给定的秒数，然后返回0。若SLEEP()被中断,它会返回1。duration或许或包括一个给定的以微秒为单位的分数部分。\n\n**UUID()**:返回一个通用唯一标识符(UUID)，UUID被设计成一个在时间和空间上都独一无二的数字。2个对UUID()的调用应产生2个不同的值，即使这些调用的执行是在两个互不相连的单独电脑上进行。\n\n**VALUES(col_name)**:在一个`INSERT … ON DUPLICATE KEY UPDATE … `语句中，你可以在UPDATE子句中使用VALUES(col_name)函数，用来访问来自该语句的INSERT部分的列值。换言之，UPDATE子句中的VALUES(col_name)访问需要被插入的col_name的值,并不会发生重复键冲突。这个函数在多行插入中特别有用。VALUES()函数只在INSERT ... UPDATE语句中有意义，而在其它情况下只会返回NULL\n\n\n### 聚合函数\n\n**AVG([DISTINCT] expr)**:返回expr的平均值。DISTINCT选项可用于返回expr的不同值的平均值。\n\n**BIT_AND(expr)**:返回expr中所有比特的bitwise AND。计算执行的精确度为64比特(BIGINT)。若找不到匹配的行，则这个函数返回18446744073709551615。(这是无符号BIGINT值，所有比特被设置为1)。\n\n**BIT_OR(expr)**:返回expr中所有比特的bitwise OR。计算执行的精确度为64比特(BIGINT)。若找不到匹配的行，则函数返回0。\n\n**BIT_XOR(expr)**:返回expr中所有比特的bitwise XOR。计算执行的精确度为64比特(BIGINT)。若找不到匹配的行，则函数返回0。\n\n**COUNT(expr)**:返回SELECT语句检索到的行中非NULL值的数目。若找不到匹配的行，则COUNT()返回0\n\n**COUNT(DISTINCT expr ,[expr ...])**:返回不同的非NULL值数目。若找不到匹配的项，则COUNT(DISTINCT)返回0\n\n**GROUP_CONCAT(expr)**:该函数返回带有来自一个组的连接的非NULL值的字符串结果。其完整的语法如下所示：\n```sql\nGROUP_CONCAT([DISTINCT] expr [,expr ...]\n\n[ORDER BY {unsigned_integer | col_name | expr }\n\n                 [ASC | DESC] [,col_name ...]]\n\n             [SEPARATOR str_val ])\n```\n\n**MIN([DISTINCT] expr), MAX([DISTINCT] expr)**:返回expr的最小值和最大值。MIN()和MAX()的取值可以是一个字符串参数；在这些情况下，它们返回最小或最大字符串值。\n\n**STD(expr),STDDEV(expr)**:返回expr的总体标准偏差。这是标准SQL的延伸。这个函数的STDDEV()形式用来提供和Oracle的兼容性。可使用标准SQL函数STDDEV_POP()进行代替\n\n**STDDEV_POP(expr)**:返回expr的总体标准偏差(VAR_POP()的平方根)。你也可以使用STD()或STDDEV(),它们具有相同的意义，然而不是标准的SQL。若找不到匹配的行，则STDDEV_POP()返回NULL\n\n**STDDEV_SAMP(expr)**:返回expr的样本标准差(VAR_SAMP()的平方根)。若找不到匹配的行，则STDDEV_SAMP()返回NULL\n\n**SUM([DISTINCT] expr)**:返回expr的总数。若返回集合中无任何行，则SUM()返回NULL。DISTINCT关键词可用于MySQL5.1中，求得expr不同值的总和。若找不到匹配的行，则SUM()返回NULL\n\n**VAR_POP(expr)**:返回expr总体标准方差。它将行视为总体，而不是一个样本，所以它将行数作为分母。你也可以使用VARIANCE(),它具有相同的意义然而不是标准的SQL\n\n**VAR_SAMP(expr)**:返回expr的样本方差。更确切的说，分母的数字是行数减去1。若找不到匹配的行，则VAR_SAMP()返回NULL\n\n**VARIANCE(expr)**:返回expr的总体标准方差。这是标准SQL的延伸。可使用标准SQL函数VAR_POP()进行代替。若找不到匹配的项，则VARIANCE()返回NULL\n\n## MySQL锁\n\n### 表级锁和行级锁了解吗？有什么区别？\n\nMyISAM仅仅支持表级锁(table-level locking)，一锁就锁整张表，这在并发写的情况下性非常差。InnoDB不光支持表级锁(table-level locking)，还支持行级锁(row-level locking)，默认为行级锁。行级锁的粒度更小，仅对相关的记录上锁即可（对一行或者多行记录加锁），所以对于并发写入操作来说，InnoDB的性能更高。\n\n**表级锁和行级锁对比**：\n\n- **表级锁**:MySQL中锁定粒度最大的一种锁（全局锁除外），是针对非索引字段加的锁，对当前操作的整张表加锁，实现简单，资源消耗也比较少，加锁快，不会出现死锁。不过，触发锁冲突的概率最高，高并发下效率极低。表级锁和存储引擎无关，MyISAM和InnoDB引擎都支持表级锁。\n- **行级锁**:MySQL中锁定粒度最小的一种锁，是针对索引字段加的锁，只针对当前操作的行记录进行加锁。行级锁能大大减少数据库操作的冲突。其加锁粒度最小，并发度高，但加锁的开销也最大，加锁慢，会出现死锁。行级锁和存储引擎有关，是在存储引擎层面实现的。\n\n### 行级锁的使用有什么注意事项？\n\nInnoDB的行锁是针对索引字段加的锁，表级锁是针对非索引字段加的锁。当我们执行`UPDATE`、`DELETE`语句时，如果`WHERE`条件中字段没有命中唯一索引或者索引失效的话，就会导致扫描全表对表中的所有行记录进行加锁。这个在我们日常工作开发中经常会遇到，一定要多多注意！不过，很多时候即使用了索引也有可能会走全表扫描，这是因为MySQL优化器的原因。\n\n### InnoDB有哪几类行锁？\n\nInnoDB行锁是通过对索引数据页上的记录加锁实现的，MySQLInnoDB支持三种行锁定方式：\n\n- **记录锁（RecordLock）**：也被称为记录锁，属于单个行记录上的锁。\n- **间隙锁（GapLock）**：锁定一个范围，不包括记录本身。\n- **临键锁（Next-KeyLock）**：RecordLock+GapLock，锁定一个范围，包含记录本身，主要目的是为了解决幻读问题（MySQL事务部分提到过）。记录锁只能锁住已经存在的记录，为了避免插入新记录，需要依赖间隙锁。\n\n在InnoDB默认的隔离级别REPEATABLE-READ下，行锁默认使用的是Next-KeyLock。但是，如果操作的索引是唯一索引或主键，InnoDB会对Next-KeyLock进行优化，将其降级为RecordLock，即仅锁住索引本身，而不是范围。\n\n> 一些大厂面试中可能会问到Next-KeyLock的加锁范围，这里推荐一篇文章：[MySQLnext-keylock加锁范围是什么？-程序员小航-2021](https://segmentfault.com/a/1190000040129107)。\n\n### 共享锁和排他锁呢？\n\n不论是表级锁还是行级锁，都存在共享锁（ShareLock，S锁）和排他锁（ExclusiveLock，X锁）这两类：\n\n- **共享锁（S锁）**：又称读锁，事务在读取记录的时候获取共享锁，允许多个事务同时获取（锁兼容）。\n- **排他锁（X锁）**：又称写锁/独占锁，事务在修改记录的时候获取排他锁，不允许多个事务同时获取。如果一个记录已经被加了排他锁，那其他事务不能再对这条事务加任何类型的锁（锁不兼容）。\n\n排他锁与任何的锁都不兼容，共享锁仅和共享锁兼容。\n\n|      | S 锁   | X 锁 |\n| :--- | :----- | :--- |\n| S 锁 | 不冲突 | 冲突 |\n| X 锁 | 冲突   | 冲突 |\n\n由于MVCC的存在，对于一般的SELECT语句，InnoDB不会加任何锁。不过，你可以通过以下语句显式加共享锁或排他锁。\n\n```sql\n# 共享锁\nSELECT ... LOCK IN SHARE MODE;\n# 排他锁\nSELECT ... FOR UPDATE;\n```\n\n### 意向锁有什么作用？\n\n如果需要用到表锁的话，如何判断表中的记录没有行锁呢，一行一行遍历肯定是不行，性能太差。我们需要用到一个叫做意向锁的东东来快速判断是否可以对某个表使用表锁。\n\n意向锁是表级锁，共有两种：\n\n- **意向共享锁（IntentionSharedLock，IS锁）**：事务有意向对表中的某些记录加共享锁（S锁），加共享锁前必须先取得该表的IS锁。\n- **意向排他锁（IntentionExclusiveLock，IX锁）**：事务有意向对表中的某些记录加排他锁（X锁），加排他锁之前必须先取得该表的IX锁。\n\n意向锁是有数据引擎自己维护的，用户无法手动操作意向锁，在为数据行加共享/排他锁之前，InooDB会先获取该数据行所在在数据表的对应意向锁。\n\n意向锁之间是互相兼容的。\n\n|       | IS 锁 | IX 锁 |\n| ----- | ----- | ----- |\n| IS 锁 | 兼容  | 兼容  |\n| IX 锁 | 兼容  | 兼容  |\n\n意向锁和共享锁和排它锁互斥（这里指的是表级别的共享锁和排他锁，意向锁不会与行级的共享锁和排他锁互斥）。\n\n|      | IS 锁 | IX 锁 |\n| ---- | ----- | ----- |\n| S 锁 | 兼容  | 互斥  |\n| X 锁 | 互斥  | 互斥  |\n\n《MySQL技术内幕InnoDB存储引擎》这本书对应的描述应该是笔误了。\n\n![img](https://oss.javaguide.cn/github/javaguide/mysql/image-20220511171419081.png)\n\n### 当前读和快照读有什么区别？\n\n**快照读**（一致性非锁定读）就是单纯的SELECT语句，但不包括下面这两类SELECT语句：\n\n```sql\nSELECT...FORUPDATE\nSELECT...LOCKINSHAREMODE\n```\n\n快照即记录的历史版本，每行记录可能存在多个历史版本（多版本技术）。快照读的情况下，如果读取的记录正在执行UPDATE/DELETE操作，读取操作不会因此去等待记录上X锁的释放，而是会去读取行的一个快照。只有在事务隔离级别RC(读取已提交)和RR（可重读）下，InnoDB才会使用一致性非锁定读：\n\n- 在RC级别下，对于快照数据，一致性非锁定读总是读取被锁定行的最新一份快照数据。\n- 在RR级别下，对于快照数据，一致性非锁定读总是读取本事务开始时的行数据版本。\n\n快照读比较适合对于数据一致性要求不是特别高且追求极致性能的业务场景。**当前读**（一致性锁定读）就是给行记录加X锁或S锁。当前读的一些常见SQL语句类型如下：\n\n```sql\n# 对读的记录加一个X锁\nSELECT...FOR UPDATE\n# 对读的记录加一个S锁\nSELECT...LOCK IN SHARE MODE\n# 对修改的记录加一个X锁\nINSERT...\nUPDATE...\nDELETE...\n```\n\n### 自增锁有了解吗？\n\n> 不太重要的一个知识点，简单了解即可。\n\n关系型数据库设计表的时候，通常会有一列作为自增主键。InnoDB中的自增主键会涉及一种比较特殊的表级锁—**自增锁（AUTO-INC Locks）**。\n\n```sql\nCREATE TABLE `sequence_id` (\n  `id` bigint(20) unsigned NOT NULL AUTO_INCREMENT,\n  `stub` char(10) NOT NULL DEFAULT '',\n  PRIMARY KEY (`id`),\n  UNIQUE KEY `stub` (`stub`)\n) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;\n```\n\n更准确点来说，不仅仅是自增主键，AUTO_INCREMENT的列都会涉及到自增锁，毕竟非主键也可以设置自增长。如果一个事务正在插入数据到有自增列的表时，会先获取自增锁，拿不到就可能会被阻塞住。这里的阻塞行为只是自增锁行为的其中一种，可以理解为自增锁就是一个接口，其具体的实现有多种。具体的配置项为innodb_autoinc_lock_mode（MySQL5.1.22引入），可以选择的值如下：\n\n| innodb_autoinc_lock_mode | 介绍                           |\n| :----------------------- | :----------------------------- |\n| 0                        | 传统模式                       |\n| 1                        | 连续模式（MySQL 8.0 之前默认） |\n| 2                        | 交错模式(MySQL 8.0 之后默认)   |\n\n交错模式下，所有的“INSERT-LIKE”语句（所有的插入语句，包括：`INSERT`、`REPLACE`、`INSERT…SELECT`、`REPLACE…SELECT`、`LOADDATA`等）都不使用表级锁，使用的是轻量级互斥锁实现，多条插入语句可以并发执行，速度更快，扩展性也更好。不过，如果你的MySQL数据库有主从同步需求并且bin log存储格式为Statement的话，不要将InnoDB自增锁模式设置为交叉模式，不然会有数据不一致性问题。这是因为并发情况下插入语句的执行顺序就无法得到保障。\n\n> 如果MySQL采用的格式为Statement，那么MySQL的主从同步实际上同步的就是一条一条的SQL语句。\n\n最后，再推荐一篇文章：[为什么MySQL的自增主键不单调也不连续](https://draveness.me/whys-the-design-mysql-auto-increment/)。\n\n### 总结\n\n#### 数据库中的锁\n\n表级锁包括：表锁、元数据锁、意向锁。\n对于表锁而言，当存储引擎不支持行级锁时，使用表锁。SQL语句没有匹配到索引时，使用表锁。\n对于元数据锁而言，对表做增删改查时，会加上MDL读锁。对表结构做变更时，会加上MDL写锁。\n对于意向锁而言，对表中的行记录加锁时，会用到意向锁。\n而对于行级锁而言，增删改查匹配到索引时，会使用行级锁\n\n全局锁就是对整个数据库实例加锁。MySQL提供了一个加全局读锁的方法，命令是Flush tables with read lock(FTWRL)。当你需要让整个库处于只读状态的时候，可以使用这个命令，之后其他线程的以下语句会被阻塞：数据更新语句（数据的增删改）、数据定义语句（包括建表、修改表结构等）和更新类事务的提交语句。你可以理解为，全局锁基本上把数据所所有的变更语句都锁住了。全局锁的典型场景应用场景是全库逻辑备份，也就是把整个库每个表都select出来存起来。上面说到全局锁会锁住所有变更语句，但这只是对于MyISAM存储引擎而言的。对于Innodb而言，其可以利用MVCC实现数据的一致性视图，从而不需要锁整个库就可以实现全库的数据备份\n\n表级锁可以分为：表锁、元数据锁、意向锁三种\n\n- 表锁，顾名思义就是对某个表加锁。那什么时候会使用表锁呢？一般情况是对应的存储引擎没有行级锁（例如：MyIASM），或者是对应的SQL语句没有匹配到索引。\n\n- 元数据锁\n  元数据，指的是我们的表结构这些元数据。元数据锁（Metadata Lock）自然是执行DDL表结构变更语句时，我们对表加上的一个锁了。那什么时候会使用元数据锁这个表级锁呢？当我们对一个表做增删改查操作的时候，会加上MDL读锁；当我们要对表结构做变更时，就会加MDL写锁。\n\n- 意向锁\n  意向锁，本质上就是空间换时间的产物，是为了提高行锁效率的一个东西。在InnoDB中，我们对某条记录进行锁定时，为了提高并发度，通常都只是锁定这一行记录，而不是锁定整个表。而当我们需要为整个表加X锁的时候，我们就需要遍历整个表的记录，如果每条记录都没有被加锁，才可以给整个表加X锁。而这个遍历过程就很费时间，这时候就有了意向锁的诞生。意向锁其实就是标记这个表有没有被锁，如果有某条记录被锁住了，那么就必须获取该表的意向锁。所以当我们需要判断这个表的记录有没有被加锁时，直接判断意向锁就可以了，减少了遍历的时间，提高了效率，是典型的用空间换时间的做法。那么什么时候会用到意向锁呢？很简单，就是在对表中的行记录加锁的时候，就会用到意向锁。\n\n那么什么时候会使用行级锁呢？\n当增删改查匹配到索引时，Innodb会使用行级锁。\n\n> [MySQL啥时候用表锁，啥时候用行锁？](https://mp.weixin.qq.com/s/v4GVad1wgW0H7tmjyQS0QA)\n\n**如果查询条件用了索引/主键，那么`select ..... for update`就会进行行锁。**\n**如果是普通字段(没有索引/主键)，那么`select ..... for update`就会进行锁表。**\n\n\n#### 如何尽可能避免死锁\n\n1. 合理的设计索引，区分度高的列放到组合索引前面，使业务SQL尽可能通过索引定位更少的行，减少锁竞争。\n2. 调整业务逻辑SQL执行顺序，避免update/delete长时间持有锁的SQL在事务前面。\n3. 避免大事务，尽量将大事务拆成多个小事务来处理，小事务发生锁冲突的几率也更小。\n4. 以固定的顺序访问表和行。比如两个更新数据的事务，事务A更新数据的顺序为1，2;事务B更新数据的顺序为2，1。这样更可能会造成死锁。\n5. 在并发比较高的系统中，不要显式加锁，特别是是在事务里显式加锁。如select…for update语句，如果是在事务里（运行了start transaction或设置了auto commit等于0）,那么就会锁定所查找到的记录。\n6. 尽量按主键/索引去查找记录，范围查找增加了锁冲突的可能性，也不要利用数据库做一些额外额度计算工作。比如有的程序会用到“select…where…order by rand();”这样的语句，由于类似这样的语句用不到索引，因此将导致整个表的数据都被锁住。\n7. 优化SQL和表设计，减少同时占用太多资源的情况。比如说，减少连接的表，将复杂SQL分解为多个简单的SQL。\n\n#### 相关文章\n\n- [这六个MySQL死锁案例，能让你理解死锁的原因！](https://mp.weixin.qq.com/s/7BuvuRFuelBTI2rn2If6cA)\n- [面试命中率90%的点：MySQL锁](https://mp.weixin.qq.com/s/3EU5Yfd3O6i1CA-YvhqoVg)\n- [一文搞懂MySQL中各种锁，写的太好了](https://mp.weixin.qq.com/s/5lX30br8IBStcnk2J1PEXg)\n\n## MySQL索引\n\n### 索引介绍\n\n**索引是一种用于快速查询和检索数据的数据结构，其本质可以看成是一种排序好的数据结构。**\n\n索引的作用就相当于书的目录。打个比方:我们在查字典的时候，如果没有目录，那我们就只能一页一页的去找我们需要查的那个字，速度很慢。如果有目录了，我们只需要先去目录里查找字的位置，然后直接翻到那一页就行了。索引底层数据结构存在很多种类型，常见的索引结构有:B树，B+树和Hash、红黑树。在MySQL中，无论是Innodb还是MyIsam，都使用了B+树作为索引结构。\n\n### 索引的优缺点\n\n**优点**：\n\n- 使用索引可以大大加快数据的检索速度（大大减少检索的数据量）,这也是创建索引的最主要的原因。\n- 通过创建唯一性索引，可以保证数据库表中每一行数据的唯一性。\n\n**缺点**：\n\n- 创建索引和维护索引需要耗费许多时间。当对表中的数据进行增删改的时候，如果数据有索引，那么索引也需要动态的修改，会降低SQL执行效率。\n- 索引需要使用物理文件存储，也会耗费一定空间。\n\n但是，**使用索引一定能提高查询性能吗**?大多数情况下，索引查询都是比全表扫描要快的。但是如果数据库的数据量不大，那么使用索引也不一定能够带来很大提升。\n\n### 索引的底层数据结构\n\n#### Hash表\n\n哈希表是键值对的集合，通过键(key)即可快速取出对应的值(value)，因此哈希表可以快速检索数据（接近O（1））。\n\n**为何能够通过key快速取出value呢**？原因在于哈希算法（也叫散列算法）。通过哈希算法，我们可以快速找到key对应的index，找到了index也就找到了对应的value。\n\n\n```java\nhash = hashfunc(key)\nindex = hash % array_size\n```\n\n![img](https://oss.javaguide.cn/github/javaguide/database/mysql20210513092328171.png)\n\n但是哈希算法有个**Hash冲突**问题，也就是说多个不同的key最后得到的index相同。通常情况下，我们常用的解决办法是**链地址法**。链地址法就是将哈希冲突数据存放在链表中。就比如JDK1.8之前HashMap就是通过链地址法来解决哈希冲突的。不过，JDK1.8以后HashMap为了减少链表过长的时候搜索时间过长引入了红黑树。\n\n![img](https://oss.javaguide.cn/github/javaguide/database/mysql20210513092224836.png)\n\n为了减少Hash冲突的发生，一个好的哈希函数应该“均匀地”将数据分布在整个可能的哈希值集合中。\n\n既然哈希表这么快，**为什么MySQL没有使用其作为索引的数据结构呢**？主要是因为Hash索引不支持顺序和范围查询。假如我们要对表中的数据进行排序或者进行范围查询，那Hash索引可就不行了。并且，每次IO只能取一个。\n\n试想一种情况:\n\n\n```java\nSELECT * FROM tb1 WHERE id < 500;\n```\n\n在这种范围查询中，优势非常大，直接遍历比500小的叶子节点就够了。而Hash索引是根据hash算法来定位的，难不成还要把1-499的数据，每个都进行一次hash计算来定位吗?这就是Hash最大的缺点了。\n\n#### B树&B+树\n\nB树也称B-树,全称为**多路平衡查找树**，B+树是B树的一种变体。B树和B+树中的B是Balanced（平衡）的意思。目前大部分数据库系统及文件系统都采用B-Tree或其变种B+Tree作为索引结构。\n\n**B树&B+树两者有何异同呢？**\n\n- B树的所有节点既存放键(key)也存放数据(data)，而B+树只有叶子节点存放key和data，其他内节点只存放key。\n- B树的叶子节点都是独立的;B+树的叶子节点有一条引用链指向与它相邻的叶子节点。\n- B树的检索的过程相当于对范围内的每个节点的关键字做二分查找，可能还没有到达叶子节点，检索就结束了。而B+树的检索效率就很稳定了，任何查找都是从根节点到叶子节点的过程，叶子节点的顺序检索很明显。\n\n在MySQL中，MyISAM引擎和InnoDB引擎都是使用B+Tree作为索引结构，但是，两者的实现方式不太一样。（下面的内容整理自《Java工程师修炼之道》）\n\n> MyISAM引擎中，B+Tree叶节点的data域存放的是数据记录的地址。在索引检索的时候，首先按照B+Tree搜索算法搜索索引，如果指定的Key存在，则取出其data域的值，然后以data域的值为地址读取相应的数据记录。这被称为“**非聚簇索引（非聚集索引）**”。\n>\n> InnoDB引擎中，其数据文件本身就是索引文件。相比MyISAM，索引文件和数据文件是分离的，其表数据文件本身就是按B+Tree组织的一个索引结构，树的叶节点data域保存了完整的数据记录。这个索引的key是数据表的主键，因此InnoDB表数据文件本身就是主索引。这被称为**聚簇索引（聚集索引）**，而其余的索引都作为**辅助索引**，辅助索引的data域存储相应记录主键的值而不是地址，这也是和MyISAM不同的地方。在根据主索引搜索时，直接找到key所在的节点即可取出数据；在根据辅助索引查找时，则需要先取出主键的值，再走一遍主索引。因此，在设计表的时候，不建议使用过长的字段作为主键，也不建议使用非单调的字段作为主键，这样会造成主索引频繁分裂。\n\n### 索引类型总结\n\n**按照数据结构维度划分**：\n\n- BTree索引：MySQL里默认和最常用的索引类型。只有叶子节点存储value，非叶子节点只有指针和key。存储引擎MyISAM和InnoDB实现BTree索引都是使用B+Tree，但二者实现方式不一样（前面已经介绍了）。\n- 哈希索引：类似键值对的形式，一次即可定位。\n- RTree索引：一般不会使用，仅支持geometry数据类型，优势在于范围查找，效率较低，通常使用搜索引擎如ElasticSearch代替。\n- 全文索引：对文本的内容进行分词，进行搜索。目前只有`CHAR`、`VARCHAR`，`TEXT`列上可以创建全文索引。一般不会使用，效率较低，通常使用搜索引擎如ElasticSearch代替。\n\n**按照底层存储方式角度划分**：\n\n- 聚簇索引（聚集索引）：索引结构和数据一起存放的索引，InnoDB中的主键索引就属于聚簇索引。\n- 非聚簇索引（非聚集索引）：索引结构和数据分开存放的索引，二级索引(辅助索引)就属于非聚簇索引。MySQL的MyISAM引擎，不管主键还是非主键，使用的都是非聚簇索引。\n\n**按照应用维度划分**：\n\n- 主键索引：加速查询+列值唯一（不可以有NULL）+表中只有一个。\n- 普通索引：仅加速查询。\n- 唯一索引：加速查询+列值唯一（可以有NULL）。\n- 覆盖索引：一个索引包含（或者说覆盖）所有需要查询的字段的值。\n- 联合索引：多列值组成一个索引，专门用于组合搜索，其效率大于索引合并。\n- 全文索引：对文本的内容进行分词，进行搜索。目前只有`CHAR`、`VARCHAR`，`TEXT`列上可以创建全文索引。一般不会使用，效率较低，通常使用搜索引擎如ElasticSearch代替。\n\nMySQL8.x中实现的索引新特性：\n\n- 隐藏索引：也称为不可见索引，不会被优化器使用，但是仍然需要维护，通常会软删除和灰度发布的场景中使用。主键不能设置为隐藏（包括显式设置或隐式设置）。\n- 降序索引：之前的版本就支持通过desc来指定索引为降序，但实际上创建的仍然是常规的升序索引。直到MySQL8.x版本才开始真正支持降序索引。另外，在MySQL8.x版本中，不再对GROUPBY语句进行隐式排序。\n- 函数索引：从MySQL8.0.13版本开始支持在索引中使用函数或者表达式的值，也就是在索引中可以包含函数或者表达式。\n\n### 主键索引(Primary Key)\n\n数据表的主键列使用的就是主键索引。一张数据表有只能有一个主键，并且主键不能为null，不能重复。在MySQL的InnoDB的表中，当没有显示的指定表的主键时，InnoDB会自动先检查表中是否有唯一索引且不允许存在null值的字段，如果有，则选择该字段为默认的主键，否则InnoDB将会自动创建一个6Byte的自增主键。\n\n![img](https://oss.javaguide.cn/github/javaguide/open-source-project/cluster-index.png)\n\n### 二级索引\n\n**二级索引（SecondaryIndex）又称为辅助索引，是因为二级索引的叶子节点存储的数据是主键。也就是说，通过二级索引，可以定位主键的位置**。\n\n唯一索引，普通索引，前缀索引等索引属于二级索引。\n\n1. **唯一索引（UniqueKey）**：唯一索引也是一种约束。唯一索引的属性列不能出现重复的数据，但是允许数据为NULL，一张表允许创建多个唯一索引。建立唯一索引的目的大部分时候都是为了该属性列的数据的唯一性，而不是为了查询效率。\n2. **普通索引（Index）**：普通索引的唯一作用就是为了快速查询数据，一张表允许创建多个普通索引，并允许数据重复和NULL。\n3. **前缀索引（Prefix）**：前缀索引只适用于字符串类型的数据。前缀索引是对文本的前几个字符创建索引，相比普通索引建立的数据更小，因为只取前几个字符。\n4. **全文索引（FullText）**：全文索引主要是为了检索大文本数据中的关键字的信息，是目前搜索引擎数据库使用的一种技术。Mysql5.6之前只有MYISAM引擎支持全文索引，5.6之后InnoDB也支持了全文索引。\n\n二级索引:\n\n![img](https://oss.javaguide.cn/github/javaguide/open-source-project/no-cluster-index.png)\n\n### 聚簇索引与非聚簇索引\n\n#### 聚簇索引（聚集索引）\n\n##### 聚簇索引介绍\n\n**聚簇索引（ClusteredIndex）即索引结构和数据一起存放的索引，并不是一种单独的索引类型。InnoDB中的主键索引就属于聚簇索引**。\n\n在MySQL中，InnoDB引擎的表的`.ibd`文件就包含了该表的索引和数据，对于InnoDB引擎表来说，该表的索引(B+树)的每个非叶子节点存储索引，叶子节点存储索引和索引对应的数据。\n\n##### 聚簇索引的优缺点\n\n**优点**：\n\n- **查询速度非常快**：聚簇索引的查询速度非常的快，因为整个B+树本身就是一颗多叉平衡树，叶子节点也都是有序的，定位到索引的节点，就相当于定位到了数据。相比于非聚簇索引，聚簇索引少了一次读取数据的IO操作。\n- **对排序查找和范围查找优化**：聚簇索引对于主键的排序查找和范围查找速度非常快。\n\n**缺点**：\n\n- **依赖于有序的数据**：因为B+树是多路平衡树，如果索引的数据不是有序的，那么就需要在插入时排序，如果数据是整型还好，否则类似于字符串或UUID这种又长又难比较的数据，插入或查找的速度肯定比较慢。\n- **更新代价大**：如果对索引列的数据被修改时，那么对应的索引也将会被修改，而且聚簇索引的叶子节点还存放着数据，修改代价肯定是较大的，所以对于主键索引来说，主键一般都是不可被修改的。\n\n#### 非聚簇索引（非聚集索引）\n\n##### 非聚簇索引介绍\n\n非聚簇索引(Non-ClusteredIndex)即索引结构和数据分开存放的索引，并不是一种单独的索引类型。二级索引(辅助索引)就属于非聚簇索引。MySQL的MyISAM引擎，不管主键还是非主键，使用的都是非聚簇索引。\n\n非聚簇索引的叶子节点并不一定存放数据的指针，因为二级索引的叶子节点就存放的是主键，根据主键再回表查数据。\n\n##### 非聚簇索引的优缺点\n\n**优点**：\n\n更新代价比聚簇索引要小。非聚簇索引的更新代价就没有聚簇索引那么大了，非聚簇索引的叶子节点是不存放数据的\n\n**缺点**：\n\n- **依赖于有序的数据**：跟聚簇索引一样，非聚簇索引也依赖于有序的数据\n- **可能会二次查询（回表）**：这应该是非聚簇索引最大的缺点了。当查到索引对应的指针或主键后，可能还需要根据指针或主键再到数据文件或表中查询。\n\n这是MySQL的表的文件截图:\n\n![img](https://oss.javaguide.cn/github/javaguide/database/mysql20210420165311654.png)\n\n聚簇索引和非聚簇索引:\n\n![img](https://oss.javaguide.cn/github/javaguide/database/mysql20210420165326946.png)\n\n##### 非聚簇索引一定回表查询吗(覆盖索引)?\n\n**非聚簇索引不一定回表查询**。试想一种情况，用户准备使用SQL查询用户名，而用户名字段正好建立了索引。\n\n\n```sql\nSELECT name FROM table WHERE name='guang19';\n```\n\n那么这个索引的key本身就是name，查到对应的name直接返回就行了，无需回表查询。即使是MYISAM也是这样，虽然MYISAM的主键索引确实需要回表，因为它的主键索引的叶子节点存放的是指针。但是！**如果SQL查的就是主键呢**？\n\n\n```sql\nSELECT id FROM table WHERE id=1;\n```\n\n主键索引本身的key就是主键，查到返回就行了。这种情况就称之为覆盖索引了。\n\n### 覆盖索引和联合索引\n\n#### 覆盖索引\n\n如果一个索引包含（或者说覆盖）所有需要查询的字段的值，我们就称之为**覆盖索引（CoveringIndex）**。我们知道在InnoDB存储引擎中，如果不是主键索引，叶子节点存储的是主键+列值。最终还是要“回表”，也就是要通过主键再查找一次。这样就会比较慢覆盖索引就是把要查询出的列和索引是对应的，不做回表操作！覆盖索引即需要查询的字段正好是索引的字段，那么直接根据该索引，就可以查到数据了，而无需回表查询。\n\n> 如主键索引，如果一条SQL需要查询主键，那么正好根据主键索引就可以查到主键。再如普通索引，如果一条SQL需要查询name，name字段正好有索引，那么直接根据这个索引就可以查到数据，也无需回表。\n\n![覆盖索引](https://oss.javaguide.cn/github/javaguide/database/mysql20210420165341868.png)\n\n我们这里简单演示一下覆盖索引的效果。\n\n1、创建一个名为cus_order的表，来实际测试一下这种排序方式。为了测试方便，cus_order这张表只有id、score、name这3个字段。\n\n\n```sql\nCREATE TABLE cus_order (\n  id int(11) unsigned NOT NULL AUTO_INCREMENT,\n  score int(11) NOT NULL,\n  name varchar(11) NOT NULL DEFAULT '',\n  PRIMARY KEY (`id`)\n) ENGINE=InnoDB AUTO_INCREMENT=100000 DEFAULT CHARSET=utf8mb4;\n```\n\n2、定义一个简单的存储过程（PROCEDURE）来插入100w测试数据。\n\n```sql\nDELIMITER ;;\nCREATE DEFINER=`root`@`%` PROCEDURE `BatchinsertDataToCusOder`(IN start_num INT,IN max_num INT)\nBEGIN\n      DECLARE i INT default start_num;\n      WHILE i < max_num DO\n          insert into cus_order(id, score, name)\n          values (i,RAND() * 1000000,CONCAT('user', i));\n          SET i = i + 1;\n      END WHILE;\n  END;;\nDELIMITER ;\n```\n\n存储过程定义完成之后，我们执行存储过程即可！\n\n```sql\n-- 插入100w+的随机数据\nCALL BatchinsertDataToCusOder(1, 1000000);\n```\n\n等待一会，100w的测试数据就插入完成了！\n\n3、创建覆盖索引并使用EXPLAIN命令分析。\n\n为了能够对这100w数据按照score进行排序，我们需要执行下面的SQL语句。\n\n```sql\n-- 降序排序\nSELECT score,name FROM cus_order ORDER BY score DESC;\n```\n\n使用EXPLAIN命令分析这条SQL语句，通过Extra这一列的Using filesort，我们发现是没有用到覆盖索引的。\n\n![img](https://oss.javaguide.cn/github/javaguide/mysql/not-using-covering-index-demo.png)\n\n不过这也是理所应当，毕竟我们现在还没有创建索引呢！\n\n我们这里以score和name两个字段建立联合索引：\n\n\n```sql\nALTER TABLE cus_order ADD INDEX id_score_name(score, name);\n```\n\n创建完成之后，再用EXPLAIN命令分析再次分析这条SQL语句。\n\n![img](https://oss.javaguide.cn/github/javaguide/mysql/using-covering-index-demo.png)\n\n通过Extra这一列的Using index，说明这条SQL语句成功使用了覆盖索引。\n\n> 关于EXPLAIN命令的详细介绍请看：[MySQL执行计划分析](https://javaguide.cn/database/mysql/mysql-query-execution-plan.html)这篇文章。\n\n#### 联合索引\n\n使用表中的多个字段创建索引，就是**联合索引**，也叫**组合索引**或**复合索引**。\n\n以score和name两个字段建立联合索引：\n\n```sql\nALTER TABLE `cus_order` ADD INDEX id_score_name(score, name);\n```\n\n#### 最左前缀匹配原则\n\n最左前缀匹配原则指的是，在使用联合索引时，MySQL会根据联合索引中的字段顺序，从左到右依次到查询条件中去匹配，如果查询条件中存在与联合索引中最左侧字段相匹配的字段，则就会使用该字段过滤一批数据，直至联合索引中全部字段匹配完成，或者在执行过程中遇到范围查询（如\">\"、\"<\"）才会停止匹配。对于\">=\"、\"<=\"、\"BETWEEN\"、\"like\"前缀匹配的范围查询，并不会停止匹配。所以，我们在使用联合索引时，可以将区分度高的字段放在最左边，这也可以过滤更多数据。\n\n> 相关阅读：[联合索引的最左匹配原则,全网都在说的一个错误结论](https://mp.weixin.qq.com/s/8qemhRg5MgXs1So5YCv0fQ)。\n\n### 索引下推\n\n索引下推（IndexConditionPushdown）是MySQL5.6版本中提供的一项索引优化功能，可以在非聚簇索引遍历过程中，对索引中包含的字段先做判断，过滤掉不符合条件的记录，减少回表次数。\n\n### 正确使用索引的一些建议\n\n#### 选择合适的字段创建索引\n\n- **不为NULL的字段**：索引字段的数据应该尽量不为NULL，因为对于数据为NULL的字段，数据库较难优化。如果字段频繁被查询，但又避免不了为NULL，建议使用0,1,true,false这样语义较为清晰的短值或短字符作为替代。\n- **被频繁查询的字段**：我们创建索引的字段应该是查询操作非常频繁的字段。\n- **被作为条件查询的字段**：被作为WHERE条件查询的字段，应该被考虑建立索引。\n- **频繁需要排序的字段**：索引已经排序，这样查询可以利用索引的排序，加快排序查询时间。\n- **被经常频繁用于连接的字段**：经常用于连接的字段可能是一些外键列，对于外键列并不一定要建立外键，只是说该列涉及到表与表的关系。对于频繁被连接查询的字段，可以考虑建立索引，提高多表连接查询的效率。\n\n#### 被频繁更新的字段应该慎重建立索引\n\n虽然索引能带来查询上的效率，但是维护索引的成本也是不小的。如果一个字段不被经常查询，反而被经常修改，那么就更不应该在这种字段上建立索引了。\n\n#### 限制每张表上的索引数量\n\n索引并不是越多越好，建议单张表索引不超过5个！索引可以提高效率同样可以降低效率。索引可以增加查询效率，但同样也会降低插入和更新的效率，甚至有些情况下会降低查询效率。因为MySQL优化器在选择如何优化查询时，会根据统一信息，对每一个可以用到的索引来进行评估，以生成出一个最好的执行计划，如果同时有很多个索引都可以用于查询，就会增加MySQL优化器生成执行计划的时间，同样会降低查询性能。\n\n#### 尽可能的考虑建立联合索引而不是单列索引\n\n因为索引是需要占用磁盘空间的，可以简单理解为每个索引都对应着一颗B+树。如果一个表的字段过多，索引过多，那么当这个表的数据达到一个体量后，索引占用的空间也是很多的，且修改索引时，耗费的时间也是较多的。如果是联合索引，多个字段在一个索引上，那么将会节约很大磁盘空间，且修改数据的操作效率也会提升。\n\n#### 注意避免冗余索引\n\n冗余索引指的是索引的功能相同，能够命中索引(a,b)就肯定能命中索引(a)，那么索引(a)就是冗余索引。如（name,city）和（name）这两个索引就是冗余索引，能够命中前者的查询肯定是能够命中后者的在大多数情况下，都应该尽量扩展已有的索引而不是创建新索引。\n\n#### 字符串类型的字段使用前缀索引代替普通索引\n\n前缀索引仅限于字符串类型，较普通索引会占用更小的空间，所以可以考虑使用前缀索引带替普通索引。\n\n#### 避免索引失效\n\n索引失效也是慢查询的主要原因之一，常见的导致索引失效的情况有下面这些：\n\n- 使用SELECT*进行查询;\n- 创建了组合索引，但查询条件未遵守最左匹配原则;\n- 在索引列上进行计算、函数、类型转换等操作;\n- 以%开头的LIKE查询比如like'%abc';\n- 查询条件中使用or，且or的前后条件中有一个列没有索引，涉及的索引都不会被使用到;\n- 发生[隐式转换](https://javaguide.cn/database/mysql/index-invalidation-caused-by-implicit-conversion.html);\n- ......\n\n#### 删除长期未使用的索引\n\n删除长期未使用的索引，不用的索引的存在会造成不必要的性能损耗。MySQL5.7可以通过查询sys库的schema_unused_indexes视图来查询哪些索引从未被使用。\n\n#### 知道如何分析语句是否走索引查询\n\n我们可以使用EXPLAIN命令来分析SQL的执行计划，这样就知道语句是否命中索引了。执行计划是指一条SQL语句在经过MySQL查询优化器的优化会后，具体的执行方式。EXPLAIN并不会真的去执行相关的语句，而是通过查询优化器对语句进行分析，找出最优的查询方案，并显示对应的信息。EXPLAIN的输出格式如下：\n\n```sql\nmysql> EXPLAIN SELECT `score`,`name` FROM `cus_order` ORDER BY `score` DESC;\n+----+-------------+-----------+------------+------+---------------+------+---------+------+--------+----------+----------------+\n| id | select_type | table     | partitions | type | possible_keys | key  | key_len | ref  | rows   | filtered | Extra          |\n+----+-------------+-----------+------------+------+---------------+------+---------+------+--------+----------+----------------+\n|  1 | SIMPLE      | cus_order | NULL       | ALL  | NULL          | NULL | NULL    | NULL | 997572 |   100.00 | Using filesort |\n+----+-------------+-----------+------------+------+---------------+------+---------+------+--------+----------+----------------+\n1 row in set, 1 warning (0.00 sec)\n```\n\n各个字段的含义如下：\n\n| **列名**      | **含义**                                     |\n| ------------- | -------------------------------------------- |\n| id            | SELECT 查询的序列标识符                      |\n| select_type   | SELECT 关键字对应的查询类型                  |\n| table         | 用到的表名                                   |\n| partitions    | 匹配的分区，对于未分区的表，值为 NULL        |\n| type          | 表的访问方法                                 |\n| possible_keys | 可能用到的索引                               |\n| key           | 实际用到的索引                               |\n| key_len       | 所选索引的长度                               |\n| ref           | 当使用索引等值查询时，与索引作比较的列或常量 |\n| rows          | 预计要读取的行数                             |\n| filtered      | 按表条件过滤后，留存的记录数的百分比         |\n| Extra         | 附加信息                                     |\n\n> 篇幅问题，我这里只是简单介绍了一下MySQL执行计划，详细介绍请看：[MySQL执行计划分析](https://javaguide.cn/database/mysql/mysql-query-execution-plan.html)这篇文章\n> [原文链接](https://javaguide.cn/database/mysql/mysql-index.html)\n\n\n### 总结\n\n索引什么情况下会失效\n1. 使用!=或者<>或者is null判断导致索引失效\n2. 类型不一致导致的索引失效，如主键为id，其他表的字段为这个表的主键,字段类型却不一样\n3. 函数导致的索引失效，如SUBSTRING(name,1,2) = 'wise'需要全表扫描\n4. 运算符导致的索引失效\n5. OR引起的索引失效，因为如果OR判断的字段中有一个没有索引的话,引擎会放弃索引而产生全表扫描\n6. 模糊搜索LIKE导致的索引失效\n7. NOT IN、NOT EXISTS导致索引失效\n8. IS NULL不走索引，IS NOT NULL走索引\n9. 联合索引如果不遵循最左前缀原则，那么索引也将失效\n10. 索引列如果使用了隐式转换也会导致索引失效\n\n### 相关文章\n\n- [如何防止MySQL索引失效](https://mp.weixin.qq.com/s/B1Dr_w3oeIsFTRtZCKS5Ow)\n- [你设计索引的原则是什么？怎么避免索引失效？](https://mp.weixin.qq.com/s/wyotVRKbBJ7LGdqFSxZOFg)\n- [超全的数据库建表/SQL/索引规范，建议贴在工位上！](https://mp.weixin.qq.com/s/k7AXALxURez5nS34KZZIxA)\n- [MySQL索引扫盲总结](https://mp.weixin.qq.com/s/hsvdbGXBcHPA0JOO7gkpjw)\n- [为什么Mysql的常用引擎都默认使用B+树作为索引？](https://mp.weixin.qq.com/s?__biz=MzkzODE3OTI0Ng==&mid=2247491020&idx=1&sn=f703a7a54cf9bccd8aed6dd384515945&source=41#wechat_redirect)\n- [为什么MySQL使用B+树，而不是B树或者Hash？](https://mp.weixin.qq.com/s/Ii9gF-TZZ84WghB8Xtwxig)\n- [MySQL索引底层：B+树详解](https://mp.weixin.qq.com/s/opbgvTX_GDytR-MS3-6yEA)\n- [为什么Mongodb索引用B树，而Mysql用B+树?](https://mp.weixin.qq.com/s/ZbkRWFT5rXIA9ZXzwTZTTA)\n- [腾讯面试官用「B+树」虐哭我了](https://mp.weixin.qq.com/s/I6g_O2lq3iI6DdnxFGGsuA)\n- [面试官:谈谈你对mysql索引的认识？](https://mp.weixin.qq.com/s?__biz=MzIwMDgzMjc3NA==&mid=2247484720&idx=1&sn=7bd7774058e7886eeb3dedb38aa8657a&chksm=96f66759a181ee4f4c177a755c3ac6b6e97fef148bbf4afea8616f4edec33bf6d4f18cda9f69&scene=21#wechat_redirect)\n- [面试的时候，如果你没掌握索引，绝对没戏！](https://mp.weixin.qq.com/s/RfA8r3IXECqmvvTJX4sz0w)\n- [什么是MySQ索引?](https://mp.weixin.qq.com/s/yxS4tpX_6fz9LBsh0UoHpw)\n- [MySQL的索引实现原理](https://mp.weixin.qq.com/s/1RdEIq4EDYpIge_84dunEw)\n- [浅入浅出MySQL索引](https://mp.weixin.qq.com/s/b5xc6mRzzqg5Bjd5scJ_ug)\n- [别再一知半解啦，索引其实就这么回事](https://mp.weixin.qq.com/s/F1EY0hY9WrzmeRp3C5C9Pw)\n- [MySQL索引凭什么让查询效率提高这么多？](https://mp.weixin.qq.com/s/HUy5RPBsxhu7g5Sr-l3XlQ)\n- [索引为什么能提高查询性能....](https://mp.weixin.qq.com/s/6N-Do7Av6y6VP_vK9CVK3A)\n- [常见索引类型](https://mp.weixin.qq.com/s/YRbHVNeJNjvQcgC4PunW6w)\n- [主键索引就是聚集索引？MySQL索引类型大梳理](https://mp.weixin.qq.com/s/iS8V65my03EQtOQAkxfMag)\n- [再聊MySQL聚簇索引](https://mp.weixin.qq.com/s/F0cEzIqecF4sWg7ZRmHKRQ)\n- [MySQL主键自增也有坑?innodb_autoinc_lock_mode](https://mp.weixin.qq.com/s/5EymS2IyB7yyYsxX5UWrtw)\n- [再有人问你MySQL索引原理，就把这篇文章甩给他！](https://mp.weixin.qq.com/s/9yeModGuGvDu5S0bW9sU6w)\n- [为什么索引可以让查询变快？终于有人说清楚了！](https://mp.weixin.qq.com/s/dKvPKUVDTM1OiP_qzNiFBg)\n- [面试官问我索引为什么这快？我好像解释不清楚了](https://mp.weixin.qq.com/s/EyYBqfZcBWP60Y1gAADeOg)\n- [一文讲清，MySQL中的二级索引](https://mp.weixin.qq.com/s/0_4Jq06LLaTEfjetLQP0iA)\n- [明明加了唯一索引，为什么还是产生重复数据？](https://mp.weixin.qq.com/s/_hdGKhB-A4ZOFZ42POmgjQ)\n- [面试官提问：什么是前缀索引？](https://mp.weixin.qq.com/s/4XfSv004Vy7hyvZsjlQXwQ)\n- [MySQL遵循最左前缀匹配原则！面试官：回去等通知吧](https://mp.weixin.qq.com/s/IYRTE00_3bXD6y3YBW9P6Q)\n- [MySQL索引15连问](https://mp.weixin.qq.com/s/N95kMxVduiA6ZA3TSexAtA)\n- [MySQL索引数据结构入门](https://mp.weixin.qq.com/s/QLQvMT2sPmmjVE_pXj5FIA)\n- [前缀索引，在性能和空间中寻找平衡](https://mp.weixin.qq.com/s/mqi7MyF183FUmgy2FXi3Tw)\n\n## MySQL三大日志\n\nMySQL日志主要包括错误日志、查询日志、慢查询日志、事务日志、二进制日志几大类。其中，比较重要的还要属二进制日志bin log（归档日志）和事务日志redo log（重做日志）和undo log（回滚日志）。\n\n![img](https://oss.javaguide.cn/github/javaguide/01.png)\n\n今天就来聊聊redo log（重做日志）、bin log（归档日志）、两阶段提交、undo log（回滚日志）。\n\n### redo log\n\nredo log（重做日志）是InnoDB存储引擎独有的，它让MySQL拥有了崩溃恢复能力。比如MySQL实例挂了或宕机了，重启时，InnoDB存储引擎会使用redo log恢复数据，保证数据的持久性与完整性。\n\n![img](https://oss.javaguide.cn/github/javaguide/02.png)\n\nMySQL中数据是以页为单位，你查询一条记录，会从硬盘把一页的数据加载出来，加载出来的数据叫数据页，会放入到Buffer Pool中。后续的查询都是先从Buffer Pool中找，没有命中再去硬盘加载，减少硬盘IO开销，提升性能。更新表数据的时候，也是如此，发现Buffer Pool里存在要更新的数据，就直接在Buffer Pool里更新。然后会把“在某个数据页上做了什么修改”记录到重做日志缓存（redo log buffer）里，接着刷盘到redo log文件里。\n\n![img](https://oss.javaguide.cn/github/javaguide/03.png)\n\n> 图片笔误提示：第4步“清空redo log buffe刷盘到redo日志中”这句话中的buffe应该是buffer。\n\n理想情况，事务一提交就会进行刷盘操作，但实际上，刷盘的时机是根据策略来进行的。\n\n> 小贴士：每条redo记录由“表空间号+数据页号+偏移量+修改数据长度+具体修改的数据”组成\n\n#### 刷盘时机\n\nInnoDB存储引擎为redo log的刷盘策略提供了innodb_flush_log_at_trx_commit参数，它支持三种策略：\n\n- **0**：设置为0的时候，表示每次事务提交时不进行刷盘操作\n- **1**：设置为1的时候，表示每次事务提交时都将进行刷盘操作（默认值）\n- **2**：设置为2的时候，表示每次事务提交时都只把redo log buffer内容写入page cache\n\ninnodb_flush_log_at_trx_commit参数默认为1，也就是说当事务提交时会调用fsync对redo log进行刷盘。另外，InnoDB存储引擎有一个后台线程，每隔1秒，就会把redo log buffer中的内容写到文件系统缓存（page cache），然后调用fsync刷盘。\n\n![img](https://oss.javaguide.cn/github/javaguide/04.png)\n\n也就是说，一个没有提交事务的redo log记录，也可能会刷盘。\n\n**为什么呢？**\n\n因为在事务执行过程redo log记录是会写入redo log buffer中，这些redo log记录会被后台线程刷盘。\n\n![img](https://oss.javaguide.cn/github/javaguide/05.png)\n\n除了后台线程每秒1次的轮询操作，还有一种情况，当redo log buffer占用的空间即将达到innodb_log_buffer_size一半的时候，后台线程会主动刷盘。下面是不同刷盘策略的流程图。\n\n##### innodb_flush_log_at_trx_commit=0\n\n![img](https://oss.javaguide.cn/github/javaguide/06.png)\n\n为0时，如果MySQL挂了或宕机可能会有1秒数据的丢失。\n\n##### innodb_flush_log_at_trx_commit=1\n\n![img](https://oss.javaguide.cn/github/javaguide/07.png)\n\n为1时，只要事务提交成功，redo log记录就一定在硬盘里，不会有任何数据丢失。如果事务执行期间MySQL挂了或宕机，这部分日志丢了，但是事务并没有提交，所以日志丢了也不会有损失。\n\n##### innodb_flush_log_at_trx_commit=2\n\n![img](https://oss.javaguide.cn/github/javaguide/09.png)\n\n为2时，只要事务提交成功，redo log buffer中的内容只写入文件系统缓存（page cache）。如果仅仅只是MySQL挂了不会有任何数据丢失，但是宕机可能会有1秒数据的丢失。\n\n#### 日志文件组\n\n硬盘上存储的redo log日志文件不只一个，而是以一个**日志文件组**的形式出现的，每个的redo日志文件大小都是一样的。比如可以配置为一组4个文件，每个文件的大小是1GB，整个redo log日志文件组可以记录4G的内容。它采用的是环形数组形式，从头开始写，写到末尾又回到头循环写，如下图所示。\n\n![img](https://oss.javaguide.cn/github/javaguide/10.png)\n\n在个**日志文件组**中还有两个重要的属性，分别是write pos、checkpoint\n\n- **write pos**是当前记录的位置，一边写一边后移\n- **checkpoint**是当前要擦除的位置，也是往后推移\n\n每次刷盘redo log记录到**日志文件组**中，write pos位置就会后移更新。每次MySQL加载**日志文件组**恢复数据时，会清空加载过的redo log记录，并把checkpoint后移更新。write pos和checkpoint之间的还空着的部分可以用来写入新的redo log记录。\n\n![img](https://oss.javaguide.cn/github/javaguide/11.png)\n\n如果write pos追上checkpoint，表示**日志文件组**满了，这时候不能再写入新的redo log记录，MySQL得停下来，清空一些记录，把checkpoint推进一下。\n\n![img](https://oss.javaguide.cn/github/javaguide/12.png)\n\n#### redo log小结\n\n相信大家都知道redo log的作用和它的刷盘时机、存储形式。现在我们来思考一个问题：**只要每次把修改后的数据页直接刷盘不就好了，还有redo log什么事**？它们不都是刷盘么？差别在哪里？\n\n\n```java\n1 Byte = 8bit\n1 KB = 1024 Byte\n1 MB = 1024 KB\n1 GB = 1024 MB\n1 TB = 1024 GB\n```\n\n实际上，数据页大小是16KB，刷盘比较耗时，可能就修改了数据页里的几Byte数据，有必要把完整的数据页刷盘吗？而且数据页刷盘是随机写，因为一个数据页对应的位置可能在硬盘文件的随机位置，所以性能是很差。如果是写redo log，一行记录可能就占几十Byte，只包含表空间号、数据页号、磁盘文件偏移量、更新值，再加上是顺序写，所以刷盘速度很快。所以用redo log形式记录修改内容，性能会远远超过刷数据页的方式，这也让数据库的并发能力更强。\n\n> 其实内存的数据页在一定时机也会刷盘，我们把这称为页合并，讲Buffer Pool的时候会对这块细说\n\n### bin log\n\nredo log它是物理日志，记录内容是“在某个数据页上做了什么修改”，属于InnoDB存储引擎。而bin log是逻辑日志，记录内容是语句的原始逻辑，类似于“给ID=2这一行的c字段加1”，属于MySQL Server层。不管用什么存储引擎，只要发生了表数据更新，都会产生bin log日志。那bin log到底是用来干嘛的？可以说MySQL数据库的**数据备份、主备、主主、主从**都离不开bin log，需要依靠bin log来同步数据，保证数据一致性。\n\n![img](https://oss.javaguide.cn/github/javaguide/01-20220305234724956.png)\n\nbin log会记录所有涉及更新数据的逻辑操作，并且是顺序写。\n\n#### 记录格式\n\nbin log日志有三种格式，可以通过bin log_format参数指定。\n\n- **statement**\n- **row**\n- **mixed**\n\n指定statement，记录的内容是SQL语句原文，比如执行一条`update T set update_time=now( )where id=1`，记录的内容如下。\n\n![img](https://oss.javaguide.cn/github/javaguide/02-20220305234738688.png)\n\n同步数据时，会执行记录的SQL语句，但是有个问题，update_time=now()这里会获取当前系统时间，直接执行会导致与原库的数据不一致。为了解决这种问题，我们需要指定为row，记录的内容不再是简单的SQL语句了，还包含操作的具体数据，记录内容如下。\n\n![img](https://oss.javaguide.cn/github/javaguide/03-20220305234742460.png)\n\nrow格式记录的内容看不到详细信息，要通过mysql bin log工具解析出来。update_time=now()变成了具体的时间update_time=1627112756247，条件后面的@1、@2、@3都是该行数据第1个~3个字段的原始值（**假设这张表只有3个字段**）。这样就能保证同步数据的一致性，通常情况下都是指定为row，这样可以为数据库的恢复与同步带来更好的可靠性。但是这种格式，需要更大的容量来记录，比较占用空间，恢复与同步时会更消耗IO资源，影响执行速度。所以就有了一种折中的方案，指定为mixed，记录的内容是前两者的混合。MySQL会判断这条SQL语句是否可能引起数据不一致，如果是，就用row格式，否则就用statement格式。\n\n#### 写入机制\n\nbin log的写入时机也非常简单，事务执行过程中，先把日志写到bin log cache，事务提交的时候，再把bin log cache写到bin log文件中。因为一个事务的bin log不能被拆开，无论这个事务多大，也要确保一次性写入，所以系统会给每个线程分配一个块内存作为bin log cache。我们可以通过bin log_cache_size参数控制单个线程bin log cache大小，如果存储内容超过了这个参数，就要暂存到磁盘（Swap）。bin log日志刷盘流程如下\n\n![img](https://oss.javaguide.cn/github/javaguide/04-20220305234747840.png)\n\n- **上图的write，是指把日志写入到文件系统的page cache，并没有把数据持久化到磁盘，所以速度比较快**\n- **上图的fsync，才是将数据持久化到磁盘的操作**\n\nwrite和fsync的时机，可以由参数sync_bin log控制，默认是0。为0的时候，表示每次提交事务都只write，由系统自行判断什么时候执行fsync。\n\n![img](https://oss.javaguide.cn/github/javaguide/05-20220305234754405.png)\n\n虽然性能得到提升，但是机器宕机，page cache里面的bin log会丢失。为了安全起见，可以设置为1，表示每次提交事务都会执行fsync，就如同**redo log日志刷盘流程**一样。最后还有一种折中方式，可以设置为N(N>1)，表示每次提交事务都write，但累积N个事务后才fsync。\n\n![img](https://oss.javaguide.cn/github/javaguide/06-20220305234801592.png)\n\n在出现IO瓶颈的场景里，将sync_bin log设置成一个比较大的值，可以提升性能。同样的，如果机器宕机，会丢失最近N个事务的bin log日志。\n\n### 两阶段提交\n\nredo log（重做日志）让InnoDB存储引擎拥有了崩溃恢复能力。\n\nbin log（归档日志）保证了MySQL集群架构的数据一致性。\n\n虽然它们都属于持久化的保证，但是侧重点不同。在执行更新语句过程，会记录redo log与bin log两块日志，以基本的事务为单位，redo log在事务执行过程中可以不断写入，而bin log只有在提交事务时才写入，所以redo log与bin log的写入时机不一样。\n\n![img](https://oss.javaguide.cn/github/javaguide/01-20220305234816065.png)\n\n回到正题，redo log与bin log两份日志之间的逻辑不一致，会出现什么问题？我们以update语句为例，假设id=2的记录，字段c值是0，把字段c值更新成1，SQL语句为`update T set c=1 where id=2`。假设执行过程中写完redo log日志后，bin log日志写期间发生了异常，会出现什么情况呢？\n\n![img](https://oss.javaguide.cn/github/javaguide/02-20220305234828662.png)\n\n由于bin log没写完就异常，这时候bin log里面没有对应的修改记录。因此，之后用bin log日志恢复数据时，就会少这一次更新，恢复出来的这一行c值是0，而原库因为redo log日志恢复，这一行c值是1，最终数据不一致。\n\n![img](https://oss.javaguide.cn/github/javaguide/03-20220305235104445.png)\n\n为了解决两份日志之间的逻辑一致问题，InnoDB存储引擎使用**两阶段提交**方案。原理很简单，将redo log的写入拆成了两个步骤prepare和commit，这就是**两阶段提交**。\n\n![img](https://oss.javaguide.cn/github/javaguide/04-20220305234956774.png)\n\n使用**两阶段提交**后，写入bin log时发生异常也不会有影响，因为MySQL根据redo log日志恢复数据时，发现redo log还处于prepare阶段，并且没有对应bin log日志，就会回滚该事务。\n\n![img](https://oss.javaguide.cn/github/javaguide/05-20220305234937243.png)\n\n再看一个场景，redo log设置commit阶段发生异常，那会不会回滚事务呢？\n\n![img](https://oss.javaguide.cn/github/javaguide/06-20220305234907651.png)\n\n并不会回滚事务，它会执行上图框住的逻辑，虽然redo log是处于prepare阶段，但是能通过事务id找到对应的bin log日志，所以MySQL认为是完整的，就会提交事务恢复数据。\n\n### undo log\n\n> 我们知道如果想要保证事务的原子性，就需要在异常发生时，对已经执行的操作进行回滚，在MySQL中，恢复机制是通过回滚日志（undo log）实现的，所有事务进行的修改都会先记录到这个回滚日志中，然后再执行相关的操作。如果执行过程中遇到异常的话，我们直接利用回滚日志中的信息将数据回滚到修改之前的样子即可！并且，回滚日志会先于数据持久化到磁盘上。这样就保证了即使遇到数据库突然宕机等情况，当用户再次启动数据库的时候，数据库还能够通过查询回滚日志来回滚将之前未完成的事务。\n\n另外，MVCC的实现依赖于：**隐藏字段、ReadView、undolog**。在内部实现中，InnoDB通过数据行的DB_TRX_ID和ReadView来判断数据的可见性，如不可见，则通过数据行的DB_ROLL_PTR找到undo log中的历史版本。每个事务读到的数据版本可能是不一样的，在同一个事务中，用户只能看到该事务创建ReadView之前已经提交的修改和该事务本身做的修改\n\n### 总结\n\n> MySQLInnoDB引擎使用redo log(重做日志)保证事务的持久性，使用undolog(回滚日志)来保证事务的原子性。\n> MySQL数据库的数据备份、主备、主主、主从都离不开bin log，需要依靠bin log来同步数据，保证数据一致性。\n> [原文链接](https://javaguide.cn/database/mysql/mysql-logs.html)\n\n#### 三大日志\n\nbin log,redo log,undo log\n\n**bin log**\n主要使用场景有两个，分别是主从复制和数据恢复\nbin log记录了数据库表结构和表数据变更，比如update/delete/insert/truncate/create。它不会记录select（因为这没有对表没有进行变更）\n\n**redo log**\n包括两部分：一个是内存中的日志缓冲(redo log buffer)，另一个是磁盘上的日志文件(redo log file)。Mysql的基本存储结构是页(记录都存在页里边)，所以MySQL是先把这条记录所在的页找到，然后把该页加载到内存中，将对应记录进行修改。现在就可能存在一个问题：如果在内存中把数据改了，还没来得及落磁盘，而此时的数据库挂了怎么办？显然这次更改就丢了。所以引入redo log解决。MySQL每执行一条DML语句，先将记录写入redo log buffer（缓冲），后续某个时间点再一次性将多个操作记录写到redo log file（磁盘）。这种先写日志，再写磁盘的技术就是MySQL里经常说到的WAL(Write-Ahead Logging)技术。redo log也是写到磁盘，不过是顺序IO所以速度很快。\n\n**bin log和 redo log区别**\n1. 存储的内容\nbin log记载的是update/delete/insert这样的SQL语句，而redo log记载的是物理修改的内容（xxxx页修改了xxx）。\n所以在搜索资料的时候会有这样的说法：redo log记录的是数据的物理变化，bin log记录的是数据的逻辑变化\n2. 功能\nredo log的作用是为持久化而生的。写完内存，如果数据库挂了，那我们可以通过redo log来恢复内存还没来得及刷到磁盘的数据，将redo log加载到内存里边，那内存就能恢复到挂掉之前的数据了。\nbin log的作用是复制和恢复而生的。主从服务器需要保持数据的一致性，通过bin log来同步数据。如果整个数据库的数据都被删除了，bin log存储着所有的数据变更情况，那么可以通过bin log来对数据进行恢复。看到这里，你会想：”如果整个数据库的数据都被删除了，那我可以用redo log的记录来恢复吗？“不能,因为功能的不同，redo log存储的是物理数据的变更，如果我们内存的数据已经刷到了磁盘了，那redo log的数据就无效了。所以redo log不会存储着历史所有数据的变更，文件的内容会被覆盖的。\n3. redo log是MySQL的InnoDB引擎所产生的。bin log无论MySQL用什么引擎，都会有的。\n\n**undo log**\n主要有两个作用：回滚和多版本控制(MVCC)\n在数据修改的时候，不仅记录了redo log，还记录undo log，如果因为某些原因导致事务失败或回滚了，可以用undo log进行回滚，undo log主要存储的也是逻辑日志，比如我们要insert一条数据了，那undo log会记录的一条对应的delete日志。我们要update一条记录时，它会记录一条对应相反的update记录。\n\n### 相关文章\n\n- [MySQL undo log、bin log、redo log](https://mp.weixin.qq.com/s/0z6GmUp0Lb1hDUo0EyYiUg)\n- [必须了解的mysql三大日志-bin log、redo log和undo log](https://mp.weixin.qq.com/s?__biz=Mzg2MDYzODI5Nw==&mid=2247494134&idx=1&sn=241d91929e6a8b7b5f92db1528286645&source=41#wechat_redirect)\n- [MySQL日志(redo log 和 undo log)都是什么鬼？](https://mp.weixin.qq.com/s/JLV8GEcF1z4D8oiDhFo6-g)\n- [MySQL不会丢失数据的秘密，就藏在它的7种日志里](https://mp.weixin.qq.com/s/d_CRZGbC6qpxdP4BYetFcA)\n- [Java代码中，如何监控Mysql的bin log？](https://mp.weixin.qq.com/s/IEG6O3xNv62moh5DYxPL2Q)\n- [手把手教你玩MySQL删库不跑路，直接把MySQL的bin log玩溜！](https://mp.weixin.qq.com/s/w4vPFHJkog2nbl68_0LOnw)\n- [MySQL的bin log的三种格式这么好玩！](https://mp.weixin.qq.com/s/FlhZsx9MBGY0ypXQMqe6wA)\n- [讲一讲MySQL数据备份杀手锏bin log](https://mp.weixin.qq.com/s/0dBwL3nwcldbS7hv7O5rOQ)\n- [3000帧动画图解MySQL为什么需要bin log、redo log和undo log](https://mp.weixin.qq.com/s/lvw89Ix73oTqc2juF4X5sg)\n- [MySQL bin log的三个业务应用场景](https://mp.weixin.qq.com/s/kqfMxkpYZ3ICkHfbv8ET1g)\n- [MySQL中bin log与redo log的区别](https://zhuanlan.zhihu.com/p/299512401)\n- [听我讲完redo log、bin log原理，面试官老脸一红](https://mp.weixin.qq.com/s/PvQxdDMdC98mpwSb7pyXvw)\n- [bin log恢复误删除的数据](https://mp.weixin.qq.com/s/JYuh2c3Tad8C_DP7LZO5bg)\n- [聊聊redo log是什么？](https://mp.weixin.qq.com/s/ayI190ZxhAoUracU3K_HTA)\n- [MySQL为什么需要redo log？](https://mp.weixin.qq.com/s/cXQ3RFwi-4JgH_x-3HJQnA)\n\n\n## InnoDB存储引擎对MVCC的实现\n\n### 一致性非锁定读和锁定读\n\n#### 一致性非锁定读\n\n对于[一致性非锁定读（Consistent Nonlocking Reads）](https://dev.mysql.com/doc/refman/5.7/en/innodb-consistent-read.html)的实现，通常做法是加一个版本号或者时间戳字段，在更新数据的同时版本号+1或者更新时间戳。查询时，将当前可见的版本号与对应记录的版本号进行比对，如果记录的版本小于可见版本，则表示该记录可见。在InnoDB存储引擎中，[多版本控制(multi versioning)](https://dev.mysql.com/doc/refman/5.7/en/innodb-multi-versioning.html)就是对非锁定读的实现。如果读取的行正在执行DELETE或UPDATE操作，这时读取操作不会去等待行上锁的释放。相反地，InnoDB存储引擎会去读取行的一个快照数据，对于这种读取历史数据的方式，我们叫它快照读(snapshot read)。在`Repeatable Read`和`Read Committed`两个隔离级别下，如果是执行普通的select语句（不包括select...lock in share mode,select...for update）则会使用一致性非锁定读（MVCC）。并且在Repeatable Read下MVCC实现了可重复读和防止部分幻读\n\n#### 锁定读\n\n如果执行的是下列语句，就是[锁定读（Locking Reads）](https://dev.mysql.com/doc/refman/5.7/en/innodb-locking-reads.html)\n\n- select ... lock in share mode\n- select ... for update\n- insert、update、delete操作\n\n在锁定读下，读取的是数据的最新版本，这种读也被称为当前读（current read）。锁定读会对读取到的记录加锁：\n\n- select ... lock in share mode：对记录加S锁，其它事务也可以加S锁，如果加x锁则会被阻塞\n- select ... for update、insert、update、delete：对记录加X锁，且其它事务不能加任何锁\n\n在一致性非锁定读下，即使读取的记录已被其它事务加上X锁，这时记录也是可以被读取的，即读取的快照数据。上面说了，在Repeatable Read下MVCC防止了部分幻读，这边的“部分”是指在一致性非锁定读情况下，只能读取到第一次查询之前所插入的数据（根据Read View判断数据可见性，Read View在第一次查询时生成）。但是！如果是当前读，每次读取的都是最新数据，这时如果两次查询中间有其它事务插入数据，就会产生幻读。所以，InnoDB在实现Repeatable Read时，如果执行的是当前读，则会对读取的记录使用Next-key Lock，来防止其它事务在间隙间插入数据\n\n### InnoDB对MVCC的实现\n\nMVCC的实现依赖于：**隐藏字段、Read View、undo log**。在内部实现中，InnoDB通过数据行的DB_TRX_ID和Read View来判断数据的可见性，如不可见，则通过数据行的DB_ROLL_PTR找到undo log中的历史版本。每个事务读到的数据版本可能是不一样的，在同一个事务中，用户只能看到该事务创建Read View之前已经提交的修改和该事务本身做的修改\n\n#### 隐藏字段\n\n在内部，InnoDB存储引擎为每行数据添加了三个[隐藏字段](https://dev.mysql.com/doc/refman/5.7/en/innodb-multi-versioning.html)：\n\n- DB_TRX_ID（6字节）：表示最后一次插入或更新该行的事务id。此外，delete操作在内部被视为更新，只不过会在记录头Record header中的deleted_flag字段将其标记为已删除\n- DB_ROLL_PTR（7字节）回滚指针，指向该行的undo log。如果该行未被更新，则为空\n- DB_ROW_ID（6字节）：如果没有设置主键且该表没有唯一非空索引时，InnoDB会使用该id来生成聚簇索引\n\n#### ReadView\n\n```c\nclass ReadView {\n  /* ... */\nprivate:\n  trx_id_t m_low_limit_id;      /* 大于等于这个ID的事务均不可见 */\n\n  trx_id_t m_up_limit_id;       /* 小于这个ID的事务均可见 */\n\n  trx_id_t m_creator_trx_id;    /* 创建该Read View的事务ID */\n\n  trx_id_t m_low_limit_no;      /* 事务Number,小于该Number的Undo Logs均可以被Purge */\n\n  ids_t m_ids;                  /* 创建Read View时的活跃事务列表 */\n\n  m_closed;                     /* 标记Read View是否close */\n}\n```\n\n[Read View](https://github.com/facebook/mysql-8.0/blob/8.0/storage/innobase/include/read0types.h#L298)主要是用来做可见性判断，里面保存了“当前对本事务不可见的其他活跃事务”\n\n主要有以下字段：\n\n- m_low_limit_id：目前出现过的最大的事务ID+1，即下一个将被分配的事务ID。大于等于这个ID的数据版本均不可见\n- m_up_limit_id：活跃事务列表m_ids中最小的事务ID，如果m_ids为空，则m_up_limit_id为m_low_limit_id。小于这个ID的数据版本均可见\n- m_ids：Read View创建时其他未提交的活跃事务ID列表。创建Read View时，将当前未提交事务ID记录下来，后续即使它们修改了记录行的值，对于当前事务也是不可见的。m_ids不包括当前事务自己和已提交的事务（正在内存中）\n- m_creator_trx_id：创建该Read View的事务ID\n\n#### undo-log\n\nundo log主要有两个作用：\n\n- 当事务回滚时用于将数据恢复到修改前的样子\n- 另一个作用是MVCC，当读取记录时，若该记录被其他事务占用或当前版本对该事务不可见，则可以通过undo log读取之前的版本数据，以此实现非锁定读\n\n在InnoDB存储引擎中undo log分为两种：insert undo log和update undo log：\n\n1. **insert undo log**：指在insert操作中产生的undo log。因为insert操作的记录只对事务本身可见，对其他事务不可见，故该undo log可以在事务提交后直接删除。不需要进行purge操作。insert时的数据初始状态：\n\n2. **update undo log**：update或delete操作中产生的undo log。该undo log可能需要提供MVCC机制，因此不能在事务提交时就进行删除。提交时放入undo log链表，等待purge线程进行最后的删除\n\n不同事务或者相同事务的对同一记录行的修改，会使该记录行的undo log成为一条链表，链首就是最新的记录，链尾就是最早的旧记录。\n\n#### 数据可见性算法\n\n在InnoDB存储引擎中，创建一个新事务后，执行每个select语句前，都会创建一个快照（ReadView），**快照中保存了当前数据库系统中正处于活跃（没有commit）的事务的ID号**。其实简单的说保存的是系统中当前不应该被本事务看到的其他事务ID列表（即m_ids）。当用户在这个事务中要读取某个记录行的时候，InnoDB会将该记录行的DB_TRX_ID与Read View中的一些变量及当前事务ID进行比较，判断是否满足可见性条件。\n\n1. 如果记录DB_TRX_ID< m_up_limit_id，那么表明最新修改该行的事务（DB_TRX_ID）在当前事务创建快照之前就提交了，所以该记录行的值对当前事务是可见的\n2. 如果DB_TRX_ID>=m_low_limit_id，那么表明最新修改该行的事务（DB_TRX_ID）在当前事务创建快照之后才修改该行，所以该记录行的值对当前事务不可见。跳到步骤5\n3. m_ids为空，则表明在当前事务创建快照之前，修改该行的事务就已经提交了，所以该记录行的值对当前事务是可见的\n4. 如果m_up_limit_id<=DB_TRX_ID< m_low_limit_id，表明最新修改该行的事务（DB_TRX_ID）在当前事务创建快照的时候可能处于“活动状态”或者“已提交状态”；所以就要对活跃事务列表m_ids进行查找（源码中是用的二分查找，因为是有序的）\n    - 如果在活跃事务列表m_ids中能找到DB_TRX_ID，表明：①在当前事务创建快照前，该记录行的值被事务ID为DB_TRX_ID的事务修改了，但没有提交；或者②在当前事务创建快照后，该记录行的值被事务ID为DB_TRX_ID的事务修改了。这些情况下，这个记录行的值对当前事务都是不可见的。跳到步骤5\n    - 在活跃事务列表中找不到，则表明“id为trx_id的事务”在修改“该记录行的值”后，在“当前事务”创建快照前就已经提交了，所以记录行对当前事务可见\n5. 在该记录行的DB_ROLL_PTR指针所指向的undolog取出快照记录，用快照记录的DB_TRX_ID跳到步骤1重新开始判断，直到找到满足的快照版本或返回空\n\n### RC和RR隔离级别下MVCC的差异\n\n在事务隔离级别RC和RR（InnoDB存储引擎的默认事务隔离级别）下，InnoDB存储引擎使用MVCC（非锁定一致性读），但它们生成Read View的时机却不同\n\n- 在RC隔离级别下的每次select查询前都生成一个Read View(m_ids列表)\n- 在RR隔离级别下只在事务开始后第一次select数据前生成一个Read View（m_ids列表）\n\n### MVCC解决不可重复读问题\n\n虽然RC和RR都通过MVCC来读取快照数据，但由于生成Read View时机不同，从而在RR级别下实现可重复读。\n\n#### 在RC下Read View生成情况\n\n**1.假设时间线来到T4，那么此时数据行id=1的版本链为**：\n\n![img](https://javaguide.cn/assets/a3fd1ec6-8f37-42fa-b090-7446d488fd04.bf41f07c.png)\n\n由于RC级别下每次查询都会生成Read View，并且事务101、102并未提交，此时103事务生成的Read View中活跃的事务m_ids为：[101,102]，m_low_limit_id为：104，m_up_limit_id为：101，m_creator_trx_id为：103\n\n- 此时最新记录的DB_TRX_ID为101，m_up_limit_id<=101< m_low_limit_id，所以要在m_ids列表中查找，发现DB_TRX_ID存在列表中，那么这个记录不可见\n- 根据DB_ROLL_PTR找到undo log中的上一版本记录，上一条记录的DB_TRX_ID还是101，不可见\n- 继续找上一条DB_TRX_ID为1，满足1< m_up_limit_id，可见，所以事务103查询到数据为name=菜花\n\n**2.时间线来到T6，数据的版本链为**：\n\n![img](https://javaguide.cn/assets/528559e9-dae8-4d14-b78d-a5b657c88391.2ff79120.png)\n\n因为在RC级别下，重新生成Read View，这时事务101已经提交，102并未提交，所以此时Read View中活跃的事务m_ids：[102]，m_low_limit_id为：104，m_up_limit_id为：102，m_creator_trx_id为：103\n\n- 此时最新记录的DB_TRX_ID为102，m_up_limit_id<=102< m_low_limit_id，所以要在m_ids列表中查找，发现DB_TRX_ID存在列表中，那么这个记录不可见\n- 根据DB_ROLL_PTR找到undo log中的上一版本记录，上一条记录的DB_TRX_ID为101，满足101< m_up_limit_id，记录可见，所以在T6时间点查询到数据为name=李四，与时间T4查询到的结果不一致，不可重复读！\n\n**3.时间线来到T9，数据的版本链为**：\n\n![img](https://javaguide.cn/assets/6f82703c-36a1-4458-90fe-d7f4edbac71a.c8de5ed7.png)\n\n重新生成ReadView，这时事务101和102都已经提交，所以m_ids为空，则m_up_limit_id=m_low_limit_id=104，最新版本事务ID为102，满足102< m_low_limit_id，可见，查询结果为name=赵六\n\n> 总结：在RC隔离级别下，事务在每次查询开始时都会生成并设置新的Read View，所以导致不可重复读\n\n#### 在RR下Read View生成情况\n\n在可重复读级别下，只会在事务开始后第一次读取数据时生成一个Read View（m_ids列表）\n\n**1.在T4情况下的版本链为**：\n\n![img](https://javaguide.cn/assets/0e906b95-c916-4f30-beda-9cb3e49746bf.3a363d10.png)\n\n在当前执行select语句时生成一个Read View，此时m_ids：[101,102]，m_low_limit_id为：104，m_up_limit_id为：101，m_creator_trx_id为：103\n\n此时和RC级别下一样：\n\n- 最新记录的DB_TRX_ID为101，m_up_limit_id<=101< m_low_limit_id，所以要在m_ids列表中查找，发现DB_TRX_ID存在列表中，那么这个记录不可见\n- 根据DB_ROLL_PTR找到undolog中的上一版本记录，上一条记录的DB_TRX_ID还是101，不可见\n- 继续找上一条DB_TRX_ID为1，满足1< m_up_limit_id，可见，所以事务103查询到数据为name=菜花\n\n**2.时间点T6情况下**：\n\n![img](https://javaguide.cn/assets/79ed6142-7664-4e0b-9023-cf546586aa39.9c5cd303.png)\n\n在RR级别下只会生成一次Read View，所以此时依然沿用m_ids：[101,102]，m_low_limit_id为：104，m_up_limit_id为：101，m_creator_trx_id为：103\n\n- 最新记录的DB_TRX_ID为102，m_up_limit_id<=102< m_low_limit_id，所以要在m_ids列表中查找，发现DB_TRX_ID存在列表中，那么这个记录不可见\n- 根据DB_ROLL_PTR找到undo log中的上一版本记录，上一条记录的DB_TRX_ID为101，不可见\n- 继续根据DB_ROLL_PTR找到undo log中的上一版本记录，上一条记录的DB_TRX_ID还是101，不可见\n- 继续找上一条DB_TRX_ID为1，满足1< m_up_limit_id，可见，所以事务103查询到数据为name=菜花\n\n**3.时间点T9情况下**：\n\n![img](https://javaguide.cn/assets/cbbedbc5-0e3c-4711-aafd-7f3d68a4ed4e.7b4a86c0.png)\n\n此时情况跟T6完全一样，由于已经生成了Read View，此时依然沿用m_ids：[101,102]，所以查询结果依然是name=菜花\n\n### MVCC➕Next-key-Lock防止幻读\n\nInnoDB存储引擎在RR级别下通过MVCC和Next-keyLock来解决幻读问题：\n\n**1、执行普通select，此时会以MVCC快照读的方式读取数据**\n\n在快照读的情况下，RR隔离级别只会在事务开启后的第一次查询生成Read View，并使用至事务提交。所以在生成Read View之后其它事务所做的更新、插入记录版本对当前事务并不可见，实现了可重复读和防止快照读下的“幻读”\n\n**2、执行select...for update/lock in share mode、insert、update、delete等当前读**\n\n在当前读下，读取的都是最新的数据，如果其它事务有插入新的记录，并且刚好在当前事务查询范围内，就会产生幻读！InnoDB使用[Next-keyLock](https://dev.mysql.com/doc/refman/5.7/en/innodb-locking.html#innodb-next-key-locks)来防止这种情况。当执行当前读时，会锁定读取到的记录的同时，锁定它们的间隙，防止其它事务在查询范围内插入数据。只要我不让你插入，就不会发生幻读\n\n> [原文链接](https://javaguide.cn/database/mysql/innodb-implementation-of-mvcc.html)\n\n## SQL语句在MySQL中的执行过程\n\n### MySQL基础架构分析\n\n#### MySQL基本架构概览\n\n下图是MySQL的一个简要架构图，从下图你可以很清晰的看到用户的SQL语句在MySQL内部是如何执行的。\n\n先简单介绍一下下图涉及的一些组件的基本作用帮助大家理解这幅图，在下节中会详细介绍到这些组件的作用。\n\n- **连接器**:身份认证和权限相关(登录MySQL的时候)。\n- **查询缓存**:执行查询语句的时候，会先查询缓存（MySQL8.0版本后移除，因为这个功能不太实用）。\n- **分析器**:没有命中缓存的话，SQL语句就会经过分析器，分析器说白了就是要先看你的SQL语句要干嘛，再检查你的SQL语句语法是否正确。\n- **优化器**:按照MySQL认为最优的方案去执行。\n- **执行器**:执行语句，然后从存储引擎返回数据。\n\n![img](https://oss.javaguide.cn/javaguide/13526879-3037b144ed09eb88.png)\n\n简单来说MySQL主要分为Server层和存储引擎层：\n\n- **Server层**：主要包括连接器、查询缓存、分析器、优化器、执行器等，所有跨存储引擎的功能都在这一层实现，比如存储过程、触发器、视图，函数等，还有一个通用的日志模块bin log日志模块。\n- **存储引擎**：主要负责数据的存储和读取，采用可以替换的插件式架构，支持InnoDB、MyISAM、Memory等多个存储引擎，其中InnoDB引擎有自有的日志模块redo log模块。现在最常用的存储引擎是InnoDB，它从MySQL5.5版本开始就被当做默认存储引擎了。\n\n#### Server层基本组件介绍\n\n##### 连接器\n\n连接器主要和身份认证和权限相关的功能相关，就好比一个级别很高的门卫一样。\n\n主要负责用户登录数据库，进行用户的身份认证，包括校验账户密码，权限等操作，如果用户账户密码已通过，连接器会到权限表中查询该用户的所有权限，之后在这个连接里的权限逻辑判断都是会依赖此时读取到的权限数据，也就是说，后续只要这个连接不断开，即使管理员修改了该用户的权限，该用户也是不受影响的。\n\n##### 查询缓存(MySQL8.0版本后移除)\n\n查询缓存主要用来缓存我们所执行的SELECT语句以及该语句的结果集。\n\n连接建立后，执行查询语句的时候，会先查询缓存，MySQL会先校验这个SQL是否执行过，以Key-Value的形式缓存在内存中，Key是查询预计，Value是结果集。如果缓存key被命中，就会直接返回给客户端，如果没有命中，就会执行后续的操作，完成后也会把结果缓存起来，方便下一次调用。当然在真正执行缓存查询的时候还是会校验用户的权限，是否有该表的查询条件。\n\nMySQL查询不建议使用缓存，因为查询缓存失效在实际业务场景中可能会非常频繁，假如你对一个表更新的话，这个表上的所有的查询缓存都会被清空。对于不经常更新的数据来说，使用缓存还是可以的。\n\n所以，一般在大多数情况下我们都是不推荐去使用查询缓存的。\n\nMySQL8.0版本后删除了缓存的功能，官方也是认为该功能在实际的应用场景比较少，所以干脆直接删掉了。\n\n##### 分析器\n\nMySQL没有命中缓存，那么就会进入分析器，分析器主要是用来分析SQL语句是来干嘛的，分析器也会分为几步：\n\n**第一步，词法分析**，一条SQL语句有多个字符串组成，首先要提取关键字，比如select，提出查询的表，提出字段名，提出查询条件等等。做完这些操作后，就会进入第二步。\n\n**第二步，语法分析**，主要就是判断你输入的SQL是否正确，是否符合MySQL的语法。\n\n完成这2步之后，MySQL就准备开始执行了，但是如何执行，怎么执行是最好的结果呢？这个时候就需要优化器上场了。\n\n##### 优化器\n\n优化器的作用就是它认为的最优的执行方案去执行（有时候可能也不是最优，这篇文章涉及对这部分知识的深入讲解），比如多个索引的时候该如何选择索引，多表查询的时候如何选择关联顺序等。\n\n可以说，经过了优化器之后可以说这个语句具体该如何执行就已经定下来。\n\n##### 执行器\n\n当选择了执行方案后，MySQL就准备开始执行了，首先执行前会校验该用户有没有权限，如果没有权限，就会返回错误信息，如果有权限，就会去调用引擎的接口，返回接口执行的结果。\n\n### 二语句分析\n\n#### 查询语句\n\n说了以上这么多，那么究竟一条SQL语句是如何执行的呢？其实我们的SQL可以分为两种，一种是查询，一种是更新（增加，修改，删除）。我们先分析下查询语句，语句如下：\n\n\n```sql\nselect * from tb_student A where A.age='18' and A.name='张三';\n```\n\n结合上面的说明，我们分析下这个语句的执行流程：\n\n- 先检查该语句是否有权限，如果没有权限，直接返回错误信息，如果有权限，在MySQL8.0版本以前，会先查询缓存，以这条SQL语句为key在内存中查询是否有结果，如果有直接缓存，如果没有，执行下一步。\n\n- 通过分析器进行词法分析，提取SQL语句的关键元素，比如提取上面这个语句是查询select，提取需要查询的表名为tb_student，需要查询所有的列，查询条件是这个表的id='1'。然后判断这个SQL语句是否有语法错误，比如关键词是否正确等等，如果检查没问题就执行下一步。\n\n- 接下来就是优化器进行确定执行方案，上面的SQL语句，可以有两种执行方案：\n\n```\na. 先查询学生表中姓名为“张三”的学生，然后判断是否年龄是18。\nb. 先找出学生中年龄18岁的学生，然后再查询姓名为“张三”的学生。\n```\n\n那么优化器根据自己的优化算法进行选择执行效率最好的一个方案（优化器认为，有时候不一定最好）。那么确认了执行计划后就准备开始执行了。\n\n- 进行权限校验，如果没有权限就会返回错误信息，如果有权限就会调用数据库引擎接口，返回引擎的执行结果。\n\n#### 更新语句\n\n以上就是一条查询SQL的执行流程，那么接下来我们看看一条更新语句如何执行的呢？SQL语句如下：\n\n```sql\nupdate tb_student A set A.age='19' where A.name='张三';\n```\n\n我们来给张三修改下年龄，在实际数据库肯定不会设置年龄这个字段的，不然要被技术负责人打的。其实这条语句也基本上会沿着上一个查询的流程走，只不过执行更新的时候肯定要记录日志啦，这就会引入日志模块了，MySQL自带的日志模块是**bin log（归档日志）**，所有的存储引擎都可以使用，我们常用的InnoDB引擎还自带了一个日志模块**redo log（重做日志）**，我们就以InnoDB模式下来探讨这个语句的执行流程。流程如下：\n\n- 先查询到张三这一条数据，如果有缓存，也是会用到缓存。\n- 然后拿到查询的语句，把age改为19，然后调用引擎API接口，写入这一行数据，InnoDB引擎把数据保存在内存中，同时记录redo log，此时redo log进入prepare状态，然后告诉执行器，执行完成了，随时可以提交。\n- 执行器收到通知后记录bin log，然后调用引擎接口，提交redo log为提交状态。\n- 更新完成。\n\n**这里肯定有同学会问，为什么要用两个日志模块，用一个日志模块不行吗**？\n\n这是因为最开始MySQL并没有InnoDB引擎（InnoDB引擎是其他公司以插件形式插入MySQL的），MySQL自带的引擎是MyISAM，但是我们知道redo log是InnoDB引擎特有的，其他存储引擎都没有，这就导致会没有crash-safe的能力(crash-safe的能力即使数据库发生异常重启，之前提交的记录都不会丢失)，bin log日志只能用来归档。\n\n并不是说只用一个日志模块不可以，只是InnoDB引擎就是通过redo log来支持事务的。那么，又会有同学问，我用两个日志模块，但是不要这么复杂行不行，为什么redo log要引入prepare预提交状态？这里我们用反证法来说明下为什么要这么做？\n\n- **先写redo log直接提交，然后写bin log**，假设写完redo log后，机器挂了，bin log日志没有被写入，那么机器重启后，这台机器会通过redo log恢复数据，但是这个时候bin log并没有记录该数据，后续进行机器备份的时候，就会丢失这一条数据，同时主从同步也会丢失这一条数据。\n- **先写binlog，然后写redo log**，假设写完了bin log，机器异常重启了，由于没有redo log，本机是无法恢复这一条记录的，但是bin log又有记录，那么和上面同样的道理，就会产生数据不一致的情况。\n\n如果采用redo log两阶段提交的方式就不一样了，写完bin log后，然后再提交redo log就会防止出现上述的问题，从而保证了数据的一致性。那么问题来了，有没有一个极端的情况呢？假设redo log处于预提交状态，bin log也已经写完了，这个时候发生了异常重启会怎么样呢？这个就要依赖于MySQL的处理机制了，MySQL的处理过程如下：\n\n- 判断redo log是否完整，如果判断是完整的，就立即提交。\n- 如果redo log只是预提交但不是commit状态，这个时候就会去判断bin log是否完整，如果完整就提交redo log,不完整就回滚事务。\n\n这样就解决了数据一致性的问题。\n\n### 总结\n\n- MySQL主要分为Server层和引擎层，Server层主要包括连接器、查询缓存、分析器、优化器、执行器，同时还有一个日志模块（bin log），这个日志模块所有执行引擎都可以共用，redo log只有InnoDB有。\n- 引擎层是插件式的，目前主要包括，MyISAM,InnoDB,Memory等。\n- 查询语句的执行流程如下：权限校验（如果命中缓存）--->查询缓存--->分析器--->优化器--->权限校验--->执行器--->引擎\n- 更新语句执行流程如下：分析器---->权限校验---->执行器--->引擎---redo log(prepare状态)--->bin log--->redo log(commit状态)\n\n> [原文链接](https://javaguide.cn/database/mysql/how-sql-executed-in-mysql.html)\n\n### sql执行顺序\n```\nfrom、on、join、where、group by、(avg,sum)、having、select、distinct、union、order by、limit\n```\n### 相关文章\n\n- [面试官：你来讲讲一条查询语句的具体执行过程](https://mp.weixin.qq.com/s?__biz=MzkzODE3OTI0Ng==&mid=2247490897&idx=1&sn=e5eaa58d426255c2c1401578d5606138&source=41#wechat_redirect)\n- [一文读懂MySQL查询语句的执行过程](https://mp.weixin.qq.com/s/dLJet2geb-dQ9hLE0oL6Qw)\n- [MySQL的执行过程及执行顺序](https://mp.weixin.qq.com/s/DS4-ObUFeGsU2l6FvF0Pcw)\n- [图解SQL执行顺序，通俗易懂！](https://mp.weixin.qq.com/s/k1Zjr6ucFZzI-w6wC2Hz3Q)\n\n## MySQL查询缓存详解\n\n缓存是一个有效且实用的系统性能优化的手段，不论是操作系统还是各种软件和网站或多或少都用到了缓存。然而，有经验的DBA都建议生产环境中把MySQL自带的QueryCache（查询缓存）给关掉。而且，从MySQL5.7.20开始，就已经默认弃用查询缓存了。在MySQL8.0及之后，更是直接删除了查询缓存的功能。这又是为什么呢？查询缓存真就这么鸡肋么?带着如下几个问题，我们正式进入本文。\n\n- MySQL查询缓存是什么？适用范围？\n- MySQL缓存规则是什么？\n- MySQL缓存的优缺点是什么？\n- MySQL缓存对性能有什么影响？\n\n### MySQL查询缓存介绍\n\nMySQL体系架构如下图所示：\n\n![img](https://oss.javaguide.cn/github/javaguide/mysql/mysql-architecture.png)\n\n为了提高完全相同的查询语句的响应速度，MySQL Server会对查询语句进行Hash计算得到一个Hash值。MySQL Server不会对SQL做任何处理，SQL必须完全一致Hash值才会一样。得到Hash值之后，通过该Hash值到查询缓存中匹配该查询的结果。\n\n- 如果匹配（命中），则将查询的结果集直接返回给客户端，不必再解析、执行查询。\n- 如果没有匹配（未命中），则将Hash值和结果集保存在查询缓存中，以便以后使用。\n\n也就是说，**一个查询语句（select）到了MySQLServer之后，会先到查询缓存看看，如果曾经执行过的话，就直接返回结果集给客户端**。\n\n![img](https://oss.javaguide.cn/javaguide/13526879-3037b144ed09eb88.png)\n\n### MySQL查询缓存管理和配置\n\n通过`showvariableslike'%query_cache%'`命令可以查看查询缓存相关的信息。\n\n8.0版本之前的话，打印的信息可能是下面这样的：\n\n```bash\nmysql> show variables like '%query_cache%';\n+------------------------------+---------+\n| Variable_name                | Value   |\n+------------------------------+---------+\n| have_query_cache             | YES     |\n| query_cache_limit            | 1048576 |\n| query_cache_min_res_unit     | 4096    |\n| query_cache_size             | 599040  |\n| query_cache_type             | ON      |\n| query_cache_wlock_invalidate | OFF     |\n+------------------------------+---------+\n6 rows in set (0.02 sec)\n```\n\n8.0以及之后版本之后，打印的信息是下面这样的：\n\n\n```bash\nmysql> show variables like '%query_cache%';\n+------------------+-------+\n| Variable_name    | Value |\n+------------------+-------+\n| have_query_cache | NO    |\n+------------------+-------+\n1 row in set (0.01 sec)\n```\n\n我们这里对8.0版本之前show variables like '%query_cache%';命令打印出来的信息进行解释。\n\n- **have_query_cache**:该MySQL Server是否支持查询缓存，如果是YES表示支持，否则则是不支持。\n- **query_cache_limit**:MySQL查询缓存的最大查询结果，查询结果大于该值时不会被缓存。\n- **query_cache_min_res_unit**:查询缓存分配的最小块的大小(字节)。当查询进行的时候，MySQL把查询结果保存在查询缓存中，但如果要保存的结果比较大，超过query_cache_min_res_unit的值，这时候MySQL将一边检索结果，一边进行保存结果，也就是说，有可能在一次查询中，MySQL要进行多次内存分配的操作。适当的调节query_cache_min_res_unit可以优化内存。\n- **query_cache_size**:为缓存查询结果分配的内存的数量，单位是字节，且数值必须是1024的整数倍。默认值是0，即禁用查询缓存。\n- **query_cache_type**:设置查询缓存类型，默认为ON。设置GLOBAL值可以设置后面的所有客户端连接的类型。客户端可以设置SESSION值以影响他们自己对查询缓存的使用。\n- **query_cache_wlock_invalidate**:如果某个表被锁住，是否返回缓存中的数据，默认关闭，也是建议的。\n\nquery_cache_type可能的值(修改query_cache_type需要重启MySQLServer)：\n\n- 0或OFF：关闭查询功能。\n- 1或ON：开启查询缓存功能，但不缓存SelectSQL_NO_CACHE开头的查询。\n- 2或DEMAND：开启查询缓存功能，但仅缓存SelectSQL_CACHE开头的查询。\n\n**建议**：\n\n- query_cache_size不建议设置的过大。过大的空间不但挤占实例其他内存结构的空间，而且会增加在缓存中搜索的开销。建议根据实例规格，初始值设置为10MB到100MB之间的值，而后根据运行使用情况调整。\n- 建议通过调整query_cache_size的值来开启、关闭查询缓存，因为修改query_cache_type参数需要重启MySQL Server生效。\n\n8.0版本之前，my.cnf加入以下配置，重启MySQL开启查询缓存\n\n```properties\nquery_cache_type=1\nquery_cache_size=600000\n```\n\n或者，MySQL执行以下命令也可以开启查询缓存\n\n```properties\nset global query_cache_type=1;\nset global query_cache_size=600000;\n```\n\n手动清理缓存可以使用下面三个SQL：\n\n- flush query cache;：清理查询缓存内存碎片。\n- reset query cache;：从查询缓存中移除所有查询。\n- flush tables；关闭所有打开的表，同时该操作会清空查询缓存中的内容。\n\n### MySQL缓存机制\n\n#### 缓存规则\n\n- 查询缓存会将查询语句和结果集保存到内存（一般是key-value的形式，key是查询语句，value是查询的结果集），下次再查直接从内存中取。\n- 缓存的结果是通过sessions共享的，所以一个client查询的缓存结果，另一个client也可以使用。\n- SQL必须完全一致才会导致查询缓存命中（大小写、空格、使用的数据库、协议版本、字符集等必须一致）。检查查询缓存时，MySQL Server不会对SQL做任何处理，它精确的使用客户端传来的查询。\n- 不缓存查询中的子查询结果集，仅缓存查询最终结果集。\n- 不确定的函数将永远不会被缓存,比如now()、curdate()、last_insert_id()、rand()等。\n- 不缓存产生告警（Warnings）的查询。\n- 太大的结果集不会被缓存(< query_cache_limit)。\n- 如果查询中包含任何用户自定义函数、存储函数、用户变量、临时表、MySQL库中的系统表，其查询结果也不会被缓存。\n- 缓存建立之后，MySQL的查询缓存系统会跟踪查询中涉及的每张表，如果这些表（数据或结构）发生变化，那么和这张表相关的所有缓存数据都将失效。\n- MySQL缓存在分库分表环境下是不起作用的。\n- 不缓存使用SQL_NO_CACHE的查询。\n- ......\n\n查询缓存SELECT选项示例：\n\n\n```sql\n-- 会缓存\nSELECT SQL_CACHE id, name FROM customer;\n-- 不会缓存\nSELECT SQL_NO_CACHE id, name FROM customer;\n```\n\n#### 缓存机制中的内存管理\n\n查询缓存是完全存储在内存中的，所以在配置和使用它之前，我们需要先了解它是如何使用内存的。MySQL查询缓存使用内存池技术，自己管理内存释放和分配，而不是通过操作系统。内存池使用的基本单位是变长的block,用来存储类型、大小、数据等信息。一个结果集的缓存通过链表把这些block串起来。block最短长度为`query_cache_min_res_unit`。当服务器启动的时候，会初始化缓存需要的内存，是一个完整的空闲块。当查询结果需要缓存的时候，先从空闲块中申请一个数据块为参数`query_cache_min_res_unit`配置的空间，即使缓存数据很小，申请数据块也是这个，因为查询开始返回结果的时候就分配空间，此时无法预知结果多大。分配内存块需要先锁住空间块，所以操作很慢，MySQL会尽量避免这个操作，选择尽可能小的内存块，如果不够，继续申请，如果存储完时有空余则释放多余的。但是如果并发的操作，余下的需要回收的空间很小，小于`query_cache_min_res_unit`，不能再次被使用，就会产生碎片。\n\n### MySQL查询缓存的优缺点\n\n**优点**\n\n- 查询缓存的查询，发生在MySQL接收到客户端的查询请求、查询权限验证之后和查询SQL解析之前。也就是说，当MySQL接收到客户端的查询SQL之后，仅仅只需要对其进行相应的权限验证之后，就会通过查询缓存来查找结果，甚至都不需要经过Optimizer模块进行执行计划的分析优化，更不需要发生任何存储引擎的交互。\n- 由于查询缓存是基于内存的，直接从内存中返回相应的查询结果，因此减少了大量的磁盘I/O和CPU计算，导致效率非常高。\n\n**缺点**\n\n- MySQL会对每条接收到的SELECT类型的查询进行Hash计算，然后查找这个查询的缓存结果是否存在。虽然Hash计算和查找的效率已经足够高了，一条查询语句所带来的开销可以忽略，但一旦涉及到高并发，有成千上万条查询语句时，hash计算和查找所带来的开销就必须重视了。\n- 查询缓存的失效问题。如果表的变更比较频繁，则会造成查询缓存的失效率非常高。表的变更不仅仅指表中的数据发生变化，还包括表结构或者索引的任何变化。\n- 查询语句不同，但查询结果相同的查询都会被缓存，这样便会造成内存资源的过度消耗。查询语句的字符大小写、空格或者注释的不同，查询缓存都会认为是不同的查询（因为他们的Hash值会不同）。\n- 相关系统变量设置不合理会造成大量的内存碎片，这样便会导致查询缓存频繁清理内存。\n\n### MySQL查询缓存对性能的影响\n\n在MySQL Server中打开查询缓存对数据库的读和写都会带来额外的消耗:\n\n- 读查询开始之前必须检查是否命中缓存。\n- 如果读查询可以缓存，那么执行完查询操作后，会查询结果和查询语句写入缓存。\n- 当向某个表写入数据的时候，必须将这个表所有的缓存设置为失效，如果缓存空间很大，则消耗也会很大，可能使系统僵死一段时间，因为这个操作是靠全局锁操作来保护的。\n- 对InnoDB表，当修改一个表时，设置了缓存失效，但是多版本特性会暂时将这修改对其他事务屏蔽，在这个事务提交之前，所有查询都无法使用缓存，直到这个事务被提交，所以长时间的事务，会大大降低查询缓存的命中。\n\n### 总结\n\nMySQL中的查询缓存虽然能够提升数据库的查询性能，但是查询同时也带来了额外的开销，每次查询后都要做一次缓存操作，失效后还要销毁。查询缓存是一个适用较少情况的缓存机制。如果你的应用对数据库的更新很少，那么查询缓存将会作用显著。比较典型的如博客系统，一般博客更新相对较慢，数据表相对稳定不变，这时候查询缓存的作用会比较明显。简单总结一下查询缓存的适用场景：\n\n- 表数据修改不频繁、数据较静态。\n- 查询（Select）重复度高。\n- 查询结果集小于1MB。\n\n对于一个更新频繁的系统来说，查询缓存缓存的作用是很微小的，在某些情况下开启查询缓存会带来性能的下降。简单总结一下查询缓存不适用的场景：\n\n- 表中的数据、表结构或者索引变动频繁\n- 重复的查询很少\n- 查询的结果集很大\n\n《高性能MySQL》这样写到：\n\n> 根据我们的经验，在高并发压力环境中查询缓存会导致系统性能的下降，甚至僵死。如果你一定要使用查询缓存，那么不要设置太大内存，而且只有在明确收益的时候才使用（数据库内容修改次数较少）。\n\n确实是这样的！实际项目中，更建议使用本地缓存（比如Caffeine）或者分布式缓存（比如Redis），性能更好，更通用一些\n> [原文链接](https://javaguide.cn/database/mysql/mysql-query-cache.html)\n\n## Explain执行计划\n\n### 什么是执行计划？\n\n**执行计划**是指一条SQL语句在经过**MySQL查询优化器**的优化会后，具体的执行方式。执行计划通常用于SQL性能分析、优化等场景。通过EXPLAIN的结果，可以了解到如数据表的查询顺序、数据查询操作的操作类型、哪些索引可以被命中、哪些索引实际会命中、每个数据表有多少行记录被查询等信息。\n\n### 如何获取执行计划？\n\nMySQL为我们提供了EXPLAIN命令，来获取执行计划的相关信息。需要注意的是，EXPLAIN语句并不会真的去执行相关的语句，而是通过查询优化器对语句进行分析，找出最优的查询方案，并显示对应的信息。EXPLAIN执行计划支持SELECT、DELETE、INSERT、REPLACE以及UPDATE语句。我们一般多用于分析SELECT查询语句，使用起来非常简单，语法如下：\n\n\n```sql\nEXPLAIN + SELECT 查询语句；\n```\n\n我们简单来看下一条查询语句的执行计划：\n\n\n```sql\nmysql> explain SELECT * FROM dept_emp WHERE emp_no IN (SELECT emp_no FROM dept_emp GROUP BY emp_no HAVING COUNT(emp_no)>1);\n+----+-------------+----------+------------+-------+-----------------+---------+---------+------+--------+----------+-------------+\n| id | select_type | table    | partitions | type  | possible_keys   | key     | key_len | ref  | rows   | filtered | Extra       |\n+----+-------------+----------+------------+-------+-----------------+---------+---------+------+--------+----------+-------------+\n|  1 | PRIMARY     | dept_emp | NULL       | ALL   | NULL            | NULL    | NULL    | NULL | 331143 |   100.00 | Using where |\n|  2 | SUBQUERY    | dept_emp | NULL       | index | PRIMARY,dept_no | PRIMARY | 16      | NULL | 331143 |   100.00 | Using index |\n+----+-------------+----------+------------+-------+-----------------+---------+---------+------+--------+----------+-------------+\n```\n\n可以看到，执行计划结果中共有12列，各列代表的含义总结如下表：\n\n| **列名**      | **含义**                                     |\n| ------------- | -------------------------------------------- |\n| id            | SELECT查询的序列标识符                       |\n| select_type   | SELECT关键字对应的查询类型                   |\n| table         | 用到的表名                                   |\n| partitions    | 匹配的分区，对于未分区的表，值为 NULL        |\n| type          | 表的访问方法                                 |\n| possible_keys | 可能用到的索引                               |\n| key           | 实际用到的索引                               |\n| key_len       | 所选索引的长度                               |\n| ref           | 当使用索引等值查询时，与索引作比较的列或常量 |\n| rows          | 预计要读取的行数                             |\n| filtered      | 按表条件过滤后，留存的记录数的百分比         |\n| Extra         | 附加信息                                     |\n\n#### 如何分析EXPLAIN结果？\n\n为了分析EXPLAIN语句的执行结果，我们需要搞懂执行计划中的重要字段。\n\n#### id\n\nSELECT标识符，是查询中SELECT的序号，用来标识整个查询中SELELCT语句的顺序。\n\nid如果相同，从上往下依次执行。id不同，id值越大，执行优先级越高，如果行引用其他行的并集结果，则该值可以为NULL。\n\n#### select_type\n\n查询的类型，主要用于区分普通查询、联合查询、子查询等复杂的查询，常见的值有：\n\n- **SIMPLE**：简单查询，不包含UNION或者子查询。\n- **PRIMARY**：查询中如果包含子查询或其他部分，外层的SELECT将被标记为PRIMARY。\n- **SUBQUERY**：子查询中的第一个SELECT。\n- **UNION**：在UNION语句中，UNION之后出现的SELECT。\n- **DERIVED**：在FROM中出现的子查询将被标记为DERIVED。\n- **UNIONRESULT**：UNION查询的结果。\n\n#### table\n\n查询用到的表名，每行都有对应的表名，表名除了正常的表之外，也可能是以下列出的值：\n\n- **<unionM,N>**:本行引用了id为M和N的行的UNION结果；\n- **< derivedN>**:本行引用了id为N的表所产生的的派生表结果。派生表有可能产生自FROM语句中的子查询。\n- **< subqueryN>**:本行引用了id为N的表所产生的的物化子查询结果。\n\n#### type（重要）\n\n查询执行的类型，描述了查询是如何执行的。所有值的顺序从最优到最差排序为：system>const>eq_ref>ref>fulltext>ref_or_null>index_merge>unique_subquery>index_subquery>range>index>ALL\n\n常见的几种类型具体含义如下：\n\n- **system**：如果表使用的引擎对于表行数统计是精确的（如：MyISAM），且表中只有一行记录的情况下，访问方法是system，是const的一种特例。\n- **const**：表中最多只有一行匹配的记录，一次查询就可以找到，常用于使用主键或唯一索引的所有字段作为查询条件。\n- **eq_ref**：当连表查询时，前一张表的行在当前这张表中只有一行与之对应。是除了system与const之外最好的join方式，常用于使用主键或唯一索引的所有字段作为连表条件。\n- **ref**：使用普通索引作为查询条件，查询结果可能找到多个符合条件的行。\n- **index_merge**：当查询条件使用了多个索引时，表示开启了Index Merge优化，此时执行计划中的key列列出了使用到的索引。\n- **range**：对索引列进行范围查询，执行计划中的key列表示哪个索引被使用了。\n- **index**：查询遍历了整棵索引树，与ALL类似，只不过扫描的是索引，而索引一般在内存中，速度更快。\n- **ALL**：全表扫描。\n\n#### possible_keys\n\npossible_keys列表示MySQL执行查询时可能用到的索引。如果这一列为NULL，则表示没有可能用到的索引；这种情况下，需要检查WHERE语句中所使用的的列，看是否可以通过给这些列中某个或多个添加索引的方法来提高查询性能。\n\n#### key（重要）\n\nkey列表示MySQL实际使用到的索引。如果为NULL，则表示未用到索引。\n\n#### key_len\n\nkey_len列表示MySQL实际使用的索引的最大长度；当使用到联合索引时，有可能是多个列的长度和。在满足需求的前提下越短越好。如果key列显示NULL，则key_len列也显示NULL。\n\n#### rows\n\nrows列表示根据表统计信息及选用情况，大致估算出找到所需的记录或所需读取的行数，数值越小越好。\n\n#### Extra（重要）\n\n这列包含了MySQL解析查询的额外信息，通过这些信息，可以更准确的理解MySQL到底是如何执行查询的。常见的值如下：\n\n- **Using filesort**：在排序时使用了外部的索引排序，没有用到表内索引进行排序。\n- **Using temporary**：MySQL需要创建临时表来存储查询的结果，常见于ORDERBY和GROUPBY。\n- **Using index**：表明查询使用了覆盖索引，不用回表，查询效率非常高。\n- **Using index condition**：表示查询优化器选择使用了索引条件下推这个特性。\n- **Using where**：表明查询使用了WHERE子句进行条件过滤。一般在没有使用到索引的时候会出现。\n- **Using join buffer（Block Nested Loop）**：连表查询的方式，表示当被驱动表的没有使用索引的时候，MySQL会先将驱动表读出来放到joinbuffer中，再遍历被驱动表与驱动表进行查询。\n\n这里提醒下，当Extra列包含Using file sort或Using temporary时，MySQL的性能可能会存在问题，需要尽可能避免\n\n> [原文链接](https://javaguide.cn/database/mysql/mysql-query-execution-plan.html)\n\n### 总结\n\n1. id: 是用来顺序标识整个查询中select语句的，在嵌套查询中id越大的语句越先执\n2. select_type:查询的类型\n- simple: 简单的SELECT（不使用UNION或子查询）\n- primary: 最外面的SELECT\n- union: UNION中的第二个或更高的SELECT语句\n- dependent union: UNION中的第二个或更高的SELECT语句，取决于外部查询\n- union result: UNION的结果\n- subquery: 在子查询中首先选择SELECT\n- dependent subquery: 子查询中的第一个SELECT，取决于外部查询\n- derived: 派生表——该临时表是从子查询派生出来的，位于from中的子查询\n- uncacheable subquery: 无法缓存结果的子查询，必须为外部查询的每一行重新计算\n- uncacheable union: 在UNION中的第二个或更晚的选择属于不可缓存的子查询\n3. table:当前行所查询的表\n4. type:访问类型，表示数据库引擎查找表的方式。常见的type类型有：all,index,range,ref,eq_ref,const。\n- all :全表扫描，表示sql语句会把表中所有表数据全部读取读取扫描一遍。效率最低，我们应尽量避免\n- index :全索引扫描,表示sql语句将会把整颗二级索引树全部读取扫描一遍，因为二级索引树的数据量比全表数据量小，所以效率比all高一些。一般查询语句中查询字段为索引字段，且无where子句时，type会为index,类似全表扫描，只是扫描表的时候按照索引次序进行而不是行。主要优点就是避免了排序,但是开销仍然非常大。如在Extra列看到Using index，说明正在使用覆盖索引，只扫描索引的数据，它比按索引次序全表扫描的开销要小很多\n- range :部分索引扫描，当查询为区间查询，且查询字段为索引字段时，这时会根据where条件对索引进行部分扫描,范围扫描，一个有限制的索引扫描。key列显示使用了哪个索引。当使用=、<>、>、>=、<、<=、IS NULL、<=>、BETWEEN或者IN操作符，用常量比较关键字列时,可以使用range\n- unique_subquery: 在使用in查询的情况下会取代eq_ref\n- index_merge: 此连接类型表示使用了索引合并优化。在这种情况下，输出行中的key列包含使用的索引列表，key_len包含所用索引的最长key部分列表\n- ref :出现于where操作符为‘=’，且where字段为非唯一索引的单表查询或联表查询。通过普通索引查询匹配的很多行时的类型\n- ref_or_null: 跟ref类似的效果，不过多一个列不能null的条件\n- eq_ref :出现于where操作符为‘=’，且where字段为唯一索引的联表查询。最多只返回一条符合条件的记录，通过使用在两个表有关联字段的时候\n- const :出现于where操作符为‘=’，且where字段为唯一索引的单表查询,此时最多只会匹配到一行,当确定最多只会有一行匹配的时候，MySQL优化器会在查询前读取它而且只读取一次，因此非常快。使用主键查询往往就是const级别的，非常高效\n- system:const的一种特例，表中只有一行数据\nfulltext: 全文索引\n**单从type字段考虑效率，const > eq_ref > ref > range > index > all**\n5. possible_keys:查询可能用到的索引,MySQL 可能采用的索引，但是并不一定使用\n6. key:mysql决定采用的索引来优化查询,真正使用的索引名称\nsql语句实际执行时使用的索引列，有时候mysql可能会选择优化效果不是最好的索引，这时，我们可以在select语句中使用force index(INDEXNAME)来强制mysql使用指定索引或使用ignore index(INDEXNAME)强制mysql忽略指定索引\n7. key_len: 索引key的长度\n8. ref: 显示了之前的表在key列记录的索引中查找值所用的列或常量\n9. rows:查询扫描的行数，预估值，不一定准确\n10. Extra:额外的查询辅助信息\n- extra列会包含一些十分重要的信息，我们可以根据这些信息进行sql优化\n- using index: sql语句没有where查询条件，使用覆盖索引，不需要回表查询即可拿到结果\n- using where: 没有使用索引/使用了索引但需要回表查询且没有使用到下推索引\n- using index && useing where: sql语句有where查询条件，且使用覆盖索引，不需要回表查询即可拿到结果。\n- Using index condition:使用索引查询，sql语句的where子句查询条件字段均为同一索引字段，且开启索引下推功能，需要回表查询即可拿到结果\n- Using index condition && using where:使用索引查询，sql语句的where子句查询条件字段存在非同一索引字段，且开启索引下推功能，需要回表查询即可拿到结果。\n- using filesort: 当语句中存在order by时，且orderby字段不是索引，这个时候mysql无法利用索引进行排序，只能用排序算法重新进行排序，会额外消耗资源。\n- Using temporary：建立了临时表来保存中间结果，查询完成之后又要把临时表删除。会很影响性能，需尽快优化\n11. filtered ：查询的表行占表的百分比\n12. partitions：匹配的分区\n\n### 相关文章\n\n- [explain都不懂，还好意思说会SQL调优?](https://mp.weixin.qq.com/s/aTkGwVU5D9u1RAbMHQconw)\n- [MySQL之Explain输出分析](https://mp.weixin.qq.com/s/NwRVW6Z8AMZ_QClTXH71ow)\n- [什么是MySQL的执行计划（Explain关键字）？](https://mp.weixin.qq.com/s/E8wJQvldwEAzxK5mEuFhog)\n- [MySQL究竟是怎么执行的(explain)](https://mp.weixin.qq.com/s/kYcrHtE82-sOqNOp_qM4Ig)\n- [EXPLAIN进行索引分析和优化](https://mp.weixin.qq.com/s/-YrZFxTbutLdEbkfa9aOKQ)\n\n\n## 相关文章\n\n### InnoDB\n\n- [InnoDB自增原理都搞不清楚，还怎么CRUD？](https://mp.weixin.qq.com/s/FHoOdlWyefQLJ34MkQgFvA)\n- [InnoDB原理篇：聊聊数据页变成索引这件事](https://mp.weixin.qq.com/s/V0BX5tRlrrZhfe2-ui2aRA)\n- [两万字详解！InnoDB锁专题！](https://mp.weixin.qq.com/s/Io64o3tbEcf150dS5_ltbQ)\n- [MySQL存储引擎InnoDB详解](https://mp.weixin.qq.com/s/6BoGlaYpdDjzZy19YhInEw)\n- [innodb是如何存数据的](https://mp.weixin.qq.com/s/sr5tQF7lNjTcvwg7Fr6HjQ)\n- [MySQL是如何查询数据的](https://mp.weixin.qq.com/s/ymWeGlaBYWYmfogVDFHo5w)\n- [MySQL是如何实现ACID的?](https://mp.weixin.qq.com/s/KbOiJ8SKJ_wFZcIyDVGD9g)\n- [一文讲清，MySQL数据库一行数据在磁盘上是怎么存储的？](https://mp.weixin.qq.com/s/qJINIZ3QcXODHSMm1RTPKQ)\n- [一文讲述MySQL所有的存储引擎](https://mp.weixin.qq.com/s/Oiefzj2b3NZxTwjnRdAFRw)\n- [MySQL的InnoDB引擎原来是这样的](https://mp.weixin.qq.com/s/s2c9L2p5kGC_InjorgDRnQ)\n- [InnoDB事务隔离级别及其实现原理](https://mp.weixin.qq.com/s/7Xx7rdl7dabbTNJOOLlFVg)\n\n### 主从复制\n\n- [MySQL的主从如何配置](https://mp.weixin.qq.com/s/OHhX9XCQJnrBYdA27ipHHg)\n- [MySQL主从复制](https://mp.weixin.qq.com/s/xES94DmApf_GGYvT1Ku5QQ)\n- [面试官：Mysql中主库跑太快，从库追不上怎么整？](https://mp.weixin.qq.com/s/2JtNBnKShIe-0pgt3nYGkg)\n- [MySQL定时备份的几种方式，这下稳了！](https://mp.weixin.qq.com/s/gUoDR4WLeHQYzyGtmR5K3w)\n- [MySQL怎么保证备份数据的一致性？](https://mp.weixin.qq.com/s/zOOBkcl8Ns7Kk-k-WaXNhw)\n- [删库不跑路！我含泪写下了MySQL数据恢复大法…](https://mp.weixin.qq.com/s/jgM0aZY7UJSA2NMnkmKO3Q)\n- [京东一面：MySQL主备延迟有哪些坑？主备切换策略](https://mp.weixin.qq.com/s/SXnwM_n8UVMkKF04cDVNtw)\n- [MySQL主从数据不一致，怎么办？](https://mp.weixin.qq.com/s/ALcivJKxtTZUHS-HTnSsXA)\n- [一文讲解MySQL的主从复制](https://mp.weixin.qq.com/s/bq-sMw6mJg4I_TCvHZ-Zcw)\n\n\n### other\n\n- [MySQL自增主键为何不是连续的呢？](https://mp.weixin.qq.com/s/zv25dNB8FXn3KrnN3uJ3uQ)\n- [线上MySQL的自增id用尽怎么办？](https://mp.weixin.qq.com/s/c6Dx1xEh2BNIuGpxQahAuw)\n- [MySQL的自增id](https://mp.weixin.qq.com/s/BHE7YTOlo6UZ2WQzRieDxA)\n- [MySQL用limit为什么会影响性能？](https://mp.weixin.qq.com/s/5o-sAOFpmjRSqIoLUNesOQ)\n- [MySQL查询limit 1000,10和limit 10速度一样快吗？深度分页如何破解](https://mp.weixin.qq.com/s/1pmVqHBS0CyFJW9ykySF_Q)\n- [MySQL体系架构简介](https://mp.weixin.qq.com/s/HdE5QiK5Qp3_sGd_cXABIg)\n- [炸裂！MySQL82张图带你飞！※](https://mp.weixin.qq.com/s/HWQbWSQE4O5ndI-NG1Hwwg)\n- [MySQL面试必会！](https://mp.weixin.qq.com/s/9Ex9tzoQCAEUZNU6iXLHgQ)\n- [MySQL都不清楚？直接挂了!](https://mp.weixin.qq.com/s/FUyRHpPgw0LFWEOPTPmT0g)\n- [最全91道MySQL面试题|附答案解析](https://mp.weixin.qq.com/s/_Gd-lBfJnhwczNQA4IJyGg)\n- [138张图带你MySQL入门](https://mp.weixin.qq.com/s/XcNZeHdaMgx35dFoB4_n4A)\n- [专治MySQL乱码，再也不想看到� �！](https://mp.weixin.qq.com/s/ar35RqaoDgasO-OY0TnDpQ)\n- [MySQL的varchar水太深了，你真的会用吗？](https://mp.weixin.qq.com/s/ImB0O-Z8IXldvt6YQk80jg)\n- [聊聊MySQL的10大经典错误](https://mp.weixin.qq.com/s/vdz0aS8qgHHHhvZWPuaE8A)\n- [MySQ8.0推出直方图，性能大大提升！](https://mp.weixin.qq.com/s/3gc8tMfnih7WQoPwTtN8IA)\n- [MySQL模糊查询再也用不着like+%了！](https://mp.weixin.qq.com/s/ZAzRtRnYGhvie8MDZvy1Gg)\n- [MySQL最大建议行数2000w,靠谱吗？](https://mp.weixin.qq.com/s/PeZ8Py3NNb4CU2BUtxpygg)\n- [图解MVCC！](https://mp.weixin.qq.com/s/1XmvVNYqt5KycmD8pJanug)\n- [再有人问你什么是MVCC，就把这篇文章发给他！](https://mp.weixin.qq.com/s/WZa5UKYgU-pYKbvovLt1YQ)\n- [MySQL最朴素的监控方式](https://mp.weixin.qq.com/s/meS5Au1o9qdrSv7505mIcA)\n- [1亿条数据批量插入MySQL，哪种方式最快](https://mp.weixin.qq.com/s/c71ATJLT6_KXtb_iiUlMjg)\n- [6种MySQL数据库平滑扩容方案剖析](https://mp.weixin.qq.com/s/LvJCi-8cF6HuLo6UXY0Ing)\n- [什么是插入意向锁？](https://mp.weixin.qq.com/s/rDdUBw803tvjHkJALY5P2w)\n- [从MySQL读取100w数据进行处理，应该怎么做](https://mp.weixin.qq.com/s/XbSADUXIz1aw0kqp7p5urQ)\n- [MySQL误删数据不用跑路了，快速恢复指南来了](https://mp.weixin.qq.com/s/vyrLOH1NRXXQd0oEnv0M1Q)\n- [MySQL单表数据最大不要超过多少行？为什么](https://mp.weixin.qq.com/s/ZCM2FYzKw24Fk8yv4a9b0w)","categories":["数据库"]},{"title":"JavaScript相关","slug":"JavaScript相关","url":"/blog/posts/f5d8fbdfd125/","content":"\n## Note\n\n### JQuery、JS常用方法\n\n```js\n// json字符串\nvar str1 = '{\"name\":\"cxh\",\"sex\":\"man\"}';\n// json对象\nvar str2 = {name:\"cxh\",sex:\"man\"};\n```\n**obj.toJSONString()** :将JSON对象转化为JSON字符。\n\n> 要引入https://github.com/douglascrockford/JSON-js/blob/master/json.js\n\n**JSON.stringify()** :将一个JavaScript值(对象或者数组)转换为一个JSON字符串,用于从一个对象解析出字符串\n\n```js\nvar a = {a:1,b:2}\nJSON.stringify(a)  // 结果：{\"a\":1,\"b\":2}\n```\n当使用ajax作为参数提交的时候需要将contentType设置为\"application/json\",将提交请求格式设置为post后台使用@RequestBody接收\n\n> [差点因为JSON.stringify丢了奖金..](https://mp.weixin.qq.com/s/JDah47ariZY3aerQ97RFKw)\n\n**JSON.parse()** :用来解析JSON字符串,转换成json对象.相似于$.parseJSON()/jQuery.parseJSON()\n\n```js\nvar str = '{\"name\":\"huangxiaojian\",\"age\":\"23\"}'\nJSON.parse(str)\n\n// {age: \"23\",name: \"huangxiaojian\",__proto__: Object}\n```\n注意：单引号写在{}外，每个属性名都必须用双引号，否则会抛出异常。\n\n**Object.assign(target1,target2,...,targetn)** ：合并对象，对象合并到第一个对象中，注意目标对象target1也会改变\n\n**join(separator)** :用于把数组中的所有元素放入一个字符串。separator可选，指定要使用的分隔符。如果省略该参数，则使用逗号作为分隔符。\n\n**split(separator,howmany)** :用于把一个字符串分割成字符串数组。separator必需。字符串或正则表达式，从该参数指定的地方分割。howmany可选。该参数可指定返回的数组的最大长度。如果设置了该参数，返回的子串不会多于这个参数指定的数组。如果没有设置该参数，整个字符串都会被分割，不考虑它的长度。\n\n**splice(index,howmany,item1,.....,itemX)** :向/从数组中删除项目，然后返回被删除的项目。index必需。整数，规定添加/删除项目的位置，使用负数可从数组结尾处规定位置。howmany必需。要删除的项目数量。如果设置为0，则不会删除项目。item1,...,itemX可选。向数组添加的新项目，在原位置添加。\n\n```js\nvar fruits = [\"Banana\", \"Orange\", \"Apple\", \"Mango\"];\nfruits.splice(2,0,\"Lemon\",\"Kiwi\");\n// fruits 输出结果： Banana,Orange,Lemon,Kiwi,Apple,Mango\n```\n**unshift(newelement1,newelement2,....,newelementX)** :可向数组的开头添加一个或更多元素，并返回新的长度。newelement1必需。向数组添加的第一个元素。newelement2可选。向数组添加的第二个元素。newelementX可选。可添加若干个元素\n\n**pop()** :删除原数组最后一项，并返回删除元素的值；如果数组为空则返回undefined\n\n**shift()** :用于把数组的第一个元素从其中删除，并返回第一个元素的值\n\n**push(newelement1,newelement2,....,newelementX)** :可向数组的末尾添加一个或多个元素，并返回新的长度。\n\n**instanceof Array  typeof()** :判断是否为数组\n\n**concat()** :返回一个新数组，是将参数添加到原数组中构成的\n\n**reverse()** :将数组反序\n\n**sort(orderfunction)** :按指定的参数对数组进行排序\n\n**slice(start,end)** :返回从原数组中指定开始下标到结束下标之间的项组成的新数组。start-必填；设定新数组的起始位置；如果是负数，则表示从数组尾部开始算起（-1指最后一个元素，-2指倒数第二个元素，以此类推）。end-可选；设定新数组的结束位置；如果不填写该参数，默认到数组结尾；如果是负数，则表示从数组尾部开始算起（-1指最后一个元素，-2指倒数第二个元素，以此类推）\n\n**siblings()** :方法返回被选元素的所有同胞元素。\n\n**prev()** :查找上一个兄弟节点，不是所有的兄弟节点\n\n**prevAll()** :查找所有之前的兄弟节点\n\n**next()** :方法返回被选元素的下一个同胞元素。\n\n**nextAll()** :方法返回被选元素的所有跟随的同胞元素。\n\n**nextUntil()** :方法返回介于两个给定参数之间的所有跟随的同胞元素\n\n**parent(expr)** :找父元素\n\n**parents(expr)** :找到所有祖先元素，不限于父元素\n\n**children(expr)** :查找所有子元素，只会找到直接的孩子节点，不会返回所有子孙\n\n**contents()** :查找下面的所有内容，包括节点和文本。\n\n**slideToggle()** :方法通过使用滑动效果（高度变化）来切换元素的可见状态。如果被选元素是可见的，则隐藏这些元素，如果被选元素是隐藏的，则显示这些元素\n\n**slideUp()** :方法用于向上滑动元素。\n\n**slideDown()** :方法用于向下滑动元素\n\n**first()** :方法返回被选元素的首个元素。\n\n**last()** :方法返回被选元素的最后一个元素。\n\n**filter()** :方法允许您规定一个标准。不匹配这个标准的元素会被从集合中删除，匹配的元素会被返回。\n\n**not()** :方法返回不匹配标准的所有元素\n\n**after()** :方法在被选元素后插入指定的内容\n\n**clone()** :方法生成被选元素的副本，包含子节点、文本和属性\n\n**toFixed(num)** :规定小数的位数，是0~20之间的值，包括0和20，有些实现可以支持更大的数值范围。如果省略了该参数，将用0代替。\n\n**substring(start,end)** :索引从0开始。start必需。一个非负的整数，规定要提取的子串的第一个字符在stringObject中的位置。end可选。一个非负的整数，比要提取的子串的最后一个字符在stringObject中的位置多1。如果省略该参数，那么返回的子串会一直到字符串的结尾。\n\n**substr(start,length)** :start必需。要抽取的子串的起始下标。必须是数值。如果是负数，那么该参数声明从字符串的尾部开始算起的位置。也就是说，-1指字符串中最后一个字符，-2指倒数第二个字符，以此类推。length可选。子串中的字符数。必须是数值。如果省略了该参数，那么返回从stringObject的开始位置到结尾的字串。\n\n**every()** :是对数组中每一项运行给定函数，如果该函数对每一项返回true,则返回true。\n\n**some()** :是对数组中每一项运行给定函数，如果该函数对任一项返回true,则返回true。\n\n```js\nvar arr = [ 1, 2, 3, 4, 5, 6 ];\nconsole.log( arr.some( function( item,index,array){\n  console.log( 'item=' + item + ',index='+index+',array='+array );\n  return item > 3;\n\n})); // true\nconsole.log( arr.every( function( item, index, array ){\n  console.log( 'item=' + item + ',index='+index+',array='+array);\n  return item > 3;\n\n})); // false\n```\n\n**find(selector)** :获得当前元素集合中每个元素的后代，通过选择器、jQuery对象或元素来筛选。\n\n**padStart(maxLength,fillString)** :填充字符串例:2000-1-1变成2000-01-01\n\n**padEnd(maxLength,fillString)**\n\n**setInterval()** :方法可按照指定的周期（以毫秒计）来调用函数或计算表达式。\nsetInterval()方法会不停地调用函数，直到**clearInterval()** 被调用或窗口被关闭。由setInterval()返回的ID值可用作clearInterval()方法的参数。\n\n**setTimeout()** :方法用于在指定的毫秒数后调用函数或计算表达式。\n\n**jQuery.extend()** :函数用于将一个或多个对象的内容合并到目标对象。\n\n1. 如果只为$.extend()指定了一个参数，则意味着参数target被省略。此时，target就是jQuery对象本身。通过这种方式，我们可以为全局对象jQuery添加新的函数。\n2. 如果多个对象具有相同的属性，则后者会覆盖前者的属性值。\n```js\njQuery.extend({\n  min: function(a, b) {\n\treturn a < b ? a : b;\n  },\n  max: function(a, b) {\n\treturn a > b ? a : b;\n  }\n});\n\njQuery.min(2, 3); // 2\njQuery.max(4, 5); // 5\n\n$.fn.extend({\n  alertWhileClick: function() {\n\t$(this).click(function() {\n\t\talert($(this).val());\n\t});\n  }\n});\n$(\"#input1\").alertWhileClick();\n```\njquery.extend()将两个或更多对象的内容合并到第一个对象\njquery.extend(target,object1,...,objectN)\n\n**replaceAll**\n\n```js\nvar str = \"dogdogdog\";\nvar str2 = str.replace(/dog/g,\"cat\");\n// 去除字符串内所有的空格：\nstr = str.replace(/\\s*/g,\"\");\n// 去除字符串内两头的空格：\nstr = str.replace(/^\\s*|\\s*$/g,\"\");\n// 去除字符串内左侧的空格：\nstr = str.replace(/^\\s*/,\"\");\n// 去除字符串内右侧的空格：\nstr = str.replace(/(\\s*$)/g,\"\");\n```\n\n**JS阻止原生态提交事件**\n\n```js\nevent.preventDefault();\n```\n\n**执行函数!function(){}()**\n\n```js\n//!function(){}()写法和(function(){})()是相同的,函数意义相同，叫做立即运行的匿名函bai数(也叫立即调用函数)。\n//js中可以这样创建一个匿名函数：\n(function(){do something...})()\n//或\n(function(){do something...}())\n//而匿名函数后面的小括号()是为了让匿名函数立即执行，其实就是一个函数调用。\n\nalert((new Function(\"x\",\"y\",\"return x*y;\"))(2,3));// \"6\"\n```\n\n### JQuery选择器\n\n> [jQuery选择器](https://tool.oschina.net/uploads/apidocs/jquery/)\n\n```js\n// >: 选择某元素后面的第一代子元素div>p选择其父元素是<div>元素的所有<p>元素。\n// ~: 选取某个元素之后的所有同级元素,.box~h2这句就是选取.box后面所有的h2,.box和h2同级,并且不需要紧邻\n// 空格: 选择某元素后面的所有子元素.不一定是直接子元素.孙子元素也可以\n// +: 可选择紧接在另一元素后的兄弟元素，且二者有相同父元素,元素需要紧邻. div + p选择所有紧随<div>元素之后的<p>元素。\n\n$(\"#select option:selected\").val();\n$(\"#select option[value='1']\").attr(\"selected\",true);\n$(\"p :eq(1)\")  // 选择第二个<p>元素：\n```\n\n### 获取div class的值\n\n```js\n$(\"#divid\").attr('class');// attr基本上选中html原生属性\n$(\"#divid\").prop('class');// prop自定义属性\n```\n\n### display属性\n\n- display:block块级元素特点：\n1. 每个块级元素都从新的一行开始，并且其后的元素也另起一行。（很霸道，一个块级元素独占一行）\n2. 元素的高度、宽度、行高以及顶和底边距都可设置。\n3. 元素宽度在不设置的情况下，是它本身父容器的100%（和父元素的宽度一致），除非设定一个宽度。\n- display:inline内联元素特点：\n1. 和其他元素都在一行上；\n2. 元素的高度、宽度及顶部和底部边距不可设置；\n3. 元素的宽度就是它包含的文字或图片的宽度，不可改变。\n- display:inline-block内联块状元素（inline-block）就是同时具备内联元素、块状元素的特点。\ninline-block元素特点：\n1. 和其他元素都在一行上；\n2. 元素的高度、宽度、行高以及顶和底边距都可设置\n\n### aria-hidden=true\n\naria-hidden=\"true\"不会导致任何事情,它只是声明作者已经隐藏了该元素.\n\n### 三种方式脱离文档流\n\n```css\nposition:absolute\nposition:fixed\nfloat\n```\n\n### Map转json\n\n```js\nfunction MapToJson(m) {\n  var str = '{';\n  var i = 1;\n  m.forEach(function (item, key, mapObj) {\n   if(mapObj.size == i){\n     str += '\"'+ key+'\":\"'+ item + '\"';\n   }else{\n     str += '\"'+ key+'\":\"'+ item + '\",';\n   }\n   i++;\n  });\n  str +='}';\n  return str;\n}\nvar jsons=JSON.parse(MapToJson(map));\n```\n\n### if(obj)判断\n\nif(jsObj){}过滤**undefined不能过,null不能过 ,''不能过 ,0不能过,{}能过,[]能过**。即`jsObj!=undefined && jsObj!=null && jsObj!='' && jsObj!=0`\n\n**if(!!jsObj)与上面等价 可以隐式转换类型**\n\n**js判断对象是否为空对象的几种方法**\n1、将json对象转化为json字符串，再判断该字符串是否为\"{}\"\n```js\nvar data = {};\nvar b = (JSON.stringify(data) == \"{}\");\nalert(b);//true\n```\n\n2、for in循环判断\n\n```js\nvar obj = {};\nvar b = function() {\nfor(var key in obj) {\n\treturn false; \n} \nreturn true;\n\nalert(b());//true\n```\n\n3、jquery的**isEmptyObject**方法\n\n此方法是jquery将2方法(for in)进行封装，使用时需要依赖jquery\n```js\nvar data = {};\nvar b = $.isEmptyObject(data);\nalert(b);//true\n```\n\n4、**Object.getOwnPropertyNames()** 方法\n\n此方法是使用Object对象的getOwnPropertyNames方法，获取到对象中的属性名，存到一个数组中，返回数组对象，我们可以通过判断数组的length来判断此对象是否为空。注意：此方法不兼容ie8，其余浏览器没有测试\n```js\nvar data = {};\nvar arr = Object.getOwnPropertyNames(data);\nalert(arr.length == 0);//true\n```\n\n5、使用ES6的**Object.keys()** 方法\n与4方法类似，是ES6的新方法,返回值也是对象中属性名组成的数组\n```js\nvar data = {};\nvar arr = Object.keys(data);\nalert(arr.length == 0);//true\n```\n\n### 滚动条位置\n\n- scrollTop:获取或设置一个元素的内容垂直滚动的像素数。//页面内容的滚动距离\n- scrollHeight:一个元素内容高度的度量，包括由于溢出导致的视图中不可见内容。//滚动内容的总大小\n- clientHeight:元素内部的高度(单位像素)，包含内边距，但不包括水平滚动条、边框和外边距。//可见网页内容高\n\n### 浏览器展示页面执行顺序\n\n$(document).ready(function) = $().ready(function) = $(function)\n当DOM（文档对象模型）已经加载(页面所有的html标签（包括图片等）都加载完了，即浏览器已经响应完了，加载完了，全部展现到浏览器界面上了。)，并且页面（包括图像）已经完全呈现时，会发生ready事件。对于一个HTML文档，浏览器的解析顺序为：按照文档流，从上到下逐步解析页面的结构。JavaScript代码作为通过标签嵌入或引入的脚本，也HTML文档的组成部分。因此，JavaScript代码在装载时的执行顺序也是根据脚本标签<script\\>的出现顺序来确定的。但是，浏览器加载JavaScript时有个特点，那就是载入之后立即就会执行（先编译后执行），因为JavaScript可能会影响DOM树的结构，所以浏览器在执行完后才能继续加载下面的HTML内容。也就是说，浏览器下载并执行JavaScript的过程会阻塞DOM树的继续建立。所以，引入的多个js文件，会按顺序分开执行。同样的，对于不同<script\\>标签嵌入的JavaScript代码，也会分开执行。同一组<script\\>标签包括的代码就是一个代码块。后引入的JavaScript文件可以调用先引入的JavaScript文件的资源，下面的代码块可以访问上面代码块的资源，反之则不行。由于JavaScript通常需要操作DOM，所以，一般把JavaScript放在</body\\>前或者文档结尾处引入。若需要在<head\\>中引入，可以通过修改window.onload或者document.ready事件，强制等到DOM加载完成后再执行相关函数。\n\n\n### 引用\n\n- 数字、字符串、布尔类型的为原始类型，是值引用\n- 数组、对象类型为地址引用\n- 值引用可以深拷贝\n- 地址引用循环到原始类型方可进行深拷贝\n\n### http状态码\n\n- 301永久重定向\n- 302临时重定向\n- 400 bad request：错误请求，一般是前端提交数据的字段名称或者是字段类型和后台的实体类不一致(后台接收的数据类型不一致)，导致无法封装。或前端提交的到后台的数据应该是json字符串类型，而前端没有将对象转化为字符串类型；\n- 403服务器理解请求但拒绝执行，一般是资源权限问题导致,无权限访问\n- 405 get post等请求类型错误\n- 406 服务器返回的数据前端无法解析,一般是返回json格式数据前端的Content-Type是text:html\n- 502 bad gateway:错误的网关,上游服务器出现问题。连接超时 我们向服务器发送请求,由于服务器当前链接太多，导致服务器方面无法给于正常的响应，产生此类报错\n- 503过载\n- 504 gateway time-out\n程序执行时间过长导致响应超时，例如程序需要执行20秒，而nginx最大响应等待时间为10秒，这样就会出现超时\n\n> [http状态码详解](http://tool.oschina.net/commons?type=5)\n> [请求头响应头等参数详解](https://mp.weixin.qq.com/s/w_FgN5tVGr1DlbqyF5g4gw)\n\n### GET与POST区别\n\n1. GET在浏览器回退时是无害的，而POST会再次提交请求。\n2. GET产生的URL地址可以被Bookmark，而POST不可以。\n3. GET请求会被浏览器主动cache，而POST不会，除非手动设置。\n4. GET请求只能进行url编码，而POST支持多种编码方式。\n5. GET请求参数会被完整保留在浏览器历史记录里，而POST中的参数不会被保留。\n6. GET请求在URL中传送的参数是有长度限制的，而POST没有。\n7. 对参数的数据类型，GET只接受ASCII字符，而POST没有限制。\n8. GET比POST更不安全，因为参数直接暴露在URL上，所以不能用来传递敏感信息。\n9. GET参数通过URL传递，POST放在Request body中。\n\n> [听我讲完GET、POST原理，面试官给我倒了杯卡布奇诺](https://mp.weixin.qq.com/s/W68JzNIoUpm9hyXinOzkMw)\n\n### ||和&&\n只要\"||\"前面为false,不管\"||\"后面是true还是false，都返回\"||\"后面的值\n只要\"||\"前面为true,不管\"||\"后面是true还是false，都返回\"||\"前面的值\n\n只要\"&&\"前面是false，无论\"&&\"后面是true还是false，结果都将返\"&&\"前面的值\n只要\"&&\"前面是true，无论\"&&\"后面是true还是false，结果都将返\"&&\"后面的值\n\n## JS关键字\n\n\n### arguments\n\narguments是js中内置的一个对象数组，存放的是调用函数的参数\narguments.callee()递归函数(callee是arguments内置的一个函数)\n```js\nfunction factorial(num) {\n  if(num<=1) {\n   return 1;\n  }else {\n   return num * factorial(num-1);\n  }\n} \n```\n相当于\n```js\nfunction factorial(num) {\n  if(num<=1) {\n   return 1;\n  }else {\n   return num * arguments.callee(num-1);\n  }\n} \n```\n\n### prototype、\\__proto\\__(两个下划线)与constructor\n\n\\__proto\\__和constructor属性是对象所独有的；prototype属性是函数所独有的，因为函数也是一种对象，所以函数也拥有\\__proto\\__和constructor属性。\\__proto\\__属性的作用就是当访问一个对象的属性时，如果该对象内部不存在这个属性，那么就会去它的\\__proto\\__属性所指向的那个对象（父对象）里找，一直找，直到\\__proto\\__属性的终点null，再往上找就相当于在null上取值，会报错。通过\\__proto\\__属性将对象连接起来的这条链路即我们所谓的原型链。大多数情况下，\\__proto\\__可以理解为“构造器的原型”，即\\__proto\\__===constructor.prototype,但是通过Object.create()创建的对象有可能不是，Object.create()方法创建一个新对象，使用现有的对象来提供新创建的对象的\\__proto_\\_\nprototype属性的作用就是让该函数所实例化的对象们都可以找到公用的属性和方法，即**book1.\\__proto\\__ === Book.prototype**。\n构造函数有什么缺点呢？构造函数的缺点就是会将构造函数内部的对象都复制一份：\n\n```js\nfunction Book(){\n    this.name ='www.flydean.com';\n    this.getName =function (){\n        console.log('flydean');\n    }\n}\nvar book1 = new Book();\nvar book2  = new Book();\nconsole.log(book1.getName  === book2.getName);//false\n```\n输出结果是false,说明每次new一个对象，对象中的方法也被拷贝了一份。而这并不是必须的。JavaScript的每个对象都继承另一个对象，后者称为“原型”（prototype）对象。只有null除外，它没有自己的原型对象。原型对象上的所有属性和方法，都能被派生对象共享。这就是JavaScript继承机制的基本设计。通过构造函数生成实例对象时，会自动为实例对象分配原型对象。每一个构造函数都有一个prototype属性，这个属性就是实例对象的原型对象。\n\n```js\n\nfunction Book(name){\n    this.name = name;\n}\nBook.prototype.author ='flydean';\nvar book1 = new Book();\nvar book2 = new Book();\nconsole.log(book1.author);//flydean\nconsole.log(book2.author);//flydean\n```\n上面例子中的author属性会被Book的所有实例所继承，Book的prototype对象，就是book1和book2的原型对象。\n原型对象的属性不是实例对象自身的属性。只要修改原型对象，变动就立刻会体现在所有实例对象上。由于原型本身也是对象，又有自己的原型，所以形成了一条原型链（prototype chain）。如果一层层地上溯，所有对象的原型最终都可以上溯到Object.prototype，即Object构造函数的prototype属性指向的那个对象。Object.prototype对象有没有它的原型呢？回答可以是有的，就是没有任何属性和方法的null对象，而null对象没有自己的原型。\n\n```js\nconsole.log(Object.getPrototypeOf(Object.prototype));//null\n```\nprototype对象有一个constructor属性，默认指向prototype对象所在的构造函数.\n```js\nfunction Book(name){\n    this.name = name;\n}\nvar book3 = new Book();\nconsole.log(book3.constructor);//function Book(name){this.name = name;}\nconsole.log(book3.constructor === Book.prototype.constructor);//true\nconsole.log(book3.hasOwnProperty(constructor));//false\n```\n还是刚刚的book，book3.constructor就是function Book本身。它也等于Book.prototype.constructor。constructor属性的含义就是指向该对象的构造函数，所有函数（此时看成对象了）最终的构造函数都指向Function。constructor属性的作用，是分辨原型对象到底属于哪个构造函数。因为prototype是一个对象，所以对象可以被赋值，也就是说prototype可以被改变：\n\n```js\nfunction A(){}\nvar a = new A();\nconsole.log(a instanceof A);//true\nfunction B(){}\nA.prototype = B.prototype;\nconsole.log(a instanceof A);//false\n```\n上面的例子中，我们修改了A.prototype，最后a instanceof A值是false。为了保证不会出现这样错误匹配的问题，我们再构建prototype的时候，一定不要直接重写整个的prototype，只需要修改其中的某个属性就好:\n```js\n// 不要这样写\nA.prototype = {\n    method1:function (){}\n}\n// 比较好的写法\nA.prototype = {\n    constructor:A,\n    method1:function (){}\n}\n// 更好的写法\nA.prototype.method1 = function (){}\n```\n#### Object的prototype操作\n**Object.getPrototypeOf**方法返回一个对象的原型。这是获取原型对象的标准方法.\n\n```js\n// 空对象的prototype是Object.prototype\nconsole.log(Object.getPrototypeOf({}) === Object.prototype);//true\n// function的prototype是Function.prototype\nfunction f(){}\nconsole.log(Object.getPrototypeOf(f)  === Function.prototype);//true\n\nfunction F(){this.name ='flydean'}\nvar f1 = new F();\nconsole.log(Object.getPrototypeOf(f1) === F.prototype);//true\nvar f2 = new f();\nconsole.log(Object.getPrototypeOf(f2) === f.prototype);//true\n```\n**Object.setPrototypeOf**方法可以为现有对象设置原型，返回一个新对象。\nObject.setPrototypeOf方法接受两个参数，第一个是现有对象，第二个是原型对象。\n\n```js\nvar a = {name: 'flydean'};\nvar b = Object.setPrototypeOf({},a);\nconsole.log(b.name);//flydean\n```\n**Object.prototype.isPrototypeOf()**\n对象实例的isPrototypeOf方法，用来判断一个对象是否是另一个对象的原型.\n\n```js\nvar a = {name: 'flydean'};\nvar b = Object.setPrototypeOf({},a);\nconsole.log(a.isPrototypeOf(b));//true\n```\n**Object.prototype.proto**\nproto属性（前后各两个下划线）可以改写某个对象的原型对象。还是刚才的例子，这次我们使用proto来改写对象的原型。\n\n```js\nvar a = {name: 'flydean'};\nvar c ={};\nc.__proto__ = a;\nconsole.log(Object.getPrototypeOf(c));//{\"name\": \"flydean\",[[Prototype]]:object}\n\n//-------\nfunction Book(name){\n    this.name = name;\n}\nconsole.log(Book.prototype)//{\"author\": \"flydean\",constructor:f Book(name)..}\nvar book1 = new Book();\nconsole.log(book1.__proto__===Book.prototype);//true\n```\nproto属性只有浏览器才需要部署，其他环境可以没有这个属性，而且前后的两根下划线，表示它本质是一个内部属性，不应该对使用者暴露。因此，应该尽量少用这个属性，而是用Object.getPrototypeof()（读取）和Object.setPrototypeOf()（设置），进行原型对象的读写操作\n\n综上，我们有三种获取原型对象的方法：\n```js\nobj.proto\nobj.constructor.prototype\nObject.getPrototypeOf(obj)\n```\n\n### Promise\n\n什么时候用\n回调地狱时使用，当代码难以维护，常常第一个的函数的输出是第二个函数的输入这种现象时使用promise。Promise对象用于表示一个异步操作的最终完成(或失败)及其结果值\n基本用法\ndemo1\n\n```js\nlet myFirstPromise = new Promise(function(resolve,reject){\n//调用then()时,当异步代码执行成功时，调用resolve(...),当异步代码失败时,调用reject(...)\n  if(){\n   resolve(111)\n  }else{\n   reject(222)\n  }\n});\nmyFirstPromise.then(function(successMessage){\n//successMessage的值是上面调用resolve(...)方法传入的值.上面不写resolve()的话不会执行这个函数\n//successMessage参数不一定非要是字符串类型，也可以是函数\n  console.log(\"Yay! \" + successMessage);\n},function(failedMessage){\n  // failedMessage的值是reject(..)传过来的参数，如果没有在上面写reject()的话不会执行此函数，写上reject()才会执行此快函数\n});\n```\ndemo2：\n\n```js\nfunction test(fun) {\n  //这里表示执行了一大堆各种代码;\n  ...\n  // 返回Promise对象\n  return new Promise(function(resolve, reject) {\n  // 这里也可以执行相关代码\n\tif (typeof fun== 'function') {\n\t   resolve(fun);\n\t } else {\n\t   reject('TypeError: '+ fun+'不是一个函数')\n\t }\n  })\n}\n// 当fun是一个function时\ntest(fun).then(function(abc) {// 参数abc是上面resolve(fun)的参数\n  abc();\n})\ntest('1234').catch(function(err) {\n// 参数err是上面reject('TypeError: '+ fun+'不是一个函数')的参数\n  console.log(err);\n})\n//promise.then(onFulfilled, onRejected)等价于promise.then(onFulfilled).catch(onRejected)\n```\n> [前端基础进阶（十五）：透彻掌握Promise的使用，读这篇就够了](https://www.jianshu.com/p/fe5f173276bd)\n> [JavaScript Promise对象](https://www.runoob.com/w3cnote/javascript-promise-object.html)\n> [mozilla promise](https://developer.mozilla.org/zh-CN/docs/Web/JavaScript/Reference/Global_Objects/Promise)\n> [写给Java程序员的前端Promise教程](https://mp.weixin.qq.com/s/92AHFPWjtH_y2_88mP3CYQ)\n\n\n### await、async\n\nasync是一个修饰符，async定义的函数会默认的返回一个Promise对象resolve的值，因此对async函数可以直接进行then操作,返回的值即为then方法的传入函数\n```js\n// 0. async基础用法测试\nasync function fun0() {\n    console.log(1)\n    return 1\n}\nfun0().then( x => { console.log(x) })  //  输出结果 1， 1，\n\nasync function funa() {\n    console.log('a')\n    return 'a'\n}\nfuna().then( x => { console.log(x) })  //  输出结果a， a，\n\nasync function funo() {\n    console.log({})\n    return {}\n}\nfuno().then( x => { console.log(x) })   // 输出结果 {}  {}\n\nasync function funp() {\n    console.log('Promise')\n    return new Promise(function(resolve, reject){\n        resolve('Promise')\n    })\n}\n\nfunp().then( x => { console.log(x) })   // 输出promise  promise\n```\nawait也是一个修饰符，await操作符用于等待一个Promise对象。它只能在异步函数async function中使用(await关键字只能放在async函数内部)，await关键字的作用就是返回Promise对象的处理结果，如果await后面并不是一个Promise的返回值，则会按照同步程序返回该值本身\n```js\n//  await关键字只能放在async函数内部，await关键字的作用就是获取Promise中返回的内容，获取的是Promise函数中resolve或者reject的值\n// 如果await后面并不是一个Promise的返回值，则会按照同步程序返回值处理,为undefined\nconst bbb = function(){ return 'string'}\n\nasync function funAsy() {\n   const a = await 1\n   const b = await new Promise((resolve, reject)=>{\n        setTimeout(function(){\n           resolve('time')\n        }, 3000)\n   })\n   const c = await bbb()\n   console.log(a, b, c)\n}\nfunAsy()  //  运行结果是3秒钟之后输出1，time,string,\n```\n\n那么由此看来async/await的综合用法如下\n\n```js\n// 1.定义一个或多个普通函数，函数必须返回Promise对象，如果返回其他类型的数据，将按照普通同步程序处理\nfunction log(time) {\n    return new Promise((resolve, reject)=> {\n        setTimeout(function(){\n           console.log(time)\n           resolve()\n        }, time)\n    })\n}\nasync function fun() {\n    await log(5000)\n    await log(10000)\n    log(1000)\n    console.log(1)\n}\nfun()\n\n// 2.如果不使用promise的方法的话\nfunction log2(time) {\n   setTimeout(function(){\n       console.log(time)\n       return 1\n    }, time)\n}\nasync function fun1() {\n    const a = await log2(5000)\n    const b = await log2(10000)\n    const c = log2(2000)\n    console.log(a)\n    console.log(1)\n}\n\nfun1()\n// 以上运行结果为：立刻输出undefined 立刻输出1  2秒后输出2000  5秒后输出5000  10秒后输出10000\n\n// 3.async/await的重要应用 \nconst asy = function(x, time) {\n    return new Promise((resolve, reject) =>{\n        setTimeout(()=>{\n            resolve(x)\n        }, time)\n    })\n}\nconst add = async function() {\n    const a = await asy(3, 5000)\n    console.log(a)\n    const b = await asy(4, 10000)\n    console.log(b)\n    const c =  await asy(5, 15000)\n    console.log(a,b,c)\n    const d = a + b +c  \n    console.log(d)\n}\nadd();\n// 5秒后输出3  又10秒后输出4 又15秒后输出5  然后立刻输出3,4,5，然后输出12\n```\n\n### js new\n\n帮我们做了这样几件事：\n1. 帮我们创建了一个空对象，作为返回对象的实例,例如：obj；\n2. 将空对象原型指向构造函数的property属性\n3. 将这个空对象赋值给函数内部的this\n4. 执行构造函数(如果构造函数内部有return语句，而且return后面跟着一个对象，new命令会返回return语句指定的对象；否则，就会不管return语句，返回this对象。)\n```js\nfunction Foo(name) {\n  this.name = name;\n  return this;\n}\nvar obj = {};\nobj.__proto__ = Foo.prototype;\nvar foo = Foo.call(obj, 'mm'); //call会改变this的指向\nconsole.log(foo);\n// 等同于\nvar foo = new Foo('mm')\nconsole.log(foo)\n```\n> [JavaScript：对象都是这样生成的!※(很重要的一篇文章)！](https://mp.weixin.qq.com/s/QJj9TnUKeXGj1HIX039etg)\n\n\n### JS this\n\nthis总是返回一个对象，简单说，就是返回属性或方法“当前”所在的对象。\n```js\nvar book = {\n    name :'flydean',\n    getName : function (){\n        return '书名：'+ this.name;\n    }\n}\nconsole.log(book.getName());\n//书名：flydean\n```\n这里this的指向是可变的，我们看一个例子：\n```js\nvar book = {\n    name :'flydean',\n    getName : function (){\n        return '书名：'+ this.name;\n    }\n}\nvar car ={\n    name :'car'\n}\ncar.getName = book.getName;\nconsole.log(car.getName());\n//书名：car\n```\n当A对象的方法被赋予B对象，该方法中的this就从指向A对象变成了指向B对象\n上面的例子中，我们把book中的getName方法赋值给了car对象，this对象现在就指向了car。如果某个方法位于多层对象的内部，这时this只是指向当前一层的对象，而不会继承更上面的层。\n\n```js\nvar book1 = {\n    name :'flydean',\n    book2: {\n        getName : function (){\n            return '书名：'+ this.name;\n        }\n    }\n}\nconsole.log(book1.book2.getName());\n//书名：undefined\n```\n上面的例子中，this是定义在对象中的函数中，如果是在函数中的函数中定义的this，代表什么呢？\n```js\nvar book3 = {\n    name :'flydean',\n    book4: function(){\n        console.log('book4');\n        var getName = function (){\n            console.log(this); //Window\n        }();\n    }\n}\nbook3.book4();\n```\n如果在函数中的函数中使用了this，那么内层的this指向的是全局的window对象。所以我们在使用的过程中要避免多层this。由于this的指向是不确定的，所以切勿在函数中包含多层的this。如果在全局环境使用this，它指的就是顶层对象window。\n数组的map和foreach方法，允许提供一个函数作为参数。这个函数内部不应该使用this。\n```js\nvar book5 ={\n    name : 'flydean',\n    author : ['max','jacken'],\n    f: function (){\n        this.author.forEach(function (item) {\n            console.log(this.name+' '+item);\n        })\n    }\n}\nbook5.f();\n//undefined max\n//undefined jacken\n```\nforeach方法的回调函数中的this，其实是指向window对象，因此取不到o.v的值。原因跟上一段的多层this是一样的，就是内层的this不指向外部，而指向顶层对象。怎么解决呢？我们使用一个中间变量：\n```js\nvar book6 ={\n    name : 'flydean',\n    author : ['max','jacken'],\n    f: function (){\n        var that = this;\n        this.author.forEach(function (item) {\n            console.log(that.name+' '+item);\n        })\n    }\n}\nbook6.f();\n//flydean max\n//flydean jacken\n```\n或者将this当作foreach方法的第二个参数，固定它的运行环境：\n```js\nvar book7 ={\n    name : 'flydean',\n    author : ['max','jacken'],\n    f: function (){\n        this.author.forEach(function (item) {\n            console.log(this.name+' '+item);\n        },this)\n    }\n}\nbook7.f();\n//flydean max\n//flydean jacken\n```\n绑定this的方法,JavaScript提供了call、apply、bind这三个方法，来切换/固定this的指向.\n\n**call**\n函数实例的call方法，可以指定函数内部this的指向（即函数执行时所在的作用域），然后在所指定的作用域中，调用该函数.\n\n```js\nvar book = {};\nvar f = function () {\n    return this;\n}\nf()  === this ; //true\nf.call(book) === book; //true\n```\n上面例子中，如果直接调用f()，那么返回的就是全局的window对象。如果传入book对象，那么返回的就是book对象。call方法的参数，应该是一个对象。如果参数为空、null和undefined，则默认传入全局对象。如果call方法的参数是一个原始值，那么这个原始值会自动转成对应的包装对象，然后传入call方法。\n\n```js\nvar f = function () {\n    return this;\n}\nconsole.log(f.call(100));\n//[Number: 100]\n```\ncall方法还可以接受多个参数。func.call(thisValue,arg1,arg2,...);call的第一个参数就是this所要指向的那个对象，后面的参数则是函数调用时所需的参数。call一般用在调用对象的原始方法：\n```js\nvar person =  {};\nperson.hasOwnProperty('getName');//false\n//覆盖person的getName方法\nperson.getName  = function(){\n    return true;\n}\nperson.hasOwnProperty('getName');//true\nObject.prototype.hasOwnProperty.call(person,'getName');//false\n```\n**apply**\napply方法的作用与call方法类似，也是改变this指向，然后再调用该函数。唯一的区别就是，它接收一个数组作为函数执行时的参数.\n```js\nfunc.apply(thisValue,[arg1,arg2,...])\n```\n**bind**\ncall和apply是改变this的指向，然后调用该函数，而bind方法用于将函数体内的this绑定到某个对象，然后返回一个新函数.\n```js\nvar d = new Date();\nconsole.log(d.getTime()); //1600755862787\nvar getTime= d.getTime;\nconsole.log(getTime());//TypeError: this is not a Date object.\n```\n上面的例子中，getTime方法里面调用了this，如果直接把d.getTime赋值给getTime变量，那么this将会指向全局的window对象，导致运行错误。我们可以这样修改：\n```js\nvar d = new Date();\nconsole.log(d.getTime()); //1600755862787\nvar getTime2= d.getTime.bind(d);\nconsole.log(getTime2());\n```\nbind比call方法和apply方法更进一步的是，除了绑定this以外，还可以绑定原函数的参数。\n```js\nvar add = function(x,y){\n    return x +this.m +  y + this.n;\n}\nvar addObj ={\n    m: 10,\n    n: 10\n}\nvar newAdd = add.bind(addObj,2);\nconsole.log(newAdd(3));//25\n```\n上面的例子中，bind将两个参数的add方法，替换成了1个参数的add方法。注意：bind每次调用都会返回一个新的函数，从而导致无法取消之前的绑定。\n\n> [理解js中this的指向](https://www.cnblogs.com/pssp/p/5216085.html)\n\n\n### class\n\nES6的class可以看作只是一个语法糖，它的绝大部分功能，ES5都可以做到，新的class写法只是让对象原型的写法更加清晰、更像面向对象编程的语法而已。\n```js\nclass Person {\n    constructor(name,sex) {\n        this.name=name;\n        this.sex =sex;\n    }\n    toString(){\n        return this.name + ' '+ this.sex;\n    }\n}\n```\n构造函数的prototype属性，在ES6的“类”上面继续存在。事实上，类的所有方法都定义在类的prototype属性上面。\n上面的类等同于：\n```js\nPerson.prototype = {\n       constructor(name,sex) {\n        this.name=name;\n        this.sex =sex;\n    }\n    toString(){\n        return this.name + ' '+ this.sex;\n    } \n}\n```\n表达式属性名\nclass还支持动态的表达式属性名：\n```js\nlet methodName = 'getName';\nclass Person {\n    constructor(name,sex) {\n        this.name=name;\n        this.sex =sex;\n    }\n\n    toString(){\n        return this.name + ' '+ this.sex;\n    }\n\n    [methodName](){\n        return this.name;\n    }\n}\n```\n静态方法\n类相当于实例的原型，所有在类中定义的方法，都会被实例继承。如果在一个方法前，加上static关键字，就表示该方法不会被实例继承，而是直接通过类来调用，这就称为“静态方法”。\n```js\nclass Person {\n    constructor(name,sex) {\n        this.name=name;\n        this.sex =sex;\n    }\n    static getSex(){\n        return '男';\n    }\n}\nconsole.log(Person.getSex()); //男\nlet  p  = new Person();\nconsole.log(p.getSex());//TypeError: p.getSex is not a function\n```\n静态属性\n静态属性指的是Class本身的属性，即Class.propName，而不是定义在实例对象（this）上的属性.\n```js\nclass Person {\n    constructor(name,sex) {\n        this.name=name;\n        this.sex =sex;\n    }\n}\nPerson.address ='address';\nconsole.log(Person.address);\n```\n目前，只有这种写法可行，因为ES6明确规定，Class内部只有静态方法，没有静态属性。\n\nclass的继承\nclass 的继承一般使用extends关键字：\n```js\nclass Boy extends Person{\n    constructor(name,sex,address) {\n        super(name,sex); //调用父类的构造函数\n        this.address =address;\n    }\n    toString() {\n        return super.toString();//调用父类的方法\n    }\n}\n```\n在子类的构造函数中，只有调用super之后，才可以使用this关键字，否则会报错。这是因为子类实例的构建，是基于对父类实例加工，只有super方法才能返回父类实例。super作为函数调用时，代表父类的构造函数。ES6要求，子类的构造函数必须执行一次super函数。super作为对象时，在普通方法中，指向父类的原型对象；在静态方法中，指向父类。上面的例子，我们在子类Boy中的toString普通方法中，调用了super.toString()，之前我们也讲了，类的所有方法都定义在类的prototype属性上面。所以super.toString就是Person中定义的toString方法。由于super指向父类的原型对象，所以定义在父类实例上的方法或属性，是无法通过super调用的。定义在父类实例上的方法或属性就是指在constructor中定义的方法或者属性。Person 类，在constructor中定义了name属性。我们看一下在Boy中的普通方法中访问会有什么问题：\n```js\nclass Boy extends Person{\n    constructor(name,sex,address) {\n        super(name,sex); //调用父类的构造函数\n        console.log(super.name);  //undefined\n        console.log(this.name);  //hanmeimei\n        this.address =address;\n    }\n    toString() {\n        return super.toString();//调用父类的方法\n    }\n    getName(){\n        console.log(super.name);  //undefined\n        console.log(this.name);    //hanmeimei\n    }\n}\nvar b =new Boy('hanmeimei','女','北京');\nb.getName();\n```\n\n### 继承\n\n构造函数的继承。\n构造函数的继承第一步是在子类的构造函数中，调用父类的构造函数,让子类实例具有父类实例的属性。然后让子类的原型指向父类的原型，这样子类就可以继承父类原型。\n\n```js\nfunction Person (){\n    this.name = 'person';\n}\n\nfunction Boy(){\n    Person.call(this);\n    this.title = 'boy';\n}\n\nBoy.prototype= Object.create(Person.prototype);\nBoy.prototype.constructor=Boy;\nBoy.prototype.getTitle=function (){console.log(this.title)};\n\nvar b =new Boy();\nb.getTitle();\nconsole.log(b);\n```\n调用父类的构造函数是初始化实例对象的属性。子类的原型指向父类的原型是为了基础父类的原型对象的属性。\n另外一种写法是Boy.prototype等于一个父类实例：\n```js\nBoy.prototype = new Person();\n```\n上面这种写法也有继承的效果，但是子类会具有父类实例的方法。有时，这可能不是我们需要的，所以不推荐使用这种写法.\nJavaScript不提供多重继承功能，即不允许一个对象同时继承多个对象。但是，可以通过变通方法，实现这个功能:\n```js\nfunction Person1 (){\n    this.name = 'person';\n}\nfunction Person2 (){\n    this.sex = '男';\n}\n\nfunction Boy(){\n    Person1.call(this);\n    Person2.call(this);\n    this.title = 'boy';\n}\n\n//继承Person1\nBoy.prototype= Object.create(Person1.prototype);\n//继承链加上Person2\nObject.assign(Boy.prototype,Person2.prototype);\n\nBoy.prototype.constructor=Boy;\nBoy.prototype.getTitle=function (){console.log(this.title)};\n\nvar b =new Boy();\nb.getTitle();\nconsole.log(b);\n//Boy { name: 'person', sex: '男', title: 'boy' }\n```\n\n### spread运算符(扩展运算符)\n\n扩展运算符`...`是ES6中引入的一个新运算符，它可以将一个数组或者类数组对象展开（或者说“拍扁”）成一系列单独的值，或者将多个值合并成一个数组\n\n```js\n// 插入数组：\n// 看看如下代码，不使用扩展语法：\nvar mid = [3, 4];\nvar arr = [1, 2, mid, 5, 6];\nconsole.log(arr);  // [1, 2, [3, 4] , 5, 6]\n// 上面这段代码将得到一个嵌套数组的数组。大部分情况，我们希望一个array（mid）展开后再插入到另一个array（arr）中。\n// 使用spread操作符我们可以这样：\nvar mid = [3, 4];\nvar arr = [1, 2, ...mid, 5, 6];\nconsole.log(arr); // [1，2，3，4，5，6]\n\n// 展开数组作为参数\n// 当一个函数接收多个参数，比如Math.max，当我们有一个数组需要找到你了的最大值，我们可以使用如下代码进行调用。\nvar arr = [2, 4, 8, 6, 0];\nfunction max(arr) {\n  return Math.max.apply(null, arr);\n}\nconsole.log(max(arr)); // 8\n\n// 如果这时候使用spread运算符会变得非常方便。\nvar arr = [2, 4, 8, 6, 0];\nvar max = Math.max(...arr);\nconsole.log(max); // 8\n\n// 复制数组\n// 用数组给新数组赋值只是获取到数组引用，并没有达到深复制的效果。\nvar arr = ['a', 'b', 'c'];\nvar arr2 = arr;\narr2.push('d');\nconsole.log(arr);// [\"a\", \"b\", \"c\", \"d\"]\n\n// 有多重方法可以实现深复制，但是使用spread操作符是最简洁的一种实现：\nvar arr = ['a', 'b', 'c'];\nvar arr2 = [...arr];\narr2.push('d');\nconsole.log(arr);  // [\"a\", \"b\", \"c\"]\n\n// 展开String\n// 如果想将字符串转为字符数组，如果不实用展开操作：\nvar str = \"hello\";\n\"hello\".split('') // [\"h\", \"e\", \"l\", \"l\", \"o\"]\n\n// 使用展开操作符可以这样写：\nvar str = \"hello\";\nvar chars = [...str]; // [\"h\", \"e\", \"l\", \"l\", \"o\"]\n\n// 展开Object\n// 我们还可以对对象进行展开，如果有两个对象，有不同的key-value,我们需要将这两个对象合并起来(需要使用Object rest spread transform)，我们可以这样：\n\nlet Obj1 = {\n key1: 'value1'\n}\n\nlet Obj2 = {\n key2: 'value3'\n}\n\nlet concatValue = {\n ...Obj1,\n ...Obj2\n}\nconsole.log(concatValue) // {key1: 'value1', key2: 'value2'}\n```\n\n## 相关文章\n\n- [Web开发技术(mozilla)](https://developer.mozilla.org/zh-CN/docs/Web)\n- [万字干货！详解JavaScript执行过程](https://mp.weixin.qq.com/s/wolPlpUDizVnzh-kBKMtxg)\n- [20分钟全面理解JavaScript事件机制](https://mp.weixin.qq.com/s/ct4AyU--sewtOaDOlO2Oxw)\n- [面试官：有了for循环为什么还要forEach？](https://mp.weixin.qq.com/s/aPFCrPGBTus_Spf1QL1WWA)\n- [每日一题」JS中的闭包是什么？](https://zhuanlan.zhihu.com/p/22486908)\n- [post请求下载excel文档解决方法](https://blog.csdn.net/weixin_44001753/article/details/114266453)\n- [不懂Vue的Java猿不是一个好的前端工程师](https://mp.weixin.qq.com/s/QoS5z9Qfokpcw42zDbiE2A)","tags":["随笔"],"categories":["JS"]},{"title":"我的笔记","slug":"我的笔记","url":"/blog/posts/8e7bdb140cc5/","content":"\n## 笔记\n\n### 代码块\n\n#### 静态代码块\n\n在类被加载的时候就运行了，而且只运行一次，并且优先于各种代码块以及构造函数。如果一个类中有多个静态代码块，会按照书写顺序依次执行。静态代码块不能存在任何方法体中静态代码块不能访问普通变量\n\n#### 构造代码块\n\n在创建对象时被调用，每次创建对象都会调用一次，但是优先于构造函数执行。需要注意的是，构造代码块不是优先于构造函数执行，而是依托于构造函数，也就是说，如果你不实例化对象，构造代码块是不会执行的\n\n#### 普通代码块\n\n和构造代码块的区别是，构造代码块是在类中定义的，而普通代码块是在方法体中定义的。且普通代码块的执行顺序和书写顺序一致\n\n静态代码块>构造代码块（类中定义）>构造函数>普通代码块(方法中定义)\n\n### 内部类\n\n#### 成员内部类\n\n也是最普通的内部类，它是外围类的一个成员，所以他是可以无限制的访问外围类的所有成员属性和方法，尽管是private的，但是外围类要访问内部类的成员属性和方法则需要通过内部类实例来访问。\n在成员内部类中要注意两点，第一：成员内部类中不能存在任何static的变量和方法；第二：成员内部类是依附于外围类的，所以只有先创建了外围类才能够创建内部类。\n\n```java\nOuterClass outerClass = new OuterClass();\nOuterClass.InnerClass innerClass = outerClass.new InnerClass();\ninnerClass.getOuterClass().display();\n```\n\n#### 局部内部类\n\n有这样一种内部类，它是嵌套在方法和作用域内的，对于这个类的使用主要是应用与解决比较复杂的问题，想创建一个类来辅助我们的解决方案，到那时又不希望这个类是公共可用的，所以就产生了局部内部类，局部内部类和成员内部类一样被编译，只是它的作用域发生了改变，它只能在该方法和属性中被使用，出了该方法和属性就会失效。\n\n#### 静态内部类\n\n使用static修饰的内部类我们称之为静态内部类，不过我们更喜欢称之为嵌套内部类。静态内部类与非静态内部类之间存在一个最大的区别，我们知道非静态内部类在编译完成之后会隐含地保存着一个引用，该引用是指向创建它的外围内，但是静态内部类却没有。没有这个引用就意味着：\n1. 它的创建是不需要依赖于外围类的。\n2. 它不能使用任何外围类的非static成员变量和方法。\n3. 在静态内部类中可以存在静态成员\n4. 静态内部类只能访问外围类的静态成员变量和方法，不能访问外围类的非静态成员变量和方法\n5. 非静态内部类中不能存在静态成员，非静态内部类中可以调用外围类的任何成员,不管是静态的还是非静态的\n6. 静态内部类可以直接创建实例不需要依赖于外围类\n7. 非静态内部的创建需要依赖于外围类方位\n8. 非静态内部类的成员需要使用非静态内部类的实例new\n\n#### 匿名内部类\n\nnew父类构造器（参数列表）|实现接口（）{\n   //匿名内部类的类体部分\n}\n在这里我们看到使用匿名内部类我们必须要继承一个父类或者实现一个接口，当然也仅能只继承一个父类或者实现一个接口。同时它也是没有class关键字，这是因为匿名内部类是直接使用new来生成一个对象的引用。当然这个引用是隐式的。\n1. 使用匿名内部类时，我们必须是继承一个类或者实现一个接口，但是两者不可兼得，同时也只能继承一个类或者实现一个接口。\n2. 匿名内部类中是不能定义构造函数的。\n3. 匿名内部类中不能存在任何的静态成员变量和静态方法。\n4. 匿名内部类为局部内部类，所以局部内部类的所有限制同样对匿名内部类生效。\n5. 匿名内部类不能是抽象的，它必须要实现继承的类或者实现的接口的所有抽象方法。\n\n```java\nThread thread = new Thread(new Runnable() {\n\t@Override\n\tpublic void run() {\n\tSystem.out.println(title);\n\t}\n });\n```\n\n#### 使用内部类的优势\n\n封装性、实现多继承※、用匿名内部类实现回调功能、解决继承及实现接口出现同名方法的问题\n\n> [详解内部类](https://www.cnblogs.com/chenssy/p/3388487.html)\n> [深入理解Java：内部类](https://mp.weixin.qq.com/s/Is2Ka2R_PWZP3XZsUroLIg)\n> [Java内部类有坑,100%内存泄露！](https://mp.weixin.qq.com/s/u8rgIwoxnXLe8GPMrFbXpQ)\n\n### 元注解\n\n#### @Retention\n\n表示需要在什么级别保存该注释信息，用于描述注解的生命周期（即：被描述的注解在什么范围内有效）,取值（RetentionPoicy）有：\n\n1. SOURCE:在源文件中有效（即源文件保留）\n2. CLASS:在class文件中有效（即class保留）\n3. RUNTIME:在运行时有效（即运行时保留）\n\n#### @Target\n\n用于描述注解的使用范围（即：被描述的注解可以用在什么地方）,取值(ElementType)有：\n1. CONSTRUCTOR:用于描述构造器\n2. FIELD:用于描述域\n3. LOCAL_VARIABLE:用于描述局部变量\n4. METHOD:用于描述方法\n5. PACKAGE:用于描述包\n6. PARAMETER:用于描述参数\n7. TYPE:用于描述类、接口(包括注解类型)或enum声明\n8. ANNOTATION_TYPE:标明注解可以用于注解声明(应用于另一个注解上)\n9. TYPE_PARAMETER:表示该注解能写在类型参数的声明语句中也就是泛型上，类型参数声明如下：<T\\>，< T extends Person>\n10. TYPE_USE:表示注解可以在任何用到该类的地方使用\n\n#### @Inherited\n\n标记这个注解是继承于哪个注解类(默认注解并没有继承于任何子类)\n\n#### @Documented\n\n标记这些注解是否包含在用户文档中。\n\n#### @Repeatable\n\n重复注解,在java8中新增了一个方法getAnnotationsByType，用于获取可重复的注解,返回类型是数组\n\n> [你知道Java中的注解是如何工作的？](https://mp.weixin.qq.com/s/kx_111lekaIzkYYYAwjwxg)\n> [深入理解Java：注解](https://mp.weixin.qq.com/s?__biz=Mzg2MDYzODI5Nw==&amp;mid=2247493932&amp;idx=1&amp;sn=c8e75d1f75467f240ad749853eae4668&amp;source=41#wechat_redirect)\n> [JDK中注解的底层原来是这样实现的](https://mp.weixin.qq.com/s?__biz=Mzg2MDYzODI5Nw==&amp;mid=2247494076&amp;idx=2&amp;sn=ba12e71724444dfd8ce10297778682aa&amp;source=41#wechat_redirect)\n> [原来注解是这么实现的啊](https://mp.weixin.qq.com/s/Ggw-uPBmxZ0VDyACreMNOw)\n\n\n### 构建工具\n\n#### maven\n\n**常用命令**\n\n- mvn package、install、deploy区别\n**package**命令完成了项目编译、单元测试、打包功能，但没有把打好的可执行jar包（war包或其它形式的包）布署到本地maven仓库和远程maven私服仓库\n**install**命令完成了项目编译、单元测试、打包功能，同时把打好的可执行jar包（war包或其它形式的包）布署到本地maven仓库，但没有布署到远程maven私服仓库\n**deploy**命令完成了项目编译、单元测试、打包功能，同时把打好的可执行jar包（war包或其它形式的包）布署到本地maven仓库和远程maven私服仓库\n\n- compile(编译 打包)\ncompile是默认的范围；如果没有提供一个范围，那该依赖的范围就是编译范围。编译范围依赖在所有的classpath中可用，同时它们也会被打包。\n\n- provided(已提供 编译不打包)\nprovided意味着打包的时候可以不用包进去，别的设施(Web Container)会提供。事实上该依赖理论上可以参与编译，测试，运行等周期。相当于compile，但是在打包阶段做了exclude的动作。\n\n- runtime(运行时 不编译打包)\nruntime表示被依赖项目无需参与项目的编译，不过后期的测试和运行周期需要其参与。与compile相比，跳过编译而已。\n\n- test (测试)\ntest范围依赖在一般的编译和运行时都不需要，它们只有在测试编译和测试运行阶段可用。\n\n- system (系统)\nMaven不会在仓库中去寻找它。如果你将一个依赖范围设置成系统范围，你必须同时提供一个systemPath元素。注意该范围是不推荐使用的（建议尽量去从公共或定制的Maven仓库中引用依赖）\n\n- import(导入)\nimport仅支持在<dependencyManagement\\>中的类型依赖项上。它表示要在指定的POM<dependencyManagement\\>部分中用有效的依赖关系列表替换的依赖关系。该scope类型的依赖项实际上不会参与限制依赖项的可传递性。\n\n- <optional\\>标签\nprojectA依赖projectB,projectB依赖projectC时,当projectA在maven引入projectB时,如果projectB写上<optional\\>true</optional\\>时,则projectA不依赖projectC,即projectA可以自己选择是否依赖projectC,如果不写或者是false的时候,则projectA引入projectB时,也会引入projectC,默认<optional\\>的值为false,即子项目必须依赖.但是像引入parent继承情况时,像这样\n```xml\n<dependencyManagement>\n    <dependencies>\n        <dependency>\n            <groupId>joda-time</groupId>\n            <artifactId>joda-time</artifactId>\n            <version>2.9.9</version>\n            <optional>true</optional>\n        </dependency>\n    </dependencies>\n</dependencyManagement>\n```\noptional选项在统一控制版本的情况下会失效\n\n**Maven打包跳过测试的5种方法**\n\n(1) 命令行方式\n```bash\n# -DskipTests=true,不执行测试用例,但编译测试用例类生成相应的class文件至target/test-classes下。\nmvn package -DskipTests=true\n# -Dmaven.test.skip=true,不执行测试用例,也不编译测试用例类。\nmvn package -Dmaven.test.skip=true\n```\n(2) pom.xml\n```xml\n<build>\n    <plugins>\n        <!-- maven打包时跳过测试 -->\n        <plugin>\n            <groupId>org.apache.maven.plugins</groupId>\n            <artifactId>maven-surefire-plugin</artifactId>\n            <configuration>\n                <skip>true</skip>\n            </configuration>\n        </plugin>\n    </plugins>\n</build>\n```\n(3) IDEA图标Skip Tests。点击选中，再用LifeCycle中的打包就会跳过测试\n(4) 打开配置，找到Build,Exxcution,Deployment–>Build Tools–>Maven–>Runner，在VM option中添加-Dmaven.test.skip=true或者-DskipTests=true，就能在打包时跳过测试。\n(5) 打开配置，找到Build,Exxcution,Deployment–>Build Tools–>Maven–>Runner，在Properties中勾选Skip Test选项。\n\n> [说一个大家都知道的Spring Boot小细节](https://mp.weixin.qq.com/s/JD2wfLGXdujoc9DOJEZFpA)\n> [Maven(五)：resources、profiles标签的实践](https://mp.weixin.qq.com/s/Kt51X_VEeC_EQSw22pqB-A)\n> [Maven最全教程，看了必懂，99%的人都收藏了！](https://mp.weixin.qq.com/s/dEmCRy_9CtRPtOP4DViDHw)\n> [如何利用Maven将代码打包成第三方公共jar包？](https://mp.weixin.qq.com/s/vq6Id8g0IBRVM3xdPRYFbA)\n\n#### Gradle\n\n- implementation，默认的scope,取代compile（已弃用）。implementation的作用域会让依赖在编译和运行时均包含在内，但是不会暴露在类库使用者的编译时。举例，如果我们的类库包含了gson，那么其他人使用我们的类库时，编译时不会出现gson的依赖。\n\n- api,和implementation类似，都是编译和运行时都可见的依赖。但是api允许我们将自己类库的依赖暴露给我们类库的使用者。\n\n- compileOnly和runtimeOnly，这两种顾名思义，一种只在编译时可见，一种只在运行时可见。而runtimeOnly和Maven的provided比较接近。runtimeOnly取代了runtime（不建议使用）\n\n- testImplementation，这种依赖在测试编译时和运行时可见，类似于Maven的test作用域。\n\n- testCompileOnly和testRuntimeOnly，这两种类似于compileOnly和runtimeOnly，但是作用于测试编译时和运行时。\n\n- compileClasspath延伸compile,compileOnly,implementation。编译类路径，在编译源代码时使用。由任务使用compileJava。runtimeClasspath延伸runtimeOnly,runtime,implementation，运行时类路径包含实现的元素以及仅运行时元素。\n\n- annotationProcessor编译期间使用的注释处理器。\n\n### String相关\n\n- String.intern()是一个Native方法，底层调用C++的StringTable::intern方法实现。当通过语句str.intern()调用intern()方法后，JVM就会在当前类的常量池中查找是否存在与str等值的String，若存在则直接返回常量池中相应Strnig的引用；若不存在，则会在常量池中创建一个等值的String，然后返回这个String在常量池中的引用,通俗的讲，是将字符串放入常量池中。是一种手动将字符串加入常量池中的方法\n- String.toUpperCase()会在内存中新建一个字符串并不会修改原来的字符串\n- String对象new出来的字符串是放在堆中，直接赋值的字符串是放在常量池中的。\n对字符串做拼接操作，即做“+”运算，分两种情况：\n①表达式右边是纯字符串常量，则存放在常量池中\n②表达式右边存在字符串引用，则存放在堆中\n```java\nString a = \"s\"; String b = \"b\";\nString c = a+b;//变量形式相加底层new String()不放常量池\nString d = \"s\"+\"b\";//非变量形式相加放到常量池\nString e = \"sb\";//放到常量池\nString f = new String(\"sb\");//不放常量池\nString g = f.intern();//手动放到常量池\nSystem.out.println(d==e);//true\nSystem.out.println(c==e);//false\nSystem.out.println(c==f);//false\nSystem.out.println(c==d);//false\nSystem.out.println(e==f);//false\nSystem.out.println(e==g);//true\n\nString s1 = new String(\"计算机\");\nString s2 = s1.intern();\nString s3 = \"计算机\";\nSystem.out.println(s2);//计算机\nSystem.out.println(s1 == s2);//false，因为一个是堆内存中的String对象一个是常量池中的String对象，\nSystem.out.println(s3 == s2);//true，因为两个都是常量池中的String对\n\nString str1 = \"str\";\nString str2 = \"ing\";\nString str3 = \"str\" + \"ing\";//常量池中的对象\nString str4 = str1 + str2; //在堆上创建的新的对象\nString str5 = \"string\";//常量池中的对象\nSystem.out.println(str3 == str4);//false\nSystem.out.println(str3 == str5);//true\nSystem.out.println(str4 == str5);//false\n\nString a = \"a\";\nString b = a;\nSystem.out.pringln(\"a == b\");true\n```\n\n- String字符串有长度限制，在编译期，要求字符串常量池中的常量不能超过65535，并且在javac执行过程中控制了最大值为65534。\n在运行期，长度不能超过Int的范围，否则会抛异常。\n\n- common-lang3包StringUtils\n```java\nisEmpty// 如果是null或者“”则返回true。\nisBlank// 如果是null或者“”或者空格或者制表符则返回true。isBlank判空更加准确。\n```\n\n> [java.lang.String的+号操作，这个谜终于要解开了！](https://mp.weixin.qq.com/s/ic5K1X6DQnBN37uv9cr-mg)\n> [基础面试，为什么面试官总喜欢问String？](https://mp.weixin.qq.com/s?__biz=Mzg2MDYzODI5Nw==&mid=2247493885&idx=1&sn=024118e8044ce958c0602db7aba2f916&source=41#wechat_redirect)\n> [深入Java源码剖析之字符串常量](https://mp.weixin.qq.com/s/YvHt9uKKOdYZDdipGJXQYA)\n> [浅谈Java中字符串的初始化及字符串操作类](https://mp.weixin.qq.com/s?__biz=MzkzODE3OTI0Ng==&mid=2247491285&idx=1&sn=c9373feb23075bbada302740ec1a1434&source=41#wechat_redirect)\n> [5个刁钻的String面试题！](https://mp.weixin.qq.com/s/xJnQq8voJvAyyyaBkrvc8A)\n> [为什么String是不可变的？](https://mp.weixin.qq.com/s/P-ijDf5IqtAwnFzDErQOyQ)\n> [90%的同学都没搞清楚的Java字符串常量池问题（图文并茂）](https://mp.weixin.qq.com/s/fwRcDog9_EU3nCEmDF_jcw)\n> [Java中你以为的String其实并不完全正确](https://mp.weixin.qq.com/s?__biz=Mzg2MDYzODI5Nw==&mid=2247494306&idx=1&sn=9184273c5833a9a363b4ebb85163c1a5&source=41#wechat_redirect)\n> [再议String-字符串常量池与String.intern()](https://mp.weixin.qq.com/s/TuGw1tDVj53hIurRfA360g)\n> [Java 9为何要将String的底层实现由char改成了byte?](https://mp.weixin.qq.com/s/AWVJIvOAMERmw7lU1th4iQ)\n> [关于Java字符串的全部，都在这份手册里了](https://mp.weixin.qq.com/s/iYO16IddRqAqn3D5VhURQQ)\n> [面试被问到了String相关的几道题，你能答上来吗？](https://mp.weixin.qq.com/s/wR-HmIn8nO2UICVmJeqgGw)\n\n### try、catch、finally\n\n1. try中的return语句先执行了但并没有立即返回，等到finally执行结束后再return\n2. finally块中的return语句会覆盖try块中的return返回\n3. 如果finally语句中没有return语句覆盖返回值，那么原来的返回值可能因为finally里的修改而改变也可能不变\n4. try块里的return语句在异常的情况下不会被执行，这样具体返回哪个看情况\n5. 当发生异常后，catch中的return执行情况与未发生异常时try中return的执行情况完全一样\n6. finally块的语句在try或catch中的return语句执行之后返回之前执行且finally里的修改语句可能影响也可能不影响try或catch中return已经确定的返回值，若finally里也有return语句则覆盖try或catch中的return语句直接返回。\n\n> [finally是在return之前还是之后执行](https://mp.weixin.qq.com/s/0fdehvtTYz8Off52YOT8Qg)\n> [面试官太难伺候？一个try-catch问出这么多花样](https://mp.weixin.qq.com/s/qYvMpbOXcJ77XQbK_MC8MA)\n\n### 几个经典问题\n\n#### equals和hashcode的关系\n\n1. 如果两个对象相同（即用equals比较返回true），那么它们的hashCode值一定要相同；\n2. 如果两个对象的hashCode相同，它们并不一定相同(即用equals比较返回false)\n\n#### 重写equals为什么需要重写hashcode\n\n比较对象先比较hashcode，hashcode相同在比较equals。如果equals为true而hashcode不同的话就会造成hashmap的key可能会重复，因为jdk判断hashmap的key是否为重复首先判断hashcode是否一致，不一致的话直接判定不是同一个对象，而equals则判定对象为一个对象违背了hashmap的设计原则，重写hashCode方法，是为了在一些算法中避免我们不想要的冲突和碰撞\n\n#### Java为什么不支持多重继承\n\n有两个类B和C继承自A；假设B和C都继承了A的方法并且进行了覆盖，编写了自己的实现；假设D通过多重继承继承了B和C，那么D应该继承B和C的重载方法，那么它应该继承的是B的还是C的？这就陷入了矛盾，所以Java不允许多重继承。\n\n#### Java8中的接口和抽象类到底还有啥区别？\n\n既然接口都能写默认方法了，那么还要抽象类干嘛呢？\n区别1：首先抽象类是一个“类”，而接口只是一个“接口”，两者的概念和应用场景不一样，这也是抽象类和接口的主要区别。\n区别2：即使在Java8中接口也能写实现方法了，但却不能写构造方法，而在抽象类是可以写构造方法的，意味着抽象类是参与类的实例化过程的，而接口则不是。抽象类不可以new\n区别3：抽象类可以有自己的各种成员变量，并且可以通过自己的非抽象方法进行改变，而接口中的变量默认全是public static final修饰的，意味着都是常量，并且不能被自己和外部修改。\n区别4：接口可以实现多继承，而抽象类只能单继承\n\n#### Comparable和Comparator区别\n\n- 一个类实现了Comparable接口，意味着该类的对象可以直接进行比较（排序），但比较（排序）的方式只有一种，很单一。一个类如果想要保持原样，又需要进行不同方式的比较（排序），就可以定制比较器（实现Comparator接口）。\n- Comparable接口在java.lang包下，而Comparator接口在java.util包下。\n- 如果对象的排序需要基于自然顺序，请选择Comparable，如果需要按照对象的不同属性进行排序，请选择Comparator\n\n#### 过滤器和拦截器的区别\n\n1. Filter需要在web.xml中配置，依赖于Servlet；Interceptor需要在SpringMVC中配置，依赖于框架；\n3. Filter的执行顺序在Interceptor之前\n3. 两者的本质区别：拦截器（Interceptor）是基于Java的反射机制，而过滤器（Filter）是基于函数回调。从灵活性上说拦截器功能更强大些，Filter能做的事情，都能做，而且可以在请求前，请求后执行，比较灵活。Filter主要是针对URL地址做一个编码的事情、过滤掉没用的参数、安全校验（比较泛的，比如登录不登录之类），太细的话，还是建议用interceptor。不过还是根据不同情况选择合适的。\n5. 拦截器可以访问action上下文、值栈里的对象，而过滤器不能访问。\n6. 在action的生命周期中，拦截器可以多次被调用，而过滤器只能在容器初始化时被调用一次。init(),destroy()各一次，dofilter()多次\n7. 拦截的请求范围不同,过滤器几乎可以对所有进入容器的请求起作用，而拦截器只会对Controller中请求或访问static目录下的资源请求起作用。\n8. 触发时机不同，过滤器Filter是在请求进入容器后，但在进入servlet之前进行预处理，请求结束是在servlet处理完以后。拦截器Interceptor是在请求进入servlet后，在进入Controller之前进行预处理的，Controller中渲染了对应的视图之后请求结束。\n\n> [过滤器、监听器、拦截器的区别](https://zhuanlan.zhihu.com/p/69060111?utm_source=wechat_timeline&amp;utm_medium=social&amp;utm_oi=1040923520439672832&amp;from=timeline)\n> [一口气说出过滤器和拦截器6个区别](https://mp.weixin.qq.com/s/PzQlCjLLM1fjPc0y4poQOw)\n> [SpringBoot过滤器、拦截器、监听器对比及使用场景](https://mp.weixin.qq.com/s/kZx10tnBJ8a6xtDgQsN1vQ)\n\n\n### 泛型\n\n- `List<? extends Object >`相当于`List<?>`\n在Java集合框架中，对于参数值是未知类型的容器类，只能读取其中元素，不能向其中添加元素,因为，其类型是未知，所以编译器无法识别添加元素的类型和容器的类型是否兼容，唯一的例外是NULL\n\n- `List<? extends T>`可以添加元素添加的元素为T或T的子类\n- `List<? super T>`可以添加元素添加的元素为T或T的父类\n\n> [为了让你理解Java泛型，费了好大心思](https://mp.weixin.qq.com/s/ilqFpf5kE0XzJnOv9SsX7Q)\n> [泛型中的T、E、K、V，还记得嘛？](https://mp.weixin.qq.com/s/5dWCpdyXNP5PeCZM8AQVAg)\n\n### CSRF和XSS的区别\n\n1. CSRF是跨站请求伪造;XSS是跨域脚本攻击。\n2. CSRF需要用户先登录网站A,获取cookie;XSS不需要登录。\n3. CSRF是利用网站A本身的漏洞,去请求网站A的api;XSS是向网站A注入JS代码,然后执行JS里的代码,篡改网站A的内容。（XSS利用的是站点内的信任用户，而CSRF则是通过伪装来自受信任用户的请求来利用受信任的网站。你可以这么理解CSRF攻击：攻击者盗用了你的身份，以你的名义向第三方网站发送恶意请求。）\n\n### web.xml详解\n\n**<context-param\\>** 元素含有一对参数名和参数值，用作应用的Servlet上下文初始化参数，参数名在整个Web应用中必须是惟一的，在web应用的整个生命周期中上下文初始化参数都存在，任意的Servlet和jsp都可以随时随地访问它。在JSP网页中可以使用下列方法来取得：\n\n```js\n${initParam.param_name}\n```\n若在Servlet可以使用下列方法来获得：\n```java\nString param_name=getServletContext().getInitParamter(\"param_name\");\n```\nServlet的ServletConfig对象拥有该Servlet的ServletContext的一个引用，所以可这样取得上下文初始化参数：\n```java\ngetServletConfig().getServletContext().getInitParameter()\n```\n也可以在Servlet中直接调用\n```java\ngetServletContext().getInitParameter()，\n```\n两者是等价的。\n\n**<listener\\>** 为web应用程序定义监听器，监听器用来监听各种事件，比如：application和session事件，所有的监听器按照相同的方式定义，功能取决去它们各自实现的接口，常用的Web事件接口有如下几个：\n\n1. ServletContextListener：用于监听Web应用的启动和关闭；\n2. ServletContextAttributeListener：用于监听ServletContext范围（application）内属性的改变；\n3. ServletRequestListener：用于监听用户的请求；\n4. ServletRequestAttributeListener：用于监听ServletRequest范围（request）内属性的改变；\n5. HttpSessionListener：用于监听用户session的开始和结束；\n6. HttpSessionAttributeListener：用于监听HttpSession范围（session）内属性的改变\n\n**<load-on-startup\\>1</load-on-startup/\\>**\nload-on-startup元素标记容器是否应该在web应用程序启动的时候就加载这个servlet，(实例化并调用其init()方法)。它的值必须是一个整数，表示servlet被加载的先后顺序。如果该元素的值为负数或者没有设置，则容器会当Servlet被请求时再加载。如果值为正整数或者0时，表示容器在应用启动时就加载并初始化这个servlet，值越小，servlet的优先级越高，就越先被加载。值相同时，容器就会自己选择顺序来加载创建Servlet实例有两个时机较小的优先加载\n\n\n### 解决跨域的几种方案\n\n服务器没有同源策略，浏览器的同源策略是为了保证浏览器的安全性，实际上跨域请求会将请求发送到服务器，只不过在返回数据的时候浏览器发出跨域警告\n\n1. JSONP\n2. CORS\n服务器中编写过滤器允许跨域访问\n```java\nHttpServletResponse httpResponse = (HttpServletResponse) response;\n//该字段必填。它的值要么是请求时Origin字段的具体值，要么是一个*，表示接受任意域名的请求\nhttpResponse.setHeader(\"Access-Control-Allow-Origin\", \"*\");\n//该字段必填。它的值是逗号分隔的一个具体的字符串(GET,POST...)或者*，表明服务器支持的所有跨域请求的方法。注意，返回的是所有支持的方法，而不单是浏览器请求的那个方法。这是为了避免多次\"预检\"请求\nhttpResponse.setHeader(\"Access-Control-Allow-Methods\", \"*\");\n//该字段可选，用来指定本次预检请求的有效期，单位为秒。在有效期间，不用发出另一条预检请求。\nhttpResponse.setHeader(\"Access-Control-Max-Age\", \"3600\");\nhttpResponse.setHeader(\"Access-Control-Allow-Headers\",\n\"Origin,X-Requested-With, Content-Type, Accept, Connection, User-Agent, Cookie\");\nhttpResponse.setHeader(\"Access-Control-Allow-Credentials\", \"true\");\nhttpResponse.setHeader(\"Content-type\", \"application/json\");\nhttpResponse.setHeader(\"Cache-Control\", \"no-cache, must-revalidate\");\nchain.doFilter(request,httpResponse);\n```\n3. 反向代理工具如nginx\n4. 服务器没有同源策略，使用HTTPClient等技术\n5. 控制器添加@CrossOrigin注解\n\n### 收藏文章\n\nJava\n\n- [java关键字名单](https://mp.weixin.qq.com/s?__biz=MzkzODE3OTI0Ng==&mid=2247491305&idx=2&sn=c8b3f2d88d16aa3ebb9f5b674dce0a55&source=41#wechat_redirect)\n- [Class.forName和ClassLoader到底有啥区别](https://mp.weixin.qq.com/s/qvKunFJfDuBesjJUIsJdXg)\n- [说说Java深拷贝和浅拷贝区别](https://mp.weixin.qq.com/s/GOSoJrArIQrctiP3ArG6lA)\n- [面试难缠的深拷贝浅拷贝，这次终于通透了](https://mp.weixin.qq.com/s/SQB4ZteKe7-fHFzL__6cSQ)\n- [简单聊聊对象浅拷贝和深拷贝，真不简单！](https://mp.weixin.qq.com/s/P1mlsdacKGF0vB566fqm3g)\n- [零拷贝技术在Java中为何这么牛](https://mp.weixin.qq.com/s/GSzbiVge-aoDHxYCzCTALw)\n- [什么是零拷贝？](https://mp.weixin.qq.com/s/Fsw01-dXjeS4X8I-zmu8hQ)\n- [天天都会写接口，但它的用途和好处有多少人能说得清楚？](https://mp.weixin.qq.com/s/R7JyIRnRYEEMNKZG-aEoMw)\n- [Java中的接口还可以这样用，你知道吗？](https://mp.weixin.qq.com/s/NN3Zy1RGpwWUIgaatAJ4HQ)\n- [10大Java面试难题，打趴无数面试者！](https://mp.weixin.qq.com/s/Fap6REHGxmnaHOPwp94iRA)※\n- [知名互联网公司Java开发岗面试知识点解析](https://mp.weixin.qq.com/s/efM15hYwgowrprWWNgbYjg)\n- [社招后端20问！](https://mp.weixin.qq.com/s/kbvWCme1T-_3_MaIvgH66g)\n- [Java面试题全梳理](https://mp.weixin.qq.com/s/3cqRu-STSX7aL_3kRQffTQ)\n- [oppo面试题](https://mp.weixin.qq.com/s/UZld1hKUgeBrYFrhQ_wjMQ)\n- [扪心自问！一百多道难搞的面试题，你能答对了多少](https://mp.weixin.qq.com/s/aj-WmrJsWnH2Utsbhurnbg)\n- [B站面试经历](https://mp.weixin.qq.com/s/ZbP8kBTKpnxKsogZyQL1nw)\n- [Java基础夺命连环16问](https://mp.weixin.qq.com/s/_JsVu1Vcj8kCl1gZc8tNXg)\n- [15个常见的Node.js面试问题及答案](https://mp.weixin.qq.com/s/-v3PCoVInPYpkhnO9y3SNQ)\n- [30道计网常考面试题含答案总结！血赚！](https://mp.weixin.qq.com/s/OY6oO-1dSxc9Flseal22zw)\n- [2W字！梳理50道经典计算机网络面试题（收藏版）](https://mp.weixin.qq.com/s/PlBwyJ4wpDeZ7J-PKw3sgA)\n- [蚂蚁金服一面：十道经典面试题解析](https://mp.weixin.qq.com/s/6KhtiAdDOljqQ4bzc7pSnw)\n- [美团面试题](https://mp.weixin.qq.com/s/2IUaDr5XRgWoVaGgPGsAxA)\n- [Java中经常被提到的SPI到底是什么](https://mp.weixin.qq.com/s/t92OtlMD_dwk1ldCVNvHBA)\n\n分布式\n\n- [Java分布式面试题集合](https://mp.weixin.qq.com/s/8L713J4zzv_PfDtfdR5CRA)\n- [分布式十二问！万字图文详解！](https://mp.weixin.qq.com/s/TnFsKK77uyfXkckt4m1Ctw)\n- [面试官：谈谈分布式一致性机制，我一脸懵逼。。](https://mp.weixin.qq.com/s/e1xvvdx8oOs8ykIMgSKiuw)\n- [面试绕不开的CAP理论，这篇文章帮你搞定！](https://mp.weixin.qq.com/s/tLfJdZ4BxlXY_wiV6-Hx8w)\n\n其他\n\n- [如何写出让CPU跑得更快的代码](https://mp.weixin.qq.com/s/g6bCfbyJ0NrtSAwN3Xooqw)\n- [程序在计算机中是如何运行起来的](https://mp.weixin.qq.com/s/Ek3gWfzrmWrRz8ypL76UTA)\n- [了解这些软件设计思想，你的思维至少上升一个段位](https://mp.weixin.qq.com/s/3dF8cy9LWh5A0oGWL2p35g)\n- [微信支付、支付宝最全接入指引，看完立刻就可以上手](https://mp.weixin.qq.com/s/emU6QyhRFXaBlkj9jerOMg)\n- [如何防止你的jar被反编译](https://mp.weixin.qq.com/s/AB1GN6UAsHW_nvctH_tuRQ)\n- [PageHelper使用中的一些坑](https://mp.weixin.qq.com/s/2FR4lmNdGmLP4qnD2F0Krw)\n- [一文参透分布式存储系统Ceph的架构设计、集群搭建](https://mp.weixin.qq.com/s/bt5Df-sY3QETgvGp3Kp8Yw)\n- [电商系统架构，常见的9个大坑](https://mp.weixin.qq.com/s/RnR3OyNA8PSz9CrCMP48kw)\n- [给定一个接口，要用户自定义动态实现并上传热部署](https://mp.weixin.qq.com/s/4Yr0J5MPvUQmDTseoJAILQ)\n- [说说布隆过滤器与布谷鸟过滤器及应用场景](https://mp.weixin.qq.com/s/q5p0v2R_FeYq2HwlrqL_xA)\n- [我们为什么要放弃RESTful，选择拥抱GraphQL](https://mp.weixin.qq.com/s/CsUIu48Yyqr0yiir4Xq4XA)\n- [轻松实现word、excel、ppt、txt等办公文件在线预览功能](https://mp.weixin.qq.com/s/m2e8dgW0NLgB9jzYd7nS2A)\n\n下载\n\n- [apache旗下安装包](https://dlcdn.apache.org/)\n- [centos rpm包（cat /etc/centos-release）](http://vault.centos.org/)\n- [PLSQL历史版本下载](https://www.allroundautomations.com/registered-plsqldev/)\n- [onekey ghpst+win7 ghost](https://www.newxitong.com/)\n- [Chrome](https://www.google.cn/chrome/)\n\n工具\n\n- [阿里云开发者藏经阁](https://developer.aliyun.com/topic/ebook)\n- [Postman详细用法示例](https://mp.weixin.qq.com/s/ccRwb3SJBQl3Fhq8pDzl_A)\n- [简单的Postman，还能玩出花](https://mp.weixin.qq.com/s/YTacVjCl90CKcn8tIhM7bQ)\n- [pdf在线转换器(easypdf.com)](https://easypdf.com/cn)\n- [pdf工具（在线签名、转换等）](https://lightpdf.com/zh/)\n- [Linux端口转发](https://blog.csdn.net/Tomorrow_Yesterday/article/details/84605297)\n- [开源中国工具](https://tool.oschina.net/)\n- [QQ浏览器工具箱](https://tool.browser.qq.com/)\n- [amCharts3官方文档](https://www.amcharts.com/docs/v3/reference/)\n- [表格组件神器：bootstrap table](https://www.cnblogs.com/landeanfen/p/4993979.html)\n- [创建地图-百度地图生成器](http://api.map.baidu.com/lbsapi/creatmap/index.html)\n- [面向程序员的精品开源字体](https://mp.weixin.qq.com/s/Dz3Z1S6vIkBksPfVyq7ehQ)\n- [idea使用的30个小技巧](https://mp.weixin.qq.com/s/XsBqQwZBUHfwBJBLgd2NTw)\n\n\n排序算法\n\n> [Java实现的排序算法](https://github.com/xmxe/demo/tree/master/study-demo/src/main/java/com/xmxe/algorithm/sort)\n\n- [漫画：“排序算法”大总结](https://mp.weixin.qq.com/s/teOGQlslb6aP4AQrx7TTzA)\n- [因为排序不明白，被面试官锤了一顿](https://mp.weixin.qq.com/s/hQkdAMICGU2Akonl0upbag)\n- [史上最好的排序和数据结构入门](https://mp.weixin.qq.com/s/8iXAupQIdbRrcsFDuKwgHA)\n\n堆\n\n- [一文告诉你Java集合中「堆」的最佳打开方式](https://mp.weixin.qq.com/s/Wy-DgsZZAeZmI19ZskERRg)\n\n链表\n\n- [一口气搞懂「链表」，就靠这20+张图了](https://mp.weixin.qq.com/s/oyXrWMfOBRBClbCg1j9TyQ)\n\n栈\n\n- [面试官问我什么是「栈」，我随手画了10张图来解释](https://mp.weixin.qq.com/s/E3WDAksQV3qjS_O8ittfBA)\n\n哈希\n\n- [不能错过！简单易懂的哈希表总结](https://mp.weixin.qq.com/s/AgkHMhITtOTf9y3jrrxsfQ)\n- [学生物的女朋友都能看懂的哈希表总结](https://mp.weixin.qq.com/s?__biz=Mzg3Mzc0NjUzMQ==&mid=2247497071&idx=1&sn=3a67993e5e4634ddf0880e6c02a10d4e&source=41#wechat_redirect)\n- [如果世界上只有一种数据结构，我选择哈希！](https://mp.weixin.qq.com/s/YA5Pj6GTMFw0nrIOseMyRg)\n\n算法复杂度\n\n- [复杂度O、Θ、Ω、o、ω，别再傻傻分不清了！](https://mp.weixin.qq.com/s/HgsQUAGWG5GOs288thM8QA)\n- [看完这篇，还不清楚时间复杂度的，请来怼我](https://mp.weixin.qq.com/s/z_pSFJPaEdPsWtV9o-4tUQ)\n- [面试时候说的复杂度都是什么？](https://mp.weixin.qq.com/s/9ZFC_vlMa0bhrXhgvWx9pw)\n\n二叉树\n\n- [漫画：什么是平衡二叉树？](https://mp.weixin.qq.com/s/Tbx-VZxca8Z2U8VpXl6GoA)\n- [不怕面试被问了！二叉树算法大盘点](https://mp.weixin.qq.com/s/T4ObPeuFzdAbkW3R5q_vLA)\n- [看懂这篇文章，玩转二叉查找树](https://mp.weixin.qq.com/s?__biz=MzkzODE3OTI0Ng==&mid=2247491062&idx=1&sn=3baf32b14a438ea29273add1bbae134e&source=41#wechat_redirect)\n- [一文弄懂二叉树三种遍历](https://mp.weixin.qq.com/s/38j1f-UR1uakiav5sPoCIw)\n- [一文高效图解二叉树面试题](https://mp.weixin.qq.com/s/QEM6Arxny0HaUasGRoSxQw)\n- [3分钟看完关于树的故事](https://mp.weixin.qq.com/s?__biz=MzkzODE3OTI0Ng==&mid=2247491085&idx=2&sn=ad6b30d8feb980354f5131d21f3c34b8&source=41#wechat_redirect)\n\n红黑树\n\n- [漫画：什么是红黑树?](https://mp.weixin.qq.com/s/X3zYwQXxq93P_XUzFmKluQ)\n- [一口气写出了7k字的红黑树总结](https://mp.weixin.qq.com/s/uTkORM6bQeUT2XDHHmOc4A)\n- [漫画算法：5分钟搞明白红黑树到底是什么](https://mp.weixin.qq.com/s/MSB-vFGqNWB26kPydBJQmQ)\n- [用超强动静图详解红黑树，简单易懂](https://mp.weixin.qq.com/s/UjP3pHXWnlIf68cplj2RRw)\n- [红黑树是怎么实现的，看这篇真的就够了！](https://mp.weixin.qq.com/s?__biz=MzkzODE3OTI0Ng==&mid=2247491027&idx=1&sn=70566688d09508c7355cfef38b1453a5&source=41#wechat_redirect)\n- [什么是红黑树？看完这篇你就明白了！](https://mp.weixin.qq.com/s/9s6c1sPN7avqwxZC7BsVUQ)\n- [图文详解红黑树，还有谁不会](https://mp.weixin.qq.com/s/Xim1IUQmGT8hKelYZwUdLA)\n\nTomcat\n\n- [面试官:Tomcat的生命周期是什么样子的？](https://mp.weixin.qq.com/s/b3MP9eTvfEdEKw-fL2jeTw)\n- [超详细的Tomcat性能监控及调优教程](https://mp.weixin.qq.com/s/3i8uRr-4EOpFizie2hmogA)\n- [Tomcat配置文件server.xml你有深入了解过没？](https://mp.weixin.qq.com/s?__biz=Mzg2MDYzODI5Nw==&mid=2247494066&idx=2&sn=14302c80a7c6e6480dd7553a5579ce06&source=41#wechat_redirect)\n- [一文拆解Tomcat高并发原理与性能调优](https://mp.weixin.qq.com/s/GR7iB5esMEczn687mwwc5Q)\n- [Tomcat调优和JVM参数优化](https://mp.weixin.qq.com/s/jmqqLbbOt3D--wksCigAOw)\n- [牛逼！硬核图解Tomcat整体架构](https://mp.weixin.qq.com/s/U49ZIWYRvIYYFkL-06aSXA)\n- [Tomcat深入理解](https://mp.weixin.qq.com/s?__biz=Mzg2MDYzODI5Nw==&mid=2247494316&idx=1&sn=fb6eb1b78ed195a977bdfbe92a7cc6f4&source=41#wechat_redirect)\n- [Tomcat请求处理流程](https://mp.weixin.qq.com/s/Miw5K0OyE4oytn4U65Hn5g)\n- [Tomcat有哪些组成部分？讲讲工作原理？](https://mp.weixin.qq.com/s/UilzaN7jQEza7wiwzw4bsg)\n- [教你用Java写一个自己的Tomcat容器，涨姿势了](https://mp.weixin.qq.com/s/RpMXgOi8WsMm19S4YOh1aw)\n\n## gist\n\n### Java相关\n\n#### fastjson转换\n\n```java\n// json = {\"a\":1,\"b\":2,\"c\":[\"d\":3,\"e\":4]}\n// json转JSONObject\nJSONObject a = json.getJSONObject(\"a\");\n// json转JSONArray\nJSONArray b = json.getJSONArray(\"c\");\n// json转对象\nMap<String,Object> map = JSONObject.parseObject(json.get(\"c\"),new TypeReference<Map<String,Object>>(){});\n// 将Java对象序列化为JSON字符串，支持各种各种Java基本类型和JavaBean,避免value为null时过滤掉字段\nJSONObject.toJSONString(data,SerializerFeature.WriteMapNullValue);\nJSON.parse() JSON.parseObject()JSON.parseArray()  json.get() ...\n```\n\n#### 获取路径\n\n```java\n// http://localhost:8080/demo/course/index.jsp\n// 返回：/demo\nrequest.getContextPath()\n// 返回当前页面所在目录下全名称:/course/index.jsp\nrequest.getServletPath()\n// 返回浏览器地址栏地址 http://localhost:8080/demo/course/index.jsp\nrequest.getRequestURL()\n// 返回包含工程名的当前页面全路径：/demo/course/index.jsp\nrequest.getRequestURI()\n// 返回 http://localhost:8080\nrequest.getScheme()+\"://\"+request.getServerName()+\":\"+ request.getServerPort()\n// 返回E:\\apache-tomcat-7.0.82\\webapps\\jnhouse\\\nrequest.getSession().getServletContext().getRealPath(File.separator)\n// 获取项目存放class文件的全路径 F:\\zhongzhu\\WebRoot\\WEB-INF\\classes\nThread.currentThread().getContextClassLoader().getResource(\"/\").getPath()\n//从classpath路径下获取资源并返回一个InputStream供读取文件\n(this)类名.class.getClassLoader().getResourceAsStream(\"name\")\n// 会在当前类所在的包结构下查找相应的资源\n类名.class.getResourceAsStream(\"name\")\n// 请求转发\nreturn “forward:forward2.html”;\n// 重定向\nreturn “redirect:redirect2.html”;\nrequest.getRequestDispatcher(\"url\").forward(request, response)\nresponse.sendRedirect(\"leader.htm\");\n\n```\n\n#### list操作\n\n```java\n// 交集 listA内容变为listA和listB都存在的对象 listB不变\nlistA.retainAll(listB)\n// 差集 listA中存在listB的内容去重 listB不变\nlistA.removeAll(listB)\n// 并集,为了去重，listA先取差集，然后追加全部的listB listB不变\nlistA.removeAll(listB) listA.addAll(listB)\n// 将一个list平均切割2块\nint sublistSize = (list.size() + 1) / 2;\nList<Integer> sublist1 = list.subList(0, sublistSize);\nList<Integer> sublist2 = list.subList(sublistSize, originalList.size());\n// 将切分后的两个子列表分别添加到新的ArrayList中\nresultList1.addAll(sublist1);\nresultList2.addAll(sublist2);\n```\n\n#### 代码分页\n\n```java\nInteger page = map.get(\"page\");\nInteger limit = map.get(\"rows\");\nInteger start = (page-1)*limit;\n//List<Map<String,Object>> subList = list.subList(start,start+limit>list.size()?list.size():start+limit)\nList<Map<String,Object>> subList = list.subList(start,Math.min(start+limit,list.size))\n```\n#### mybatis foreach批量添加/更新写法\n\n```xml\n<insert id=\"insertList\" parameterType=\"map\">\n begin try\n  insert into gs_job_pfr_mx (tag_code,val,time) values\n  <foreach collection=\"list\" item=\"item\" separator=\",\"  index=\"index\">\n   (#{item.tag},#{item.value},GETDATE())\n  </foreach>\n end try\n begin catch\n  update gs_job_pfr_mx set val =\n<foreach collection=\"list\" item=\"item\" separator=\" \"open=\"case tag_code\" close=\"end\">\n  when #{item.tag} then #{item.value}\n </foreach>,\n  time = GETDATE()\n  where tag_code in\n <foreach collection=\"list\" item=\"item\" separator=\",\" open=\"(\" close=\")\">\n    #{item.tag}\n  </foreach>\n end catch\n </insert>\n\n<!-- trim标签-->\n<update id=\"updateYsz\" parameterType=\"tblYsz\">\n    update tblysz set\n        <trim suffixOverrides=\",\">\n            <if test=\"name!=null\">name=#{name},</if>\n            <if test=\"jgdm!=null\">jgdm=#{jgdm},</if>\n            <if test=\"xz!=null\">xz=#{xz},</if>\n        </trim>\n     <where>id=#{id}</where>\n </update>\n\n<!-- choose标签 -->\n<select id=\"selectByIdOrName\" resultMap=\"BaseResultMap\" parameterType=\"com.homejim.mybatis.entity.Student\">\n    select\n    <include refid=\"Base_Column_List\" />\n    from student\n    where 1=1\n    <choose>\n      <when test=\"studentId != null\">\n        and student_id=#{studentId}\n      </when>\n      <when test=\"name != null and name != ''\">\n        and name=#{name}\n      </when>\n      <otherwise>\n        and 1=2\n      </otherwise>\n    </choose>\n  </select>\n\n<!-- resultMap-->\n<resultMap id=\"BaseResultMap\"type=\"com.xmxe.entity.User\">\n<id property=\"id\" column=\"id\" jdbcType=\"INTEGER\" javaType=\"java.lang.IntegerINTEGER\"/>\n<result property=\"username\" column=\"username\" jdbcType=\"VARCHAR\" javaType=\"java.lang.String\"/>\n<result property=\"password\" column=\"password\" jdbcType=\"VARCHAR\" javaType=\"java.lang.String\"/>\n</resultMap>\n\n<sql id=\"commonsSql\">\n\tid,username,password\n</sql>\n\n```\n\n> [MyBatis动态SQL](https://mp.weixin.qq.com/s/-aIum139UB_dt7OIaiccfA)\n> [10种超好用的MyBatis写法](https://mp.weixin.qq.com/s/G7JpSRbEAw0dDbSj3lneOw)\n\n#### mybatis连接数据库\n\n```java\nString mybatisConfig= \"mybatis-config.xml\";\nInputStream is = Resources.getResourceAsStream(mybatisConfig);\nSqlSessionFactory sessionFactory = new SqlSessionFactoryBuilder().build(is);\nSqlSession session = sessionFactory.openSession();\nsession.getMapper(Mapper.class).findByid();\n```\n\n#### 正则表达式\n\n匹配数字，包括小数\n```js\nvar patten = /^[+-]?(0|([1-9]\\d*))(\\.\\d+)?$/\n```\n[+-] 中括号表示其内的内容都是符合要求的匹配，所以这个表示“+”或\"-\"\nMatcher 类中group(0) 表示正则表达式中符合条件的字符串。\nMatcher 类中group(1) 表示正则表达式中符合条件的字符串中的第一个()中的字符串。\nMatcher 类中group(2) 表示正则表达式中符合条件的字符串中的第二个()中的字符串。\nMatcher 类中group(3) 表示正则表达式中符合条件的字符串中的第三个()中的字符串。\ngroup是针对()来说的，group(0)就是指的整个串，group(1)指的是第一个括号里的东西，group(2)指的第二个括号里的东西。\n```java\nString line = \"123ra9040 123123aj234 adf12322ad 222jsk22\";\nString pattern = \"(\\\\d+)([a-z]+)(\\\\d+)\";\n// 创建Pattern对象\nPattern r = Pattern.compile(pattern);\n// 现在创建matcher对象\nMatcher m = r.matcher(line);\n// m.find是否找到正则表达式中符合条件的字符串\nwhile (m.find()) {\n    // 拿到上面匹配到的数据\n    System.out.println(\"Found value: \" + m.group(0) );\n    System.out.println(\"Found value: \" + m.group(1) );\n    System.out.println(\"Found value: \" + m.group(2) );\n    System.out.println(\"Found value: \" + m.group(3) );\n}\n```\ngroup(0)对应着((//d+)([a-z]+)(//d+))所匹配的数据123ra9040或者123123aj234或者222jsk22\ngroup(2)输出的数据是group(0)中所匹配的数据,第二个括号的表达式,也就是([a-z]+)匹配到是数据ra或者aj或者jsk\ngroup(3)输出的数据是group(0)中所匹配的数据,第三个括号的表达式,也就是(//d+)匹配到是数据9040或者234或者22\n\nJS使用正则表达式\n```js\n// 可以和java正则表达式共用，如果不使用new RegExp()，则和java正则表达式书写方式有差异\nvar patten = new RegExp(\"...\")\npatten.text(要匹配的内容)\n```\n\n> [对正则表达式，这么多年你还在害怕吗](https://mp.weixin.qq.com/s/40UCh57rBu9LafTfjEQh1g)\n> [正则表达式手册](https://tool.oschina.net/uploads/apidocs/jquery/regexp.html)\n> [常用正则表达式最强整理（速查手册）](https://mp.weixin.qq.com/s/ABCqcJPNf1VJ7ByGrVlS_A)\n> [给懒人开发者的一份正则表达式指南](https://mp.weixin.qq.com/s/j5rAEyu1k4edpk5T8NuGFw)\n\n#### 冒泡排序\n\n```java\nint[] arr = {2,5,4,1};\nfor(int i=0;i<arr.length-1;i++){\n　　for(int j=0;j<arr.length-1-i;j++){\n　　　　if(arr[j]>arr[j+1]){\n　　　　　　int temp=arr[j];\n　　　　　　arr[j]=arr[j+1];\n　　　　　　arr[j+1]=temp;\n　　　　}\n　　}\n} \n```\n\n#### JDK8 base64转换\n\n```java\nString str = \"str\";\nString encoded = Base64.getEncoder().encodeToString(str.getBytes( StandardCharsets.UTF_8));\nString decoded = new String(Base64.getDecoder().decode(encoded), StandardCharsets.UTF_8);\n```\n\n#### poi excel\n\n```java\npublic void creatExcel(File file,String code,String ts){\n\tString[] header={\"有功功率\",\"时间\",\"pss输出信号\",\"时间\"};\n\tWorkbook wb = new XSSFWorkbook();\n\tSheet sheet = wb.createSheet(\"sheet1\");\n\tRow rowFirst = sheet.createRow(0);\n\tfor(int i = 0;i<header.length;i++){\n\t\tsheet.setColumnWidth(i, 5000);\n\t}\n\tfor(int i = 0;i<header.length;i++){\n\t\tCell cell = rowFirst.createCell(i);\n\t\tcell.setCellValue(header[i]);\n\t}\n\tfor(int i =0;i<maxSize;i++){\n\t\tRow row = sheet.createRow(i+1);\n\t\trow.createCell(0).setCellValue(\"\");\n\t\trow.createCell(1).setCellValue(\"\");\n\t\trow.createCell(2).setCellValue(\"\");\n\t\trow.createCell(3).setCellValue(\"\");\n\t}\n    try{// 指定本地文件流\n\t    OutputStream os = new FileOutputStream(file);\n        // excel写入\n        wb.write(os);\n        os.close();\n\t}catch(Exception e){\n\t\te.printStackTrace();\n\t}\n}\n\npublic void excel(HttpServletRequest request,HttpServletResponse response){\n\tString[] handers = {\"id\",\"书名\",\"作者\",\"价格\"};\n\tList<Book> list = masterMapper.querySome(null,1,5);\n\ttry{\n\t\tString filedisplay = \"test.xlsx\";\n\t\tfiledisplay = URLEncoder.encode(filedisplay, \"UTF-8\");\t\t\t\n\t\t//由浏览器指定下载路径\n\t\t//response.reset();\t\t\t\n\t\t//response.setContentType(\"application/x-download\");\n\t\t//response.setContentType(\"application/vnd.ms-excel;charset=utf-8\");\n\t\tresponse.addHeader(\"Content-Disposition\", \"attachment;filename=\"+ filedisplay);\n\t\trequest.setCharacterEncoding(\"UTF-8\");\n\t\tresponse.setContentType(\"APPLICATION/OCTET-STREAM\");\n\t\tresponse.setHeader(\"Content-Dispostion\",\"attachment;filename=\".concat(filedisplay));\n\t\t\t\n\t\tHSSFWorkbook wb = new HSSFWorkbook();//创建工作簿\n\t\tHSSFSheet sheet = wb.createSheet(\"操作\");//第一个sheet\n\t\tHSSFRow rowFirst = sheet.createRow(0);//第一个sheet第一行为标题\n\t\trowFirst.setHeight((short) 500);\n        HSSFCellStyle cellStyle = wb.createCellStyle();// 创建单元格样式对象\n        cellStyle.setAlignment(HorizontalAlignment.CENTER); // 居中\n        cellStyle.setVerticalAlignment(VerticalAlignment.CENTER);\n\t\tfor (int i = 0; i < handers.length; i++) {\n\t\t  sheet.setColumnWidth(i, 4000);// 设置列宽\n\t\t}\n\t\t//写标题了\n\t\tfor (int i = 0; i < handers.length; i++) {\n\t\t\t//获取第一行的每一个单元格\n\t\t\tHSSFCell cell = rowFirst.createCell(i);\n\t\t\t//往单元格里面写入值\n\t\t\tcell.setCellValue(handers[i]);\n\t\t\tcell.setCellStyle(cellStyle);\n\t\t}\n\t\tfor (int i = 0; i < list.size(); i++) {\n\t\t\tBook u = list.get(i);\t\t\t\n\t\t\t//创建数据行\n\t\t\tHSSFRow row = sheet.createRow(i + 1);\t\t\t\t\n\t\t\trow.setHeight((short) 400); // 设置每行的高度\n\t\t\t//设置对应单元格的值\n\t        row.createCell(0).setCellValue(u.getId());\n            row.getCell(0).setCellStyle(cellStyle);\n            row.createCell(1).setCellValue(u.getBookname());\n            row.getCell(1).setCellStyle(cellStyle);\n            row.createCell(2).setCellValue(u.getBookauthor());\n            row.getCell(2).setCellStyle(cellStyle);\n            row.createCell(3).setCellValue(u.getBookprice());\n            row.getCell(3).setCellStyle(cellStyle);\n\n\t}\n        OutputStream os = response.getOutputStream();\n        wb.write(os);\n        os.close();\n        wb.close();\n\t}catch(Exception e){\n\t\te.printStackTrace();\n\t}\n}\n```\n\n#### ServletRequest常用方法\n\n```java\nObject getAttribute(String name)\t\n// 以Object形式返回指定属性的值，如果不存在给定名称的属性，则返回null。\nEnumeration\tgetAttributeNames()\t\n// 返回包含此请求可用属性的名称的Enumeration。如果该请求没有可用的属性，则此方法返回一个空的Enumeration。\nString getCharacterEncoding()\t\n// 返回此请求正文中使用的字符编码的名称。如果该请求未指定字符编码，则此方法返回null\nvoid setCharacterEncoding(String env)\t\n// 重写此请求正文中使用的字符编码的名称。必须在使用getReader() 读取请求参数或读取输入之前调用此方法。否则，此方法没有任何效果。\nint getContentLength()\t\n// 返回请求正文的长度（以字节为单位），并使输入流可以使用它，如果长度未知，则返回-1。对于HTTP servlet，返回的值与CGI变量 CONTENT_LENGTH的值相同。\nString getContentType()\t\n// 返回请求正文的MIME类型，如果该类型未知，则返回null。对于HTTP servlet，返回的值与CGI变量CONTENT_TYPE的值相同。\nServletInputStream getInputStream()\t\n// 使用ServletInputStream以二进制数据形式获取请求正文。可调用此方法或getReader读取正文，而不是两种方法都调用。\nString getParameter(String name)\t\n// 以String形式返回请求参数的值，如果该参数不存在，则返回null。请求参数是与请求一起发送的额外信息。对于HTTP servlet，参数包含在查询字符串或发送的表单数据中。\nEnumeration\tgetParameterNames()\t\n// 返回包含此请求中所包含参数的名称的String对象的Enumeration。如果该请求没有参数，则此方法返回一个空的Enumeration。\nString[] getParameterValues(String name)\t\n// 返回包含给定请求参数拥有的所有值的String对象数组，如果该参数不存在，则返回null。\nMap<K, V> getParameterMap()\t\n// 返回此请求的参数的 java.util.Map。请求参数是与请求一起发送的额外信息。对于HTTP servlet，参数包含在查询字符串或发送的表单数据中。\nString getProtocol()\t\n// 以protocol/majorVersion.minorVersion的形式（例如HTTP/1.1）返回请求使用的协议的名称和版本。对于HTTP servlet，返回的值与CGI变量SERVER_PROTOCOL的值相同。\nString getScheme()\t\n// 返回用于发出此请求的方案的名称，例如http、https或ftp。不同方案具有不同的构造URL的规则，这一点已在RFC 1738中注明。\nString getServerName()\t\n// 返回请求被发送到的服务器的主机名。它是Host头值“:”（如果有）之前的那部分的值，或者解析的服务器名称或服务器IP地址。\nint\tgetServerPort()\t// 返回请求被发送到的端口号。它是Host头值“:”（如果有）之后的那部分的值，或者接受客户端连接的服务器端口。\nBufferedReader getReader()\t\n// 使用BufferedReader以字符数据形式获取请求正文。读取器根据正文上使用的字符编码转换字符数据。可调用此方法或getInputStream读取正文，而不是两种方法都调用。\nString getRemoteAddr()\t\n// 返回发送请求的客户端或最后一个代理的Internet Protocol (IP)地址。对于HTTP servlet，返回的值与CGI变量REMOTE_ADDR的值相同。\nString getRemoteHost()\t\n// 返回发送请求的客户端或最后一个代理的完全限定名称。如果引擎无法或没有选择解析主机名（为了提高性能），则此方法返回以点分隔的字符串形式的IP地址。对于HTTP servlet，返回的值与CGI变量REMOTE_HOST的值相同。\nvoid setAttribute(String name, Object o)\t\n// 存储此请求中的属性。在请求之间重置属性。此方法常常与RequestDispatcher一起使用。\nvoid removeAttribute(String name)\t\n// 从此请求中移除属性。此方法不是普遍需要的，因为属性只在处理请求期间保留。\nLocale getLocale()\t\n// 基于Accept-Language头，返回客户端将用来接受内容的首选Locale。如果客户端请求没有提供Accept-Language头，则此方法返回服务器的默认语言环境。\nEnumeration\tgetLocales()\t\n// 返回Locale对象的Enumeration，这些对象以首选语言环境开头，按递减顺序排列，指示基于Accept-Language头客户端可接受的语言环境。如果客户端请求没有提供Accept-Language头，则此方法返回包含一个Locale的Enumeration，即服务器的默认语言环境。\nboolean\tisSecure()\t\n// 返回一个boolean值，指示此请求是否是使用安全通道（比如HTTPS）发出的。\nRequestDispatcher getRequestDispatcher(String path)\t\n// 返回一个RequestDispatcher对象，它充当位于给定路径上的资源的包装器。可以使用RequestDispatcher对象将请求转发给资源，或者在响应中包含资源。资源可以是动态的，也可以是静态的。\nString getRealPath(String path)\t\n// 从Java Servlet API的版本2.1起，请改用ServletContext#getRealPath\nint getRemotePort()\t\n// 返回发送请求的客户端或最后一个代理的Internet Protocol (IP)源端口。\nString getLocalName()\t\n// 返回接收请求的Internet Protocol (IP)接口的主机名。\nString getLocalAddr()\t\n// 返回接收请求的接口的Internet Protocol (IP)地址。\nint getLocalPort()\t\n// 返回接收请求的接口的Internet Protocol (IP)端口号。\n```\n\n#### HttpServletRequest\n\n```java\nCookies\tgetCookies()\t\n// 返回包含客户端随此请求一起发送的所有Cookie对象的数组。\nlong getDateHeader(String name)\t\n// 以表示Date对象的long值的形式返回指定请求头的值。\nString getHeader(String name)\t\n// 以String的形式返回指定请求头的值。\nEnumeration\tgetHeaders(String name)\t\n// 以String对象的Enumeration的形式返回指定请求头的所有值。\nEnumeration\tgetHeaderNames()\t\n// 返回此请求包含的所有头名称的枚举。如果该请求没有头，则此方法返回一个空枚举。\nint getIntHeader(String name)\t\n// 以int的形式返回指定请求头的值。\nString getMethod()\t\n// 返回用于发出此请求的HTTP方法的名称，例如GET、POST或PUT。返回的值与CGI变量REQUEST_METHOD的值相同。\nString getPathInfo()\t\n// 返回与客户端发出此请求时发送的URL相关联的额外路径信息。额外路径信息位于servlet路径之后但在查询字符串之前，并且将以“/”字符开头。\nString getPathTranslated()\t\n// 返回在servlet名称之后但在查询字符串之前的额外路径信息，并将它转换为实际路径。返回的值与CGI变量PATH_TRANSLATED的值相同。\nString getContextPath()\t\n// 返回请求URI指示请求上下文的那一部分。请求URI中首先出现的总是上下文路径。路径以“/”字符开头但不以“/”字符结束。对于默认（根）上下文中的servlet，此方法返回“”。容器不会解码此字符串。\nString getQueryString()\t\n// 返回包含在请求URL中路径后面的查询字符串。如果URL没有查询字符串，则此方法返回null。返回的值与CGI变量QUERY_STRING的值相同。\nString getRequestedSessionId()\t\n// 返回客户端指定的会话ID。此值可能不同于此请求的当前有效会话的ID。如果客户端没有指定会话ID，则此方法返回null。\nString getRequestURL()\t\n// 重新构造客户端用于发出请求的URL。返回的URL包含一个协议、服务器名称、端口号、服务器路径，但是不包含查询字符串参数。\nString getServletPath()\t\n// 返回此请求调用servlet的URL部分。此路径以“/”字符开头，包括servlet名称或到servlet的路径，但不包括任何额外路径信息或查询字符串。返回的值与CGI变量SCRIPT_NAME的值相同。\nHttpSession\tgetSession(boolean create)\t\n// 返回与此请求关联的当前HttpSession，如果没有当前会话并且create为true，则返回一个新会话。\nHttpSession\tgetSession()\t\n// 返回与此请求关联的当前会话，如果该请求没有会话，则创建一个会话。\nboolean\tisRequestedSessionIdValid()\t\n// 检查请求的会话ID是否仍然有效。\nboolean\tisRequestedSessionIdFromCookie()\t\n// 检查请求的会话ID是否是作为cookie进入的。\nboolean\tisRequestedSessionIdFromURL()\t\n// 检查请求的会话ID是否是作为请求URL的一部分进入的。\n```\n\n#### 解决会话重放攻击demo\n\n```js\nvar random = Math.random()*1000000000000000;\n```\n```java\nString random = request.getParameter(\"random\");\nList list = (ArrayList)request.getSession().getAttribute(\"randoms\")\nif(list.contains(random)) {\n\treturn null;\n}\nlist.add(random);\nrequest.getSession().setAttribute(\"randoms\")\n```\n处理流程:请求带一个随机数，后台从session中取出存放这个随机数的list，判断list里面是否包含这个随机数，如果包含，证明请求被重复发送，不做处理，如果不包含，则证明是第一次请求，将随机数放进list放入session里面，之后处理业务逻辑\n\n### 前端相关\n\n#### jsp获取session数据\n\n```js\n<%=session.getAttribute(\"name\")%>\nor\n${sessionScope.name}\n```\n\n#### easyui combobox下拉框设置checkbox全选\n\n```js\nfunction initCombobox(id,data){\n\tvar value = \"\";\n\t//加载下拉框复选框\n\t$('#'+id).combobox({\n        data:data, //后台获取下拉框数据的url\n        method:'post',\n        panelHeight:200,//设置为固定高度，combobox出现竖直滚动条\n        valueField:'id',\n        textField:'text',\n        multiple:true,\n        editable:false,\n        formatter: function (row) { //formatter方法就是实现了在每个下拉选项前面增加checkbox框的方法\n            var opts = $(this).combobox('options');\n            var checkbox_id = row.check_id;\n            if(checkbox_id){\n            \treturn '<input type=\"checkbox\" class=\"combobox-checkbox\" id=\"' + checkbox_id +'\">' + row[opts.textField]\n            }\n            return '<input type=\"checkbox\" class=\"combobox-checkbox\">' + row[opts.textField]\n        },\n        onLoadSuccess: function () {//下拉框数据加载成功调用\n            var opts = $(this).combobox('options');\n            var target = this;\n            var values = $(target).combobox('getValues');//获取选中的值的values\n            $.map(values, function (value) {\n                var el = opts.finder.getEl(target, value);\n                el.find('input.combobox-checkbox')._propAttr('checked', true); \n            })\n        },\n        onSelect: function (row) { //选中一个选项时调用\n        \tvar opts = $(this).combobox('options');\n            //当点击所有时，则勾中所有的选项\n            if (row.text === \"全选\") {\n            \tvar data = $(\"#\"+id).combobox('getData');;\n            \tvar show_type = $(\"#show_type\").combobox('getValue');\n                for (var i = 0; i < data.length; i++) {\n                \t//获取选中的值的values\n                    $(\"#\"+id).val($(this).combobox('getValues'));\n                    // 选择全部和按照机组展示全选时才选中发电类型 机组类型 供热状态 否则不选择这三个\n                \tif(show_type != '6' && show_type != ''){\n                \t\tvar field = data[i][opts.valueField];\n                \t\tif(field == 'fdlx' || field == 'grqk' || field == 'unit_status'){\n                \t\t\tcontinue;\n                \t\t}\n                \t}\n        \t\t   //设置选中值所对应的复选框为选中状态\n                    var el = opts.finder.getEl(this, data[i][opts.valueField]);\n                    el.find('input.combobox-checkbox')._propAttr('checked', true);\n                }\n                var list = [];\n                $.map(opts.data, function (opt) {\t\n                \t// 选择全部和按照机组展示全选时才选中发电类型 机组类型 供热状态 否则不选择这三个\n                \tif(show_type != '6' && show_type != ''){\t\n                \t\tif(opt.id != 'fdlx' && opt.id != 'grqk' && opt.id != 'unit_status'){\n                \t\t\tlist.push(opt.id);\n                \t\t}\n                \t}else{\n                \t\tlist.push(opt.id);\n                \t}\n                    \n                });\n                $(\"#\"+id).combobox('setValues', list); // combobox全选\n            \n            } else {\n                //获取选中的值的values\n                $(\"#\"+id).val($(this).combobox('getValues'));\n    \t\t   //设置选中值所对应的复选框为选中状态\n                var el = opts.finder.getEl(this, row[opts.valueField]);\n                el.find('input.combobox-checkbox')._propAttr('checked', true);\n            }\n\n        },\n        onUnselect: function (row) {//不选中一个选项时调用\n            var opts = $(this).combobox('options');\n            if (row.text === \"全选\") {\n                var a = $(\"#\"+id).combobox('getData');\n                for (var i = 0; i < a.length; i++) {\n                \t $(\"#\"+id).val($(this).combobox('getValues'));\n                     var el = opts.finder.getEl(this, data[i][opts.valueField]);\n                     el.find('input.combobox-checkbox')._propAttr('checked', false);\n                }\n                $(\"#\"+id).combobox('clear');//清空选中项\n            } else {\n            \t //获取选中的值的values\n                $(\"#\"+id).val($(this).combobox('getValues'));\n                var el = opts.finder.getEl(this, row[opts.valueField]);\n                el.find('input.combobox-checkbox')._propAttr('checked', false);\n\n            }\n        }\n    });\n}\n```\n\n### 其他\n\n#### 二叉树遍历\n\n前序遍历A-B-D-F-G-H-I-E-C\n中序遍历F-D-H-G-I-B-E-A-C\n后序遍历F-H-I-G-D-E-B-C-A\n前序(根左右)，中序(左根右)，后序(左右根)\n\n![](/images/ecs.jpg)\n\n#### py import\n\n```python\nimport math\nprint math.pi\n# 等价于\nfrom math import pi\nprint pi\n```\n\n#### excel查看路径和文件名\n```\n在任一单元格输入=CELL(\"filename\")即可\n```\n\n#### windows杀掉进程\n\n```bash\ntaskkill /pid pid\n# /f 强制\n# /t 终止进程和他启用的子进程\ntaskkill /? # 查看相关用法\ntasklist # 查看所有进程\n```\n\n#### cron表达式\n\n3/12 第三分钟开始,每12分钟触发\n12,15,17 第12分钟、15分钟、17分钟触发\n3-19 看使用在哪个域 如果在minute域则表示在第3分钟到第19分钟每分钟触发一次 在second域则表示从第3秒到第19秒每秒触发一次\n24 43 2 ? 1-12 3 * 每周三2点43分24秒触发一次\n\n## 进制转换\n\n### 十进制转换二进制\n\n一直除以2最后的商+余数开始排序\n\n5/2=2余1，2/2 =1余0，所以5的二进制为101\n\n6/2=3余0，3/2=1余1，所以6的2进制是110\n\n### 二进制转换十进制\n\n1         0          1\n\n1* 2的2次方 + 0 * 2的1次方 + 1 * 2 的0次方 = 5\n\n### 运算符\n\n#### \"<<\"左移运算符\n\n**转换为二进制后左移 例:2<<3 即2的二进制左移三位**\n\n2的二进制为2/2=1余0 ， 10左移三位后补0 （00000010）（00010000）\n\nn<<3 可以转换为 n * 2 ^ 3\n\n#### \">>\"右移运算符\n\n**转换为二进制后右移 例:8>>3 即8的二进制右移三位**\n\n8的二进制为8/2=4余0 ,4/2=2余0, 2/2=1余0, 1000右移三位（删除后三位） （0000001000）（000000001）为1\n\nn>>3 可以转换为 n/2^3\n\n#### & 运算符\n\n如果相对应位都是1，则结果为1，否则为0\n\n#### | 运算符\n\n如果相对应位都是0，则结果为0，否则为1\n\n#### ^ 运算符\n\n如果相对应位值相同，则结果为0，否则为1\n\n#### 比较\n\n^: 可以不借助第三块空间的方式交换两个变量的值\n&: 在某些情况下可以取代%的运算\n\n某些情况：当我们拿着一个正数去%上2的n次方数的时候,其实结果等价于我们拿着这个正数&上2的n次方数-1的结果\n\nx % 2(n) == x & 2(n)-1\n\n17 % 4 == 17 & 3\n\n99 % 64 == 99 & 63\n\n### 文章\n\n- [你可能不知道的位运算技巧](https://mp.weixin.qq.com/s/INYjYfwhPU7uZcNmzBTpNQ)\n\n## 数据库相关知识点\n\n### SQL查询慢的原因\n\n1. sql没加索引\n2. sql索引不生效:\n    - 隐式的类型转换，索引失效,\n    - 查询条件包含or，可能导致索引失效,\n    - like通配符可能导致索引失效,\n    - 查询条件不满足联合索引的最左匹配原则,\n    - 在索引列上使用mysql的内置函数,\n    - 对索引进行列运算（如，+、-、\\*、/）,索引不生效,\n    - 索引字段上使用（!=或者<>），索引可能失效,\n    - 索引字段上使用is null,is not null，索引可能失效,\n    - 左右连接，关联的字段编码格式不一样,\n    - 优化器选错了索引\n3. limit深分页问题\n4. 单表数据量太大\n5. join或者子查询过多\n6. in元素过多\n7. 数据库在刷脏页\n8. order by文件排序\n9. 拿不到锁\n10. delete + in子查询不走索引！\n11. group by使用临时表\n12. 系统硬件或网络资源\n\n> [盘点MySQL慢查询的12个原因](https://mp.weixin.qq.com/s/qCA7hICMktxeYO7Jc0oVVg)\n\n### SQL优化\n\n1. 查询SQL尽量不要使用`select *`，而是select具体字段。\n    > [为啥不建议使用Select *？](https://mp.weixin.qq.com/s/2ActOr3bivQSMdyIZA5mbw)\n\n2. 如果知道查询结果只有一条或者只要最大/最小一条记录，建议用limit 1\n3. 应尽量避免在where子句中使用or来连接条件\n4. 优化limit分页\n5. 优化你的like语句\n6. 使用where条件限定要查询的数据，避免返回多余的行\n7. 尽量避免在索引列上使用mysql的内置函数\n8. 应尽量避免在where子句中对字段进行表达式操作，这将导致系统放弃使用索引而进行全表扫\n9. Inner join 、left join、right join，优先使用Inner join，如果是left join，左边表结果尽量小\n10. 应尽量避免在where子句中使用!=或<>操作符，否则将引擎放弃使用索引而进行全表扫描。\n11. 使用联合索引时，注意索引列的顺序，一般遵循最左匹配原则。\n12. 对查询进行优化，应考虑在where及order by涉及的列上建立索引，尽量避免全表扫描。\n13. 如果插入数据过多，考虑批量插入。\n14. 在适当的时候，使用覆盖索引。\n15. 慎用distinct关键字\n16. 删除冗余和重复索引\n17. 如果数据量较大，优化你的修改/删除语句。\n18. where子句中考虑使用默认值代替null。\n19. 不要有超过5个以上的表连接\n20. exist&in的合理利用\n21. 尽量用union all替换union\n22. 索引不宜太多，一般5个以内。\n23. 尽量使用数字型字段，若只含数值信息的字段尽量不要设计为字符型\n24. 索引不适合建在有大量重复数据的字段上，如性别这类型数据库字段。\n25. 尽量避免向客户端返回过多数据量。\n26. 当在SQL语句中连接多个表时,请使用表的别名，并把别名前缀于每一列上，这样语义更加清晰\n27. 尽可能使用varchar/nvarchar代替char/nchar。\n28. 为了提高group by语句的效率，可以在执行到该语句前，把不需要的记录过滤掉。\n29. 如果字段类型是字符串，where时一定用引号括起来，否则索引失效\n30. 使用explain分析你SQL的计划\n\n> [SQL优化的N种方法](https://mp.weixin.qq.com/s/5zsHr2tKOOkRN6mFa65tvw)\n> [吐血整理！书写高质量SQL的30条建议](https://mp.weixin.qq.com/s/4ByWZVc5jVmuVcFqkkIC_A)\n> [MySQL数据量太大优化方案](https://mp.weixin.qq.com/s/Ky_jak-u7vCX4vXkAONN_w)\n> [Mysql数据库查询好慢，除了索引，还能因为什么？](https://mp.weixin.qq.com/s/qJkEgNRlUwBAe41IdtJ_hQ)\n> [MySQL常用优化指南，面试再也不怕了！](https://mp.weixin.qq.com/s/eoKViDLmAB3ZaPuGgsk_zQ)\n> [深入理解为什么MySQL全表扫描很慢？](https://mp.weixin.qq.com/s/Q9yb1Aa-PQCW1DguRzLUdg)\n> [专业解决MySQL查询速度慢与性能差！](https://mp.weixin.qq.com/s/Ow1vuBST7YUh5J5SYHT6dg)\n> [记一次神奇的SQL查询经历，group by慢查询优化](https://mp.weixin.qq.com/s/VmAD31GYPV8HcE-m2aIrnA)\n> [SQL子查询怎么优化？写的很深！](https://mp.weixin.qq.com/s/9RJxCqpKt0LTaSG0SHIttA)\n> [SQL优化极简法则，还有谁不会？](https://mp.weixin.qq.com/s/NS2trUx9nVN5gNquVVbruw)\n> [如何写出一手好SQL？很有必要！](https://mp.weixin.qq.com/s/Z1OOMWBqeF_QQBvhbwo-sw)\n> [每个后端都应该知道的八个提升SQL性能的Tips](https://mp.weixin.qq.com/s/9LIn8H78gNyYnKZrIHLbgA)\n> [10个经典场景带你玩转SQL优化](https://mp.weixin.qq.com/s/BgOFpBBVvvlx-Jq0ZQg3AQ)\n> [聊聊sql优化的15个小技巧](https://mp.weixin.qq.com/s/Cirhr8SmBzyllAI3nQTXag)\n> [阿里一面：SQL优化有哪些技巧？](https://mp.weixin.qq.com/s/ERpnzP4UvS-RUS2cmGmxKg)\n> [如何让JOIN跑得更快？](https://mp.weixin.qq.com/s/w5oy8tg8sQ3GB8hw4esVDw)\n> [SQL优化万能公式：5大步骤+10个案例](https://mp.weixin.qq.com/s/wjDNsGFYxfgK5oAn9kdN9Q)\n> [SQL优化21连击+思维导图](https://mp.weixin.qq.com/s/A8OM449_JhKPIZa4P3w08Q)\n\n\n### 水平拆分、垂直拆分\n\n- 水平拆分，就是把一个表的数据给弄到多个库的多个表里去，但是每个库的表结构都一样，只不过每个库表放的数据是不同的，所有库表的数据加起来就是全部数据。水平拆分的意义，就是将数据均匀放更多的库里，然后用多个库来扛更高的并发，还有就是用多个库的存储容量来进行扩容。\n- 垂直拆分，就是把一个有很多字段的表给拆分成多个表，或者是多个库上去。每个库表的结构都不一样，每个库表都包含部分字段。一般来说，会将较少的访问频率很高的字段放到一个表里去，然后将较多的访问频率很低的字段放到另外一个表里去。因为数据库是有缓存的，你访问频率高的行字段越少，就可以在缓存里缓存更多的行，性能就越好。这个一般在表层面做的较多一些。\n\n> [分库分表的四个面时连环炮问题](https://mp.weixin.qq.com/s/_wDGx0UfG2hXo80JvvpBAw)\n> [这四种情况下，才是考虑分库分表的时候！](https://mp.weixin.qq.com/s/jqry9LnCVBt64d_DdpRXJg)\n> [分库分表？如何做到永不迁移数据和避免热点？](https://mp.weixin.qq.com/s/fbZ8SpHh8ScV8Xt50PGb6A)\n> [这应该是最详尽的MySQL分库分表文章了](https://mp.weixin.qq.com/s/4eII2YyVA_snLn4kswtNcQ)\n> [好好的系统，为什么要分库分表？](https://mp.weixin.qq.com/s/Q6VbgQaz0NVFG-8L4GC7bQ)\n\n\n### 主键、外键\n\n- **主键（主码）**：主键用于唯一标识一个元组，不能有重复，不允许为空。一个表只能有一个主键。\n- **外键（外码）**：外键用来和其他表建立联系用，外键是另一表的主键，外键是可以有重复的，可以是空值。一个表可以有多个外键。\n\n#### 为什么不推荐使用外键与级联？\n\n对于外键和级联，阿里巴巴开发手册这样说到：\n\n> 【强制】不得使用外键与级联，一切外键概念必须在应用层解决。\n>\n> 说明:以学生和成绩的关系为例，学生表中的student_id是主键，那么成绩表中的student_id则为外键。如果更新学生表中的student_id，同时触发成绩表中的student_id更新，即为级联更新。外键与级联更新适用于单机低并发，不适合分布式、高并发集群;级联更新是强阻塞，存在数据库更新风暴的风险;外键影响数据库的插入速度\n\n为什么不要用外键呢？大部分人可能会这样回答：\n\n1. **增加了复杂性**：a.每次做DELETE或者UPDATE都必须考虑外键约束，会导致开发的时候很痛苦,测试数据极为不方便;b.外键的主从关系是定的，假如那天需求有变化，数据库中的这个字段根本不需要和其他表有关联的话就会增加很多麻烦。\n2. **增加了额外工作**：数据库需要增加维护外键的工作，比如当我们做一些涉及外键字段的增，删，更新操作之后，需要触发相关操作去检查，保证数据的的一致性和正确性，这样会不得不消耗资源；（个人觉得这个不是不用外键的原因，因为即使你不使用外键，你在应用层面也还是要保证的。所以，我觉得这个影响可以忽略不计。）\n3. **对分库分表不友好**：因为分库分表下外键是无法生效的。\n4. ......\n\n我个人觉得上面这种回答不是特别的全面，只是说了外键存在的一个常见的问题。实际上，我们知道外键也是有很多好处的，比如：\n\n1. 保证了数据库数据的一致性和完整性；\n2. 级联操作方便，减轻了程序代码量；\n3. ......\n\n所以说，不要一股脑的就抛弃了外键这个概念，既然它存在就有它存在的道理，如果系统不涉及分库分表，并发量不是很高的情况还是可以考虑使用外键的。\n\n#### 关于数据库外键是否应该使用\n\n外键提供的几种在更新和删除时的不同行为都可以帮助我们保证数据库中数据的一致性和引用合法性，但是外键的使用也需要数据库承担额外的开销，在大多数服务都可以水平扩容的今天，高并发场景中使用外键确实会影响服务的吞吐量上限。在数据库之外手动实现外键的功能是可能的，但是却会带来很多维护上的成本或者需要我们在数据一致性上做出一些妥协。我们可以从可用性、一致性几个方面分析使用外键、模拟外键以及不使用外键的差异：不使用外键牺牲了数据库中数据的一致性，但是却能够减少数据库的负载；模拟外键将一部分工作移到了数据库之外，我们可能需要放弃一部分一致性以获得更高的可用性，但是为了这部分可用性，我们会付出更多的研发与维护成本，也增加了与数据库之间的网络通信次数；使用外键保证了数据库中数据的一致性，也将全部的计算任务全部交给了数据库；在大多数不需要高并发或者对一致性有较强要求的系统中，我们可以直接使用数据库提供的外键帮助我们对数据进行校验，但是在对一致性要求不高的、复杂的场景或者大规模的团队中，不使用外键也确实可以为数据库减负，而大团队也有更多的时间和精力去设计其他的方案，例如：分布式的关系型数据库。当我们考虑应不应该在数据库中使用外键时，需要关注的核心我们的数据库承担这部分计算任务后会不会影响系统的可用性，在使用时也不应该一刀切的决定用或者不用外键，应该根据具体的场景做决策，我们在这里介绍了两个使用外键时可能遇到的问题：RESTRICT外键会在更新和删除关系表中的数据时对外键约束的合法性进行检查，保证外键不会引用到不存在的记录；CASCADE外键会在更新和删除关系表中的数据时触发对关联记录的更新和删除，在数据量较大的数据库中可能会有数量级的放大效果\n\n### 数据库三大范式\n\n数据库范式有3种：\n\n- 1NF(第一范式)：属性不可再分。\n- 2NF(第二范式)：1NF的基础之上，消除了非主属性对于码的部分函数依赖。\n- 3NF(第三范式)：3NF在2NF的基础之上，消除了非主属性对于码的传递函数依赖。\n\n#### 1NF(第一范式)\n\n属性（对应于表中的字段）不能再被分割，也就是这个字段只能是一个值，不能再分为多个其他的字段了。**1NF是所有关系型数据库的最基本要求**，也就是说关系型数据库中创建的表一定满足第一范式。\n\n#### 2NF(第二范式)\n\n2NF在1NF的基础之上，消除了非主属性对于码的部分函数依赖。如下图所示，展示了第一范式到第二范式的过渡。第二范式在第一范式的基础上增加了一个列，这个列称为主键，非主属性都依赖于主键。\n\n![第二范式](https://oss.javaguide.cn/github/javaguide/csdn/bd1d31be3779342427fc9e462bf7f05c.png)\n\n一些重要的概念：\n\n- **函数依赖（functionaldependency）**：若在一张表中，在属性（或属性组）X的值确定的情况下，必定能确定属性Y的值，那么就可以说Y函数依赖于X，写作X→Y。\n- **部分函数依赖（partialfunctionaldependency）**：如果X→Y，并且存在X的一个真子集X0，使得X0→Y，则称Y对X部分函数依赖。比如学生基本信息表R中（学号，身份证号，姓名）当然学号属性取值是唯一的，在R关系中，（学号，身份证号）->（姓名），（学号）->（姓名），（身份证号）->（姓名）；所以姓名部分函数依赖与（学号，身份证号）；\n- **完全函数依赖（Fullfunctionaldependency）**：在一个关系中，若某个非主属性数据项依赖于全部关键字称之为完全函数依赖。比如学生基本信息表R（学号，班级，姓名）假设不同的班级学号有相同的，班级内学号不能相同，在R关系中，（学号，班级）->（姓名），但是（学号）->(姓名)不成立，（班级）->(姓名)不成立，所以姓名完全函数依赖与（学号，班级）；\n- **传递函数依赖**：在关系模式R(U)中，设X，Y，Z是U的不同的属性子集，如果X确定Y、Y确定Z，且有X不包含Y，Y不确定X，（X∪Y）∩Z=空集合，则称Z传递函数依赖(transitivefunctionaldependency)于X。传递函数依赖会导致数据冗余和异常。传递函数依赖的Y和Z子集往往同属于某一个事物，因此可将其合并放到一个表中。比如在关系R(学号,姓名,系名，系主任)中，学号→系名，系名→系主任，所以存在非主属性系主任对于学号的传递函数依赖。。\n\n#### 3NF(第三范式)\n\n3NF在2NF的基础之上，消除了非主属性对于码的传递函数依赖。符合3NF要求的数据库设计，基本上解决了数据冗余过大，插入异常，修改异常，删除异常的问题。比如在关系R(学号,姓名,系名，系主任)中，学号→系名，系名→系主任，所以存在非主属性系主任对于学号的传递函数依赖，所以该表的设计，不符合3NF的要求。\n\n#### 总结\n\n- 1NF：数据库表的每一列都是不可分割的基本数据项，同一列中不能有多个值，不能在一列中存放多个属性。例如员工信息表，不能在信息一列中放入电话，住址等信息，应该单独设计成电话、住址各一列\n- 2NF：每个表必须有主键(Primary key),其他数据元素与主键一一对应。通常称这种关系为函数依赖(Functional dependence)关系，即表中其他数据元素都依赖于主键,或称该数据元素惟一地被主键所标识,例如学生表（学生id，姓名，成绩，合格状态），其中合格状态这一列不依赖于学生信息，而依赖于成绩，所以不符合第二范式\n- 3NF:要求一个数据库表中不包含已在其它表中已包含的非主关键字信息,例如学生表（学生id，姓名，班级id，班级位置）班级表(班级id，班级名，班级位置)，学生表里已经有了班级的id，可以推断出班级位置，无需在学生表里存入班级位置信息\nBCNF:所有非主属性对每一个候选键都是完全函数依赖；所有的主属性对每一个不包含它的候选键，也是完全函数依赖；没有任何属性完全函数依赖于非候选键的任何一组属性\n\n注意事项：\n1. 第二范式与第三范式的本质区别：在于有没有分出两张表。\n第二范式是说一张表中包含了多种不同实体的属性，那么必须要分成多张表，第三范式是要求已经分好了多张表的话，一张表中只能有另一张标的ID，而不能有其他任何信息，（其他任何信息，一律用主键在另一张表中查询）。\n2. 必须先满足第一范式才能满足第二范式，必须同时满足第一第二范式才能满足第三范式。\n三大范式只是一般设计数据库的基本理念，可以建立冗余较小、结构合理的数据库。如果有特殊情况，当然要特殊对待，数据库设计最重要的是看需求跟性能，需求>性能>表结构。所以不能一味的去追求范式建立数据库。\n\n### 五大约束\n\n数据库中的五大约束包括：\n1. 主键约束（Primay Key Coustraint）唯一性，非空性；\n2. 唯一约束 （Unique Counstraint）唯一性，可以空，但只能有一个\n3. 默认约束 (Default Counstraint)该数据的默认值；\n4. 外键约束 (Foreign Key Counstraint)需要建立两表间的关系；\n5. 非空约束（Not Null Counstraint）:设置非空约束，该字段不能为空。\n\n### 相关文章\n\n- [数据库面试题（函数和存储过程区别）](https://mp.weixin.qq.com/s/gCT048J16KO3ucNtzvAWxA)\n- [吊打面试官？2020年数据库高频面试题|原力计划](https://mp.weixin.qq.com/s/zx2PY8A_L5wnVc7goMY5WQ)\n- [JDBC查询各种姿势（普通、流式、游标）](https://mp.weixin.qq.com/s?__biz=Mzg2MDYzODI5Nw==&mid=2247494238&idx=1&sn=e44bf7e2ba678a562246e8831d5d8dc4&source=41#wechat_redirect)\n- [1.2W字的SQL语法速成手册](https://mp.weixin.qq.com/s/AUXNm2LyVl4NtKGEzBaWFg)\n- [使用uuid作为数据库主键，被技术总监怼了一顿！](https://mp.weixin.qq.com/s/BhMjFRqlDYKQNaCKj2tnkQ)\n- [一直认为count(1)比count(\\*)效果高，被同事鄙视了。](https://mp.weixin.qq.com/s/zdxkbtHxdgGtgck1zjZmWw)\n- [我说用count(\\*)统计行数，面试官让我回去等消息..](https://mp.weixin.qq.com/s/7ipMLtkeJhCAtxKlEvOFzw)\n- [8种常见SQL错误用法](https://mp.weixin.qq.com/s/Siyd8kkZEsSV5g0D2qsCHg)\n- [常见的SQL面试题：经典50例](https://mp.weixin.qq.com/s/SZnwP5Guu4RraxBI6l6nFg)\n- [一文搞定关系数据库设计要领，值得收藏！](https://mp.weixin.qq.com/s/NbZnNOZTUZaavEAKD-CSYw)\n- [常用数据库SQL命令详解（上）](https://mp.weixin.qq.com/s/hd7-IjhRW5DyWZi9MdEG0w)\n- [常用数据库SQL命令详解（下）](https://mp.weixin.qq.com/s/gidYvMFQGM8VaiQkP_9IjQ)\n- [输入SQL到返回数据，到底发生了什么？](https://mp.weixin.qq.com/s/jhej5m8qs0isEzJC-SpLfA)\n- [一条SQL查询语句是如何执行的？](https://mp.weixin.qq.com/s/M9PmFMmXToJOMyzVWj0Z5g)\n- [数据库主键一定要自增吗？有哪些场景不建议自增？](https://mp.weixin.qq.com/s/Xu_SfM-DzKarybGpspwIHA)\n- [慢SQL，压垮团队的最后一根稻草](https://mp.weixin.qq.com/s/2ea_no_oonNb58OqInRn5g)\n- [聊聊数据库建表的15个小技巧](https://mp.weixin.qq.com/s/03lD4P0rE909Kkvm_7vLNg)\n- [百亿级数据分库分表后怎么分页查询？](https://mp.weixin.qq.com/s/FPdl_-jqgeIXSGOh-MQNFQ)\n- [加密的手机号，如何模糊查询？](https://mp.weixin.qq.com/s/PhGDmIumM0nmp8CA6nuzfQ)\n\n## 怎么保证幂等性\n\n### 幂等性的几种方案\n\n- 唯一索引，防止新增脏数据\n- 前端限制:页面的提交按钮只能被点击提交一次\n  后端解决方案：\n    1. 集群环境：采用token加redis（redis单线程的，处理需要排队）\n    2. 单JVM环境：采用token加redis或token加jvm内存\n    3. 处理流程：请求前先生成token，重复代表处理过.数据提交前向授权系统申请token，系统根据相关信息生成token并判断是否可以返回（根据代码逻辑将生成的token与已生成且保存的token做对比，token一致的话代表已经生成了一次，本次不允许返回），将生成的token放到redis或jvm内存，并设置token的有效时间，返回给客户端\n  \n      ![](images/mideng.jpg)\n       客户端携带token请求服务端，服务端查询redis，如果有的话处理请求并删除token，没有的话代表非法请求不做处理\n\n> 现在很多系统处理请求已经不做是否登陆的校验，而是根据用户名密码申请token，将token保存到cookie或者Local Storage或者form隐藏域，请求时携带token，服务端校验处理请求\n> [Token多平台身份认证架构设计思路](https://mp.weixin.qq.com/s/X4J56Y2dLzkBVwCC2hlmaQ)\n> [Token登录认证详解](https://blog.csdn.net/GreenSky_Test/article/details/116056661)\n\n- 悲观锁获取数据的时候加锁获取\n```sql\nselect * from table_xxx where id='xxx' for update;\n```\n注意：id字段一定是主键或者唯一索引\n悲观锁使用时一般伴随事务一起使用，数据锁定时间可能会很长，根据实际情况选用\n\n- 乐观锁:乐观锁只是在更新数据那一刻锁表，其他时间不锁表，所以相对于悲观锁，效率更高。乐观锁的实现方式多种多样可以通过version或者其他状态条件：\n```sql\n-- 通过版本号实现\nupdate table_xxx set name=#name#,version=version+1 where version=#version#\n-- 通过条件限制\nupdate tablexxx set avaiamount=avaiamount-#subAmount# where avaiamount-#subAmount# >= 0\n```\n\n- 分布式锁\n\n- 状态机幂等\n在设计单据相关的业务，或者是任务相关的业务，肯定会涉及到状态机(状态变更图)，就是业务单据上面有个状态，状态在不同的情况下会发生变更，一般情况下存在有限状态机,如果状态机已经处于下一个状态，这时候来了一个上一个状态的变更，理论上是不能够变更的，这样的话，保证了有限状态机的幂等。注意：订单等单据类业务，存在很长的状态流转，一定要深刻理解状态机，对业务系统设计能力提高有很大帮助\n\n- 利用唯一请求编号去重，借用Redis做这个去重——只要这个唯一请求编号在redis存在，证明处理过，那么就认为是重复的\n\n#### 幂等性相关文章\n\n- [拒绝接口裸奔！开放API接口签名验证！](https://mp.weixin.qq.com/s/eVHScVN2kcuZaokzs6-QxA)\n- [面试问：你的项目是如何处理重复请求/并发请求的？](https://mp.weixin.qq.com/s/BENMPLwH8WG60UVL5vXj0A)\n- [Spring Boot实现接口幂等性的4种方案](https://mp.weixin.qq.com/s/nza76CX-UJxspSTl52B8eQ)\n- [高并发下如何保证接口的幂等性？](https://mp.weixin.qq.com/s/3FJQYVoh_MDXBT0wHoKksQ)\n- [分布式幂等性如何保证](https://mp.weixin.qq.com/s/Hd4T8aqrx8gTmkLRyl7hcA)\n- [消息幂等（去重）通用解决方案，写得真好](https://mp.weixin.qq.com/s/Rax8Qb-DrYNpkbc6eTtRlg)\n- [面试官：给我一个避免消息重复消费的解决方案？](https://mp.weixin.qq.com/s/aKStFQXAlFF-1dOax5xg4Q)\n- [处理接口幂等性的两种常见方案|手把手教你](https://mp.weixin.qq.com/s?__biz=MzI1NDY0MTkzNQ==&amp;mid=2247497775&amp;idx=1&amp;sn=02f389bfbdb280314e4db32660cd5bc5&amp;scene=21#wechat_redirect)\n- [如何防止订单重复支付？](https://mp.weixin.qq.com/s/ryhX9usir_d-O8eJuItWzA)\n- [保证接口数据安全的10种方案](https://mp.weixin.qq.com/s/aQwcHv36y0Du4w9HxT5FSw)\n- [如何防止接口重复提交？（上）](https://mp.weixin.qq.com/s/6WIlEnyYtpO50hAsO4EFNg)\n- [如何防止接口重复提交？（中）](https://mp.weixin.qq.com/s/RrMGoR4DgC7esfWjsX0-Lg)\n- [如何防止接口重复提交？（下）](https://mp.weixin.qq.com/s/Wedb1MyybIXdQEp5xvnxLw)\n- [并发扣款，如何保证一致性](https://mp.weixin.qq.com/s/2bhhEM8UG8AgIga0RxpW3g)\n- [一种非侵入式幂等性的Java实现](https://mp.weixin.qq.com/s/bktHm67TBWnNi6-xsM3-NA)\n\n### 微服务系统如何设计一个安全的API\n\n1. 身份认证问题\n一般情况下服务端均会向客户端颁发appId、partnerId等类似于标识用户身份的唯一ID，此ID关联用户密钥，一旦服务端异常或者受到工具，可以追溯来源，调整ID状态或者来强制下线用户\n2. 信息泄露问题\n信息泄露主要是要控制报文在网络传输中不要明文传输，根据对称和非对称加密算法的特点，目前主流的做法是使用混合加密，主要流程是，服务端创建RSA密钥对，将公钥传输给客户端，同时客户端创建AES密钥，使用AES密钥加密明文得到密文，接着使用公钥加密AES密钥，最后将加密后的AES密钥和密文传输到服务端。服务端使用自己的私钥解密加密后的AES密钥得到AES密钥，接着使用AES密钥解密密文得到明文\n3. 请求被篡改问题\n防止请求被篡改主要是要做好加签和验签\n4. 重放攻击问题\n黑客监听到请求后，重复请求攻击服务端，服务端如何识别是非法请求\n在请求参数中增加timestamp、randomString参数【此参数是从服务端实时请求的】，服务端在接收到请求后timestamp时间戳和服务端相差1分中之内的才放行，接着判断randomString是否已经存在，如果存在则不响应\n\n### 秒杀\n\n#### 步骤\n\n写脚本肯定需要知道步骤是什么，然后才能用代码去复刻下来嘛。\n\n1、下载浏览器驱动，这里我用的是chrome浏览器，先看一下自己的版本号，在设置可以看到。\n\n然后在[网站](http://chromedriver.storage.googleapis.com/index.html)找好对应的版本去下载\n\n2、接下来就是设置秒杀时间\n\n3、打开浏览器输入淘宝网址\n\n4、登录账号，进入购物车页面\n\n5、点击选择按钮\n\n6、秒杀时间到了，立刻下单！\n\n#### 操作开始\n\n导入依赖：\n\n```xml\n<dependency>\n      <groupId>org.seleniumhq.selenium</groupId>\n      <artifactId>selenium-java</artifactId>\n      <version>3.141.59</version>\n</dependency>\n```\n\n下面是完整的代码\n\n```java\npublic void taoBao() throws Exception {\n    // 浏览器驱动路径\n    System.setProperty(\"webdriver.chrome.driver\",\"D:\\\\JDK\\\\chromedriver.exe\");\n    // 设置秒杀时间\n    SimpleDateFormat sdf = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss SSSSSSSSS\");\n    Date date = sdf.parse(\"2022-04-14 14:07:00 000000000\");\n    // 1、打开浏览器\n    ChromeDriver browser = new ChromeDriver();\n    Actions actions = new Actions(browser);\n    // 2、输入网址\n    browser.get(\"https://www.taobao.com\");\n    Thread.sleep(3000);\n    // 3、点击登录\n    browser.findElement(By.linkText(\"亲，请登录\")).click();\n    Thread.sleep(2000);\n    // 4、扫码登录\n    browser.findElement(By.className(\"icon-qrcode\")).click();\n    Thread.sleep(4000);\n    // 5、进入购物车页面\n    browser.get(\"https://cart.taobao.com/cart.htm\");\n    Thread.sleep(3000);\n    // 6、点击选择第一个按钮\n    browser.findElement(By.xpath(\"//*[@id=\\\"J_Order_s_2207407355826_1\\\"]/div[1]/div/div/label\")).click();\n    Thread.sleep(2000);\n    while (true){\n        //当前时间\n        Date now = new Date();\n        System.out.println(now);\n        if(now.after(date)){\n            if(browser.findElement(By.linkText(\"结 算\")).isEnabled()){\n                browser.findElement(By.linkText(\"结 算\")).click();\n                System.out.println(\"结算成功\");\n                break;\n            }\n\n        }\n    }\n    Thread.sleep(5000);\n}\n```\n\n这里说一下会遇到的问题：\n\n1. 这里使用的是扫码登录，需要用手机淘宝扫码进行登录\n2. Thread.sleep(4000);就是系统休息4秒钟，如果扫码登录时间大于4秒会报错，可以根据电脑网速来设置\n3. browser.findElement(By.xpath(\"xxx\")).click();这个是选择购物车第一个商家的所有商品，里面xxx需要更改。当然其他参数怎么修改可以根据这个对应来修改。\n\n进入购物车页面后按F12，然后点左上角那个箭头，然后选择店铺左边的按钮，这样下面代码块就对应到了指定的代码位置\n\n![图片](https://img-blog.csdnimg.cn/b2f3301c394e498eaa8fb7bb87477317.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA6L6w5YWuaW5n,size_20,color_FFFFFF,t_70,g_se,x_16)\n\n右键这一行，然后选择copy→Copy XPath，这个XPath就是browser.findElement(By.xpath(\"xxx\")).click();的xxx内容\n\n![图片](https://img-blog.csdnimg.cn/c816562669b74a729443ff134c5f0b07.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA6L6w5YWuaW5n,size_20,color_FFFFFF,t_70,g_se,x_16)\n\n如果以上操作都没有问题，那么你就可以启动程序啦！！成功后你会发现，脚本居然如此简单！！\n\n- [面试官：你给我画一下秒杀系统的架构图](https://mp.weixin.qq.com/s/EYx78REElJE3ASm1PCG2aw)\n- [秒杀场景的九个细节，细思极恐！](https://mp.weixin.qq.com/s/Uq0zM_ceoFupYDKqqpvR4Q)\n- [秒杀系统设计最全攻略](https://mp.weixin.qq.com/s/2LpNQTmSFu0InYnC-Q7bJA)\n- [下次二面再回答不好“秒杀系统“设计原理，我就捶死自己...](https://mp.weixin.qq.com/s/YHhbonAxNntonTo-UhDV6Q)\n- [如何设计订单系统](https://mp.weixin.qq.com/s/AtXswrVQPGD_jfSqzKe_qQ)\n- [千万级高并发秒杀系统设计套路](https://mp.weixin.qq.com/s/KmPY2NOfpftFi6TpVSW23Q)\n\n\n### 相关文章\n\n- [生成订单30分钟未支付，则自动取消，该怎么实现?](https://mp.weixin.qq.com/s/ijv_4_qWTrG-kA9jSbz1iw)\n- [一口气说出6种实现延时消息的方案](https://mp.weixin.qq.com/s/VcbZTsoD5-ioc4x7s6mj7Q)\n- [面试官：怎么不用定时任务实现关闭订单？](https://mp.weixin.qq.com/s/Oc188nkq4-s9ivt7Ki_M1A)\n- [再有人问你如何实现订单到期关闭，就把这篇文章发给他](https://mp.weixin.qq.com/s/BG1PqUWX0XwJX6aMCXCgvw)\n- [订单超时怎么处理？阿里用这种方案](https://mp.weixin.qq.com/s/pGZj1jVHKPhUJKlKNnm5CQ)\n\n## 反射\n\n### 何为反射\n\n如果说大家研究过框架的底层原理或者咱们自己写过框架的话，一定对反射这个概念不陌生。反射之所以被称为框架的灵魂，主要是因为它赋予了我们在运行时分析类以及执行类中方法的能力。通过反射你可以获取任意一个类的所有属性和方法，你还可以调用这些方法和属性。\n\n### 反射的应用场景\n\n像咱们平时大部分时候都是在写业务代码，很少会接触到直接使用反射机制的场景。但是，这并不代表反射没有用。相反，正是因为反射，你才能这么轻松地使用各种框架。像Spring/SpringBoot、MyBatis等等框架中都大量使用了反射机制。这些框架中也大量使用了动态代理，而动态代理的实现也依赖反射。比如下面是通过JDK实现动态代理的示例代码，其中就使用了反射类`Method`来调用指定的方法。\n\n\n```java\npublic class DebugInvocationHandler implements InvocationHandler {\n    /**\n     * 代理类中的真实对象\n     */\n    private final Object target;\n\n    public DebugInvocationHandler(Object target) {\n        this.target = target;\n    }\n\n\n    public Object invoke(Object proxy, Method method, Object[] args) throws InvocationTargetException, IllegalAccessException {\n        System.out.println(\"before method \" + method.getName());\n        Object result = method.invoke(target, args);\n        System.out.println(\"after method \" + method.getName());\n        return result;\n    }\n}\n```\n\n另外，像Java中的一大利器注解的实现也用到了反射。为什么你使用Spring的时候，一个@Component注解就声明了一个类为Spring Bean呢？为什么你通过一个@Value注解就读取到配置文件中的值呢？究竟是怎么起作用的呢？这些都是因为你可以基于反射分析类，然后获取到类/属性/方法/方法的参数上的注解。你获取到注解之后，就可以做进一步的处理。\n\n### 反射机制的优缺点\n\n**优点**：可以让咱们的代码更加灵活、为各种框架提供开箱即用的功能提供了便利\n\n**缺点**：让我们在运行时有了分析操作类的能力，这同样也增加了安全问题。比如可以无视泛型参数的安全检查（泛型参数的安全检查发生在编译时）。另外，反射的性能也要稍差点，不过，对于框架来说实际是影响不大的。\n\n> 相关阅读：[Java Reflection:Why is it so slow?](https://stackoverflow.com/questions/1392351/java-reflection-why-is-it-so-slow)\n> [原文链接](https://javaguide.cn/java/basis/reflection.html)\n\n\n### 相关文章\n\n- [Java反射机制你还不会？那怎么看Spring源码？](https://mp.weixin.qq.com/s/jV9kE2ajB40f3fOU_lT9ng)\n- [Java反射是什么？看这篇绝对会了！](https://mp.weixin.qq.com/s/QbacsQwTyvBJi12LYPNKJw)\n- [学会这篇反射，我就可以去吹牛逼了。](https://mp.weixin.qq.com/s/Dyg4qSqiyjSJTne8yvUYpQ)\n- [深入理解Java：类加载机制及反射](https://mp.weixin.qq.com/s/kTYLjg_FlKBdAAQQvSAF9g)\n","tags":["随笔"]},{"title":"消息队列-MQ","slug":"消息队列-MQ","url":"/blog/posts/442dc7ae24f1/","content":"\n\n## 什么是消息队列？\n\n我们可以把消息队列看作是一个存放消息的容器，当我们需要使用消息的时候，直接从容器中取出消息供自己使用即可。由于队列Queue是一种先进先出的数据结构，所以消费消息时也是按照顺序来消费的。\n\n![Message queue](https://guide-blog-images.oss-cn-shenzhen.aliyuncs.com/消息队列/message-queue-small.png)\n\n参与消息传递的双方称为**生产者**和**消费者**，生产者负责发送消息，消费者负责处理消息。\n\n我们知道操作系统中的进程通信的一种很重要的方式就是消息队列。我们这里提到的消息队列稍微有点区别，更多指的是各个服务以及系统内部各个组件/模块之前的通信，属于一种中间件。\n\n> 维基百科是这样介绍中间件的：\n> 中间件（英语：Middleware），又译中间件、中介层，是一类提供系统软件和应用软件之间连接、便于软件各部件之间的沟通的软件，应用软件可以借助中间件在不同的技术架构之间共享信息与资源。中间件位于客户机服务器的操作系统之上，管理着计算资源和网络通信。\n\n简单来说：**中间件就是一类为应用软件服务的软件，应用软件是为用户服务的，用户不会接触或者使用到中间件**。\n\n除了消息队列之外，常见的中间件还有RPC框架、分布式组件、HTTP服务器、任务调度框架、配置中心、数据库层的分库分表工具和数据迁移工具等等。\n\n> 关于中间件比较详细的介绍可以参考阿里巴巴淘系技术的一篇回答：https://www.zhihu.com/question/19730582/answer/1663627873。\n\n随着分布式和微服务系统的发展，消息队列在系统设计中有了更大的发挥空间，使用消息队列可以降低系统耦合性、实现任务异步、有效地进行流量削峰，是分布式和微服务系统中重要的组件之一。\n\n## 消息队列有什么用？\n\n通常来说，使用消息队列能为我们的系统带来下面三点好处：\n\n1. **通过异步处理提高系统性能（减少响应所需时间）**\n2. **削峰/限流**\n3. **降低系统耦合性**\n\n### 通过异步处理提高系统性能（减少响应所需时间）\n\n![通过异步处理提高系统性能](https://my-blog-to-use.oss-cn-beijing.aliyuncs.com/2019-11/Asynchronous-message-queue.png)\n\n将用户的请求数据存储到消息队列之后就立即返回结果。随后，系统再对消息进行消费。\n\n因为用户请求数据写入消息队列之后就立即返回给用户了，但是请求数据在后续的业务校验、写数据库等操作中可能失败。因此，使用消息队列进行异步处理之后，需要适当修改业务流程进行配合，比如用户在提交订单之后，订单数据写入消息队列，不能立即返回用户订单提交成功，需要在消息队列的订单消费者进程真正处理完该订单之后，甚至出库后，再通过电子邮件或短信通知用户订单成功，以免交易纠纷。这就类似我们平时手机订火车票和电影票。\n\n### 削峰/限流\n\n**先将短时间高并发产生的事务消息存储在消息队列中，然后后端服务再慢慢根据自己的能力去消费这些消息，这样就避免直接把后端服务打垮掉**。\n\n举例：在电子商务一些秒杀、促销活动中，合理使用消息队列可以有效抵御促销活动刚开始大量订单涌入对系统的冲击。如下图所示：\n\n![削峰](https://my-blog-to-use.oss-cn-beijing.aliyuncs.com/2019-11/削峰-消息队列.png)\n\n### 降低系统耦合性\n\n使用消息队列还可以降低系统耦合性。我们知道如果模块之间不存在直接调用，那么新增模块或者修改模块就对其他模块影响较小，这样系统的可扩展性无疑更好一些。\n\n![解耦](https://my-blog-to-use.oss-cn-beijing.aliyuncs.com/2019-11/消息队列-解耦.png)\n\n生产者（客户端）发送消息到消息队列中去，接受者（服务端）处理消息，需要消费的系统直接去消息队列取消息进行消费即可而不需要和其他系统有耦合，这显然也提高了系统的扩展性。\n\n消息队列使用发布-订阅模式工作，消息发送者（生产者）发布消息，一个或多个消息接受者（消费者）订阅消息。从上图可以看到消息发送者（生产者）和消息接受者（消费者）之间没有直接耦合，消息发送者将消息发送至分布式消息队列即结束对消息的处理，消息接受者从分布式消息队列获取该消息后进行后续处理，并不需要知道该消息从何而来。对新增业务，只要对该类消息感兴趣，即可订阅该消息，对原有系统和业务没有任何影响，从而实现网站业务的可扩展性设计。消息接受者对消息进行过滤、处理、包装后，构造成一个新的消息类型，将消息继续发送出去，等待其他消息接受者订阅该消息。因此基于事件（消息对象）驱动的业务架构可以是一系列流程。\n\n另外，为了避免消息队列服务器宕机造成消息丢失，会将成功发送到消息队列的消息存储在消息生产者服务器上，等消息真正被消费者服务器处理后才删除消息。在消息队列服务器宕机后，生产者服务器会选择分布式消息队列服务器集群中的其他服务器发布消息。\n\n**备注**：不要认为消息队列只能利用发布-订阅模式工作，只不过在解耦这个特定业务环境下是使用发布-订阅模式的。除了发布-订阅模式，还有点对点订阅模式（一个消息只有一个消费者），我们比较常用的是发布-订阅模式。另外，这两种消息模型是JMS提供的，AMQP协议还提供了另外5种消息模型。\n\n### 实现分布式事务\n\n我们知道分布式事务的解决方案之一就是MQ事务。\n\nRocketMQ、Kafka、Pulsar、QMQ都提供了事务相关的功能。事务允许事件流应用将消费，处理，生产消息整个过程定义为一个原子操作。\n\n![分布式事务详解-MQ事务](https://guide-blog-images.oss-cn-shenzhen.aliyuncs.com/github/javaguide/csdn/07b338324a7d8894b8aef4b659b76d92.png)\n\n## 使用消息队列会带来哪些问题？\n\n- **系统可用性降低**：系统可用性在某种程度上降低，为什么这样说呢？在加入MQ之前，你不用考虑消息丢失或者说MQ挂掉等等的情况，但是，引入MQ之后你就需要去考虑了！\n- **系统复杂性提高**：加入MQ之后，你需要保证消息没有被重复消费、处理消息丢失的情况、保证消息传递的顺序性等等问题！\n- **一致性问题**：我上面讲了消息队列可以实现异步，消息队列带来的异步确实可以提高系统响应速度。但是，万一消息的真正消费者并没有正确消费消息怎么办？这样就会导致数据不一致的情况了!\n\n## JMS和AMQP\n\n### JMS是什么？\n\nJMS（JAVA Message Service,java消息服务）是Java的消息服务，JMS的客户端之间可以通过JMS服务进行异步的消息传输。**JMS（JAVA Message Service，Java消息服务）API是一个消息服务的标准或者说是规范**，允许应用程序组件基于JavaEE平台创建、发送、接收和读取消息。它使分布式通信耦合度更低，消息服务更加可靠以及异步性。\n\nJMS定义了五种不同的消息正文格式以及调用的消息类型，允许你发送并接收以一些不同形式的数据：\n\n- StreamMessage：Java原始值的数据流\n- MapMessage：一套名称-值对\n- TextMessage：一个字符串对象\n- ObjectMessage：一个序列化的Java对象\n- BytesMessage：一个字节的数据流\n\n**ActiveMQ（已被淘汰）就是基于JMS规范实现的。**\n\n### JMS两种消息模型\n\n#### 点到点（P2P）模型\n\n使用队列（Queue）作为消息通信载体；满足生产者与消费者模式，一条消息只能被一个消费者使用，未被消费的消息在队列中保留直到被消费或超时。比如：我们生产者发送100条消息的话，两个消费者来消费一般情况下两个消费者会按照消息发送的顺序各自消费一半（也就是你一个我一个的消费）\n\n#### 发布/订阅（Pub/Sub）模型\n\n发布订阅模型（Pub/Sub）使用主题（Topic）作为消息通信载体，类似于广播模式；发布者发布一条消息，该消息通过主题传递给所有的订阅者，在一条消息广播之后才订阅的用户则是收不到该条消息的。\n\n### AMQP是什么？\n\nAMQP，即Advanced Message Queuing Protocol，一个提供统一消息服务的应用层标准高级消息队列协议（二进制应用层协议），是应用层协议的一个开放标准，为面向消息的中间件设计，兼容JMS。基于此协议的客户端与消息中间件可传递消息，并不受客户端/中间件同产品，不同的开发语言等条件的限制。\n\n**RabbitMQ就是基于AMQP协议实现的。**\n\n### JMS vs AMQP\n\n|   对比方向   | JMS                                     | AMQP                                                         |\n| :----------: | :-------------------------------------- | :----------------------------------------------------------- |\n|     定义     | Java API                                | 协议                                                         |\n|    跨语言    | 否                                      | 是                                                           |\n|    跨平台    | 否                                      | 是                                                           |\n| 支持消息类型 | 提供两种消息模型：①Peer-2-Peer;②Pub/sub | 提供了五种消息模型：①direct exchange；②fanout exchange；③topic change；④headers exchange；⑤system exchange。本质来讲，后四种和JMS的pub/sub模型没有太大差别，仅是在路由机制上做了更详细的划分； |\n| 支持消息类型 | 支持多种消息类型 ，我们在上面提到过     | byte[]（二进制）                                             |\n\n**总结：**\n\n- AMQP为消息定义了线路层（wire-levelprotocol）的协议，而JMS所定义的是API规范。在Java体系中，多个client均可以通过JMS进行交互，不需要应用修改代码，但是其对跨平台的支持较差。而AMQP天然具有跨平台、跨语言特性。\n- JMS支持`TextMessage`、`MapMessage`等复杂的消息类型；而AMQP仅支持`byte[]`消息类型（复杂的类型可序列化后发送）。\n- 由于Exchange提供的路由算法，AMQP可以提供多样化的路由方式来传递消息到消息队列，而JMS仅支持队列和主题/订阅方式两种。\n\n## RPC和消息队列的区别\n\nRPC和消息队列都是分布式微服务系统中重要的组件之一，下面我们来简单对比一下两者：\n\n- **从用途来看**：RPC主要用来解决两个服务的远程通信问题，不需要了解底层网络的通信机制。通过RPC可以帮助我们调用远程计算机上某个服务的方法，这个过程就像调用本地方法一样简单。消息队列主要用来降低系统耦合性、实现任务异步、有效地进行流量削峰。\n- **从通信方式来看**：RPC是双向直接网络通讯，消息队列是单向引入中间载体的网络通讯。\n- **从架构上来看**：消息队列需要把消息存储起来，RPC则没有这个要求，因为前面也说了RPC是双向直接网络通讯。\n- **从请求处理的时效性来看**：通过RPC发出的调用一般会立即被处理，存放在消息队列中的消息并不一定会立即被处理。\n\nRPC和消息队列本质上是网络通讯的两种不同的实现机制，两者的用途不同，万不可将两者混为一谈。\n\n## 消息队列技术选型\n\n### Kafka\n\n![img](https://guide-blog-images.oss-cn-shenzhen.aliyuncs.com/github/javaguide/high-performance/message-queue/kafka-logo.png)\n\nKafka是LinkedIn开源的一个分布式流式处理平台，已经成为Apache顶级项目，早期被用来用于处理海量的日志，后面才慢慢发展成了一款功能全面的高性能消息队列。\n\n流式处理平台具有三个关键功能：\n\n1. **消息队列**：发布和订阅消息流，这个功能类似于消息队列，这也是Kafka也被归类为消息队列的原因。\n2. **容错的持久方式存储记录消息流**：Kafka会把消息持久化到磁盘，有效避免了消息丢失的风险。\n3. **流式处理平台**：在消息发布的时候进行处理，Kafka提供了一个完整的流式处理类库。\n\nKafka是一个分布式系统，由通过高性能TCP网络协议进行通信的服务器和客户端组成，可以部署在在本地和云环境中的裸机硬件、虚拟机和容器上。\n\n在Kafka2.8之前，Kafka最被大家诟病的就是其重度依赖于Zookeeper做元数据管理和集群的高可用。在Kafka2.8之后，引入了基于Raft协议的KRaft模式，不再依赖Zookeeper，大大简化了Kafka的架构，让你可以以一种轻量级的方式来使用Kafka。不过，要提示一下：如果要使用KRaft模式的话，建议选择较高版本的Kafka，因为这个功能还在持续完善优化中。Kafka3.3.1版本是第一个将KRaft（KafkaRaft）共识协议标记为生产就绪的版本。\n\n![img](https://guide-blog-images.oss-cn-shenzhen.aliyuncs.com/github/javaguide/high-performance/message-queue/kafka3.3.1-kraft-%20production-ready.png)\n\n> [Kafka官网](http://kafka.apache.org/)\n> [Kafka更新记录](https://kafka.apache.org/downloads)\n\n#### Kafka配置\n\n**consumer**\n\n```properties\n# 消费者所属消费组的唯一标识\ngroup.id\n# 一次拉取请求的最大消息数，默认500条\nmax.poll.records\n# 指定拉取消息线程最长空闲时间，默认300000ms\nmax.poll.interval.ms\n# 检测消费者是否失效的超时时间，默认10000ms\nsession.timeout.ms\n# 消费者心跳时间，默认3000ms\nheartbeat.interval.ms\n# 连接集群broker地址\nbootstrap.servers\n# 是否开启自动提交消费位移的功能，默认true\nenable.auto.commit\n# 自动提交消费位移的时间间隔，默认5000ms\nauto.commit.interval.ms\n# 消费者的分区配置策略,默认RangeAssignor\npartition.assignment.strategy\n# 如果分区没有初始偏移量，或者当前偏移量服务器上不存在时，将使用的偏移量设置，earliest从头开始消费，latest从最近的开始消费，none抛出异常，如果存在已经提交的offest时,不管设置为earliest或者latest都会从已经提交的offest处开始消费,如果不存在已经提交的offest时,earliest表示从头开始消费,latest表示从最新的数据消费,也就是新产生的数据.none topic各分区都存在已提交的offset时，从提交的offest处开始消费；只要有一个分区不存在已提交的offset，则抛出异常.kafka-0.10.1.X版本之前: auto.offset.reset的值为smallest,和,largest.(offest保存在zk中).kafka-0.10.1.X版本之后: auto.offset.reset的值更改为:earliest,latest,和none(offest保存在kafka的一个特殊的topic,名为:__consumer_offsets里面)\nauto.offset.reset\n# 消费者客户端一次请求从Kafka拉取消息的最小数据量，如果Kafka返回的数据量小于该值，会一直等待，直到满足这个配置大小，默认1b\nfetch.min.bytes\n# 消费者客户端一次请求从Kafka拉取消息的最大数据量，默认50MB\nfetch.max.bytes\n# 从Kafka拉取消息时，在不满足fetch.min.bytes条件时，等待的最大时间，默认500ms\nfetch.max.wait.ms\n# 强制刷新元数据时间，毫秒，默认300000，5分钟\nmetadata.max.age.ms\n# 设置从每个分区里返回给消费者的最大数据量，区别于fetch.max.bytes，默认1MB\nmax.partition.fetch.bytes\n# Socket发送缓冲区大小，默认128kb,-1将使用操作系统的设置\nsend.buffer.bytes\n# Socket发送缓冲区大小，默认64kb,-1将使用操作系统的设置\nreceive.buffer.bytes\n# 消费者客户端的id\nclient.id\n# 连接失败后，尝试连接Kafka的时间间隔，默认50ms\nreconnect.backoff.ms\n# 尝试连接到Kafka，生产者客户端等待的最大时间，默认1000ms\nreconnect.backoff.max.ms\n# 消息发送失败重试时间间隔，默认100ms\nretry.backoff.ms\n# 样本计算时间窗口，默认30000ms\nmetrics.sample.window.ms\n# 用于维护metrics的样本数量，默认2\nmetrics.num.samples\n# metrics日志记录级别，默认info\nmetrics.log.level\n# 类的列表，用于衡量指标，默认空list\nmetric.reporters\n# 自动检查CRC32记录的消耗\ncheck.crcs\n# key反序列化方式\nkey.deserializer\n# value反序列化方式\nvalue.deserializer\n# 设置多久之后关闭空闲连接，默认540000ms\nconnections.max.idle.ms\n# 客户端将等待请求的响应的最大时间,如果在这个时间内没有收到响应，客户端将重发请求，超过重试次数将抛异常，默认30000ms\nrequest.timeout.ms\n# 设置消费者api超时时间，默认60000ms\ndefault.api.timeout.ms\n# 自定义拦截器\ninterceptor.classes\n# 内部的主题:consumer_offsets和一transaction_state。该参数用来指定Kafka中的内部主题是否可以向消费者公开，默认值为true。如果设置为true，那么只能使用subscribe(Collection)的方式而不能使用subscribe(Pattern)的方式来订阅内部主题，设置为false则没有这个限制。\nexclude.internal.topics\n# 用来配置消费者的事务隔离级别。如果设置为“read committed”，那么消费者就会忽略事务未提交的消息，即只能消费到LSO(LastStableOffset)的位置，默认情况下为“read_uncommitted”，即可以消费到HW(High Watermark)处的位置\nisolation.level\nkey.deserializer = org.apache.kafka.common.serialization.StringDeserializer\nvalue.deserializer = org.apache.kafka.common.serialization.StringDeserializer\n\n```\n**producer**\n\n```properties\n# Snappy压缩技术是Google开发的，它可以在提供较好的压缩比的同时，减少对CPU的使用率并保证好的性能，所以建议在同时考虑性能和带宽的情况下使用。Gzip压缩技术通常会使用更多的CPU和时间，但会产生更好的压缩比，所以建议在网络带宽更受限制的情况下使用，默认不压缩，该参数可以设置成snappy、gzip或lz4对发送给broker的消息进行压缩\ncompression.type=Gzip\n# 请求的最大字节数。这也是对最大消息大小的有效限制。注意：server具有自己对消息大小的限制，这些大小和这个设置不同。此项设置将会限制producer每次批量发送请求的数目，以防发出巨量的请求。\nmax.request.size=1048576\n# TCP的接收缓存SO_RCVBUF空间大小，用于读取数据\nreceive.buffer.bytes=32768\n# client等待请求响应的最大时间,如果在这个时间内没有收到响应，客户端将重发请求，超过重试次数发送失败\nrequest.timeout.ms=30000\n# TCP的发送缓存SO_SNDBUF空间大小，用于发送数据\nsend.buffer.bytes=131072\n# 指定server等待来自followers的确认的最大时间，根据acks的设置，超时则返回error\ntimeout.ms=30000\n# 在block前一个connection上允许最大未确认的requests数量。当设为1时，即是消息保证有序模式，注意：这里的消息保证有序是指对于单个Partition的消息有顺序，因此若要保证全局消息有序，可以只使用一个Partition，当然也会降低性能\nmax.in.flight.requests.per.connection=5\n# 在第一次将数据发送到某topic时，需先fetch该topic的metadata，得知哪些服务器持有该topic的partition，该值为最长获取metadata时间\nmetadata.fetch.timeout.ms=60000\n# 连接失败时，当我们重新连接时的等待时间\nreconnect.backoff.ms=50\n# 在重试发送失败的request前的等待时间，防止若目的Broker完全挂掉的情况下Producer一直陷入死循环发送，折中的方法\nretry.backoff.ms=100\n# metrics系统维护可配置的样本数量，在一个可修正的window size\nmetrics.sample.window.ms=30000\n# 用于维护metrics的样本数\nmetrics.num.samples=2\n# 类的列表，用于衡量指标。实现MetricReporter接口\nmetric.reporters=[]\n# 强制刷新metadata的周期，即使leader没有变化\nmetadata.max.age.ms=300000\n# 与broker会话协议，取值：LAINTEXT,SSL,SASL_PLAINTEXT,SASL_SSL\nsecurity.protocol=PLAINTEXT\n# 分区类，实现Partitioner接口\npartitioner.class=class org.apache.kafka.clients.producer.internals.DefaultPartitioner\n# 控制block的时长，当buffer空间不够或者metadata丢失时产生block\nmax.block.ms=60000\n# 关闭达到该时间的空闲连接\nconnections.max.idle.ms=540000\n# 当向server发出请求时，这个字符串会发送给server，目的是能够追踪请求源\nclient.id=\"\"\n# acks=0配置适用于实现非常高的吞吐量,acks=all这是最安全的模式。Server完成producer request前需要确认的数量。acks=0时，producer不会等待确认，直接添加到socket等待发送；acks=1时，等待leader写到local log就行；acks=all或acks=-1时，等待isr中所有副本确认（注意：确认都是broker接收到消息放入内存就直接返回确认，不是需要等待数据写入磁盘后才返回确认，这也是kafka快的原因）\nacks = all\n# 发生错误时，重传次数。当开启重传时，需要将`max.in.flight.requests.per.connection`设置为1，否则可能导致失序\nretries = 0\n# 发送到同一个partition的消息会被先存储在batch中，该参数指定一个batch可以使用的内存大小，单位是byte。不一定需要等到batch被填满才能发送Producer可以将发往同一个Partition的数据做成一个Produce Request发送请求，即Batch批处理，以减少请求次数，该值即为每次批处理的大小。另外每个Request请求包含多个Batch，每个Batch对应一个Partition，且一个Request发送的目的Broker均为这些partition的leader副本。若将该值设为0，则不会进行批处理\nbatch.size = 16384\n# 生产者在发送消息前等待linger.ms，从而等待更多的消息加入到batch中。如果batch被填满或者linger.ms达到上限，就把batch中的消息发送出去,Producer默认会把两次发送时间间隔内收集到的所有Requests进行一次聚合然后再发送，以此提高吞吐量，而linger.ms则更进一步，这个参数为每次发送增加一些delay，以此来聚合更多的Message。官网解释翻译：producer会将request传输之间到达的所有records聚合到一个批请求。通常这个值发生在欠负载情况下，record到达速度快于发送。但是在某些场景下，client即使在正常负载下也期望减少请求数量。这个设置就是如此，通过人工添加少量时延，而不是立马发送一个record,producer会等待所给的时延，以让其他records发送出去，这样就会被聚合在一起。这个类似于TCP的Nagle算法。该设置给了batch的时延上限：当我们获得一个partition的batch.size大小的records，就会立即发送出去，而不管该设置；但是如果对于这个partition没有累积到足够的record，会linger指定的时间等待更多的records出现。该设置的默认值为0(无时延)。例如，设置linger.ms=5，会减少request发送的数量，但是在无负载下会增加5ms的发送时延。\nlinger.ms = 1\n# Producer可以用来缓存数据的内存大小。该值实际为RecordAccumulator类中的BufferPool，即Producer所管理的最大内存。如果数据产生速度大于向broker发送的速度，producer会阻塞max.block.ms，超时则抛出异常\nbuffer.memory = 33554432\nkey.serializer = org.apache.kafka.common.serialization.StringSerializer\nvalue.serializer = org.apache.kafka.common.serialization.StringSerializer\n```\n\n#### Kafka是什么？主要应用场景有哪些？\n\nKafka是一个分布式流式处理平台。这到底是什么意思呢？\n\n流平台具有三个关键功能：\n\n1. **消息队列**：发布和订阅消息流，这个功能类似于消息队列，这也是Kafka也被归类为消息队列的原因。\n2. **容错的持久方式存储记录消息流**：Kafka会把消息持久化到磁盘，有效避免了消息丢失的风险。\n3. **流式处理平台**：在消息发布的时候进行处理，Kafka提供了一个完整的流式处理类库。\n\nKafka主要有两大应用场景：\n\n1. **消息队列**：建立实时流数据管道，以可靠地在系统或应用程序之间获取数据。\n2. **数据处理**：构建实时的流数据处理程序来转换或处理数据流。\n\n#### 和其他消息队列相比,Kafka的优势在哪里？\n\n我们现在经常提到Kafka的时候就已经默认它是一个非常优秀的消息队列了，我们也会经常拿它跟RocketMQ、RabbitMQ对比。我觉得Kafka相比其他消息队列主要的优势如下：\n\n1. **极致的性能**：基于Scala和Java语言开发，设计中大量使用了批量处理和异步的思想，最高可以每秒处理千万级别的消息。\n2. **生态系统兼容性无可匹敌**：Kafka与周边生态系统的兼容性是最好的没有之一，尤其在大数据和流计算领域。\n\n实际上在早期的时候Kafka并不是一个合格的消息队列，早期的Kafka在消息队列领域就像是一个衣衫褴褛的孩子一样，功能不完备并且有一些小问题比如丢失消息、不保证消息可靠性等等。当然，这也和LinkedIn最早开发Kafka用于处理海量的日志有很大关系，哈哈哈，人家本来最开始就不是为了作为消息队列滴，谁知道后面误打误撞在消息队列领域占据了一席之地。随着后续的发展，这些短板都被Kafka逐步修复完善。所以，Kafka作为消息队列不可靠这个说法已经过时！\n\n#### 队列模型了解吗？Kafka的消息模型知道吗？\n\n> 题外话：早期的JMS和AMQP属于消息服务领域权威组织所做的相关的标准，我在[《消息队列其实很简单》](https://github.com/Snailclimb/JavaGuide#数据通信中间件)这篇文章中介绍过。但是，这些标准的进化跟不上消息队列的演进速度，这些标准实际上已经属于废弃状态。所以，可能存在的情况是：不同的消息队列都有自己的一套消息模型。\n\n##### 队列模型：早期的消息模型\n\n![队列模型](https://my-blog-to-use.oss-cn-beijing.aliyuncs.com/2019-11/队列模型23.png)\n\n使用队列（Queue）作为消息通信载体，满足生产者与消费者模式，一条消息只能被一个消费者使用，未被消费的消息在队列中保留直到被消费或超时。比如：我们生产者发送100条消息的话，两个消费者来消费一般情况下两个消费者会按照消息发送的顺序各自消费一半（也就是你一个我一个的消费。）\n\n**队列模型存在的问题：**\n\n假如我们存在这样一种情况：我们需要将生产者产生的消息分发给多个消费者，并且每个消费者都能接收到完整的消息内容。这种情况，队列模型就不好解决了。很多比较杠精的人就说：我们可以为每个消费者创建一个单独的队列，让生产者发送多份。这是一种非常愚蠢的做法，浪费资源不说，还违背了使用消息队列的目的。\n\n##### 发布-订阅模型:Kafka消息模型\n\n发布-订阅模型主要是为了解决队列模型存在的问题。\n\n![发布订阅模型](https://guide-blog-images.oss-cn-shenzhen.aliyuncs.com/java-guide-blog/发布订阅模型.png)\n\n发布订阅模型（Pub-Sub）使用主题（Topic）作为消息通信载体，类似于广播模式；发布者发布一条消息，该消息通过主题传递给所有的订阅者，在一条消息广播之后才订阅的用户则是收不到该条消息的。\n\n在发布-订阅模型中，如果只有一个订阅者，那它和队列模型就基本是一样的了。所以说，发布-订阅模型在功能层面上是可以兼容队列模型的。\n\n**Kafka采用的就是发布-订阅模型。**\n\n> **RocketMQ的消息模型和Kafka基本是完全一样的。唯一的区别是Kafka中没有队列这个概念，与之对应的是Partition（分区）。**\n\n#### 什么是Producer、Consumer、Broker、Topic、Partition？\n\nKafka将生产者发布的消息发送到Topic（主题）中，需要这些消息的消费者可以订阅这些Topic（主题），如下图所示：\n\n![img](https://guide-blog-images.oss-cn-shenzhen.aliyuncs.com/github/javaguide/high-performance/message-queue20210507200944439.png)\n\n上面这张图也为我们引出了，Kafka比较重要的几个概念：\n\n1. **Producer（生产者）**:产生消息的一方。\n2. **Consumer（消费者）**:消费消息的一方。\n3. **Broker（代理）**:可以看作是一个独立的Kafka实例。多个KafkaBroker组成一个KafkaCluster。\n\n同时，你一定也注意到每个Broker中又包含了Topic以及Partition这两个重要的概念：\n\n- **Topic（主题）**:Producer将消息发送到特定的主题，Consumer通过订阅特定的Topic(主题)来消费消息。\n- **Partition（分区）**:Partition属于Topic的一部分。一个Topic可以有多个Partition，并且同一Topic下的Partition可以分布在不同的Broker上，这也就表明一个Topic可以横跨多个Broker。这正如我上面所画的图一样。\n\n> 划重点：Kafka中的Partition（分区）实际上可以对应成为消息队列中的队列。这样是不是更好理解一点？\n\n#### Kafka的多副本机制了解吗？带来了什么好处？\n\n还有一点我觉得比较重要的是Kafka为分区（Partition）引入了多副本（Replica）机制。分区（Partition）中的多个副本之间会有一个叫做leader的家伙，其他副本称为follower。我们发送的消息会被发送到leader副本，然后follower副本才能从leader副本中拉取消息进行同步。\n\n> 生产者和消费者只与leader副本交互。你可以理解为其他副本只是leader副本的拷贝，它们的存在只是为了保证消息存储的安全性。当leader副本发生故障时会从follower中选举出一个leader,但是follower中如果有和leader同步程度达不到要求的参加不了leader的竞选。\n\n**Kafka的多分区（Partition）以及多副本（Replica）机制有什么好处呢？**\n\n1. Kafka通过给特定Topic指定多个Partition,而各个Partition可以分布在不同的Broker上,这样便能提供比较好的并发能力（负载均衡）。\n2. Partition可以指定对应的Replica数,这也极大地提高了消息存储的安全性,提高了容灾能力，不过也相应的增加了所需要的存储空间。\n\n#### Zookeeper在Kafka中的作用知道吗？\n\n> 要想搞懂zookeeper在Kafka中的作用一定要自己搭建一个Kafka环境然后自己进zookeeper去看一下有哪些文件夹和Kafka有关，每个节点又保存了什么信息。一定不要光看不实践，这样学来的也终会忘记！这部分内容参考和借鉴了这篇文章：https://www.jianshu.com/p/a036405f989c。\n\nZooKeeper主要为Kafka提供元数据的管理的功能。\n\n从图中我们可以看出，Zookeeper主要为Kafka做了下面这些事情：\n\n1. **Broker注册**：在Zookeeper上会有一个专门用来进行Broker服务器列表记录的节点。每个Broker在启动时，都会到Zookeeper上进行注册，即到`/brokers/ids`下创建属于自己的节点。每个Broker就会将自己的IP地址和端口等信息记录到该节点中去\n2. **Topic注册**：在Kafka中，同一个Topic的消息会被分成多个分区并将其分布在多个Broker上，这些分区信息及与Broker的对应关系也都是由Zookeeper在维护。比如我创建了一个名字为my-topic的主题并且它有两个分区，对应到zookeeper中会创建这些文件夹：`/brokers/topics/my-topic/Partitions/0`、`/brokers/topics/my-topic/Partitions/1`\n3. **负载均衡**：上面也说过了Kafka通过给特定Topic指定多个Partition,而各个Partition可以分布在不同的Broker上,这样便能提供比较好的并发能力。对于同一个Topic的不同Partition，Kafka会尽力将这些Partition分布到不同的Broker服务器上。当生产者产生消息后也会尽量投递到不同Broker的Partition里面。当Consumer消费的时候，Zookeeper可以根据当前的Partition数量以及Consumer数量来实现动态负载均衡。\n4. ......\n\n#### Kafka如何保证消息的消费顺序？\n\n我们在使用消息队列的过程中经常有业务场景需要严格保证消息的消费顺序，比如我们同时发了2个消息，这2个消息对应的操作分别对应的数据库操作是：\n\n1. 更改用户会员等级。\n2. 根据会员等级计算订单价格。\n\n假如这两条消息的消费顺序不一样造成的最终结果就会截然不同。\n\n我们知道Kafka中Partition(分区)是真正保存消息的地方，我们发送的消息都被放在了这里。而我们的Partition(分区)又存在于Topic(主题)这个概念中，并且我们可以给特定Topic指定多个Partition。\n\n![img](https://my-blog-to-use.oss-cn-beijing.aliyuncs.com/2019-11/KafkaTopicPartionsLayout.png)\n\n每次添加消息到Partition(分区)的时候都会采用尾加法，如上图所示。**Kafka只能为我们保证Partition(分区)中的消息有序**。\n\n> 消息在被追加到Partition(分区)的时候都会分配一个特定的偏移量（offset）。Kafka通过偏移量（offset）来保证消息在分区内的顺序性。\n\n所以，我们就有一种很简单的保证消息消费顺序的方法：**1个Topic只对应一个Partition**。这样当然可以解决问题，但是破坏了Kafka的设计初衷。Kafka中发送1条消息的时候，可以指定topic,partition,key,data（数据）4个参数。如果你发送消息的时候指定了Partition的话，所有消息都会被发送到指定的Partition。并且，同一个key的消息可以保证只发送到同一个partition，这个我们可以采用表/对象的id来作为key。\n\n总结一下，对于如何保证Kafka中消息消费的顺序，有了下面两种方法：\n\n1. 1个Topic只对应一个Partition。\n2. （推荐）发送消息的时候指定key/Partition。\n\n#### Kafka如何保证消息不丢失\n\n**生产者丢失消息的情况**\n\n生产者(Producer)调用`send`方法发送消息之后，消息可能因为网络问题并没有发送过去。所以，我们不能默认在调用`send`方法发送消息之后消息发送成功了。为了确定消息是发送成功，我们要判断消息发送的结果。但是要注意的是Kafka生产者(Producer)使用`send`方法发送消息实际上是异步的操作，我们可以通过`get()`方法获取调用结果，但是这样也让它变为了同步操作，示例代码如下：\n\n> 详细代码见我的这篇文章：[Kafka系列第三篇！10分钟学会如何在Spring Boot程序中使用Kafka作为消息队列](https://mp.weixin.qq.com/s?__biz=Mzg2OTA0Njk0OA==&mid=2247486269&idx=2&sn=ec00417ad641dd8c3d145d74cafa09ce&chksm=cea244f6f9d5cde0c8eb233fcc4cf82e11acd06446719a7af55230649863a3ddd95f78d111de&token=1633957262&lang=zh_CN#rd)\n\n```java\nSendResult<String, Object> sendResult = kafkaTemplate.send(topic, o).get();\nif (sendResult.getRecordMetadata() != null) {\n  logger.info(\"生产者成功发送消息到\" + sendResult.getProducerRecord().topic() + \"-> \" + sendResult.getProducerRecord().value().toString());\n}\n```\n\n但是一般不推荐这么做！可以采用为其添加回调函数的形式，示例代码如下：\n\n\n```java\nListenableFuture<SendResult<String, Object>> future = kafkaTemplate.send(topic, o);\nfuture.addCallback(result -> logger.info(\"生产者成功发送消息到topic:{} partition:{}的消息\", result.getRecordMetadata().topic(), result.getRecordMetadata().partition()),\n                ex -> logger.error(\"生产者发送消失败，原因：{}\", ex.getMessage()));\n```\n\n如果消息发送失败的话，我们检查失败的原因之后重新发送即可！\n\n另外这里推荐为Producer的`retries`（重试次数）设置一个比较合理的值，一般是3，但是为了保证消息不丢失的话一般会设置比较大一点。设置完成之后，当出现网络问题之后能够自动重试消息发送，避免消息丢失。另外，建议还要设置重试间隔，因为间隔太小的话重试的效果就不明显了，网络波动一次你3次一下子就重试完了\n\n**消费者丢失消息的情况**\n\n我们知道消息在被追加到Partition(分区)的时候都会分配一个特定的偏移量（offset）。偏移量(offset)表示Consumer当前消费到的Partition(分区)的所在的位置。Kafka通过偏移量（offset）可以保证消息在分区内的顺序性。\n\n![kafka offset](https://my-blog-to-use.oss-cn-beijing.aliyuncs.com/2019-11/kafka-offset.jpg)\n\n当消费者拉取到了分区的某个消息之后，消费者会自动提交了offset。自动提交的话会有一个问题，试想一下，当消费者刚拿到这个消息准备进行真正消费的时候，突然挂掉了，消息实际上并没有被消费，但是offset却被自动提交了。\n\n**解决办法也比较粗暴，我们手动关闭自动提交offset，每次在真正消费完消息之后再自己手动提交offset**。但是，细心的朋友一定会发现，这样会带来消息被重新消费的问题。比如你刚刚消费完消息之后，还没提交offset，结果自己挂掉了，那么这个消息理论上就会被消费两次。\n\n**Kafka弄丢了消息**\n\n我们知道Kafka为分区（Partition）引入了多副本（Replica）机制。分区（Partition）中的多个副本之间会有一个叫做leader的家伙，其他副本称为follower。我们发送的消息会被发送到leader副本，然后follower副本才能从leader副本中拉取消息进行同步。生产者和消费者只与leader副本交互。你可以理解为其他副本只是leader副本的拷贝，它们的存在只是为了保证消息存储的安全性。试想一种情况：假如leader副本所在的broker突然挂掉，那么就要从follower副本重新选出一个leader，但是leader的数据还有一些没有被follower副本的同步的话，就会造成消息丢失。\n\n**设置acks=all**\n\n解决办法就是我们设置**acks=all**。acks是Kafka生产者(Producer)很重要的一个参数。acks的默认值即为1，代表我们的消息被leader副本接收之后就算被成功发送。当我们配置**acks=all**表示只有所有ISR列表的副本全部收到消息时，生产者才会接收到来自服务器的响应.这种模式是最高级别的，也是最安全的，可以确保不止一个Broker接收到了消息.该模式的延迟会很高.\n\n**设置replication.factor>=3**\n\n为了保证leader副本能有follower副本能同步消息，我们一般会为topic设置**replication.factor>=3**。这样就可以保证每个分区(partition)至少有3个副本。虽然造成了数据冗余，但是带来了数据的安全性。\n\n**设置min.insync.replicas>1**\n\n一般情况下我们还需要设置**min.insync.replicas>1**，这样配置代表消息至少要被写入到2个副本才算是被成功发送。**min.insync.replicas**的默认值为1，在实际生产中应尽量避免默认值1。\n\n但是，为了保证整个Kafka服务的高可用性，你需要确保**replication.factor>min.insync.replicas**。为什么呢？设想一下假如两者相等的话，只要是有一个副本挂掉，整个分区就无法正常工作了。这明显违反高可用性！一般推荐设置成**replication.factor=min.insync.replicas+1**。\n\n**设置unclean.leader.election.enable=false**\n\n> Kafka0.11.0.0版本开始unclean.leader.election.enable参数的默认值由原来的true改为false\n\n我们最开始也说了我们发送的消息会被发送到leader副本，然后follower副本才能从leader副本中拉取消息进行同步。多个follower副本之间的消息同步情况不一样，当我们配置了**unclean.leader.election.enable=false**的话，当leader副本发生故障时就不会从follower副本中和leader同步程度达不到要求的副本中选择出leader，这样降低了消息丢失的可能性。\n\n> [Kafka是如何保证消息不丢失的](https://mp.weixin.qq.com/s/q-MrJgr8WrB_HGOFRUUWKQ)\n\n#### Kafka如何保证消息不重复消费\n\n**kafka出现消息重复消费的原因：**\n\n- 服务端侧已经消费的数据没有成功提交offset（根本原因）。\n- Kafka侧由于服务端处理业务时间长或者网络链接等等原因让Kafka认为服务假死，触发了分区rebalance。\n\n**解决方案：**\n\n- 消费消息服务做幂等校验，比如Redis的set、MySQL的主键等天然的幂等功能。这种方法最有效。\n\n- 将`enable.auto.commit`参数设置为false，关闭自动提交，开发者在代码中手动提交offset。那么这里会有个问题：什么时候提交offset合适？\n\n  - 处理完消息再提交：依旧有消息重复消费的风险，和自动提交一样\n  - 拉取到消息即提交：会有消息丢失的风险。允许消息延时的场景，一般会采用这种方式。然后，通过定时任务在业务不繁忙（比如凌晨）的时候做数据兜底\n\n> [原文链接](https://javaguide.cn/high-performance/message-queue/kafka-questions-01.html)\n\n#### 相关文章\n\n- [Kafka安装及快速入门](https://mp.weixin.qq.com/s?__biz=Mzg2MDYzODI5Nw==&mid=2247493725&idx=1&sn=96eb8c16aa8cfe8336334d93c4034ed2&source=41#wechat_redirect)\n- [带你涨姿势的认识一下kafka](https://mp.weixin.qq.com/s?__biz=MzkzODE3OTI0Ng==&mid=2247491187&idx=1&sn=f0f092e675677d87ea0675f65bb1eb14&source=41#wechat_redirect)\n- [Kafka Shell基本命令](https://www.cnblogs.com/xiaodf/p/6093261.html)\n- [漫画：图解Kafka，看本篇就足够啦](https://mp.weixin.qq.com/s/FP3z5JMRB9Oo5pr22kr3XA)\n- [你能说出Kafka这些原理吗？](https://mp.weixin.qq.com/s/lXds_G5LLs7CArMLyTAF6g)\n- [Kafka基本架构及原理](https://mp.weixin.qq.com/s?__biz=Mzg2MDYzODI5Nw==&mid=2247493937&idx=1&sn=3b2710a62cbb51b3f7c607bfd97664c1&source=41#wechat_redirect)\n- [9个Kafka面试题，你会几个？](https://mp.weixin.qq.com/s/LAXrK6WT1bv7I63BmvpXQg)\n- [Kafka中副本机制的设计和原理](https://mp.weixin.qq.com/s/yIPIABpAzaHJvGoJ6pv0kg)\n- [从面试角度一文学完Kafka](https://mp.weixin.qq.com/s/Ndwrlst-aVdUpcNz-inM3Q)\n- [支持百万级TPS，Kafka是怎么做到的？](https://mp.weixin.qq.com/s/Fvd_RcdMHzcwm3fBlpT8vw)\n- [Kafka深度剖析](https://mp.weixin.qq.com/s?__biz=Mzg2MDYzODI5Nw==&mid=2247494362&idx=1&sn=f16c4f215dd83c81957b0b431b147902&source=41#wechat_redirect)\n- [Kafka性能篇：为何Kafka这么\"快\"](https://mp.weixin.qq.com/s/2wGCEWisbynCIjRsO2gxDg)\n- [Kafka为什么吞吐量大、速度快？](https://mp.weixin.qq.com/s/fOhlF6crYGXRNHjV0nLQLQ)\n- [图解kafka架构与工作原理](https://mp.weixin.qq.com/s/V3BlPGOuGPnmT36a500o7g)\n- [面试官：聊一聊kafka线上会遇到哪些问题？](https://mp.weixin.qq.com/s/cmDqGpTiHKRqcSaQVesJVA)\n- [《面试八股文》之Kafka21卷](https://mp.weixin.qq.com/s/l75MIcG4cf8C59z1JcfygA)\n- [20,000+字，彻底搞懂Kafka](https://mp.weixin.qq.com/s/nKjKtJ5lOmYLjUrgS5qwsQ)\n- [一文掌握Kafka生产者发送消息流程原理](https://mp.weixin.qq.com/s/xe3GMo8DxZUHh-y-L9B2Nw)\n\n### RocketMQ\n\n![img](https://guide-blog-images.oss-cn-shenzhen.aliyuncs.com/github/javaguide/high-performance/message-queue/rocketmq-logo.png)\n\nRocketMQ是阿里开源的一款云原生“消息、事件、流”实时数据处理平台，借鉴了Kafka，已经成为Apache顶级项目。\n\nRocketMQ的核心特性（摘自RocketMQ官网）：\n\n- 云原生：生与云，长与云，无限弹性扩缩，K8s友好\n- 高吞吐：万亿级吞吐保证，同时满足微服务与大数据场景。\n- 流处理：提供轻量、高扩展、高性能和丰富功能的流计算引擎。\n- 金融级：金融级的稳定性，广泛用于交易核心链路。\n- 架构极简：零外部依赖，Shared-nothing架构。\n- 生态友好：无缝对接微服务、实时计算、数据湖等周边生态。\n\n根据官网介绍：\n\n> ApacheRocketMQ自诞生以来，因其架构简单、业务功能丰富、具备极强可扩展性等特点被众多企业开发者以及云厂商广泛采用。历经十余年的大规模场景打磨，RocketMQ已经成为业内共识的金融级可靠业务消息首选方案，被广泛应用于互联网、大数据、移动互联网、物联网等领域的业务场景。\n> [RocketMQ官网](https://rocketmq.apache.org/)（文档很详细，推荐阅读）\n> [RocketMQ更新记录](https://github.com/apache/rocketmq/releases)\n\n#### RocketMQ基础知识总结\n\n##### 队列模型和主题模型\n\n在谈RocketMQ的技术架构之前，我们先来了解一下两个名词概念——**队列模型**和**主题模型**。\n\n###### 队列模型\n\n就像我们理解队列一样，消息中间件的队列模型就真的只是一个队列。\n\n![img](https://my-blog-to-use.oss-cn-beijing.aliyuncs.com/2019-11/16ef3834ae653469.jpg)\n\n在一开始我跟你提到了一个广播的概念，也就是说如果我们此时我们需要将一个消息发送给多个消费者(比如此时我需要将信息发送给短信系统和邮件系统)，这个时候单个队列即不能满足需求了。\n\n当然你可以让Producer生产消息放入多个队列中，然后每个队列去对应每一个消费者。问题是可以解决，创建多个队列并且复制多份消息是会很影响资源和性能的。而且，这样子就会导致生产者需要知道具体消费者个数然后去复制对应数量的消息队列，这就违背我们消息中间件的解耦这一原则。\n\n###### 主题模型\n\n那么有没有好的方法去解决这一个问题呢？有，那就是主题模型或者可以称为发布订阅模型。\n\n> 感兴趣的同学可以去了解一下设计模式里面的观察者模式并且手动实现一下，我相信你会有所收获的。\n\n在主题模型中，消息的生产者称为**发布者（Publisher）**，消息的消费者称为**订阅者（Subscriber）**，存放消息的容器称为**主题（Topic）**。\n\n其中，发布者将消息发送到指定主题中，订阅者需要提前订阅主题才能接受特定主题的消息。\n\n![img](https://my-blog-to-use.oss-cn-beijing.aliyuncs.com/2019-11/16ef3837887d9a54sds.jpg)\n\n###### RocketMQ中的消息模型\n\nRocketMQ中的消息模型就是按照主题模型所实现的。其实对于主题模型的实现来说每个消息中间件的底层设计都是不一样的，就比如Kafka中的分区，RocketMQ中的队列，RabbitMQ中的Exchange。我们可以理解为主题模型/发布订阅模型就是一个标准，那些中间件只不过照着这个标准去实现而已。\n\n所以，RocketMQ中的主题模型到底是如何实现的呢？画一张图尝试着去理解一下。\n\n![img](https://my-blog-to-use.oss-cn-beijing.aliyuncs.com/2019-11/16ef383d3e8c9788.jpg)\n\n我们可以看到在整个图中有ProducerGroup、Topic、ConsumerGroup三个角色。\n\n- ProducerGroup生产者组：代表某一类的生产者，比如我们有多个秒杀系统作为生产者，这多个合在一起就是一个ProducerGroup生产者组，它们一般生产相同的消息。\n- ConsumerGroup消费者组：代表某一类的消费者，比如我们有多个短信系统作为消费者，这多个合在一起就是一个ConsumerGroup消费者组，它们一般消费相同的消息。\n- Topic主题：代表一类消息，比如订单消息，物流消息等等。\n\n你可以看到图中生产者组中的生产者会向主题发送消息，而主题中存在多个队列，生产者每次生产消息之后是指定主题中的某个队列发送消息的。每个主题中都有多个队列(分布在不同的Broker中，如果是集群的话，Broker又分布在不同的服务器中)，集群消费模式下，一个消费者集群多台机器共同消费一个topic的多个队列，一个队列只会被一个消费者消费。如果某个消费者挂掉，分组内其它消费者会接替挂掉的消费者继续消费。就像上图中Consumer1和Consumer2分别对应着两个队列，而Consumer3是没有队列对应的，所以一般来讲要控制消费者组中的消费者个数和主题中队列个数相同。当然也可以消费者个数小于队列个数，只不过不太建议。如下图。\n\n![img](https://my-blog-to-use.oss-cn-beijing.aliyuncs.com/2019-11/16ef3850c808d707.jpg)\n\n**每个消费组在每个队列上维护一个消费位置**，为什么呢？\n\n因为我们刚刚画的仅仅是一个消费者组，我们知道在发布订阅模式中一般会涉及到多个消费者组，而每个消费者组在每个队列中的消费位置都是不同的。如果此时有多个消费者组，那么消息被一个消费者组消费完之后是不会删除的(因为其它消费者组也需要呀)，它仅仅是为每个消费者组维护一个**消费位移（offset）**，每次消费者组消费完会返回一个成功的响应，然后队列再把维护的消费位移加一，这样就不会出现刚刚消费过的消息再一次被消费了。\n\n![img](https://my-blog-to-use.oss-cn-beijing.aliyuncs.com/2019-11/16ef3857fefaa079.jpg)\n\n可能你还有一个问题，**为什么一个主题中需要维护多个队列**？\n\n答案是提高并发能力。的确，每个主题中只存在一个队列也是可行的。你想一下，如果每个主题中只存在一个队列，这个队列中也维护着每个消费者组的消费位置，这样也可以做到发布订阅模式。如下图。\n\n![img](https://my-blog-to-use.oss-cn-beijing.aliyuncs.com/2019-11/16ef38600cdb6d4b.jpg)\n\n但是，这样我生产者是不是只能向一个队列发送消息？又因为需要维护消费位置所以一个队列只能对应一个消费者组中的消费者，这样是不是其他的Consumer就没有用武之地了？从这两个角度来讲，并发度一下子就小了很多。\n\n所以总结来说，RocketMQ通过使用在一个Topic中配置多个队列并且每个队列维护每个消费者组的消费位置实现了主题模式/发布订阅模式。\n\n##### RocketMQ的架构图\n\nRocketMQ技术架构中有四大角色NameServer、Broker、Producer、Consumer。\n\n- Broker：主要负责消息的存储、投递和查询以及服务高可用保证。说白了就是消息队列服务器嘛，生产者生产消息到Broker，消费者从Broker拉取消息并消费。\n\n  这里，我还得普及一下关于Broker、Topic和队列的关系。上面我讲解了Topic和队列的关系——一个Topic中存在多个队列，那么这个Topic和队列存放在哪呢？\n\n  **一个Topic分布在多个Broker上，一个Broker可以配置多个Topic，它们是多对多的关系**。\n\n  如果某个Topic消息量很大，应该给它多配置几个队列(上文中提到了提高并发能力)，并且尽量多分布在不同Broker上，以减轻某个Broker的压力。\n\n  Topic消息量都比较均匀的情况下，如果某个broker上的队列越多，则该broker压力越大。\n\n![img](https://my-blog-to-use.oss-cn-beijing.aliyuncs.com/2019-11/16ef38687488a5a4.jpg)\n\n  > 所以说我们需要配置多个Broker。\n\n- NameServer：不知道你们有没有接触过ZooKeeper和SpringCloud中的Eureka，它其实也是一个注册中心，主要提供两个功能：Broker管理和路由信息管理。说白了就是Broker会将自己的信息注册到NameServer中，此时NameServer就存放了很多Broker的信息(Broker的路由表)，消费者和生产者就从NameServer中获取路由表然后照着路由表的信息和对应的Broker进行通信(生产者和消费者定期会向NameServer去查询相关的Broker的信息)。\n\n- Producer：消息发布的角色，支持分布式集群方式部署。说白了就是生产者。\n\n- Consumer：消息消费的角色，支持分布式集群方式部署。支持以push推，pull拉两种模式对消息进行消费。同时也支持集群方式和广播方式的消费，它提供实时消息订阅机制。说白了就是消费者。\n\n![img](https://my-blog-to-use.oss-cn-beijing.aliyuncs.com/2019-11/16ef386c6d1e8bdb.jpg)\n\n嗯？你可能会发现一个问题，这老家伙NameServer干啥用的，这不多余吗？直接Producer、Consumer和Broker直接进行生产消息，消费消息不就好了么？但是，我们上文提到过Broker是需要保证高可用的，如果整个系统仅仅靠着一个Broker来维持的话，那么这个Broker的压力会不会很大？所以我们需要使用多个Broker来保证负载均衡。如果说，我们的消费者和生产者直接和多个Broker相连，那么当Broker修改的时候必定会牵连着每个生产者和消费者，这样就会产生耦合问题，而NameServer注册中心就是用来解决这个问题的。\n\n> 如果还不是很理解的话，可以去看我介绍SpringCloud的那篇文章，其中介绍了Eureka注册中心。\n\n当然，RocketMQ中的技术架构肯定不止前面那么简单，因为上面图中的四个角色都是需要做集群的。给出一张官网的架构图，尝试理解一下。\n\n![img](https://my-blog-to-use.oss-cn-beijing.aliyuncs.com/2019-11/16ef386fa3be1e53.jpg)\n\n第一、我们的Broker做了集群并且还进行了主从部署，由于消息分布在各个Broker上，一旦某个Broker宕机，则该Broker上的消息读写都会受到影响。所以Rocketmq提供了master/slave的结构，salve定时从master同步数据(同步刷盘或者异步刷盘)，如果master宕机，则slave提供消费服务，但是不能写入消息。\n\n第二、为了保证HA，我们的NameServer也做了集群部署，但是请注意它是去中心化的。也就意味着它没有主节点，你可以很明显地看出NameServer的所有节点是没有进行InfoReplicate的，在RocketMQ中是通过单个Broker和所有NameServer保持长连接，并且在每隔30秒Broker会向所有Nameserver发送心跳，心跳包含了自身的Topic配置信息，这个步骤就对应这上面的RoutingInfo。\n\n第三、在生产者需要向Broker发送消息的时候，需要先从NameServer获取关于Broker的路由信息，然后通过轮询的方法去向每个队列中生产数据以达到负载均衡的效果。\n\n第四、消费者通过NameServer获取所有Broker的路由信息后，向Broker发送Pull请求来获取消息数据。Consumer可以以两种模式启动——广播（Broadcast）和集群（Cluster）。广播模式下，一条消息会发送给同一个消费组中的所有消费者，集群模式下消息只会发送给一个消费者。\n\n##### 如何解决顺序消费、重复消费\n\n其实，这些东西都是我在介绍消息队列带来的一些副作用的时候提到的，也就是说，这些问题不仅仅挂钩于RocketMQ，而是应该每个消息中间件都需要去解决的。在上面我介绍RocketMQ的技术架构的时候我已经向你展示了它是如何保证高可用的，这里不涉及运维方面的搭建，如果你感兴趣可以自己去官网上照着例子搭建属于你自己的RocketMQ集群。\n\n> 其实Kafka的架构基本和RocketMQ类似，只是它注册中心使用了Zookeeper、它的分区就相当于RocketMQ中的队列。还有一些小细节不同会在后面提到。\n\n###### 顺序消费\n\n在上面的技术架构介绍中，我们已经知道了**RocketMQ在主题上是无序的、它只有在队列层面才是保证有序的**。\n\n这又扯到两个概念——**普通顺序**和**严格顺序**。\n\n所谓普通顺序是指消费者通过**同一个消费队列收到的消息是有顺序的**，不同消息队列收到的消息则可能是无顺序的。普通顺序消息在Broker**重启情况下不会保证消息顺序性**(短暂时间)。\n\n所谓严格顺序是指消费者收到的所有消息均是有顺序的。严格顺序消息即使在异常情况下也会保证消息的顺序性。但是，严格顺序看起来虽好，实现它可会付出巨大的代价。如果你使用严格顺序模式，Broker集群中只要有一台机器不可用，则整个集群都不可用。你还用啥？现在主要场景也就在binlog同步。一般而言，我们的MQ都是能容忍短暂的乱序，所以推荐使用普通顺序模式。那么，我们现在使用了普通顺序模式，我们从上面学习知道了在Producer生产消息的时候会进行轮询(取决你的负载均衡策略)来向同一主题的不同消息队列发送消息。那么如果此时我有几个消息分别是同一个订单的创建、支付、发货，在轮询的策略下这三个消息会被发送到不同队列，因为在不同的队列此时就无法使用RocketMQ带来的队列有序特性来保证消息有序性了。\n\n![img](https://my-blog-to-use.oss-cn-beijing.aliyuncs.com/2019-11/16ef3874585e096e.jpg)\n\n那么，怎么解决呢？其实很简单，我们需要处理的仅仅是将同一语义下的消息放入同一个队列(比如这里是同一个订单)，那我们就可以使用Hash取模法来保证同一个订单在同一个队列中就行了。\n\n###### 重复消费\n\n就两个字——幂等。在编程中一个*幂等*操作的特点是其任意多次执行所产生的影响均与一次执行的影响相同。比如说，这个时候我们有一个订单的处理积分的系统，每当来一个消息的时候它就负责为创建这个订单的用户的积分加上相应的数值。可是有一次，消息队列发送给订单系统FrancisQ的订单信息，其要求是给FrancisQ的积分加上500。但是积分系统在收到FrancisQ的订单信息处理完成之后返回给消息队列处理成功的信息的时候出现了网络波动(当然还有很多种情况，比如Broker意外重启等等)，这条回应没有发送成功。\n\n那么，消息队列没收到积分系统的回应会不会尝试重发这个消息？问题就来了，我再发这个消息，万一它又给FrancisQ的账户加上500积分怎么办呢？所以我们需要给我们的消费者实现幂等，也就是对同一个消息的处理结果，执行多少次都不变。\n\n那么如何给业务实现幂等呢？这个还是需要结合具体的业务的。你可以使用写入Redis来保证，因为Redis的key和value就是天然支持幂等的。当然还有使用数据库插入法，基于数据库的唯一键来保证重复数据不会被插入多条。不过最主要的还是需要根据特定场景使用特定的解决方案，你要知道你的消息消费是否是完全不可重复消费还是可以忍受重复消费的，然后再选择强校验和弱校验的方式。毕竟在CS领域还是很少有技术银弹的说法。而在整个互联网领域，幂等不仅仅适用于消息队列的重复消费问题，这些实现幂等的方法，也同样适用于，在其他场景中来解决重复请求或者重复调用的问题。比如将HTTP服务设计成幂等的，解决前端或者APP重复提交表单数据的问题，也可以将一个微服务设计成幂等的，解决RPC框架自动重试导致的重复调用问题。\n\n##### 分布式事务\n\n如何解释分布式事务呢？事务大家都知道吧？要么都执行要么都不执行。在同一个系统中我们可以轻松地实现事务，但是在分布式架构中，我们有很多服务是部署在不同系统之间的，而不同服务之间又需要进行调用。比如此时我下订单然后增加积分，如果保证不了分布式事务的话，就会出现A系统下了订单，但是B系统增加积分失败或者A系统没有下订单，B系统却增加了积分。前者对用户不友好，后者对运营商不利，这是我们都不愿意见到的。那么，如何去解决这个问题呢？如今比较常见的分布式事务实现有2PC、TCC和事务消息(half半消息机制)。每一种实现都有其特定的使用场景，但是也有各自的问题，都不是完美的解决方案。\n\n在RocketMQ中使用的是事务消息加上事务反查机制来解决分布式事务问题的。我画了张图，大家可以对照着图进行理解。\n\n![img](https://my-blog-to-use.oss-cn-beijing.aliyuncs.com/2019-11/16ef38798d7a987f.png)\n\n在第一步发送的half消息，它的意思是在事务提交之前，对于消费者来说，这个消息是不可见的。\n\n> 那么，如何做到写入消息但是对用户不可见呢？RocketMQ事务消息的做法是：如果消息是half消息，将备份原消息的主题与消息消费队列，然后改变主题为RMQ_SYS_TRANS_HALF_TOPIC。由于消费组未订阅该主题，故消费端无法消费half类型的消息，然后RocketMQ会开启一个定时任务，从Topic为RMQ_SYS_TRANS_HALF_TOPIC中拉取消息进行消费，根据生产者组获取一个服务提供者发送回查事务状态请求，根据事务状态来决定是提交或回滚消息。\n\n你可以试想一下，如果没有从第5步开始的事务反查机制，如果出现网路波动第4步没有发送成功，这样就会产生MQ不知道是不是需要给消费者消费的问题，他就像一个无头苍蝇一样。在RocketMQ中就是使用的上述的事务反查来解决的，而在Kafka中通常是直接抛出一个异常让用户来自行解决。\n\n你还需要注意的是，在MQServer指向系统B的操作已经和系统A不相关了，也就是说在消息队列中的分布式事务是——本地事务和存储消息到消息队列才是同一个事务。这样也就产生了事务的最终一致性，因为整个过程是异步的，每个系统只要保证它自己那一部分的事务就行了。\n\n##### 消息堆积问题\n\n在上面我们提到了消息队列一个很重要的功能——削峰。那么如果这个峰值太大了导致消息堆积在队列中怎么办呢？其实这个问题可以将它广义化，因为产生消息堆积的根源其实就只有两个——生产者生产太快或者消费者消费太慢。我们可以从多个角度去思考解决这个问题，当流量到峰值的时候是因为生产者生产太快，我们可以使用一些限流降级的方法，当然你也可以增加多个消费者实例去水平扩展增加消费能力来匹配生产的激增。如果消费者消费过慢的话，我们可以先检查是否是消费者出现了大量的消费错误，或者打印一下日志查看是否是哪一个线程卡死，出现了锁资源不释放等等的问题。\n\n> 当然，最快速解决消息堆积问题的方法还是增加消费者实例，不过同时你还需要增加每个主题的队列数量。\n> \n> 别忘了在RocketMQ中，**一个队列只会被一个消费者消费**，如果你仅仅是增加消费者实例就会出现我一开始给你画架构图的那种情况。\n\n![img](https://my-blog-to-use.oss-cn-beijing.aliyuncs.com/2019-11/16ef387d939ab66d.jpg)\n\n##### 回溯消费\n\n回溯消费是指Consumer已经消费成功的消息，由于业务上需求需要重新消费，在RocketMQ中，Broker在向Consumer投递成功消息后，消息仍然需要保留。并且重新消费一般是按照时间维度，例如由于Consumer系统故障，恢复后需要重新消费1小时前的数据，那么Broker要提供一种机制，可以按照时间维度来回退消费进度。RocketMQ支持按照时间回溯消费，时间维度精确到毫秒。\n\n##### RocketMQ的刷盘机制\n\n在Topic中的队列是以什么样的形式存在的？队列中的消息又是如何进行存储持久化的呢？在上文中提到的**同步刷盘**和**异步刷盘**又是什么呢？它们会给持久化带来什么样的影响呢？\n\n###### 同步刷盘和异步刷盘\n\n![img](https://my-blog-to-use.oss-cn-beijing.aliyuncs.com/2019-11/16ef387fba311cda.jpg)\n\n如上图所示，在同步刷盘中需要等待一个刷盘成功的ACK，同步刷盘对MQ消息可靠性来说是一种不错的保障，但是性能上会有较大影响，一般地适用于金融等特定业务场景。而异步刷盘往往是开启一个线程去异步地执行刷盘操作。消息刷盘采用后台异步线程提交的方式进行，降低了读写延迟，提高了MQ的性能和吞吐量，一般适用于如发验证码等对于消息保证要求不太高的业务场景。一般地，异步刷盘只有在Broker意外宕机的时候会丢失部分数据，你可以设置Broker的参数FlushDiskType来调整你的刷盘策略(ASYNC_FLUSH或者SYNC_FLUSH)。\n\n###### 同步复制和异步复制\n\n上面的同步刷盘和异步刷盘是在单个结点层面的，而同步复制和异步复制主要是指的Borker主从模式下，主节点返回消息给客户端的时候是否需要同步从节点。\n\n- 同步复制：也叫“同步双写”，也就是说，只有消息同步双写到主从节点上时才返回写入成功。\n- 异步复制：消息写入主节点之后就直接返回写入成功。\n\n然而，很多事情是没有完美的方案的，就比如我们进行消息写入的节点越多就更能保证消息的可靠性，但是随之的性能也会下降，所以需要程序员根据特定业务场景去选择适应的主从复制方案。那么，异步复制会不会也像异步刷盘那样影响消息的可靠性呢？答案是不会的，因为两者就是不同的概念，对于消息可靠性是通过不同的刷盘策略保证的，而像异步同步复制策略仅仅是影响到了可用性。为什么呢？其主要原因是RocketMQ是不支持自动主从切换的，当主节点挂掉之后，生产者就不能再给这个主节点生产消息了。比如这个时候采用异步复制的方式，在主节点还未发送完需要同步的消息的时候主节点挂掉了，这个时候从节点就少了一部分消息。但是此时生产者无法再给主节点生产消息了，消费者可以自动切换到从节点进行消费(仅仅是消费)，所以在主节点挂掉的时间只会产生主从结点短暂的消息不一致的情况，降低了可用性，而当主节点重启之后，从节点那部分未来得及复制的消息还会继续复制。\n\n在单主从架构中，如果一个主节点挂掉了，那么也就意味着整个系统不能再生产了。那么这个可用性的问题能否解决呢？一个主从不行那就多个主从的呗，别忘了在我们最初的架构图中，每个Topic是分布在不同Broker中的。\n\n![img](https://my-blog-to-use.oss-cn-beijing.aliyuncs.com/2019-11/16ef38687488a5a4.jpg)\n\n但是这种复制方式同样也会带来一个问题，那就是无法保证严格顺序。在上文中我们提到了如何保证的消息顺序性是通过将一个语义的消息发送在同一个队列中，使用Topic下的队列来保证顺序性的。如果此时我们主节点A负责的是订单A的一系列语义消息，然后它挂了，这样其他节点是无法代替主节点A的，如果我们任意节点都可以存入任何消息，那就没有顺序性可言了。\n\n而在RocketMQ中采用了Dledger解决这个问题。他要求在写入消息的时候，要求至少消息复制到半数以上的节点之后，才给客⼾端返回写⼊成功，并且它是⽀持通过选举来动态切换主节点的。\n\n> 也不是说Dledger是个完美的方案，至少在Dledger选举过程中是无法提供服务的，而且他必须要使用三个节点或以上，如果多数节点同时挂掉他也是无法保证可用性的，而且要求消息复制半数以上节点的效率和直接异步复制还是有一定的差距的。\n\n###### 存储机制\n\n还记得上面我们一开始的三个问题吗？到这里第三个问题已经解决了。但是，在Topic中的队列是以什么样的形式存在的？队列中的消息又是如何进行存储持久化的呢？还未解决，其实这里涉及到了RocketMQ是如何设计它的存储结构了。我首先想大家介绍RocketMQ消息存储架构中的三大角色——CommitLog、ConsumeQueue和IndexFile。\n\n- CommitLog：消息主体以及元数据的存储主体，存储Producer端写入的消息主体内容,消息内容不是定长的。单个文件大小默认1G，文件名长度为20位，左边补零，剩余为起始偏移量，比如00000000000000000000代表了第一个文件，起始偏移量为0，文件大小为1G=1073741824；当第一个文件写满了，第二个文件为00000000001073741824，起始偏移量为1073741824，以此类推。消息主要是顺序写入日志文件，当文件满了，写入下一个文件。\n- ConsumeQueue：消息消费队列，引入的目的主要是提高消息消费的性能(我们再前面也讲了)，由于RocketMQ是基于主题Topic的订阅模式，消息消费是针对主题进行的，如果要遍历commitlog文件中根据Topic检索消息是非常低效的。Consumer即可根据ConsumeQueue来查找待消费的消息。其中，ConsumeQueue（逻辑消费队列）作为消费消息的索引，保存了指定Topic下的队列消息在CommitLog中的起始物理偏移量offset，消息大小size和消息Tag的HashCode值。consumequeue文件可以看成是基于topic的commitlog索引文件，故consumequeue文件夹的组织方式如下：topic/queue/file三层组织结构，具体存储路径为：$HOME/store/consumequeue/{topic}/{queueId}/{fileName}。同样consumequeue文件采取定长设计，每一个条目共20个字节，分别为8字节的commitlog物理偏移量、4字节的消息长度、8字节taghashcode，单个文件由30W个条目组成，可以像数组一样随机访问每一个条目，每个ConsumeQueue文件大小约5.72M；\n- IndexFile：IndexFile（索引文件）提供了一种可以通过key或时间区间来查询消息的方法。这里只做科普不做详细介绍。\n\n总结来说，整个消息存储的结构，最主要的就是CommitLoq和ConsumeQueue。而ConsumeQueue你可以大概理解为Topic中的队列。\n\n![img](https://my-blog-to-use.oss-cn-beijing.aliyuncs.com/2019-11/16ef3884c02acc72.png)\n\nRocketMQ采用的是混合型的存储结构，即为Broker单个实例下所有的队列共用一个日志数据文件来存储消息。有意思的是在同样高并发的Kafka中会为每个Topic分配一个存储文件。这就有点类似于我们有一大堆书需要装上书架，RockeMQ是不分书的种类直接成批的塞上去的，而Kafka是将书本放入指定的分类区域的。而RocketMQ为什么要这么做呢？原因是提高数据的写入效率，不分Topic意味着我们有更大的几率获取成批的消息进行数据写入，但也会带来一个麻烦就是读取消息的时候需要遍历整个大文件，这是非常耗时的。所以，在RocketMQ中又使用了ConsumeQueue作为每个队列的索引文件来提升读取消息的效率。我们可以直接根据队列的消息序号，计算出索引的全局位置（索引序号*索引固定⻓度20），然后直接读取这条索引，再根据索引中记录的消息的全局位置，找到消息。\n\n![img](https://my-blog-to-use.oss-cn-beijing.aliyuncs.com/2019-11/16ef388763c25c62.jpg)\n\n首先，在最上面的那一块就是我刚刚讲的你现在可以直接把ConsumerQueue理解为Queue。在图中最左边说明了红色方块代表被写入的消息，虚线方块代表等待被写入的。左边的生产者发送消息会指定Topic、QueueId和具体消息内容，而在Broker中管你是哪门子消息，他直接全部顺序存储到了CommitLog。而根据生产者指定的Topic和QueueId将这条消息本身在CommitLog的偏移(offset)，消息本身大小，和tag的hash值存入对应的ConsumeQueue索引文件中。而在每个队列中都保存了ConsumeOffset即每个消费者组的消费位置，而消费者拉取消息进行消费的时候只需要根据ConsumeOffset获取下一个未被消费的消息就行了。\n\n为什么CommitLog文件要设计成固定大小的长度呢？提醒：**内存映射机制**\n\n> [RocketMQ高级功能代码实现](https://mp.weixin.qq.com/s/WVZRVMUACXuYwuJLjGIUHA)\n\n#### RocketMQ常见面试题总结\n\n##### 单机版消息中心\n\n一个消息中心，最基本的需要支持多生产者、多消费者，例如下：\n\n\n```java\nclass Scratch {\n    public static void main(String[] args) {\n        // 实际中会有nameserver服务来找到broker,具体位置以及broker主从信息\n        Broker broker = new Broker();\n        Producer producer1 = new Producer();\n        producer1.connectBroker(broker);\n        Producer producer2 = new Producer();\n        producer2.connectBroker(broker);\n\n        Consumer consumer1 = new Consumer();\n        consumer1.connectBroker(broker);\n        Consumer consumer2 = new Consumer();\n        consumer2.connectBroker(broker);\n\n        for (int i = 0; i < 2; i++) {\n            producer1.asyncSendMsg(\"producer1 send msg\" + i);\n            producer2.asyncSendMsg(\"producer2 send msg\" + i);\n        }\n        System.out.println(\"broker has msg:\" + broker.getAllMagByDisk());\n\n        for (int i = 0; i < 1; i++) {\n            System.out.println(\"consumer1 consume msg：\" + consumer1.syncPullMsg());\n        }\n        for (int i = 0; i < 3; i++) {\n            System.out.println(\"consumer2 consume msg：\" + consumer2.syncPullMsg());\n        }\n    }\n\n}\n\nclass Producer {\n\n    private Broker broker;\n\n    public void connectBroker(Broker broker) {\n        this.broker = broker;\n    }\n\n    public void asyncSendMsg(String msg) {\n        if (broker == null) {\n            throw new RuntimeException(\"please connect broker first\");\n        }\n        new Thread(() -> {\n            broker.sendMsg(msg);\n        }).start();\n    }\n}\n\nclass Consumer {\n    private Broker broker;\n\n    public void connectBroker(Broker broker) {\n        this.broker = broker;\n    }\n\n    public String syncPullMsg() {\n        return broker.getMsg();\n    }\n\n}\n\nclass Broker {\n\n    // 对应RocketMQ中MessageQueue，默认情况下1个Topic包含4个MessageQueue\n    private LinkedBlockingQueue<String> messageQueue = new LinkedBlockingQueue(Integer.MAX_VALUE);\n\n    // 实际发送消息到broker服务器使用Netty发送\n    public void sendMsg(String msg) {\n        try {\n            messageQueue.put(msg);\n            // 实际会同步或异步落盘，异步落盘使用的定时任务定时扫描落盘\n        } catch (InterruptedException e) {\n\n        }\n    }\n\n    public String getMsg() {\n        try {\n            return messageQueue.take();\n        } catch (InterruptedException e) {\n\n        }\n        return null;\n    }\n\n    public String getAllMagByDisk() {\n        StringBuilder sb = new StringBuilder(\"\\n\");\n        messageQueue.iterator().forEachRemaining((msg) -> {\n            sb.append(msg + \"\\n\");\n        });\n        return sb.toString();\n    }\n}\n```\n\n问题：\n\n1. 没有实现真正执行消息存储落盘\n2. 没有实现NameServer去作为注册中心，定位服务\n3. 使用LinkedBlockingQueue作为消息队列，注意，参数是无限大，在真正RocketMQ也是如此是无限大，理论上不会出现对进来的数据进行抛弃，但是会有内存泄漏问题（阿里巴巴开发手册也因为这个问题，建议我们使用自制线程池）\n4. 没有使用多个队列（即多个LinkedBlockingQueue），RocketMQ的顺序消息是通过生产者和消费者同时使用同一个MessageQueue来实现，但是如果我们只有一个MessageQueue，那我们天然就支持顺序消息\n5. 没有使用MappedByteBuffer来实现文件映射从而使消息数据落盘非常的快（实际RocketMQ使用的是FileChannel+DirectBuffer）\n\n##### 分布式消息中心\n\n###### 问题与解决\n\n**消息丢失的问题**\n\n1. 当你系统需要保证百分百消息不丢失，你可以使用生产者每发送一个消息，Broker同步返回一个消息发送成功的反馈消息\n2. 即每发送一个消息，同步落盘后才返回生产者消息发送成功，这样只要生产者得到了消息发送生成的返回，事后除了硬盘损坏，都可以保证不会消息丢失\n3. 但是这同时引入了一个问题，同步落盘怎么才能快？\n\n**同步落盘怎么才能快**\n\n1. 使用FileChannel+DirectBuffer池，使用堆外内存，加快内存拷贝\n2. 使用数据和索引分离，当消息需要写入时，使用commitlog文件顺序写，当需要定位某个消息时，查询index文件来定位，从而减少文件IO随机读写的性能损耗\n\n**消息堆积的问题**\n\n1. 后台定时任务每隔72小时，删除旧的没有使用过的消息信息\n2. 根据不同的业务实现不同的丢弃任务，具体参考线程池的AbortPolicy，例如FIFO/LRU等（RocketMQ没有此策略）\n3. 消息定时转移，或者对某些重要的TAG型（支付型）消息真正落库\n\n**定时消息的实现**\n\n1. 实际RocketMQ没有实现任意精度的定时消息，它只支持某些特定的时间精度的定时消息\n2. 实现定时消息的原理是：创建特定时间精度的MessageQueue，例如生产者需要定时1s之后被消费者消费，你只需要将此消息发送到特定的Topic，例如：MessageQueue-1表示这个MessageQueue里面的消息都会延迟一秒被消费，然后Broker会在1s后发送到消费者消费此消息，使用newSingleThreadScheduledExecutor实现\n\n**顺序消息的实现**\n\n1. 与定时消息同原理，生产者生产消息时指定特定的MessageQueue，消费者消费消息时，消费特定的MessageQueue，其实单机版的消息中心在一个MessageQueue就天然支持了顺序消息\n2. 注意：同一个MessageQueue保证里面的消息是顺序消费的前提是：消费者是串行的消费该MessageQueue，因为就算MessageQueue是顺序的，但是当并行消费时，还是会有顺序问题，但是串行消费也同时引入了两个问题：\n\n> 1. 引入锁来实现串行\n> 2. 前一个消费阻塞时后面都会被阻塞\n\n**分布式消息的实现**\n\n1. 需要前置知识：2PC\n2. RocketMQ4.3起支持，原理为2PC，即两阶段提交，prepared->commit/rollback\n3. 生产者发送事务消息，假设该事务消息Topic为Topic1-Trans，Broker得到后首先更改该消息的Topic为Topic1-Prepared，该Topic1-Prepared对消费者不可见。然后定时回调生产者的本地事务A执行状态，根据本地事务A执行状态，来是否将该消息修改为Topic1-Commit或Topic1-Rollback，消费者就可以正常找到该事务消息或者不执行等\n\n> 注意，就算是事务消息最后回滚了也不会物理删除，只会逻辑删除该消息\n\n**消息的push实现**\n\n1. 注意，RocketMQ已经说了自己会有低延迟问题，其中就包括这个消息的push延迟问题\n2. 因为这并不是真正的将消息主动的推送到消费者，而是Broker定时任务每5s将消息推送到消费者\n3. pull模式需要我们手动调用consumer拉消息，而push模式则只需要我们提供一个listener即可实现对消息的监听，而实际上，RocketMQ的push模式是基于pull模式实现的，它没有实现真正的push。\n4. push方式里，consumer把轮询过程封装了，并注册MessageListener监听器，取到消息后，唤醒MessageListener的consumeMessage()来消费，对用户而言，感觉消息是被推送过来的。\n\n**消息重复发送的避免**\n\n1. RocketMQ会出现消息重复发送的问题，因为在网络延迟的情况下，这种问题不可避免的发生，如果非要实现消息不可重复发送，那基本太难，因为网络环境无法预知，还会使程序复杂度加大，因此默认允许消息重复发送\n2. RocketMQ让使用者在消费者端去解决该问题，即需要消费者端在消费消息时支持幂等性的去消费消息\n3. 最简单的解决方案是每条消费记录有个消费状态字段，根据这个消费状态字段来判断是否消费或者使用一个集中式的表，来存储所有消息的消费状态，从而避免重复消费\n4. 具体实现可以查询关于消息幂等消费的解决方案\n\n**广播消费与集群消费**\n\n1. 消息消费区别：广播消费，订阅该Topic的消息者们都会消费每个消息。集群消费，订阅该Topic的消息者们只会有一个去消费某个消息\n2. 消息落盘区别：具体表现在消息消费进度的保存上。广播消费，由于每个消费者都独立的去消费每个消息，因此每个消费者各自保存自己的消息消费进度。而集群消费下，订阅了某个Topic，而旗下又有多个MessageQueue，每个消费者都可能会去消费不同的MessageQueue，因此总体的消费进度保存在Broker上集中的管理\n\n**RocketMQ不使用ZooKeeper作为注册中心的原因，以及自制的NameServer优缺点？**\n\n1. ZooKeeper作为支持顺序一致性的中间件，在某些情况下，它为了满足一致性，会丢失一定时间内的可用性，RocketMQ需要注册中心只是为了发现组件地址，在某些情况下，RocketMQ的注册中心可以出现数据不一致性，这同时也是NameServer的缺点，因为NameServer集群间互不通信，它们之间的注册信息可能会不一致\n2. 另外，当有新的服务器加入时，NameServer并不会立马通知到Producer，而是由Producer定时去请求NameServer获取最新的Broker/Consumer信息（这种情况是通过Producer发送消息时，负载均衡解决）\n\n**其它**\n\n![img](https://leran2deeplearnjavawebtech.oss-cn-beijing.aliyuncs.com/somephoto/RocketMQ流程.png)\n\n加分项\n\n1. 包括组件通信间使用Netty的自定义协议\n2. 消息重试负载均衡策略（具体参考Dubbo负载均衡策略）\n3. 消息过滤器（Producer发送消息到Broker，Broker存储消息信息，Consumer消费时请求Broker端从磁盘文件查询消息文件时,在Broker端就使用过滤服务器进行过滤）\n4. Broker同步双写和异步双写中Master和Slave的交互\n5. Broker在4.5.0版本更新中引入了基于Raft协议的多副本选举，之前这是商业版才有的特性[ISSUE-1046](http://rocketmq.apache.org/release_notes/release-notes-4.5.0/)\n\n#### 相关文章\n\n- [阿里二面：RocketMQ消息积压了，增加消费者有用吗？](https://mp.weixin.qq.com/s/804KWy4Gf9EIo9GiNRg5Fg)\n- [如何基于RocketMQ设计一套全链路消息不丢失方案？](https://mp.weixin.qq.com/s/_Gl8MqcSa5gASLXSxyhHCg)\n- [通过这三个文件彻底搞懂rocketmq的存储原理](https://mp.weixin.qq.com/s/4k_g85aktaBhwSl9iu6r5A)\n- [RocketMQ集群Broker挂了，会造成什么影响？](https://mp.weixin.qq.com/s/vIqOEeNBCSQJDB-qNYukTQ)\n- [RocketMQ高级功能代码实现](https://mp.weixin.qq.com/s/WVZRVMUACXuYwuJLjGIUHA)\n- [解读RocketMQ5.0全新的高可用设计](https://mp.weixin.qq.com/s/uXL5LKYBMzNsmWtRk6MYAg)\n\n### RabbitMQ\n\n![img](https://guide-blog-images.oss-cn-shenzhen.aliyuncs.com/github/javaguide/high-performance/message-queue/rabbitmq-logo.png)\n\n#### RabbitMQ基础知识总结\n\n##### 简介\n\nRabbitMQ是采用Erlang语言实现AMQP(AdvancedMessageQueuingProtocol，高级消息队列协议)的消息中间件，它最初起源于金融系统，用于在分布式系统中存储转发消息。\n\nRabbitMQ发展到今天，被越来越多的人认可，这和它在易用性、扩展性、可靠性和高可用性等方面的卓著表现是分不开的。RabbitMQ的具体特点可以概括为以下几点：\n\n- **可靠性**：RabbitMQ使用一些机制来保证消息的可靠性，如持久化、传输确认及发布确认等。\n- **灵活的路由**：在消息进入队列之前，通过交换器来路由消息。对于典型的路由功能，RabbitMQ己经提供了一些内置的交换器来实现。针对更复杂的路由功能，可以将多个交换器绑定在一起，也可以通过插件机制来实现自己的交换器。这个后面会在我们讲RabbitMQ核心概念的时候详细介绍到。\n- **扩展性**：多个RabbitMQ节点可以组成一个集群，也可以根据实际业务情况动态地扩展集群中节点。\n- **高可用性**：队列可以在集群中的机器上设置镜像，使得在部分节点出现问题的情况下队列仍然可用。\n- **支持多种协议**：RabbitMQ除了原生支持AMQP协议，还支持STOMP、MQTT等多种消息中间件协议。\n- **多语言客户端**：RabbitMQ几乎支持所有常用语言，比如Java、Python、Ruby、PHP、C#、JavaScript等。\n- **易用的管理界面**：RabbitMQ提供了一个易用的用户界面，使得用户可以监控和管理消息、集群中的节点等。在安装RabbitMQ的时候会介绍到，安装好RabbitMQ就自带管理界面。\n- **插件机制**：RabbitMQ提供了许多插件，以实现从多方面进行扩展，当然也可以编写自己的插件。感觉这个有点类似Dubbo的SPI机制\n\n> [RabbitMQ官网](https://www.rabbitmq.com/)\n> [RabbitMQ更新记录](https://www.rabbitmq.com/news.html)\n\n##### RabbitMQ核心概念\n\nRabbitMQ整体上是一个生产者与消费者模型，主要负责接收、存储和转发消息。可以把消息传递的过程想象成：当你将一个包裹送到邮局，邮局会暂存并最终将邮件通过邮递员送到收件人的手上，RabbitMQ就好比由邮局、邮箱和邮递员组成的一个系统。从计算机术语层面来说，RabbitMQ模型更像是一种交换机模型。\n\nRabbitMQ的整体模型架构。\n\n![图1-RabbitMQ的整体模型架构](https://oss.javaguide.cn/github/javaguide/rabbitmq/96388546.jpg)\n\n\n**Producer(生产者)和Consumer(消费者)**\n\n- **Producer(生产者)**:生产消息的一方（邮件投递者）\n- **Consumer(消费者)**:消费消息的一方（邮件收件人）\n\n消息一般由2部分组成：消息头（或者说是标签Label）和消息体。消息体也可以称为payLoad,消息体是不透明的，而消息头则由一系列的可选属性组成，这些属性包括routing-key（路由键）、priority（相对于其他消息的优先权）、delivery-mode（指出该消息可能需要持久性存储）等。生产者把消息交由RabbitMQ后，RabbitMQ会根据消息头把消息发送给感兴趣的Consumer(消费者)。\n\n**Exchange(交换器)**\n\n在RabbitMQ中，消息并不是直接被投递到Queue(消息队列) 中的，中间还必须经过Exchange(交换器) 这一层，Exchange(交换器) 会把我们的消息分配到对应的Queue(消息队列) 中。\n\n**Exchange(交换器)** 用来接收生产者发送的消息并将这些消息路由给服务器中的队列中，如果路由不到，或许会返回给Producer(生产者)，或许会被直接丢弃掉。这里可以将RabbitMQ中的交换器看作一个简单的实体。\n\nRabbitMQ的Exchange(交换器)有4种类型，不同的类型对应着不同的路由策略：**direct(默认)，fanout,topic,和headers**，不同类型的Exchange转发消息的策略有所区别。这个会在介绍ExchangeTypes(交换器类型) 的时候介绍到。\n\nExchange(交换器)示意图如下：\n\n![Exchange(交换器)示意图](https://oss.javaguide.cn/github/javaguide/rabbitmq/24007899.jpg)\n\n生产者将消息发给交换器的时候，一般会指定一个RoutingKey(路由键)，用来指定这个消息的路由规则，而这个RoutingKey需要与交换器类型和绑定键(BindingKey)联合使用才能最终生效。\n\nRabbitMQ中通过Binding(绑定)将Exchange(交换器)与Queue(消息队列)关联起来，在绑定的时候一般会指定一个BindingKey(绑定建),这样RabbitMQ就知道如何正确将消息路由到队列了,如下图所示。一个绑定就是基于路由键将交换器和消息队列连接起来的路由规则，所以可以将交换器理解成一个由绑定构成的路由表。Exchange和Queue的绑定可以是多对多的关系。\n\nBinding(绑定)示意图：\n\n![Binding(绑定)示意图](https://oss.javaguide.cn/github/javaguide/rabbitmq/70553134.jpg)\n\n生产者将消息发送给交换器时，需要一个RoutingKey,当BindingKey和RoutingKey相匹配时，消息会被路由到对应的队列中。在绑定多个队列到同一个交换器的时候，这些绑定允许使用相同的BindingKey。BindingKey并不是在所有的情况下都生效，它依赖于交换器类型，比如fanout类型的交换器就会无视，而是将消息路由到所有绑定到该交换器的队列中。\n\n**Queue(消息队列)**\n\n**Queue(消息队列)** 用来保存消息直到发送给消费者。它是消息的容器，也是消息的终点。一个消息可投入一个或多个队列。消息一直在队列里面，等待消费者连接到这个队列将其取走。RabbitMQ中消息只能存储在队列中，这一点和Kafka这种消息中间件相反。Kafka将消息存储在topic（主题） 这个逻辑层面，而相对应的队列逻辑只是topic实际存储文件中的位移标识。RabbitMQ的生产者生产消息并最终投递到队列中，消费者可以从队列中获取消息并消费。多个消费者可以订阅同一个队列，这时队列中的消息会被平均分摊（Round-Robin，即轮询）给多个消费者进行处理，而不是每个消费者都收到所有的消息并处理，这样避免消息被重复消费。RabbitMQ不支持队列层面的广播消费,如果有广播消费的需求，需要在其上进行二次开发,这样会很麻烦，不建议这样做。\n\n**Broker（消息中间件的服务节点）**\n\n对于RabbitMQ来说，一个RabbitMQBroker可以简单地看作一个RabbitMQ服务节点，或者RabbitMQ服务实例。大多数情况下也可以将一个RabbitMQBroker看作一台RabbitMQ服务器。\n\n下图展示了生产者将消息存入RabbitMQBroker,以及消费者从Broker中消费数据的整个流程。\n\n![消息队列的运转过程](https://oss.javaguide.cn/github/javaguide/rabbitmq/67952922.jpg)\n\n**ExchangeTypes(交换器类型)**\n\nRabbitMQ常用的ExchangeType有**fanout**、**direct**、**topic**、**headers**这四种（AMQP规范里还提到两种ExchangeType，分别为system与自定义，这里不予以描述）。\n\n**①fanout**\n\nfanout类型的Exchange路由规则非常简单，它会把所有发送到该Exchange的消息路由到所有与它绑定的Queue中，不需要做任何判断操作，所以fanout类型是所有的交换机类型里面速度最快的。fanout类型常用来广播消息。\n\n**②direct**\n\ndirect类型的Exchange路由规则也很简单，它会把消息路由到那些Bindingkey与RoutingKey完全匹配的Queue中。\n\n![direct类型交换器](https://oss.javaguide.cn/github/javaguide/rabbitmq/37008021.jpg)\n\n以上图为例，如果发送消息的时候设置路由键为“warning”,那么消息会路由到Queue1和Queue2。如果在发送消息的时候设置路由键为\"Info”或者\"debug”，消息只会路由到Queue2。如果以其他的路由键发送消息，则消息不会路由到这两个队列中。\n\ndirect类型常用在处理有优先级的任务，根据任务的优先级把消息发送到对应的队列，这样可以指派更多的资源去处理高优先级的队列。\n\n**③topic**\n\n前面讲到direct类型的交换器路由规则是完全匹配BindingKey和RoutingKey，但是这种严格的匹配方式在很多情况下不能满足实际业务的需求。topic类型的交换器在匹配规则上进行了扩展，它与direct类型的交换器相似，也是将消息路由到BindingKey和RoutingKey相匹配的队列中，但这里的匹配规则有些不同，它约定：\n\n- RoutingKey为一个点号“．”分隔的字符串（被点号“．”分隔开的每一段独立的字符串称为一个单词），如“com.rabbitmq.client”、“java.util.concurrent”、“com.hidden.client”;\n- BindingKey和RoutingKey一样也是点号“．”分隔的字符串；\n- BindingKey中可以存在两种特殊字符串“*”和“#”，用于做模糊匹配，其中“*”用于匹配一个单词，“#”用于匹配多个单词(可以是零个)。\n\n![topic类型交换器](https://oss.javaguide.cn/github/javaguide/rabbitmq/73843.jpg)\n\n以上图为例：\n\n- 路由键为“com.rabbitmq.client”的消息会同时路由到Queue1和Queue2;\n- 路由键为“com.hidden.client”的消息只会路由到Queue2中；\n- 路由键为“com.hidden.demo”的消息只会路由到Queue2中；\n- 路由键为“java.rabbitmq.demo”的消息只会路由到Queue1中；\n- 路由键为“java.util.concurrent”的消息将会被丢弃或者返回给生产者（需要设置mandatory参数），因为它没有匹配任何路由键。\n\n**④headers(不推荐)**\n\nheaders类型的交换器不依赖于路由键的匹配规则来路由消息，而是根据发送的消息内容中的headers属性进行匹配。在绑定队列和交换器时指定一组键值对，当发送消息到交换器时，RabbitMQ会获取到该消息的headers(也是一个键值对的形式)，对比其中的键值对是否完全匹配队列和交换器绑定时指定的键值对，如果完全匹配则消息会路由到该队列，否则不会路由到该队列。headers类型的交换器性能会很差，而且也不实用，基本上不会看到它的存在。\n\n##### 安装RabbitMQ\n\n通过Docker安装非常方便，只需要几条命令就好了，我这里是只说一下常规安装方法。\n\n前面提到了RabbitMQ是由Erlang语言编写的，也正因如此，在安装RabbitMQ之前需要安装Erlang。\n\n注意：在安装RabbitMQ的时候需要注意RabbitMQ和Erlang的版本关系，如果不注意的话会导致出错，两者对应关系如下:\n\n![RabbitMQ和Erlang的版本关系](https://oss.javaguide.cn/github/javaguide/rabbitmq/RabbitMQ-Erlang.png)\n\n###### 安装erlang\n\n**1. 下载erlang安装包**\n\n在官网下载然后上传到Linux上或者直接使用下面的命令下载对应的版本。\n\n```bash\nwget https://erlang.org/download/otp_src_19.3.tar.gz\n```\n\nerlang官网下载：[https://www.erlang.org/downloads](https://www.erlang.org/downloads)\n\n**2. 解压erlang安装包**\n\n\n```bash\ntar -xvzf otp_src_19.3.tar.gz\n```\n\n**3. 删除erlang安装包**\n\n```bash\nrm -rf otp_src_19.3.tar.gz\n```\n\n**4. 安装erlang的依赖工具**\n\n```bash\nyum -y install make gcc gcc-c++ kernel-devel m4 ncurses-devel openssl-devel unixODBC-devel\n```\n\n**5. 进入erlang安装包解压文件对erlang进行安装环境的配置**\n\n新建一个文件夹\n\n\n```bash\nmkdir erlang\n```\n\n对erlang进行安装环境的配置\n\n```bash\n ./configure --prefix=/usr/local/erlang --without-javac\n```\n\n**6. 编译安装**\n\n```bash\n make && make install\n```\n\n**7. 验证一下erlang是否安装成功了**\n\n```bash\n./bin/erl\n```\n\n运行下面的语句输出“hello world”\n\n```erlang\n io:format(\"hello world~n\", []).\n```\n\n![输出“hello world”](https://oss.javaguide.cn/github/javaguide/rabbitmq/49570541.jpg)\n\n大功告成，我们的erlang已经安装完成。\n\n**8. 配置erlang环境变量**\n\n```bash\nvim profile\n```\n\n追加下列环境变量到文件末尾\n\n```bash\n#erlang\nERL_HOME=/usr/local/erlang\nPATH=$ERL_HOME/bin:$PATH\nexport ERL_HOME PATH\n```\n\n运行下列命令使配置文件`profile`生效\n\n```bash\nsource /etc/profile\n```\n\n输入erl查看erlang环境变量是否配置正确\n\n```bash\nerl\n```\n\n![输入erl查看erlang环境变量是否配置正确](https://my-blog-to-use.oss-cn-beijing.aliyuncs.com/18-12-12/62504246.jpg)\n\n###### 安装RabbitMQ\n\n**1. 下载rpm**\n\n\n```bash\nwget https://www.rabbitmq.com/releases/rabbitmq-server/v3.6.8/rabbitmq-server-3.6.8-1.el7.noarch.rpm\n```\n\n或者直接在[官网下载](https://www.rabbitmq.com/install-rpm.html)\n\n**2. 安装rpm**\n\n```bash\nrpm --import https://www.rabbitmq.com/rabbitmq-release-signing-key.asc\n```\n\n紧接着执行：\n\n```bash\nyum install rabbitmq-server-3.6.8-1.el7.noarch.rpm\n```\n\n中途需要你输入\"y\"才能继续安装。\n\n**3. 开启web管理插件**\n\n```bash\nrabbitmq-plugins enable rabbitmq_management\n```\n\n**4. 设置开机启动**\n\n```bash\nchkconfig rabbitmq-server on\n```\n\n**5. 启动服务**\n\n```bash\nservice rabbitmq-server start\n```\n\n**6. 查看服务状态**\n\n```bash\nservice rabbitmq-server status\n```\n\n**7. 访问RabbitMQ控制台**\n\n浏览器访问：http://你的ip地址:15672/\n\n默认用户名和密码：guest/guest;但是需要注意的是：guest用户只是被容许从localhost访问。官网文档描述如下：\n\n```bash\n“guest” user can only connect via localhost\n```\n\n**解决远程访问RabbitMQ远程访问密码错误**\n\n新建用户并授权\n\n```bash\n[root@SnailClimb rabbitmq]# rabbitmqctl add_user root root\nCreating user \"root\" ...\n[root@SnailClimb rabbitmq]# rabbitmqctl set_user_tags root administrator\n\nSetting tags for user \"root\" to [administrator] ...\n[root@SnailClimb rabbitmq]# \n[root@SnailClimb rabbitmq]# rabbitmqctl set_permissions -p / root \".*\" \".*\" \".*\"\nSetting permissions for user \"root\" in vhost \"/\" ...\n```\n\n再次访问:http://你的ip地址:15672/,输入用户名和密码：root root\n\n![RabbitMQ控制台](https://oss.javaguide.cn/github/javaguide/rabbitmq/45835332.jpg)\n\n\n#### RabbitMQ常见面试题总结\n\n##### RabbitMQ是什么？\n\nRabbitMQ是一个在AMQP（AdvancedMessageQueuingProtocol）基础上实现的，可复用的企业消息系统。它可以用于大型软件系统各个模块之间的高效通信，支持高并发，支持可扩展。它支持多种客户端如：Python、Ruby、.NET、Java、JMS、C、PHP、ActionScript、XMPP、STOMP等，支持AJAX，持久化，用于在分布式系统中存储转发消息，在易用性、扩展性、高可用性等方面表现不俗。\n\nRabbitMQ是使用Erlang编写的一个开源的消息队列，本身支持很多的协议：AMQP，XMPP,SMTP,STOMP，也正是如此，使的它变的非常重量级，更适合于企业级的开发。它同时实现了一个Broker构架，这意味着消息在发送给客户端时先在中心队列排队，对路由(Routing)、负载均衡(Loadbalance)或者数据持久化都有很好的支持。\n\n> PS:也可能直接问什么是消息队列？消息队列就是一个使用队列来通信的组件。\n\n##### RabbitMQ特点?\n\n- **可靠性**:RabbitMQ使用一些机制来保证可靠性，如持久化、传输确认及发布确认等。\n- **灵活的路由**:在消息进入队列之前，通过交换器来路由消息。对于典型的路由功能，RabbitMQ己经提供了一些内置的交换器来实现。针对更复杂的路由功能，可以将多个交换器绑定在一起，也可以通过插件机制来实现自己的交换器。\n- **扩展性**:多个RabbitMQ节点可以组成一个集群，也可以根据实际业务情况动态地扩展集群中节点。\n- **高可用性**:队列可以在集群中的机器上设置镜像，使得在部分节点出现问题的情况下队列仍然可用。\n- **多种协议**:RabbitMQ除了原生支持AMQP协议，还支持STOMP，MQTT等多种消息中间件协议。\n- **多语言客户端**:RabbitMQ几乎支持所有常用语言，比如Java、Python、Ruby、PHP、C#、JavaScript等。\n- **管理界面**:RabbitMQ提供了一个易用的用户界面，使得用户可以监控和管理消息、集群中的节点等。\n- **插件机制**:RabbitMQ提供了许多插件，以实现从多方面进行扩展，当然也可以编写自己的插件。\n\n##### AMQP是什么?\n\nRabbitMQ就是AMQP协议的Erlang的实现(当然RabbitMQ还支持STOMP2、MQTT3等协议)AMQP的模型架构和RabbitMQ的模型架构是一样的，生产者将消息发送给交换器，交换器和队列绑定。\n\nRabbitMQ中的交换器、交换器类型、队列、绑定、路由键等都是遵循的AMQP协议中相应的概念。目前RabbitMQ最新版本默认支持的是AMQP0-9-1。\n\n**AMQP协议的三层**：\n\n- **ModuleLayer**:协议最高层，主要定义了一些客户端调用的命令，客户端可以用这些命令实现自己的业务逻辑。\n- **SessionLayer**:中间层，主要负责客户端命令发送给服务器，再将服务端应答返回客户端，提供可靠性同步机制和错误处理。\n- **TransportLayer**:最底层，主要传输二进制数据流，提供帧的处理、信道服用、错误检测和数据表示等。\n\n**AMQP模型的三大组件**：\n\n- **交换器（Exchange）**：消息代理服务器中用于把消息路由到队列的组件。\n- **队列（Queue）**：用来存储消息的数据结构，位于硬盘或内存中。\n- **绑定（Binding）**：一套规则，告知交换器消息应该将消息投递给哪个队列。\n\n##### 说说生产者Producer和消费者Consumer?\n\n**生产者**:\n\n- 消息生产者，就是投递消息的一方。\n- 消息一般包含两个部分：消息体(`payload`)和标签(`Label`)。\n\n**消费者**：\n\n- 消费消息，也就是接收消息的一方。\n- 消费者连接到RabbitMQ服务器，并订阅到队列上。消费消息时只消费消息体，丢弃标签。\n\n##### 说说Broker服务节点、Queue队列、Exchange交换器？\n\n- **Broker**：可以看做RabbitMQ的服务节点。一般请下一个Broker可以看做一个RabbitMQ服务器。\n- **Queue**:RabbitMQ的内部对象，用于存储消息。多个消费者可以订阅同一队列，这时队列中的消息会被平摊（轮询）给多个消费者进行处理。\n- **Exchange**:生产者将消息发送到交换器，由交换器将消息路由到一个或者多个队列中。当路由不到时，或返回给生产者或直接丢弃。\n\n##### 什么是死信队列？如何导致的？\n\nDLX，全称为Dead-Letter-Exchange，死信交换器，死信邮箱。当消息在一个队列中变成死信(deadmessage)之后，它能被重新被发送到另一个交换器中，这个交换器就是DLX，绑定DLX的队列就称之为死信队列。\n\n**导致的死信的几种原因**：\n\n- 消息被拒(`Basic.Reject/Basic.Nack`)且`requeue=false`。\n- 消息TTL过期。\n- 队列满了，无法再添加。\n\n##### 什么是延迟队列？RabbitMQ怎么实现延迟队列？\n\n延迟队列指的是存储对应的延迟消息，消息被发送以后，并不想让消费者立刻拿到消息，而是等待特定时间后，消费者才能拿到这个消息进行消费。\n\nRabbitMQ本身是没有延迟队列的，要实现延迟消息，一般有两种方式：\n\n1. 通过RabbitMQ本身队列的特性来实现，需要使用RabbitMQ的死信交换机（Exchange）和消息的存活时间TTL（TimeToLive）。\n2. 在RabbitMQ3.5.7及以上的版本提供了一个插件（rabbitmq-delayed-message-exchange）来实现延迟队列功能。同时，插件依赖Erlang/OPT18.0及以上。\n\n也就是说，AMQP协议以及RabbitMQ本身没有直接支持延迟队列的功能，但是可以通过TTL和DLX模拟出延迟队列的功能。\n\n##### 什么是优先级队列？\n\nRabbitMQ自V3.5.0有优先级队列实现，优先级高的队列会先被消费。\n\n可以通过`x-max-priority`参数来实现优先级队列。不过，当消费速度大于生产速度且Broker没有堆积的情况下，优先级显得没有意义。\n\n##### RabbitMQ有哪些工作模式？\n\n- 简单模式\n- work工作模式\n- pub/sub发布订阅模式\n- Routing路由模式\n- Topic主题模式\n\n##### RabbitMQ消息怎么传输？\n\n由于TCP链接的创建和销毁开销较大，且并发数受系统资源限制，会造成性能瓶颈，所以RabbitMQ使用信道的方式来传输数据。信道（Channel）是生产者、消费者与RabbitMQ通信的渠道，信道是建立在TCP链接上的虚拟链接，且每条TCP链接上的信道数量没有限制。就是说RabbitMQ在一条TCP链接上建立成百上千个信道来达到多个线程处理，这个TCP被多个线程共享，每个信道在RabbitMQ都有唯一的ID，保证了信道私有性，每个信道对应一个线程使用。\n\n##### 如何保证消息的可靠性？\n\n消息到MQ的过程中搞丢，MQ自己搞丢，MQ到消费过程中搞丢。\n\n- 生产者到RabbitMQ：事务机制和Confirm机制，注意：事务机制和Confirm机制是互斥的，两者不能共存，会导致RabbitMQ报错。\n- RabbitMQ自身：持久化、集群、普通模式、镜像模式。\n- RabbitMQ到消费者：basicAck机制、死信队列、消息补偿机制。\n\n##### 如何保证RabbitMQ消息的顺序性？\n\n- 拆分多个queue(消息队列)，每个queue(消息队列)一个consumer(消费者)，就是多一些queue(消息队列)而已，确实是麻烦点；\n- 或者就一个queue(消息队列)但是对应一个consumer(消费者)，然后这个consumer(消费者)内部用内存队列做排队，然后分发给底层不同的worker来处理。\n\n##### 如何保证RabbitMQ高可用的？\n\nRabbitMQ是比较有代表性的，因为是基于主从（非分布式）做高可用性的，我们就以RabbitMQ为例子讲解第一种MQ的高可用性怎么实现。RabbitMQ有三种模式：单机模式、普通集群模式、镜像集群模式。\n\n**单机模式**\n\nDemo级别的，一般就是你本地启动了玩玩儿的?，没人生产用单机模式。\n\n**普通集群模式**\n\n意思就是在多台机器上启动多个RabbitMQ实例，每个机器启动一个。你创建的queue，只会放在一个RabbitMQ实例上，但是每个实例都同步queue的元数据（元数据可以认为是queue的一些配置信息，通过元数据，可以找到queue所在实例）。\n\n你消费的时候，实际上如果连接到了另外一个实例，那么那个实例会从queue所在实例上拉取数据过来。这方案主要是提高吞吐量的，就是说让集群中多个节点来服务某个queue的读写操作。\n\n**镜像集群模式**\n\n这种模式，才是所谓的RabbitMQ的高可用模式。跟普通集群模式不一样的是，在镜像集群模式下，你创建的queue，无论元数据还是queue里的消息都会存在于多个实例上，就是说，每个RabbitMQ节点都有这个queue的一个完整镜像，包含queue的全部数据的意思。然后每次你写消息到queue的时候，都会自动把消息同步到多个实例的queue上。RabbitMQ有很好的管理控制台，就是在后台新增一个策略，这个策略是镜像集群模式的策略，指定的时候是可以要求数据同步到所有节点的，也可以要求同步到指定数量的节点，再次创建queue的时候，应用这个策略，就会自动将数据同步到其他的节点上去了。这样的好处在于，你任何一个机器宕机了，没事儿，其它机器（节点）还包含了这个queue的完整数据，别的consumer都可以到其它节点上去消费数据。坏处在于，第一，这个性能开销也太大了吧，消息需要同步到所有机器上，导致网络带宽压力和消耗很重！RabbitMQ一个queue的数据都是放在一个节点里的，镜像集群下，也是每个节点都放这个queue的完整数据。\n\n##### 如何解决消息挤压问题？\n\n**临时紧急扩容**。先修复consumer的问题，确保其恢复消费速度，然后将现有consumer都停掉。新建一个topic，partition是原来的10倍，临时建立好原先10倍的queue数量。然后写一个临时的分发数据的consumer程序，这个程序部署上去消费积压的数据，消费之后不做耗时的处理，直接均匀轮询写入临时建立好的10倍数量的queue。接着临时征用10倍的机器来部署consumer，每一批consumer消费一个临时queue的数据。这种做法相当于是临时将queue资源和consumer资源扩大10倍，以正常的10倍速度来消费数据。等快速消费完积压数据之后，得恢复原先部署的架构，重新用原先的consumer机器来消费消息。\n\n##### 如何解决消息队列的延时以及过期失效问题？\n\nRabbtiMQ是可以设置过期时间的，也就是TTL。如果消息在queue中积压超过一定的时间就会被RabbitMQ给清理掉，这个数据就没了。那这就是第二个坑了。这就不是说数据会大量积压在mq里，而是大量的数据会直接搞丢。我们可以采取一个方案，就是批量重导，这个我们之前线上也有类似的场景干过。就是大量积压的时候，我们当时就直接丢弃数据了，然后等过了高峰期以后，比如大家一起喝咖啡熬夜到晚上12点以后，用户都睡觉了。这个时候我们就开始写程序，将丢失的那批数据，写个临时程序，一点一点的查出来，然后重新灌入mq里面去，把白天丢的数据给他补回来。也只能是这样了。假设1万个订单积压在mq里面，没有处理，其中1000个订单都丢了，你只能手动写程序把那1000个订单给查出来，手动发到mq里去再补一次。\n\n#### 相关文章\n\n- [四种策略确保RabbitMQ消息发送可靠性！你用哪种？](https://mp.weixin.qq.com/s/hj8iqASSOk2AgdtkuLPCCQ)\n- [新来个技术总监，把RabbitMQ讲的那叫一个透彻，佩服！](https://mp.weixin.qq.com/s/RzxiXBQk3zjHt9PVI3JSQw)\n- [RabbitMQ高可用之如何确保消息成功消费](https://mp.weixin.qq.com/s/5szA0KBpFn9G3DeS9C0U3w)\n- [RabbitMQ中的消息会过期吗](https://mp.weixin.qq.com/s/fFqzuN2AnLazoCyahL3EFw)\n- [RabbitMQ七种消息传递形式](https://mp.weixin.qq.com/s/inhSj-nIiDCfUf24TrE2XA)\n- [使用rabbitmq延时队列实现超时取消订单](https://mp.weixin.qq.com/s/V_XZ2vC2jZmzxm-0ThhiTg)\n- [面试官：引入RabbitMQ后，你如何保证全链路数据100%不丢失？](https://mp.weixin.qq.com/s/38XaTeHdgiH5GfCIQiWlnQ)\n- [RabbitMQ详解](https://mp.weixin.qq.com/s/_G_hd8OX9D76Pd5TmM3_BQ)\n- [RabbitMQ的AMQP协议都是些什么内容呢](https://mp.weixin.qq.com/s/JDkIP7Kzi2uWcWoy9k7SQA)\n- [RabbitMQ有哪几种消息模式？](https://mp.weixin.qq.com/s/yM5NFnt0vZ9A5LaQt3-Kbw)\n- [非常强悍的RabbitMQ总结，写得真好！](https://mp.weixin.qq.com/s/pARhOtb0jDVYpAJbxrEx7w)\n\n\n### Pulsar\n\n![img](https://guide-blog-images.oss-cn-shenzhen.aliyuncs.com/github/javaguide/high-performance/message-queue/pulsar-logo.png)\n\nPulsar是下一代云原生分布式消息流平台，最初由Yahoo开发，已经成为Apache顶级项目。\n\nPulsar集消息、存储、轻量化函数式计算为一体，采用计算与存储分离架构设计，支持多租户、持久化存储、多机房跨区域数据复制，具有强一致性、高吞吐、低延时及高可扩展性等流数据存储特性，被看作是云原生时代实时消息流传输、存储和计算最佳解决方案。\n\nPulsar的关键特性如下（摘自官网）：\n\n- 是下一代云原生分布式消息流平台。\n- Pulsar的单个实例原生支持多个集群，可跨机房在集群间无缝地完成消息复制。\n- 极低的发布延迟和端到端延迟。\n- 可无缝扩展到超过一百万个topic。\n- 简单的客户端API，支持Java、Go、Python和C++。\n- 主题的多种订阅模式（独占、共享和故障转移）。\n- 通过ApacheBookKeeper提供的持久化消息存储机制保证消息传递。\n- 由轻量级的serverless计算框架PulsarFunctions实现流原生的数据处理。\n- 基于PulsarFunctions的serverlessconnector框架PulsarIO使得数据更易移入、移出ApachePulsar。\n- 分层式存储可在数据陈旧时，将数据从热存储卸载到冷/长期存储（如S3、GCS）中。\n\n> [Pulsar官网](https://pulsar.apache.org/)\n> [Pulsar更新记录](https://github.com/apache/pulsar/releases)\n\n### ActiveMQ\n\n目前已经被淘汰，不推荐使用，不建议学习。\n\n### 如何选择？\n\n> 参考《Java工程师面试突击第1季-中华石杉老师》\n\n| 对比方向 | 概要                                                         |\n| -------- | ------------------------------------------------------------ |\n| 吞吐量   | 万级的ActiveMQ和RabbitMQ的吞吐量（ActiveMQ的性能最差）要比十万级甚至是百万级的RocketMQ和Kafka低一个数量级。 |\n| 可用性   | 都可以实现高可用。ActiveMQ和RabbitMQ都是基于主从架构实现高可用性。RocketMQ基于分布式架构。Kafka也是分布式的，一个数据多个副本，少数机器宕机，不会丢失数据，不会导致不可用 |\n| 时效性   |RabbitMQ基于Erlang开发，所以并发能力很强，性能极其好，延时很低，达到微秒级，其他几个都是ms级。 |\n| 功能支持 | Pulsar的功能更全面，支持多租户、多种消费模式和持久性模式等功能，是下一代云原生分布式消息流平台。 |\n| 消息丢失 | ActiveMQ和RabbitMQ丢失的可能性非常低，Kafka、RocketMQ和Pulsar理论上可以做到0丢失。 |\n\n**总结：**\n\n- ActiveMQ的社区算是比较成熟，但是较目前来说，ActiveMQ的性能比较差，而且版本迭代很慢，不推荐使用，已经被淘汰了。\n- RabbitMQ在吞吐量方面虽然稍逊于Kafka、RocketMQ和Pulsar，但是由于它基于Erlang开发，所以并发能力很强，性能极其好，延时很低，达到微秒级。但是也因为RabbitMQ基于Erlang开发，所以国内很少有公司有实力做Erlang源码级别的研究和定制。如果业务场景对并发量要求不是太高（十万级、百万级），那这几种消息队列中，RabbitMQ或许是你的首选。\n- RocketMQ和Pulsar支持强一致性，对消息一致性要求比较高的场景可以使用。\n- RocketMQ阿里出品，Java系开源项目，源代码我们可以直接阅读，然后可以定制自己公司的MQ，并且RocketMQ有阿里巴巴的实际业务场景的实战考验。\n- Kafka的特点其实很明显，就是仅仅提供较少的核心功能，但是提供超高的吞吐量，ms级的延迟，极高的可用性以及可靠性，而且分布式可以任意扩展。同时Kafka最好是支撑较少的topic数量即可，保证其超高吞吐量。Kafka唯一的一点劣势是有可能消息重复消费，那么对数据准确性会造成极其轻微的影响，在大数据领域中以及日志采集中，这点轻微影响可以忽略这个特性天然适合大数据实时计算以及日志收集。如果是大数据领域的实时计算、日志采集等场景，用Kafka是业内标准的，绝对没问题，社区活跃度很高，绝对不会黄，何况几乎是全世界这个领域的事实性规范。\n\n> [7个方面综合对比Kafka、RabbitMQ、RocketMQ、ActiveMQ四个分布式消息队列](https://mp.weixin.qq.com/s/7PDuuEChls6MDUCiI8_FaA)\n\n\n## 经典面试题\n\n### 为什么使用消息队列\n**解耦**\n传统模式下系统间的耦合性太强。怎么说呢，举个例子：系统A通过接口调用发送数据到B、C、D三个系统，如果将来E系统接入或者B系统不需要接入了，那么系统A还需要修改代码，非常麻烦。如果系统A产生了一条比较关键的数据，那么它就要时时刻刻考虑B、C、D、E四个系统如果挂了该咋办？这条数据它们是否都收到了？显然，系统A跟其它系统严重耦合。而如果我们将数据（消息）写入消息队列，需要消息的系统直接自己从消息队列中消费。这样下来，系统A就不需要去考虑要给谁发送数据，不需要去维护这个代码，也不需要考虑其他系统是否调用成功、失败超时等情况，反正我只负责生产，别的我不管。\n\n**异步**\n先来看传统同步的情况，举个例子：系统A接收一个用户请求，需要进行写库操作，还需要同样的在B、C、D三个系统中进行写库操作。如果A自己本地写库只要1ms，而B、C、D三个系统写库分别要100ms、200ms、300ms,最终请求总延时是1 + 100 + 200 + 300 = 601ms，用户体验大打折扣。如果使用消息队列，那么系统A就只需要发送3条消息到消息队列中就行了，假如耗时5ms，A系统从接受一个请求到返回响应给用户，总时长是1 + 5 = 6ms，对于用户而言，体验好感度直接拉满。\n\n**削峰**\n如果没有使用缓存或者消息队列，那么系统就是直接基于数据库MySQL的，如果有那么一个高峰期，产生了大量的请求涌入MySQL，毫无疑问，系统将会直接崩溃。那如果我们使用消息队列，假设MySQL每秒钟最多处理1k条数据，而高峰期瞬间涌入了5k条数据，不过，这5k条数据涌入了消息队列。这样，我们的系统就可以从消息队列中根据数据库的能力慢慢的来拉取请求，不要超过自己每秒能处理的最大请求数量就行。也就是说消息队列每秒钟5k个请求进来，1k个请求出去，假设高峰期1个小时，那么这段时间就可能有几十万甚至几百万的请求积压在消息队列中。不过这个短暂的高峰期积压是完全可以的，因为高峰期过了之后，每秒钟就没有那么多的请求进入消息队列了，但是数据库依然会按照每秒1k个请求的速度处理。所以只要高峰期一过，系统就会快速的将积压的消息给处理掉。\n\n### MQ怎么保证消息可靠性？MQ怎么保证消息100%消费？如何保证消息不会丢失？\n我们需要保障MQ消息的可靠性，需要从三个层面/维度解决：生产者100%投递、MQ持久化、消费者100%消费，这里的100%消费指的是消息不少消费，也不多消费。\n\n**消息生产阶段**：从消息被生产出来，然后提交给MQ的过程中，只要能正常收到MQ Broker的ack确认响应，就表示发送成功，所以只要处理好返回值和异常，这个阶段是不会出现消息丢失的。\n**消息存储阶段**：这个阶段一般会直接交给MQ消息中间件来保证，但是你要了解它的原理，比如Broker会做副本，保证一条消息至少同步两个节点再返回ack。\n**消息消费阶段**：消费端从Broker上拉取消息，只要消费端在收到消息后，不立即发送消费确认给Broker，而是等到执行完业务逻辑后，再发送消费确认，也能保证消息的不丢失。\n\n**以kafka为例**\n一、生产者端\n\n1. Kafka消息发送端有个ACK机制。设置ack参数：ack=0，表示不重试，Kafka不需要返回ack，极有可能各种原因造成丢失；ack=1，表示Leader写入成功就返回ack了，Follower不一定同步成功；ack=all或ack=-1，表示ISR列表中的所有Follower同步完成再返回ack。只要能正常收到MQ Broker的ack确认响应，就表示发送成功，所以只要处理好返回值和异常，这个阶段是不会出现消息丢失的。\n2. 设置参数unclean.leader.election.enable:false，禁止选举ISR以外的Follower为Leader，只能从ISR列表中的节点中选举Leader；可能会牺牲Kafka的可用性，但是能够提高消息的可靠性。\n3. 重试机制，设置tries>1，表示消息重发次数。\n4. 设置最小同步副本数min.insync.replicas>1，没满足该值前，Kafka不提供读写服务，写操作会异常。\n总结：生产消息时通过设置最小同步副本数和ACK机制，可以让MQ在性能与可靠性上达到平衡。\n\n二、消费者端\n手工提交offset（偏移量）\n\n**以RocketMQ为例**\n需要确保生产者、broker、消费者三者都不丢失数据。\n1. 生产者不丢失消息\n方案1：confirm消息确认机制(同步发送消息) + 失败重试；\n方案2：事务消息机制；\n2. broker不丢失消息，开启同步刷盘策略 + 主从架构同步机制。\n只要让一个master Broker收到消息之后同步写入磁盘，同时同步复制给其他slave Broker，再返回成功响应给生产者，此时就可以保证MQ自己不会弄丢消息\n3. 消费者不丢失消息，采用RocketMQ的消费者天然就可以保证你处理完消息之后，才会提交消息的offset到broker去，不过别采用多线程异步处理消息的方式。\n\n### 如何保证mq有序且只消费一次?怎么解决消息被重复消费的问题？\n\n最简单的实现方案，就是在数据库(redis或者关系库)中建一张消息日志表，这个表有两个字段：消息ID和消息执行状态\n\n### 消息积压\n增加消费端数量\n\n### MQ高可用的相关文章\n\n- [消息队列消息丢失和消息重复发送的处理策略](https://mp.weixin.qq.com/s/Yu95odAgq7VVzCa5ttkwow)\n- [消息队列如何保证消息不丢失，且只被消费一次，这篇就教会你](https://mp.weixin.qq.com/s/wzYvFEowBHB-MJNQU6N3OQ)\n- [面试官：如何保障消息100%投递成功、消息幂等性？](https://mp.weixin.qq.com/s/ON4avaV8pOjmJVll3K90RQ)\n- [如何保证MQ消息是有序的？](https://mp.weixin.qq.com/s/tNyJhqzOyQFE2fIEygFyTw)\n- [消息被重复消费，怎么避免？有什么好的解决方案？](https://mp.weixin.qq.com/s/T3aEpibbP1u48LTat1KxHQ)\n- [如何保证消息队列里的数据顺序执行？](https://mp.weixin.qq.com/s/b_2OZkTIViA4IbfsfwwbxA)\n- [面试基操：MQ怎么保障消息可靠性？](https://mp.weixin.qq.com/s/F6JsNbLfeo_63kCzcBgogA)\n- [使用MQ的时候，怎么确保消息100%不丢失](https://mp.weixin.qq.com/s/yVatpQzVd--dlbR5ImBkWg)\n\n### other\n\n- [什么是MQ？](https://mp.weixin.qq.com/s/mXX2hjRQ7qlPs0wK98Kb4Q)\n- [消息队列原理和选型：Kafka、RocketMQ、RabbitMQ和ActiveMQ](https://mp.weixin.qq.com/s/upKoeiT2EvMBBlqyRKiv8g)\n- [一篇文章把RabbitMQ、RocketMQ、Kafka三元归一](https://mp.weixin.qq.com/s/pXjLoPkyLNj9AHFoHI-GdA)\n- [这个队列的思路真的好，现在它是我简历上的亮点了。](https://mp.weixin.qq.com/s/mtEdb-vJGF7irOJ26QbnvA)\n- [面试官必问的3道MQ面试题，还有谁不会？？](https://mp.weixin.qq.com/s/8-zvNtXlad0pACg7Yhh4TA)\n","categories":["技术栈"]},{"title":"锁","slug":"锁","url":"/blog/posts/35e64501a595/","content":"\n## 悲观锁\n\n悲观锁总是假设最坏的情况，认为共享资源每次被访问的时候就会出现问题(比如共享数据被修改)，所以每次在获取资源操作的时候都会上锁，这样其他线程想拿到这个资源就会阻塞直到锁被上一个持有者释放。也就是说，**共享资源每次只给一个线程使用，其它线程阻塞，用完后再把资源转让给其它线程**。像Java中synchronized和ReentrantLock等独占锁就是悲观锁思想的实现。**悲观锁通常多用于写比较多的情况下（多写场景），避免频繁失败和重试影响性能**。\n\n悲观锁适用于数据库高并发的大量写入操作，本次事务提交之前（事务提交时会释放事务过程中的锁），外界无法修改这些记录。具体使用:mysql数据库首先要设置事务不允许自动提交```set autocommit=0```然后```select name from table where id = ? for update```将查询的结果上锁,这样的话其他得事务查询的话如果不加for update能够正常查询，否则会一直显示正在查询中 并且其他事务不可以对此数据做增删改操作,当上锁的查询语句的事务提交后，即执行commit后其他事务才可以对数据进行其他的操作。悲观锁可以避免冲突的发生，但是会降低效率。\n**如果查询条件用了索引/主键，那么select ..... for update就会进行锁行。**\n**如果是普通字段(没有索引/主键)，那么select ..... for update就会进行锁表。**\n\n## 乐观锁\n\n### 乐观锁实现\n\n乐观锁总是假设最好的情况，认为共享资源每次被访问的时候不会出现问题，线程可以不停地执行，无需加锁也无需等待，只是在提交修改的时候去验证对应的资源（也就是数据）是否被其它线程修改了（具体方法可以使用版本号机制或CAS算法）。\n\n在Java中`java.util.concurrent.atomic`包下面的原子变量类就是使用了乐观锁的一种实现方式**CAS**实现的。**乐观锁通常多于写比较少的情况下（多读场景），避免频繁加锁影响性能，大大提升了系统的吞吐量。**\n\n\n大多是基于数据版本(Version)记录机制实现。何谓数据版本？即为数据增加一个版本标识，在基于数据库表的版本解决方案中，一般是通过为数据库表增加一个“version”字段来实现。读取出数据时，将此版本号一同读出，之后更新时，对此版本号加一。此时，将提交数据的版本数据与数据库表对应记录的当前版本信息进行比对，如果提交的数据版本号大于数据库表当前版本号，则予以更新，否则认为是过期数据。对于上面修改用户帐户信息的例子而言，假设数据库中帐户信息表中有一个version字段，当前值为1；而当前帐户余额字段（balance）为$100。操作员A此时将其读出（version=1），并从其帐户余额中扣除$50（$100-$50）。2在操作员A操作的过程中，操作员B也读入此用户信息（version=1），并从其帐户余额中扣除$20（$100-$20）。3操作员A完成了修改工作，将数据版本号加一（version=2），连同帐户扣除后余额（balance=$50），提交至数据库更新，此时由于提交数据版本大于数据库记录当前版本，数据被更新，数据库记录version更新为2。4操作员B完成了操作，也将版本号加一（version=2）试图向数据库提交数据（balance=$80），但此时比对数据库记录版本时发现，操作员B提交的数据版本号为2，数据库记录当前版本也为2，不满足“提交版本必须大于记录当前版本才能执行更新“的乐观锁策略，因此，操作员B的提交被驳回。这样，就避免了操作员B用基于version=1的旧数据修改的结果覆盖操作员A的操作结果的可能。从上面的例子可以看出，乐观锁机制避免了长事务中的数据库加锁开销（操作员A和操作员B操作过程中，都没有对数据库数据加锁），大大提升了大并发量下的系统整体性能表现。需要注意的是，乐观锁机制往往基于系统中的数据存储逻辑，因此也具备一定的局限性，如在上例中，由于乐观锁机制是在我们的系统中实现，来自外部系统的用户余额更新操作不受我们系统的控制，因此可能会造成脏数据被更新到数据库中。在系统设计阶段，我们应该充分考虑到这些情况出现的可能性，并进行相应调整（如将乐观锁策略在数据库存储过程中实现，对外只开放基于此存储过程的数据更新途径，而不是将数据库表直接对外公开）\n```sql\nupdate table set status=#{status},name=#{name},version=version+1 where id=#{id} and version=#{version}\n```\n\n### 乐观锁存在哪些问题\n\n#### ABA问题\n\n如果一个变量V初次读取的时候是A值，并且在准备赋值的时候检查到它仍然是A值，那我们就能说明它的值没有被其他线程修改过了吗？很明显是不能的，因为在这段时间它的值可能被改为其他值，然后又改回A，那CAS操作就会误认为它从来没有被修改过。这个问题被称为CAS操作的**ABA问题**。\n\nABA问题的解决思路是在变量前面追加上**版本号或者时间戳**。JDK1.5以后的AtomicStampedReference类就是用来解决ABA问题的，其中的compareAndSet()方法就是首先检查当前引用是否等于预期引用，并且当前标志是否等于预期标志，如果全部相等，则以原子方式将该引用和该标志的值设置为给定的更新值。\n\n```java\npublic boolean compareAndSet(V   expectedReference,\n                             V   newReference,\n                             int expectedStamp,\n                             int newStamp) {\n    Pair<V> current = pair;\n    return\n        expectedReference == current.reference &&\n        expectedStamp == current.stamp &&\n        ((newReference == current.reference &&\n          newStamp == current.stamp) ||\n         casPair(current, Pair.of(newReference, newStamp)));\n}\n```\n\n#### 循环时间长开销大\n\nCAS经常会用到自旋操作来进行重试，也就是不成功就一直循环执行直到成功。如果长时间不成功，会给CPU带来非常大的执行开销。如果JVM能支持处理器提供的pause指令那么效率会有一定的提升，pause指令有两个作用：\n\n1. 可以延迟流水线执行指令，使CPU不会消耗过多的执行资源，延迟的时间取决于具体实现的版本，在一些处理器上延迟时间是零。\n2. 可以避免在退出循环的时候因内存顺序冲而引起CPU流水线被清空，从而提高CPU的执行效率。\n\n#### 只能保证一个共享变量的原子操作\n\nCAS只对单个共享变量有效，当操作涉及跨多个共享变量时CAS无效。但是从JDK1.5开始，提供了**AtomicReference**类来保证引用对象之间的原子性，你可以把多个变量放在一个对象里来进行CAS操作.所以我们可以使用锁或者利用**AtomicReference**类把多个共享变量合并成一个共享变量来操作。\n\n## 分布式锁\n\n### 分布式锁介绍\n\n对于单机多线程来说，在Java中，我们通常使用ReetrantLock类、synchronized关键字这类JDK自带的本地锁来控制一个JVM进程内的多个线程对本地共享资源的访问。这些线程访问共享资源是互斥的，同一时刻只有一个线程可以获取到本地锁访问共享资源。\n\n分布式系统下，不同的服务/客户端通常运行在独立的JVM进程上。如果多个JVM进程共享同一份资源的话，使用本地锁就没办法实现资源的互斥访问了。于是，分布式锁就诞生了。举个例子：系统的订单服务一共部署了3份，都对外提供服务。用户下订单之前需要检查库存，为了防止超卖，这里需要加锁以实现对检查库存操作的同步访问。由于订单服务位于不同的JVM进程中，本地锁在这种情况下就没办法正常工作了。我们需要用到分布式锁，这样的话，即使多个线程不在同一个JVM进程中也能获取到同一把锁，进而实现共享资源的互斥访问。这些独立的进程中的线程访问共享资源是互斥的，同一时刻只有一个线程可以获取到分布式锁访问共享资源。\n\n一个最基本的分布式锁需要满足：\n\n- **互斥**：任意一个时刻，锁只能被一个线程持有；\n- **高可用**：锁服务是高可用的。并且，即使客户端的释放锁的代码逻辑出现问题，锁最终一定还是会被释放，不会影响其他线程对共享资源的访问。\n- **可重入**：一个节点获取了锁之后，还可以再次获取锁。\n\n通常情况下，我们一般会选择基于Redis或者ZooKeeper实现分布式锁，Redis用的要更多一点，我这里也以Redis为例介绍分布式锁的实现。\n\n### 基于Redis实现分布式锁\n\n#### 如何基于Redis实现一个最简易的分布式锁？\n\n不论是本地锁还是分布式锁，核心都在于“互斥”。在Redis中，`SETNX`命令是可以帮助我们实现互斥。`SETNX`即**SET if Not Exists**(对应Java中的`setIfAbsent`方法)，如果key不存在的话，才会设置key的值。如果key已经存在，`SETNX`啥也不做。\n\n\n```bash\n> SETNX lockKey uniqueValue\n(integer) 1\n> SETNX lockKey uniqueValue\n(integer) 0\n```\n\n释放锁的话，直接通过`DEL`命令删除对应的key即可。\n\n```bash\n> DEL lockKey\n(integer) 1\n```\n\n为了防止误删到其他的锁，这里我们建议使用Lua脚本通过key对应的value（唯一值）来判断。选用Lua脚本是为了保证解锁操作的原子性。因为Redis在执行Lua脚本时，可以以原子性的方式执行，从而保证了锁释放操作的原子性。\n\n```lua\n// 释放锁时，先比较锁对应的value值是否相等，避免锁的误释放\nif redis.call(\"get\",KEYS[1]) == ARGV[1] then\n    return redis.call(\"del\",KEYS[1])\nelse\n    return 0\nend\n```\n\n这是一种最简易的Redis分布式锁实现，实现方式比较简单，性能也很高效。不过，这种方式实现分布式锁存在一些问题。就比如应用程序遇到一些问题比如释放锁的逻辑突然挂掉，可能会导致锁无法被释放，进而造成共享资源无法再被其他线程/进程访问。\n\n#### 为什么要给锁设置一个过期时间？\n\n为了避免锁无法被释放，我们可以想到的一个解决办法就是：**给这个key（也就是锁）设置一个过期时间**。\n\n\n```bash\n127.0.0.1:6379> SET lockKey uniqueValue EX 3 NX\nOK\n```\n\n- **lockKey**：加锁的锁名；\n- **uniqueValue**：能够唯一标示锁的随机字符串；\n- **NX**：只有当lockKey对应的key值不存在的时候才能SET成功；\n- **EX**：过期时间设置（秒为单位）EX3标示这个锁有一个3秒的自动过期时间。与EX对应的是PX（毫秒为单位），这两个都是过期时间设置。\n\n一定要保证设置指定key的值和过期时间是一个原子操作!!不然的话，依然可能会出现锁无法被释放的问题。这样确实可以解决问题，不过，这种解决办法同样存在漏洞：如果操作共享资源的时间大于过期时间，就会出现锁提前过期的问题，进而导致分布式锁直接失效。如果锁的超时时间设置过长，又会影响到性能。你或许在想：如果操作共享资源的操作还未完成，锁过期时间能够自己续期就好了！\n\n#### 如何实现锁的优雅续期？\n\n对于Java开发的小伙伴来说，已经有了现成的解决方案：**[Redisson](https://github.com/redisson/redisson)**。其他语言的解决方案，可以在[Redis官方文档](https://redis.io/topics/distlock)中找到\n\n![DistributedlockswithRedis](https://guide-blog-images.oss-cn-shenzhen.aliyuncs.com/github/javaguide/redis-distributed-lock.png)\n\nRedisson是一个开源的Java语言Redis客户端，提供了很多开箱即用的功能，不仅仅包括多种分布式锁的实现。并且，Redisson还支持Redis单机、RedisSentinel、RedisCluster等多种部署架构。Redisson中的分布式锁自带自动续期机制，使用起来非常简单，原理也比较简单，其提供了一个专门用来监控和续期锁的**WatchDog（看门狗）**，如果操作共享资源的线程还未执行完成的话，WatchDog会不断地延长锁的过期时间，进而保证锁不会因为超时而被释放。看门狗名字的由来于`getLockWatchdogTimeout()`方法，这个方法返回的是看门狗给锁续期的过期时间，默认为30秒（[redisson-3.17.6](https://github.com/redisson/redisson/releases/tag/redisson-3.17.6)）。\n\n\n```java\n// 默认30秒，支持修改\nprivate long lockWatchdogTimeout = 30 * 1000;\n\npublic Config setLockWatchdogTimeout(long lockWatchdogTimeout) {\n    this.lockWatchdogTimeout = lockWatchdogTimeout;\n    return this;\n}\npublic long getLockWatchdogTimeout() {\n  \treturn lockWatchdogTimeout;\n}\n```\n\n`renewExpiration()`方法包含了看门狗的主要逻辑：\n\n\n```java\nprivate void renewExpiration() {\n         //......\n        Timeout task = commandExecutor.getConnectionManager().newTimeout(new TimerTask() {\n            @Override\n            public void run(Timeout timeout) throws Exception {\n                //......\n                // 异步续期，基于Lua脚本\n                CompletionStage<Boolean> future = renewExpirationAsync(threadId);\n                future.whenComplete((res, e) -> {\n                    if (e != null) {\n                        // 无法续期\n                        log.error(\"Can't update lock \" + getRawName() + \" expiration\", e);\n                        EXPIRATION_RENEWAL_MAP.remove(getEntryName());\n                        return;\n                    }\n\n                    if (res) {\n                        // 递归调用实现续期\n                        renewExpiration();\n                    } else {\n                        // 取消续期\n                        cancelExpirationRenewal(null);\n                    }\n                });\n            }\n         // 延迟internalLockLeaseTime/3（默认10s，也就是30/3）再调用\n        }, internalLockLeaseTime / 3, TimeUnit.MILLISECONDS);\n\n        ee.setTimeout(task);\n    }\n```\n\n默认情况下，每过10秒，看门狗就会执行续期操作，将锁的超时时间设置为30秒。看门狗续期前也会先判断是否需要执行续期操作，需要才会执行续期，否则取消续期操作。WatchDog通过调用`renewExpirationAsync()`方法实现锁的异步续期：\n\n```java\nprotected CompletionStage<Boolean> renewExpirationAsync(long threadId) {\n    return evalWriteAsync(getRawName(), LongCodec.INSTANCE, RedisCommands.EVAL_BOOLEAN,\n            // 判断是否为持锁线程，如果是就执行续期操作，就锁的过期时间设置为30s（默认）\n            \"if (redis.call('hexists', KEYS[1], ARGV[2]) == 1) then \" +\n                    \"redis.call('pexpire', KEYS[1], ARGV[1]); \" +\n                    \"return 1; \" +\n                    \"end; \" +\n                    \"return 0;\",\n            Collections.singletonList(getRawName()),\n            internalLockLeaseTime, getLockName(threadId));\n}\n```\n\n可以看出，`renewExpirationAsync`方法其实是调用Lua脚本实现的续期，这样做主要是为了保证续期操作的原子性。这里以Redisson的分布式可重入锁`RLock`为例来说明如何使用Redisson实现分布式锁：\n\n```java\n// 1.获取指定的分布式锁对象\nRLock lock = redisson.getLock(\"lock\");\n// 2.拿锁且不设置锁超时时间，具备Watch Dog自动续期机制\nlock.lock();\n// 3.执行业务\n...\n// 4.释放锁\nlock.unlock();\n```\n\n只有未指定锁超时时间，才会使用到Watch Dog自动续期机制。\n\n```java\n// 手动给锁设置过期时间，不具备Watch Dog自动续期机制\nlock.lock(10, TimeUnit.SECONDS);\n```\n\n如果使用Redis来实现分布式锁的话，还是比较推荐直接基于Redisson来做的。\n\n#### 如何实现可重入锁？\n\n所谓可重入锁指的是在一个线程中可以多次获取同一把锁，比如一个线程在执行一个带锁的方法，该方法中又调用了另一个需要相同锁的方法，则该线程可以直接执行调用的方法即可重入，而无需重新获得锁。像Java中的synchronized和ReentrantLock都属于可重入锁。\n\n**不可重入的分布式锁基本可以满足绝大部分业务场景了，一些特殊的场景可能会需要使用可重入的分布式锁。**\n\n可重入分布式锁的实现核心思路是线程在获取锁的时候判断是否为自己的锁，如果是的话，就不用再重新获取了。为此，我们可以为每个锁关联一个可重入计数器和一个占有它的线程。当可重入计数器大于0时，则锁被占有，需要判断占有该锁的线程和请求获取锁的线程是否为同一个。实际项目中，我们不需要自己手动实现，推荐使用我们上面提到的**Redisson**，其内置了多种类型的锁比如可重入锁（ReentrantLock）、自旋锁（SpinLock）、公平锁（FairLock）、多重锁（MultiLock）、红锁（RedLock）、读写锁（ReadWriteLock）。\n\n#### Redis如何解决集群情况下分布式锁的可靠性？\n\n为了避免单点故障，生产环境下的Redis服务通常是集群化部署的。Redis集群下，上面介绍到的分布式锁的实现会存在一些问题。由于Redis集群数据同步到各个节点时是异步的，如果在Redis主节点获取到锁后，在没有同步到其他节点时，Redis主节点宕机了，此时新的Redis主节点依然可以获取锁，所以多个应用服务就可以同时获取到锁。针对这个问题，Redis之父antirez设计了[Redlock算法](https://redis.io/topics/distlock)来解决。Redlock算法的思想是让客户端向Redis集群中的多个独立的Redis实例依次请求申请加锁，如果客户端能够和半数以上的实例成功地完成加锁操作，那么我们就认为，客户端成功地获得分布式锁，否则加锁失败。即使部分Redis节点出现问题，只要保证Redis集群中有半数以上的Redis节点可用，分布式锁服务就是正常的。Redlock是直接操作Redis节点的，并不是通过Redis集群操作的，这样才可以避免Redis集群主从切换导致的锁丢失问题。Redlock实现比较复杂，性能比较差，发生时钟变迁的情况下还存在安全性隐患。《数据密集型应用系统设计》一书的作者MartinKleppmann曾经专门发文（[Howtododistributedlocking-MartinKleppmann-2016](https://martin.kleppmann.com/2016/02/08/how-to-do-distributed-locking.html)）怼过Redlock，他认为这是一个很差的分布式锁实现。感兴趣的朋友可以看看[Redis锁从面试连环炮聊到神仙打架](https://mp.weixin.qq.com/s?__biz=Mzg3NjU3NTkwMQ==&mid=2247505097&idx=1&sn=5c03cb769c4458350f4d4a321ad51f5a&source=41#wechat_redirect)这篇文章，有详细介绍到antirez和MartinKleppmann关于Redlock的激烈辩论。实际项目中不建议使用Redlock算法，成本和收益不成正比。如果不是非要实现绝对可靠的分布式锁的话，其实单机版Redis就完全够了，实现简单，性能也非常高。如果你必须要实现一个绝对可靠的分布式锁的话，可以基于ZooKeeper来做，只是性能会差一些。\n\n> [图解Redis分布式锁](https://mp.weixin.qq.com/s/2cFtKhPX1QZ-ZxnZarYJrA)\n\n### 基于ZooKeeper实现分布式锁\n\nRedis实现分布式锁性能较高，ZooKeeper实现分布式锁可靠性更高。实际项目中，我们应该根据业务的具体需求来选择。\n\n#### 如何基于ZooKeeper实现分布式锁？\n\n客户端连接zookeeper，并在/lock下创建临时的且有序的子节点，第一个客户端对应的子节点为/lock/lock-0000000000，第二个为/lock/lock-0000000001，以此类推；客户端获取/lock下的子节点列表，判断自己创建的子节点是否为当前子节点列表中序号最小的子节点，如果是则认为获得锁，否则监听刚好在自己之前一位的子节点删除消息，获得子节点变更通知后重复此步骤直至获得锁；执行业务代码；完成业务流程后，删除对应的子节点释放锁。ZooKeeper分布式锁是基于**临时顺序节点和Watcher（事件监听器）** 实现的。\n\n获取锁：\n\n1. 首先我们要有一个持久节点/locks，客户端获取锁就是在locks下创建临时顺序节点。\n2. 假设客户端1创建了/locks/lock1节点，创建成功之后，会判断lock1是否是/locks下最小的子节点。\n3. 如果lock1是最小的子节点，则获取锁成功。否则，获取锁失败。\n4. 如果获取锁失败，则说明有其他的客户端已经成功获取锁。客户端1并不会不停地循环去尝试加锁，而是在前一个节点比如/locks/lock0上注册一个事件监听器。这个监听器的作用是当前一个节点释放锁之后通知客户端1（避免无效自旋），这样客户端1就加锁成功了。\n\n释放锁：\n\n1. 成功获取锁的客户端在执行完业务流程之后，会将对应的子节点删除。\n2. 成功获取锁的客户端在出现故障之后，对应的子节点由于是临时顺序节点，也会被自动删除，避免了锁无法被释放。\n3. 我们前面说的事件监听器其实监听的就是这个子节点删除事件，子节点删除就意味着锁被释放。\n\n实际项目中，推荐使用**Curator**来实现ZooKeeper分布式锁。**Curator**是Netflix公司开源的一套ZooKeeper Java客户端框架，相比于ZooKeeper自带的客户端zookeeper来说，Curator的封装更加完善，各种API都可以比较方便地使用。Curator主要实现了下面四种锁：\n\n- **InterProcessMutex**：分布式可重入排它锁\n- **InterProcessSemaphoreMutex**：分布式不可重入排它锁\n- **InterProcessReadWriteLock**：分布式读写锁\n- **InterProcessMultiLock**：将多个锁作为单个实体管理的容器，获取锁的时候获取所有锁，释放锁也会释放所有锁资源（忽略释放失败的锁）。\n\n\n```java\nCuratorFramework client = ZKUtils.getClient();\nclient.start();\n// 分布式可重入排它锁\nInterProcessLock lock1 = new InterProcessMutex(client, lockPath1);\n// 分布式不可重入排它锁\nInterProcessLock lock2 = new InterProcessSemaphoreMutex(client, lockPath2);\n// 将多个锁作为一个整体\nInterProcessMultiLock lock = new InterProcessMultiLock(Arrays.asList(lock1, lock2));\n\nif (!lock.acquire(10, TimeUnit.SECONDS)) {\n  \tthrow new IllegalStateException(\"不能获取多锁\");\n}\nSystem.out.println(\"已获取多锁\");\nSystem.out.println(\"是否有第一个锁: \" + lock1.isAcquiredInThisProcess());\nSystem.out.println(\"是否有第二个锁: \" + lock2.isAcquiredInThisProcess());\ntry {\n    // 资源操作\n \t \t\tresource.use(); \n} finally {\n    System.out.println(\"释放多个锁\");\n    lock.release(); \n}\nSystem.out.println(\"是否有第一个锁: \" + lock1.isAcquiredInThisProcess());\nSystem.out.println(\"是否有第二个锁: \" + lock2.isAcquiredInThisProcess());\nclient.close();\n```\n\n#### 为什么要用临时顺序节点？\n\n每个数据节点在ZooKeeper中被称为znode，它是ZooKeeper中数据的最小单元。我们通常是将znode分为4大类：\n\n- **持久（PERSISTENT）节点**：一旦创建就一直存在即使ZooKeeper集群宕机，直到将其删除。\n- **临时（EPHEMERAL）节点**：临时节点的生命周期是与客户端会话（session）绑定的，会话消失则节点消失。并且，临时节点只能做叶子节点，不能创建子节点。\n- **持久顺序（PERSISTENT_SEQUENTIAL）节点**：除了具有持久（PERSISTENT）节点的特性之外，子节点的名称还具有顺序性。比如`/node1/app0000000001`、`/node1/app0000000002`。\n- **临时顺序（EPHEMERAL_SEQUENTIAL）节点**：除了具备临时（EPHEMERAL）节点的特性之外，子节点的名称还具有顺序性。\n\n可以看出，临时节点相比持久节点，最主要的是对会话失效的情况处理不一样，临时节点会话消失则对应的节点消失。这样的话，如果客户端发生异常导致没来得及释放锁也没关系，会话失效节点自动被删除，不会发生死锁的问题。\n\n使用Redis实现分布式锁的时候，我们是通过过期时间来避免锁无法被释放导致死锁问题的，而ZooKeeper直接利用临时节点的特性即可。假设不适用顺序节点的话，所有尝试获取锁的客户端都会对持有锁的子节点加监听器。当该锁被释放之后，势必会造成所有尝试获取锁的客户端来争夺锁，这样对性能不友好。使用顺序节点之后，只需要监听前一个节点就好了，对性能更友好。\n\n#### 为什么要设置对前一个节点的监听？\n\n> Watcher（事件监听器），是ZooKeeper中的一个很重要的特性。ZooKeeper允许用户在指定节点上注册一些Watcher，并且在一些特定事件触发的时候，ZooKeeper服务端会将事件通知到感兴趣的客户端上去，该机制是ZooKeeper实现分布式协调服务的重要特性。\n\n同一时间段内，可能会有很多客户端同时获取锁，但只有一个可以获取成功。如果获取锁失败，则说明有其他的客户端已经成功获取锁。获取锁失败的客户端并不会不停地循环去尝试加锁，而是在前一个节点注册一个事件监听器。这个事件监听器的作用是：当前一个节点对应的客户端释放锁之后（也就是前一个节点被删除之后，监听的是删除事件），通知获取锁失败的客户端（唤醒等待的线程，Java中的`wait/notifyAll`），让它尝试去获取锁，然后就成功获取锁了。\n\n#### 如何实现可重入锁？\n\n这里以Curator的`InterProcessMutex`对可重入锁的实现来介绍（源码地址：[InterProcessMutex.java](https://github.com/apache/curator/blob/master/curator-recipes/src/main/java/org/apache/curator/framework/recipes/locks/InterProcessMutex.java)）。\n\n当我们调用`InterProcessMutex#acquire`方法获取锁的时候，会调用`InterProcessMutex#internalLock`方法。\n\n```java\n// 获取可重入互斥锁，直到获取成功为止\n@Override\npublic void acquire() throws Exception {\n  if (!internalLock(-1, null)) {\n    throw new IOException(\"Lost connection while trying to acquire lock: \" + basePath);\n  }\n}\n```\n\n`internalLock`方法会先获取当前请求锁的线程，然后从`threadData(ConcurrentMap<Thread,LockData>类型)`中获取当前线程对应的lockData。lockData包含锁的信息和加锁的次数，是实现可重入锁的关键。第一次获取锁的时候，lockData为null。获取锁成功之后，会将当前线程和对应的lockData放到threadData中\n\n```java\nprivate boolean internalLock(long time, TimeUnit unit) throws Exception {\n  // 获取当前请求锁的线程\n  Thread currentThread = Thread.currentThread();\n  // 拿对应的lockData\n  LockData lockData = threadData.get(currentThread);\n  // 第一次获取锁的话，lockData为null\n  if (lockData != null) {\n    // 当前线程获取过一次锁之后\n    // 因为当前线程的锁存在，lockCount自增后返回，实现锁重入.\n    lockData.lockCount.incrementAndGet();\n    return true;\n  }\n  // 尝试获取锁\n  String lockPath = internals.attemptLock(time, unit, getLockNodeBytes());\n  if (lockPath != null) {\n    LockData newLockData = new LockData(currentThread, lockPath);\n     // 获取锁成功之后，将当前线程和对应的lockData放到threadData中\n    threadData.put(currentThread, newLockData);\n    return true;\n  }\n\n  return false;\n}\n```\n\nLockData是InterProcessMutex中的一个静态内部类。\n\n```java\nprivate final ConcurrentMap<Thread, LockData> threadData = Maps.newConcurrentMap();\n\nprivate static class LockData\n{\n    // 当前持有锁的线程\n    final Thread owningThread;\n    // 锁对应的子节点\n    final String lockPath;\n    // 加锁的次数\n    final AtomicInteger lockCount = new AtomicInteger(1);\n\n    private LockData(Thread owningThread, String lockPath)\n    {\n      this.owningThread = owningThread;\n      this.lockPath = lockPath;\n    }\n}\n```\n\n如果已经获取过一次锁，后面再来获取锁的话，直接就会在`if (lockData != null)`这里被拦下了，然后就会执行`lockData.lockCount.incrementAndGet();`将加锁次数加 1。整个可重入锁的实现逻辑非常简单，直接在客户端判断当前线程有没有获取锁，有的话直接将加锁次数加1就可以了。\n\n### 总结\n\n这篇文章我们介绍了分布式锁的基本概念以及实现分布式锁的两种常见方式。至于具体选择Redis还是ZooKeeper来实现分布式锁，还是要看业务的具体需求。如果对性能要求比较高的话，建议使用Redis实现分布式锁。如果对可靠性要求比较高的话，建议使用ZooKeeper实现分布式锁。\n\n> [原文链接](https://javaguide.cn/distributed-system/distributed-lock.html)\n\n### 相关文章\n\n- [关于分布式锁的面试题都在这里了](https://mp.weixin.qq.com/s/Y7-cwHq1Z2hs2CWooFgVyg)\n- [分布式锁之Zookeeper](https://mp.weixin.qq.com/s/xKVdMvmNvy-1iV0tqx70Ng)\n- [电商防超卖的锁的解决方案](https://mp.weixin.qq.com/s/q1E5WnBKmoHZ59Euyk0kVw)\n- [阿里面试官：分布式锁到底用Redis好？还是Zookeeper好？](https://mp.weixin.qq.com/s/5T0nrf5LBSqOiPd99OB3tw)\n- [分布式锁用Redis还是Zookeeper？](https://mp.weixin.qq.com/s/_P3zWS1QzPdQp10Jb1bSOA)\n- [Redis分布式锁到底安全吗](https://mp.weixin.qq.com/s/O9XZxwAcrCY-ninASw0I5Q)\n- [Redis分布式锁深入探究](https://mp.weixin.qq.com/s/9SPMfKpv4iF_9m2F9ZTa3A)\n- [七种方案！探讨Redis分布式锁的正确使用姿势](https://mp.weixin.qq.com/s/LTS3MKBU2FbRivXyQJcptA)\n- [聊聊redis分布式锁的8大坑](https://mp.weixin.qq.com/s/vnVyvTeKvT_ZfMHr5_7CNg)\n- [这才叫细：带你深入理解Redis分布式锁](https://mp.weixin.qq.com/s/yUH8jH9NTPLk24J_BQ0JfQ)\n- [RedisTemplate分布式锁演变、Redission分布式锁实现](https://mp.weixin.qq.com/s/XnDxJgs9fp-zAGLZ5g2vsw)\n- [年轻人，看看Redisson分布式锁—可重入锁吧！太重要了](https://mp.weixin.qq.com/s/BlDsXWOcqpudORSiyI05Lg)\n- [Redis分布式锁的正确实现原理演化历程与Redisson实战总结](https://mp.weixin.qq.com/s/PKGYoerpjWGeDzDE2iCFag)\n- [从零到一编码实现Redis分布式锁](https://mp.weixin.qq.com/s/fwpHS07LcLLe82_cGOUxKA)\n- [面试官问：Redis分布式锁如何自动续期？](https://mp.weixin.qq.com/s/x9YFF4QtHrCu3abSQHEL-A)\n\n## Java中的锁\n\n### synchronized\n\n#### synchronized是什么？有什么用？\n\nsynchronized是Java中的一个关键字，翻译成中文是同步的意思，主要解决的是多个线程之间访问资源的同步性，可以保证被它修饰的方法或者代码块在任意时刻只能有一个线程执行。在Java早期版本中，synchronized属于重量级锁，效率低下。这是因为监视器锁（monitor）是依赖于底层的操作系统的Mutex Lock来实现的，Java的线程是映射到操作系统的原生线程之上的。如果要挂起或者唤醒一个线程，都需要操作系统帮忙完成，而操作系统实现线程之间的切换时需要从用户态转换到内核态，这个状态之间的转换需要相对比较长的时间，时间成本相对较高。不过，在Java6之后，synchronized引入了大量的优化如自旋锁、适应性自旋锁、锁消除、锁粗化、偏向锁、轻量级锁等技术来减少锁操作的开销，这些优化让synchronized锁的效率提升了很多。因此，synchronized还是可以在实际项目中使用的，像JDK源码、很多开源框架都大量使用了synchronized。\n\n#### 如何使用synchronized？\n\nsynchronized关键字的使用方式主要有下面3种：\n\n1. **修饰实例方法**\n2. **修饰静态方法**\n3. **修饰代码块**\n\n**1、修饰实例方法（锁当前对象实例）**\n\n给当前对象实例加锁，进入同步代码前要获得当前对象实例的锁。\n\n```java\nsynchronized void method() {\n    //业务代码\n}\n```\n\n**2、修饰静态方法（锁当前类）**\n\n给当前类加锁，会作用于类的所有对象实例，进入同步代码前要获得当前class的锁。这是因为静态成员不属于任何一个实例对象，归整个类所有，不依赖于类的特定实例，被类的所有实例共享。\n\n```java\nsynchronized static void method() {\n    //业务代码\n}\n```\n\n静态synchronized方法和非静态synchronized方法之间的调用互斥么？不互斥！如果一个线程A调用一个实例对象的非静态synchronized方法，而线程B需要调用这个实例对象所属类的静态synchronized方法，是允许的，不会发生互斥现象，因为**访问静态synchronized方法占用的锁是当前类的锁，而访问非静态synchronized方法占用的锁是当前实例对象锁**。\n\n**3、修饰代码块（锁指定对象/类）**\n\n对括号里指定的对象/类加锁：\n\n- **synchronized(object)表示进入同步代码库前要获得给定对象的锁**。\n- **synchronized(类.class)表示进入同步代码前要获得给定Class的锁**\n\n```java\nsynchronized(this) {\n    //业务代码\n}\n```\n\n**总结：**\n\n- **synchronized关键字加到static静态方法和synchronized(class)代码块上都是是给Class类上锁**；\n- **synchronized关键字加到实例方法上是给对象实例上锁**；\n- **尽量不要使用synchronized(String a)因为JVM中，字符串常量池具有缓存功能**。\n\n#### 构造方法可以用synchronized修饰么？\n\n先说结论：**构造方法不能使用synchronized关键字修饰**。构造方法本身就属于线程安全的，不存在同步的构造方法一说。\n\n#### synchronized底层原理了解吗？\n\nsynchronized关键字底层原理属于JVM层面的东西。\n\n##### synchronized同步语句块的情况\n\n```java\npublic class SynchronizedDemo {\n    public void method() {\n        synchronized (this) {\n            System.out.println(\"synchronized代码块\");\n        }\n    }\n}\n```\n\n通过JDK自带的`javap`命令查看`SynchronizedDemo`类的相关字节码信息：首先切换到类的对应目录执行`javac SynchronizedDemo.java`命令生成编译后的.class文件，然后执行`javap -c -s -v -l SynchronizedDemo.class`。\n\n![synchronized关键字原理](https://my-blog-to-use.oss-cn-beijing.aliyuncs.com/2019-6/synchronized关键字原理.png)\n\n从上面我们可以看出：synchronized同步语句块的实现使用的是monitorenter和monitorexit指令，其中monitorenter指令指向同步代码块的开始位置，monitorexit指令则指明同步代码块的结束位置。当执行monitorenter指令时，线程试图获取锁也就是获取对象监视器monitor的持有权。\n\n> 在Java虚拟机(HotSpot)中，Monitor是基于C++实现的，由[ObjectMonitor](https://github.com/openjdk-mirror/jdk7u-hotspot/blob/50bdefc3afe944ca74c3093e7448d6b889cd20d1/src/share/vm/runtime/objectMonitor.cpp)实现的。每个对象中都内置了一个ObjectMonitor对象。\n>\n> 另外，wait/notify等方法也依赖于monitor对象，这就是为什么只有在同步的块或者方法中才能调用wait/notify等方法，否则会抛出java.lang.IllegalMonitorStateException的异常的原因。\n\n在执行monitorenter时，会尝试获取对象的锁，如果锁的计数器为0则表示锁可以被获取，获取后将锁计数器设为1也就是加1。\n\n![执行monitorenter获取锁](https://guide-blog-images.oss-cn-shenzhen.aliyuncs.com/github/javaguide/java/concurrent/synchronized-get-lock-code-block.png)\n\n对象锁的的拥有者线程才可以执行monitorexit指令来释放锁。在执行monitorexit指令后，将锁计数器设为0，表明锁被释放，其他线程可以尝试获取锁。\n\n![执行monitorexit释放锁](https://guide-blog-images.oss-cn-shenzhen.aliyuncs.com/github/javaguide/java/concurrent/synchronized-release-lock-block.png)\n\n如果获取对象锁失败，那当前线程就要阻塞等待，直到锁被另外一个线程释放为止。\n\n##### synchronized修饰方法的的情况\n\n\n```java\npublic class SynchronizedDemo2 {\n    public synchronized void method() {\n        System.out.println(\"synchronized方法\");\n    }\n}\n```\n\n![synchronized关键字原理](https://my-blog-to-use.oss-cn-beijing.aliyuncs.com/2019-6/synchronized关键字原理2.png)\n\nsynchronized修饰的方法并没有monitorenter指令和monitorexit指令，取得代之的确实是ACC_SYNCHRONIZED标识，该标识指明了该方法是一个同步方法。JVM通过该ACC_SYNCHRONIZED访问标志来辨别一个方法是否声明为同步方法，从而执行相应的同步调用。\n\n如果是实例方法，JVM会尝试获取实例对象的锁。如果是静态方法，JVM会尝试获取当前class的锁。\n\n##### 总结\n\nsynchronized同步语句块的实现使用的是monitorenter和monitorexit指令，其中monitorenter指令指向同步代码块的开始位置，monitorexit指令则指明同步代码块的结束位置。synchronized修饰的方法并没有monitorenter指令和monitorexit指令，取得代之的确实是ACC_SYNCHRONIZED标识，该标识指明了该方法是一个同步方法。\n\n**不过两者的本质都是对对象监视器monitor的获取。**\n\n> 相关推荐：[Java锁与线程的那些事](https://tech.youzan.com/javasuo-yu-xian-cheng-de-na-xie-shi/)\n> 🧗🏻进阶一下：学有余力的小伙伴可以抽时间详细研究一下对象监视器monitor。\n\n#### JDK1.6之后的synchronized底层做了哪些优化？\n\nJDK1.6对锁的实现引入了大量的优化，如偏向锁、轻量级锁、自旋锁、适应性自旋锁、锁消除、锁粗化等技术来减少锁操作的开销。\n\n锁主要存在四种状态，依次是：无锁状态、偏向锁状态、轻量级锁状态、重量级锁状态，他们会随着竞争的激烈而逐渐升级。注意锁可以升级不可降级，这种策略是为了提高获得锁和释放锁的效率。关于这几种优化的详细信息可以查看下面这篇文章：[Java6及以上版本对synchronized的优化](https://www.cnblogs.com/wuqinglong/p/9945618.html)。\n\n#### synchronized和volatile有什么区别？\n\n`synchronized`关键字和`volatile`关键字是两个互补的存在，而不是对立的存在！\n\n- volatile关键字是线程同步的轻量级实现，所以volatile性能肯定比synchronized关键字要好。但是volatile关键字只能用于变量而synchronized关键字可以修饰方法以及代码块。\n- volatile关键字能保证数据的可见性，但不能保证数据的原子性。synchronized关键字两者都能保证。\n- volatile关键字主要用于解决变量在多个线程之间的可见性，而synchronized关键字解决的是多个线程之间访问资源的同步性。\n\n\n> [原文链接](https://javaguide.cn/java/concurrent/java-concurrent-questions-02.html)\n\n\n#### 相关文章\n\n- [synchronized关键字和Lock、区别](https://mp.weixin.qq.com/s/lt9pL7aMknGZwEHzWe6hCA)\n- [Synchronized天天用，实现原理你懂吗？](https://mp.weixin.qq.com/s/LcykdAXYF3Oj3xTl221_rQ)\n- [关于Synchronized的一个点，网上99%的文章都错了](https://mp.weixin.qq.com/s/clnJLeDbG0pxwd6EUewQwQ)\n- [13张图，深入理解Synchronized](https://mp.weixin.qq.com/s/zP1XnF_wvUD0HRS4cDCdVg)\n- [synchronized中的4个优化，你知道几个？](https://mp.weixin.qq.com/s/KmYoVfiCbr3y4HBQnzL1HQ)\n- [使用了synchronized，为啥还有线程安全问题！？](https://mp.weixin.qq.com/s/3elpJFnCb_fswt8tsNyu4g)\n- [synchronized加锁this和class的区别](https://mp.weixin.qq.com/s/ewXpaeMYNx7FAW_fYoQbPg)\n- [为什么wait方法要在synchronized中调用？](https://mp.weixin.qq.com/s/5wCL3LSpT44iMgVM9k9CaA)\n- [synchronized底层了解一下...](https://mp.weixin.qq.com/s/OSi1fEoGHA1qdC0dJMs55Q)\n- [深入理解Synchronized](https://mp.weixin.qq.com/s/Z6ZuE8oefnNHlNfCyQbohw)\n- [synchronized的8种用法,你会几种？](https://mp.weixin.qq.com/s/n8WxaUtkXkDOju5QkTASOQ)\n- [synchronized的用法,你知道多少？](https://mp.weixin.qq.com/s/tM4mncQwIkt9zBR1W_dBcA)\n\n### ReentrantLock\n\n#### ReentrantLock是什么？\n\nReentrantLock实现了Lock接口，是一个可重入且独占式的锁，和synchronized关键字类似。不过，ReentrantLock更灵活、更强大，增加了轮询、超时、中断、公平锁和非公平锁等高级功能。\n\n\n```java\npublic class ReentrantLock implements Lock, java.io.Serializable {}\n```\n\nReentrantLock里面有一个内部类Sync，Sync继承AQS（AbstractQueuedSynchronizer），添加锁和释放锁的大部分操作实际上都是在Sync中实现的。Sync有公平锁FairSync和非公平锁NonfairSync两个子类。\n\n![img](https://guide-blog-images.oss-cn-shenzhen.aliyuncs.com/github/javaguide/java/concurrent/reentrantlock-class-diagram.png)\n\nReentrantLock默认使用非公平锁，也可以通过构造器来显示的指定使用公平锁。\n\n\n```java\n// 传入一个boolean值，true时为公平锁，false时为非公平锁\npublic ReentrantLock(boolean fair) {\n    sync = fair ? new FairSync() : new NonfairSync();\n}\n```\n\n从上面的内容可以看出，ReentrantLock的底层就是由AQS来实现的。关于AQS的相关内容推荐阅读[AQS详解](https://javaguide.cn/java/concurrent/aqs.html)这篇文章。\n\n#### 公平锁和非公平锁有什么区别？\n\n- **公平锁**:锁被释放之后，先申请的线程先得到锁。性能较差一些，因为公平锁为了保证时间上的绝对顺序，上下文切换更频繁。\n- **非公平锁**：锁被释放之后，后申请的线程可能会先获取到锁，是随机或者按照其他优先级排序的。性能更好，但可能会导致某些线程永远无法获取到锁。\n\n#### synchronized和ReentrantLock有什么区别？\n\n##### 两者都是可重入锁\n\n可重入锁也叫递归锁，指的是线程可以再次获取自己的内部锁。比如一个线程获得了某个对象的锁，此时这个对象锁还没有释放，当其再次想要获取这个对象的锁的时候还是可以获取的，如果是不可重入锁的话，就会造成死锁。JDK提供的所有现成的Lock实现类，包括synchronized关键字锁都是可重入的。在下面的代码中，method1()和method2()都被synchronized关键字修饰，method1()调用了method2()。\n\n```java\npublic class ReentrantLockDemo {\n    public synchronized void method1() {\n        System.out.println(\"方法1\");\n        method2();\n    }\n\n    public synchronized void method2() {\n        System.out.println(\"方法2\");\n    }\n}\n```\n\n由于synchronized锁是可重入的，同一个线程在调用method1()时可以直接获得当前对象的锁，执行method2()的时候可以再次获取这个对象的锁，不会产生死锁问题。假如synchronized是不可重入锁的话，由于该对象的锁已被当前线程所持有且无法释放，这就导致线程在执行method2()时获取锁失败，会出现死锁问题。\n\n##### synchronized依赖于JVM而ReentrantLock依赖于API\n\nsynchronized是依赖于JVM实现的，前面我们也讲到了虚拟机团队在JDK1.6为synchronized关键字进行了很多优化，但是这些优化都是在虚拟机层面实现的，并没有直接暴露给我们。\n\nReentrantLock是JDK层面实现的（也就是API层面，需要lock()和unlock()方法配合try/finally语句块来完成），所以我们可以通过查看它的源代码，来看它是如何实现的。\n\n##### ReentrantLock比synchronized增加了一些高级功能\n\n相比synchronized，ReentrantLock增加了一些高级功能。主要来说主要有三点：\n\n- **等待可中断**:ReentrantLock提供了一种能够中断等待锁的线程的机制，通过lock.lockInterruptibly()来实现这个机制。也就是说正在等待的线程可以选择放弃等待，改为处理其他事情。\n- **可实现公平锁**:ReentrantLock可以指定是公平锁还是非公平锁。而synchronized只能是非公平锁。所谓的公平锁就是先等待的线程先获得锁。ReentrantLock默认情况是非公平的，可以通过ReentrantLock类的ReentrantLock(boolean fair)构造方法来制定是否是公平的。\n- **可实现选择性通知（锁可以绑定多个条件）**:synchronized关键字与wait()和notify()/notifyAll()方法相结合可以实现等待/通知机制。ReentrantLock类当然也可以实现，但是需要借助于Condition接口与newCondition()方法。\n\n如果你想使用上述功能，那么选择ReentrantLock是一个不错的选择。\n\n关于Condition接口的补充：\n\n> Condition是JDK1.5之后才有的，它具有很好的灵活性，比如可以实现多路通知功能也就是在一个Lock对象中可以创建多个Condition实例（即对象监视器），线程对象可以注册在指定的Condition中，从而可以有选择性的进行线程通知，在调度线程上更加灵活。在使用notify()/notifyAll()方法进行通知时，被通知的线程是由JVM选择的，用ReentrantLock类结合Condition实例可以实现“选择性通知”，这个功能非常重要，而且是Condition接口默认提供的。而synchronized关键字就相当于整个Lock对象中只有一个Condition实例，所有的线程都注册在它一个身上。如果执行notifyAll()方法的话就会通知所有处于等待状态的线程，这样会造成很大的效率问题。而Condition实例的signalAll()方法，只会唤醒注册在该Condition实例中的所有等待线程。\n\n#### 可中断锁和不可中断锁有什么区别？\n\n- **可中断锁**：获取锁的过程中可以被中断，不需要一直等到获取锁之后才能进行其他逻辑处理。ReentrantLock就属于是可中断锁。\n- **不可中断锁**：一旦线程申请了锁，就只能等到拿到锁以后才能进行其他的逻辑处理。synchronized就属于是不可中断锁。\n\n> [聊一聊ReentrantLock类的一些玩法](https://mp.weixin.qq.com/s/totowqfBABfFr95VKpppwQ)\n> [demo](https://github.com/xmxe/demo/blob/master/study-demo/src/main/java/com/xmxe/jdkfeature/thread/juc/ReentrantLockTest.java)\n\n\n### ReentrantReadWriteLock\n\nReentrantReadWriteLock在实际项目中使用的并不多，面试中也问的比较少，简单了解即可。JDK1.8引入了性能更好的读写锁StampedLock。\n\n#### ReentrantReadWriteLock是什么？\n\nReentrantReadWriteLock实现了ReadWriteLock，是一个可重入的读写锁，既可以保证多个线程同时读的效率，同时又可以保证有写入操作时的线程安全。\n\n\n```java\npublic class ReentrantReadWriteLock implements ReadWriteLock, java.io.Serializable{\n}\npublic interface ReadWriteLock {\n    Lock readLock();\n    Lock writeLock();\n}\n```\n\n- 一般锁进行并发控制的规则：读读互斥、读写互斥、写写互斥。\n- 读写锁进行并发控制的规则：读读不互斥、读写互斥、写写互斥（只有读读不互斥）。\n\nReentrantReadWriteLock其实是两把锁，一把是WriteLock(写锁)，一把是ReadLock`（读锁）。读锁是共享锁，写锁是独占锁。读锁可以被同时读，可以同时被多个线程持有，而写锁最多只能同时被一个线程持有。和ReentrantLock一样，ReentrantReadWriteLock底层也是基于AQS实现的。\n\n![img](https://guide-blog-images.oss-cn-shenzhen.aliyuncs.com/github/javaguide/java/concurrent/reentrantreadwritelock-class-diagram.png)\n\nReentrantReadWriteLock也支持公平锁和非公平锁，默认使用非公平锁，可以通过构造器来显示的指定。\n\n\n```java\n// 传入一个boolean值，true时为公平锁，false时为非公平锁\npublic ReentrantReadWriteLock(boolean fair) {\n    sync = fair ? new FairSync() : new NonfairSync();\n    readerLock = new ReadLock(this);\n    writerLock = new WriteLock(this);\n}\n```\n\n#### ReentrantReadWriteLock适合什么场景？\n\n由于ReentrantReadWriteLock既可以保证多个线程同时读的效率，同时又可以保证有写入操作时的线程安全。因此，在读多写少的情况下，使用ReentrantReadWriteLock能够明显提升系统性能。\n\n#### 共享锁和独占锁有什么区别？\n\n- **共享锁**：一把锁可以被多个线程同时获得。\n- **独占锁**：一把锁只能被一个线程获得。\n\n#### 线程持有读锁还能获取写锁吗？\n\n- 在线程持有读锁的情况下，该线程不能取得写锁(因为获取写锁的时候，如果发现当前的读锁被占用，就马上获取失败，不管读锁是不是被当前线程持有)。\n- 在线程持有写锁的情况下，该线程可以继续获取读锁（获取读锁时如果发现写锁被占用，只有写锁没有被当前线程占用的情况才会获取失败）。\n\n> 读写锁的源码分析，推荐阅读[聊聊Java的几把JVM级锁](https://mp.weixin.qq.com/s/h3VIUyH9L0v14MrQJiiDbw)，写的很不错。\n\n#### 读锁为什么不能升级为写锁？\n\n写锁可以降级为读锁，但是读锁却不能升级为写锁。这是因为读锁升级为写锁会引起线程的争夺，毕竟写锁属于是独占锁，这样的话，会影响性能。另外，还可能会有死锁问题发生。举个例子：假设两个线程的读锁都想升级写锁，则需要对方都释放自己锁，而双方都不释放，就会产生死锁。\n\n### StampedLock\n\nStampedLock面试中问的比较少，不是很重要，简单了解即可。\n\n#### StampedLock是什么？\n\nStampedLock是JDK1.8引入的性能更好的读写锁，不可重入且不支持条件变量Conditon。不同于一般的Lock类，StampedLock并不是直接实现Lock或ReadWriteLock接口，而是基于CLH锁独立实现的（AQS也是基于这玩意）。\n\n\n```java\npublic class StampedLock implements java.io.Serializable {}\n```\n\nStampedLock提供了三种模式的读写控制模式：读锁、写锁和乐观读。\n\n- **写锁**：独占锁，一把锁只能被一个线程获得。当一个线程获取写锁后，其他请求读锁和写锁的线程必须等待。类似于ReentrantReadWriteLock的写锁，不过这里的写锁是不可重入的。\n- **读锁**（悲观读）：共享锁，没有线程获取写锁的情况下，多个线程可以同时持有读锁。如果己经有线程持有写锁，则其他线程请求获取该读锁会被阻塞。类似于ReentrantReadWriteLock的读锁，不过这里的读锁是不可重入的。\n- **乐观读**：允许多个线程获取乐观读以及读锁。同时允许一个写线程获取写锁。\n\n另外，StampedLock还支持这三种锁在一定条件下进行相互转换。\n\n```java\nlong tryConvertToWriteLock(long stamp){}\nlong tryConvertToReadLock(long stamp){}\nlong tryConvertToOptimisticRead(long stamp){}\n```\n\nStampedLock在获取锁的时候会返回一个long型的数据戳，该数据戳用于稍后的锁释放参数，如果返回的数据戳为0则表示锁获取失败。当前线程持有了锁再次获取锁还是会返回一个新的数据戳，这也是StampedLock不可重入的原因。\n\n```java\n// 写锁\npublic long writeLock() {\n    long s, next;  // bypass acquireWrite in fully unlocked case only\n    return ((((s = state) & ABITS) == 0L &&\n             U.compareAndSwapLong(this, STATE, s, next = s + WBIT)) ?\n            next : acquireWrite(false, 0L));\n}\n// 读锁\npublic long readLock() {\n    long s = state, next;  // bypass acquireRead on common uncontended case\n    return ((whead == wtail && (s & ABITS) < RFULL &&\n             U.compareAndSwapLong(this, STATE, s, next = s + RUNIT)) ?\n            next : acquireRead(false, 0L));\n}\n// 乐观读\npublic long tryOptimisticRead() {\n    long s;\n    return (((s = state) & WBIT) == 0L) ? (s & SBITS) : 0L;\n}\n```\n\n#### StampedLock的性能为什么更好？\n\n相比于传统读写锁多出来的乐观读是StampedLock比ReadWriteLock性能更好的关键原因。StampedLock的乐观读允许一个写线程获取写锁，所以不会导致所有写线程阻塞，也就是当读多写少的时候，写线程有机会获取写锁，减少了线程饥饿的问题，吞吐量大大提高。\n\n#### StampedLock适合什么场景？\n\n和ReentrantReadWriteLock一样，StampedLock同样适合读多写少的业务场景，可以作为ReentrantReadWriteLock的替代品，性能更好。不过，需要注意的是StampedLock不可重入，不支持条件变量Conditon，对中断操作支持也不友好（使用不当容易导致CPU飙升）。如果你需要用到ReentrantLock的一些高级性能，就不太建议使用StampedLock了。另外，StampedLock性能虽好，但使用起来相对比较麻烦，一旦使用不当，就会出现生产问题。强烈建议你在使用StampedLock之前，看看[StampedLock官方文档中的案例](https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/locks/StampedLock.html)。\n\n#### StampedLock的底层原理了解吗？\n\nStampedLock不是直接实现Lock或ReadWriteLock接口，而是基于CLH锁实现的（AQS也是基于这玩意），CLH锁是对自旋锁的一种改良，是一种隐式的链表队列。StampedLock通过CLH队列进行线程的管理，通过同步状态值state来表示锁的状态和类型。\n\nStampedLock的原理和AQS原理比较类似，这里就不详细介绍了，感兴趣的可以看看下面这两篇文章：\n\n> [AQS详解](https://javaguide.cn/java/concurrent/aqs.html)\n> [StampedLock底层原理分析](https://segmentfault.com/a/1190000015808032)\n>\n> [原文链接](https://javaguide.cn/java/concurrent/java-concurrent-questions-02.html)\n\n\n## 其他锁\n\n- [聊聊13种锁的实现方式](https://mp.weixin.qq.com/s/IxC46bZW99mjfdvIYyEqmg)\n- [一文足以了解什么是Java中的锁](https://mp.weixin.qq.com/s/ws0gp-cbbAteJ9kYg7d_-A)\n- [图解Java中那18把锁](https://mp.weixin.qq.com/s/IfPkIxoTn5eqZExclaCUTA)\n- [一文详解Java的几把JVM级锁](https://mp.weixin.qq.com/s/M5WTYg_aGciaHma2X2hV-w)\n- [一文图解带你了解Java中的那些锁！](https://mp.weixin.qq.com/s/JbtY4Rf6NMqhmNY0RqkF2Q)\n- [彻底说清楚JAVA锁的种类以及区别](https://mp.weixin.qq.com/s/rFYTWTwSVgwwhrn4ol0lPg)\n- [老大吩咐的可重入分布式锁，终于完美的实现了](https://mp.weixin.qq.com/s/bl4OWKUKPFD2VlcdArHBhQ)\n- [你用对锁了吗?浅谈Java\"锁\"事](https://mp.weixin.qq.com/s/09hgu1Z9DY2-zhksu4wFRQ)\n- [一文看懂JUC之AQS机制](https://mp.weixin.qq.com/s/HEylBNG8-uIHrUwDFE8GYA)\n- [『图解Java并发』面试必问的CAS原理你会了吗？](https://mp.weixin.qq.com/s/Gn0yyQiALJzD5nWRwMME8g)\n- [了解这两个接口后，阿里多线程面试题秒AC](https://mp.weixin.qq.com/s/x6h3xYaAdIPTnHxJTlb6cQ)\n- [1.3w字，一文详解死锁！](https://mp.weixin.qq.com/s/4mJIRUShBXcxmsR6otmuwQ)\n- [重磅出击，20张图带你彻底了解ReentrantLock加锁解锁的原理](https://mp.weixin.qq.com/s/9LX3G-99RnEY5dAY0KRGOw)\n- [一文掌握ReentrantLock加解锁原理](https://mp.weixin.qq.com/s/7j6eSX-p9TrySjCgbNeivw)\n- [什么是自旋锁？自旋的好处和后果是什么](https://mp.weixin.qq.com/s/ft9GJlrL90mfY4Yz-6OhQA)","categories":["Java"]},{"title":"Java中的volatile关键字","slug":"Java中的volatile关键字","url":"/blog/posts/75fb1533e3f4/","content":"\n> [VolatileTest.java](https://github.com/xmxe/demo/blob/master/study-demo/src/main/java/com/xmxe/jdkfeature/thread/VolatileTest.java)\n\n### 如何保证变量的可见性？\n\n在Java中，volatile关键字可以保证变量的可见性，如果我们将变量声明为**volatile**，这就指示JVM，这个变量是共享且不稳定的，每次使用它都到主存中进行读取。\n\n![img](https://guide-blog-images.oss-cn-shenzhen.aliyuncs.com/github/javaguide/java/concurrent/jmm.png)\n\n![JMM(Java内存模型)强制在主存中进行读取](https://guide-blog-images.oss-cn-shenzhen.aliyuncs.com/github/javaguide/java/concurrent/jmm2.png)\n\nvolatile关键字其实并非是Java语言特有的，在C语言里也有，它最原始的意义就是禁用CPU缓存。如果我们将一个变量使用volatile修饰，这就指示编译器，这个变量是共享且不稳定的，每次使用它都到主存中进行读取。\n\nvolatile关键字能保证数据的可见性，但不能保证数据的原子性。synchronized关键字两者都能保证。\n\n### 如何禁止指令重排序？\n\n在Java中，volatile关键字除了可以保证变量的可见性，还有一个重要的作用就是防止JVM的指令重排序。如果我们将变量声明为volatile，在对这个变量进行读写操作的时候，会通过插入特定的内存屏障的方式来禁止指令重排序。\n\n在Java中，Unsafe类提供了三个开箱即用的内存屏障相关的方法，屏蔽了操作系统底层的差异：\n\n```java\npublic native void loadFence();\npublic native void storeFence();\npublic native void fullFence();\n```\n\n理论上来说，你通过这个三个方法也可以实现和volatile禁止重排序一样的效果，只是会麻烦一些。面试中面试官经常会说：“单例模式了解吗？来给我手写一下！给我解释一下双重检验锁方式实现单例模式的原理呗！”\n\n**双重校验锁实现对象单例（线程安全）**：\n\n```java\npublic class Singleton {\n\n    private volatile static Singleton uniqueInstance;\n\n    private Singleton() {\n    }\n\n    public  static Singleton getUniqueInstance() {\n       //先判断对象是否已经实例过，没有实例化过才进入加锁代码\n        if (uniqueInstance == null) {\n            //类对象加锁\n            synchronized (Singleton.class) {\n                if (uniqueInstance == null) {\n                    uniqueInstance = new Singleton();\n                }\n            }\n        }\n        return uniqueInstance;\n    }\n}\n```\n\nuniqueInstance采用volatile关键字修饰也是很有必要的，`uniqueInstance = new Singleton();`这段代码其实是分为三步执行：\n\n1. 为`uniqueInstance`分配内存空间\n2. 初始化`uniqueInstance`\n3. 将`uniqueInstance`指向分配的内存地址\n\n但是由于JVM具有指令重排的特性，执行顺序有可能变成1->3->2。指令重排在单线程环境下不会出现问题，但是在多线程环境下会导致一个线程获得还没有初始化的实例。例如，线程T1执行了1和3，此时T2调用getUniqueInstance()后发现uniqueInstance不为空，因此返回uniqueInstance，但此时uniqueInstance还未被初始化。\n\n### volatile可以保证原子性么？\n\n**volatile关键字能保证变量的可见性，但不能保证对变量的操作是原子性的**。\n\n我们通过下面的代码即可证明：\n\n```java\n\npublic class VolatoleAtomicityDemo {\n    public volatile static int inc = 0;\n\n    public void increase() {\n        inc++;\n    }\n\n    public static void main(String[] args) throws InterruptedException {\n        ExecutorService threadPool = Executors.newFixedThreadPool(5);\n        VolatoleAtomicityDemo volatoleAtomicityDemo = new VolatoleAtomicityDemo();\n        for (int i = 0; i < 5; i++) {\n            threadPool.execute(() -> {\n                for (int j = 0; j < 500; j++) {\n                    volatoleAtomicityDemo.increase();\n                }\n            });\n        }\n        // 等待1.5秒，保证上面程序执行完成\n        Thread.sleep(1500);\n        System.out.println(inc);\n        threadPool.shutdown();\n    }\n}\n```\n\n正常情况下，运行上面的代码理应输出`2500`。但你真正运行了上面的代码之后，你会发现每次输出结果都小于`2500`。为什么会出现这种情况呢？不是说好了，volatile可以保证变量的可见性嘛！也就是说，如果volatile能保证inc++操作的原子性的话。每个线程中对inc变量自增完之后，其他线程可以立即看到修改后的值。5个线程分别进行了500次操作，那么最终inc的值应该是5*500=2500。\n\n很多人会误认为自增操作`inc++`是原子性的，实际上，`inc++`其实是一个复合操作，包括三步：\n\n1. 读取inc的值。\n2. 对inc加1。\n3. 将inc的值写回内存。\n\nvolatile是无法保证这三个操作是具有原子性的，有可能导致下面这种情况出现：\n\n1. 线程1对inc进行读取操作之后，还未对其进行修改。线程2又读取了inc的值并对其进行修改（+1），再将inc的值写回内存。\n2. 线程2操作完毕后，线程1对inc的值进行修改（+1），再将inc的值写回内存。\n\n这也就导致两个线程分别对inc进行了一次自增操作后，inc实际上只增加了1。其实，如果想要保证上面的代码运行正确也非常简单，利用synchronized、Lock或者AtomicInteger都可以。\n\n使用**synchronized**改进：\n\n```java\npublic synchronized void increase() {\n    inc++;\n}\n```\n\n使用**AtomicInteger**改进：\n\n```java\npublic AtomicInteger inc = new AtomicInteger();\n\npublic void increase() {\n    inc.getAndIncrement();\n}\n```\n\n使用**ReentrantLock**改进：\n\n```java\nLock lock = new ReentrantLock();\npublic void increase() {\n    lock.lock();\n    try {\n        inc++;\n    } finally {\n        lock.unlock();\n    }\n}\n```\n\n### 原理\n\n并发编程中的三个概念:原子性问题，可见性问题，有序性问题\n**原子性**：即一个操作或者多个操作,要么全部执行并且执行的过程不会被任何因素打断，要么就都不执行。例如银行转账\n**可见性**：是指当多个线程访问同一个变量时，一个线程修改了这个变量的值，其他线程能够立即看得到修改的值\n**有序性**：即程序执行的顺序按照代码的先后顺序执行\n要想并发程序正确地执行，必须要保证原子性、可见性以及有序性。只要有一个没有被保证，就有可能会导致程序运行不正确。\n\ncpu很快,但是cpu从内存里读取数据和写入数据是很慢，为了提高读写内存速度，硬件工程师在cpu上加了一块高速缓存(工作内存),cpu从主内存中读取数据，在高速缓存(工作内存)中建立副本变量，在操作完副本变量后将变量更新到主内存，而volatile关键字则是告诉jvm，它所修饰的变量不保留拷贝，直接访问主内存中的数据，这样就保证了可见性，其实就是如果一个变量声明成是volatile的，那么当我读变量时，总是能读到它的最新值，这里最新值是指不管其它哪个线程对该变量做了写操作，都会立刻被更新到主存里，我也能从主存里读到这个刚写入的值。也就是说volatile关键字可以保证可见性以及有序性，但不保证原子性\n\nJMM（java内存模型）允许编译器和处理器对指令重排序的，但是规定了as-if-serial语义，即不管怎么重排序，程序的执行结果不能改变\n**volatile禁止指令重排(指令重排序)，不支持原子性操作**\nvolatile变量规则：对一个volatile域的写，happens-before（发生前）于后续对这个volatile域的读,即写在读之前\n\n### 使用场景\n\n您只能在有限的一些情形下使用volatile变量替代锁。要使volatile变量提供理想的线程安全，必须同时满足下面两个条件：\n1. 对变量的写操作不依赖于当前值。\n2. 该变量没有包含在具有其他变量的不变式中。\n\nvolatile最适用一个线程写，多个线程读的场合。如果有多个线程并发写操作，仍然需要使用锁或者线程安全的容器或者原子变量来代替\n\nJMM具备一些先天的有序性,即不需要通过任何手段就可以保证的有序性，通常称为happens-before原则。<<JSR-133：Java Memory Model and Thread Specification>>定义了如下happens-before规则\n1. 程序顺序规则：一个线程中的每个操作，happens-before于该线程中的任意后续操作\n2. 监视器锁规则：对一个线程的解锁，happens-before于随后对这个线程的加锁\n3. volatile变量规则：对一个volatile域的写，happens-before于后续对这个volatile域的读\n4. 传递性：如果A happens-before B,且B happens-before C,那么A happens-before C\n5. start()规则：如果线程A执行操作ThreadB_start()(启动线程B),那么A线程的ThreadB_start()happens-before于B中的任意操作\n6. join()原则：如果A执行ThreadB.join()并且成功返回，那么线程B中的任意操作happens-before于线程A从ThreadB.join()操作成功返回。\n7. interrupt()原则：对线程interrupt()方法的调用先行发生于被中断线程代码检测到中断事件的发生，可以通过Thread.interrupted()方法检测是否有中断发生\n8. finalize()原则：一个对象的初始化完成先行发生于它的finalize()方法的开始\n\n**System.out.println()会影响volatile可见性**\n因为println()使用synchronized修饰的，JMM关于synchronized的两条规定:\n\n1. 线程解锁前，必须把共享变量的最新值刷新到主内存中\n2. 线程加锁时，将清空工作内存中共享变量的值，从而使用共享变量时需要从主内存中重新获取最新的值\n\n### 相关文章\n\n- [面试：说说Java中的volatile关键词？](https://mp.weixin.qq.com/s/0a-74XQagO0TkN5b5RMD3A)\n- [volatile和synchronized到底啥区别？多图文讲解告诉你](https://mp.weixin.qq.com/s/U8WoqH1YRdh1GyFiPDw0KQ)\n- [面试:Java并发之volatile我彻底懂了](https://mp.weixin.qq.com/s/2srKmbZlgmUuuT7kCrL0RQ)\n- [面试官最爱的volatile关键字](https://mp.weixin.qq.com/s?__biz=MzI4MDYwMDc3MQ==&mid=2247486266&idx=1&sn=7beaca0358914b3606cde78bfcdc8da3&chksm=ebb74296dcc0cb805a45ca9c0501b7c2c37e8f2586295210896d18e3a0c72b01bea765924ce5&mpshare=1&scene=24&srcid=&key=c8fbfa031bd0c4166acd110fd54b85e9b3568f80a3f4c2d80add2f4add0ced46d1d3a0cf139c0ca64877a98635727a7fc593b850f8082d1fcf77a5ebf067fc1476285146d13d691f80b64b930006a341&ascene=0&uin=MjYwNzAzMzYzNw%3D%3D&devicetype=iMac+MacBookAir6%2C2+OSX+OSX+10.14.2+build(18C54)&version=12020810&nettype=WIFI&lang=zh_CN&fontScale=100&pass_ticket=hbg9AwR77rok2jxxdwyHyTHBDzwwC7lR8aEfF6HfW4KgJwsj0ruOpw8iNsUK%2B5kK)\n- [26张图带你彻底搞懂volatile关键字！](https://mp.weixin.qq.com/s/ShZwxqQb2Y-it27QawB9PQ)\n- [掉了两根头发，可算是把volatile整明白了](https://mp.weixin.qq.com/s/qX-IxNw86kw5Lk_6D07x1w)\n- [面试官：谈谈CPU Cache工作原理，Cache一致性？我懵了。。](https://mp.weixin.qq.com/s/Md1s1t_V1gPNZr0JsoLebQ)","categories":["Java"]},{"title":"ThreadLocal","slug":"ThreadLocal","url":"/blog/posts/7b1b83ae7caa/","content":"\n> [ThreadLocalTest.java](https://github.com/xmxe/demo/blob/master/study-demo/src/main/java/com/xmxe/jdkfeature/thread/ThreadLocalTest.java)\n\n### ThreadLocal有什么用？\n\n通常情况下，我们创建的变量是可以被任何一个线程访问并修改的。如果想实现每一个线程都有自己的专属本地变量该如何解决呢？JDK中自带的ThreadLocal类正是为了解决这样的问题。ThreadLocal类主要解决的就是让每个线程绑定自己的值，可以将ThreadLocal类形象的比喻成存放数据的盒子，盒子中可以存储每个线程的私有数据。如果你创建了一个ThreadLocal变量，那么访问这个变量的每个线程都会有这个变量的本地副本，这也是ThreadLocal变量名的由来。他们可以使用get()和set()方法来获取默认值或将其值更改为当前线程所存的副本的值，从而避免了线程安全问题。再举个简单的例子：两个人去宝屋收集宝物，这两个共用一个袋子的话肯定会产生争执，但是给他们两个人每个人分配一个袋子的话就不会出现这样的问题。如果把这两个人比作线程的话，那么ThreadLocal就是用来避免这两个线程竞争的。**ThreadLocal使用场景**:当需要存储线程私有变量的时候、当需要实现线程安全的变量时、当需要减少线程资源竞争的时候。\n\n### 如何使用ThreadLocal？\n\n```java\nimport java.text.SimpleDateFormat;\nimport java.util.Random;\n\npublic class ThreadLocalExample implements Runnable{\n\n     // SimpleDateFormat不是线程安全的，所以每个线程都要有自己独立的副本\n    private static final ThreadLocal<SimpleDateFormat> formatter = ThreadLocal.withInitial(() -> new SimpleDateFormat(\"yyyyMMdd HHmm\"));\n\n    public static void main(String[] args) throws InterruptedException {\n        ThreadLocalExample obj = new ThreadLocalExample();\n        for(int i=0 ; i<10; i++){\n            Thread t = new Thread(obj, \"\"+i);\n            Thread.sleep(new Random().nextInt(1000));\n            t.start();\n        }\n    }\n\n    @Override\n    public void run() {\n        System.out.println(\"Thread Name= \"+Thread.currentThread().getName()+\" default Formatter = \"+formatter.get().toPattern());\n        try {\n            Thread.sleep(new Random().nextInt(1000));\n        } catch (InterruptedException e) {\n            e.printStackTrace();\n        }\n        //formatter pattern is changed here by thread, but it won't reflect to other threads\n        formatter.set(new SimpleDateFormat());\n\n        System.out.println(\"Thread Name= \"+Thread.currentThread().getName()+\" formatter = \"+formatter.get().toPattern());\n    }\n\n}\n```\n\n输出结果:\n\n```text\nThread Name= 0 default Formatter = yyyyMMdd HHmm\nThread Name= 0 formatter = yy-M-d ah:mm\nThread Name= 1 default Formatter = yyyyMMdd HHmm\nThread Name= 2 default Formatter = yyyyMMdd HHmm\nThread Name= 1 formatter = yy-M-d ah:mm\nThread Name= 3 default Formatter = yyyyMMdd HHmm\nThread Name= 2 formatter = yy-M-d ah:mm\nThread Name= 4 default Formatter = yyyyMMdd HHmm\nThread Name= 3 formatter = yy-M-d ah:mm\nThread Name= 4 formatter = yy-M-d ah:mm\nThread Name= 5 default Formatter = yyyyMMdd HHmm\nThread Name= 5 formatter = yy-M-d ah:mm\nThread Name= 6 default Formatter = yyyyMMdd HHmm\nThread Name= 6 formatter = yy-M-d ah:mm\nThread Name= 7 default Formatter = yyyyMMdd HHmm\nThread Name= 7 formatter = yy-M-d ah:mm\nThread Name= 8 default Formatter = yyyyMMdd HHmm\nThread Name= 9 default Formatter = yyyyMMdd HHmm\nThread Name= 8 formatter = yy-M-d ah:mm\nThread Name= 9 formatter = yy-M-d ah:mm\n```\n\n从输出中可以看出，虽然Thread-0已经改变了formatter的值，但Thread-1默认格式化值与初始化值相同，其他线程也一样。上面有一段代码用到了创建ThreadLocal变量的那段代码用到了Java8的知识，它等于下面这段代码，如果你写了下面这段代码的话，IDEA会提示你转换为Java8的格式。因为ThreadLocal类在Java8中扩展，使用一个新的方法withInitial(),将Supplier功能接口作为参数。\n\n```java\nprivate static final ThreadLocal<SimpleDateFormat> formatter = new ThreadLocal<SimpleDateFormat>(){\n    @Override\n    protected SimpleDateFormat initialValue(){\n        return new SimpleDateFormat(\"yyyyMMdd HHmm\");\n    }\n};\n```\n\n### ThreadLocal原理了解吗？\n\n从Thread类源代码入手。\n\n```java\npublic class Thread implements Runnable {\n    //......\n    //与此线程有关的ThreadLocal值。由ThreadLocal类维护\n    ThreadLocal.ThreadLocalMap threadLocals = null;\n\n    //与此线程有关的InheritableThreadLocal值。由InheritableThreadLocal类维护\n    ThreadLocal.ThreadLocalMap inheritableThreadLocals = null;\n    //......\n}\n```\n\n从上面Thread类源代码可以看出Thread类中有一个threadLocals和一个inheritableThreadLocals变量，它们都是ThreadLocalMap类型的变量,我们可以把ThreadLocalMap理解为ThreadLocal类实现的定制化的HashMap。默认情况下这两个变量都是null，只有当前线程调用ThreadLocal类的set或get方法时才创建它们，实际上调用这两个方法的时候，我们调用的是ThreadLocalMap类对应的get()、set()方法。\n\nThreadLocal类的set()方法\n\n```java\npublic void set(T value) {\n    // 获取当前请求的线程\n    Thread t = Thread.currentThread();\n    // 取出Thread类内部的threadLocals变量(哈希表结构)\n    ThreadLocalMap map = getMap(t);\n    if (map != null)\n        // 将需要存储的值放入到这个哈希表中\n        map.set(this, value);\n    else\n        createMap(t, value);\n}\nThreadLocalMap getMap(Thread t) {\n    return t.threadLocals;\n}\n```\n\n通过上面这些内容，我们足以通过猜测得出结论：最终的变量是放在了当前线程的ThreadLocalMap中，并不是存在ThreadLocal上，ThreadLocal可以理解为只是ThreadLocalMap的封装，传递了变量值。ThrealLocal类中可以通过Thread.currentThread()获取到当前线程对象后，直接通过getMap(Thread t)可以访问到该线程的ThreadLocalMap对象。\n\n**每个Thread中都具备一个ThreadLocalMap，而ThreadLocalMap可以存储以ThreadLocal为key，Object对象为value的键值对**\n\n```java\nThreadLocalMap(ThreadLocal<?> firstKey, Object firstValue) {\n    //......\n}\n```\n\n比如我们在同一个线程中声明了两个ThreadLocal对象的话，Thread内部都是使用仅有的那个ThreadLocalMap存放数据的，ThreadLocalMap的key就是ThreadLocal对象，value就是ThreadLocal对象调用set方法设置的值。ThreadLocal数据结构如下图所示：\n\n![ThreadLocal数据结构](https://guide-blog-images.oss-cn-shenzhen.aliyuncs.com/github/javaguide/java/concurrent/threadlocal-data-structure.png)\n\nThreadLocalMap是ThreadLocal的静态内部类。\n\n![ThreadLocal内部类](https://guide-blog-images.oss-cn-shenzhen.aliyuncs.com/github/javaguide/java/concurrent/thread-local-inner-class.png)\n\n**总结ThreadLocal原理**：每个线程是一个Thread实例，其内部维护一个threadLocals的实例成员，其类型是ThreadLocal.ThreadLocalMap。通过实例化ThreadLocal实例，我们可以对当前运行的线程设置一些线程私有的变量，通过调用ThreadLocal的set和get方法存取。ThreadLocal本身并不是一个容器，我们存取的value实际上存储在ThreadLocalMap中，ThreadLocal只是作为TheadLocalMap的key。每个线程实例都对应一个TheadLocalMap实例，我们可以在同一个线程里实例化很多个ThreadLocal来存储很多种类型的值，这些ThreadLocal实例分别作为key，对应各自的value，最终存储在Entry table数组中。\n当调用ThreadLocal的set/get进行赋值/取值操作时，首先获取当前线程的ThreadLocalMap实例，然后就像操作一个普通的map一样，进行put和get。\n\n### ThreadLocal详解\n\n#### ThreadLocal代码演示\n\n我们先看下ThreadLocal使用示例：\n\n```java\npublic class ThreadLocalTest {\n    private List<String> messages = Lists.newArrayList();\n\n    public static final ThreadLocal<ThreadLocalTest> holder = ThreadLocal.withInitial(ThreadLocalTest::new);\n\n    public static void add(String message) {\n        holder.get().messages.add(message);\n    }\n\n    public static List<String> clear() {\n        List<String> messages = holder.get().messages;\n        holder.remove();\n\n        System.out.println(\"size: \" + holder.get().messages.size());\n        return messages;\n    }\n\n    public static void main(String[] args) {\n        ThreadLocalTest.add(\"一枝花算不算浪漫\");\n        System.out.println(holder.get().messages);\n        ThreadLocalTest.clear();\n    }\n}\n```\n\n打印结果：\n\n```java\n[一枝花算不算浪漫]\nsize: 0\n```\n\nThreadLocal对象可以提供线程局部变量，每个线程Thread拥有一份自己的副本变量，多个线程互不干扰。\n\n#### ThreadLocal的数据结构\n\nThread类有一个类型为ThreadLocal.ThreadLocalMap的实例变量threadLocals，也就是说每个线程有一个自己的ThreadLocalMap。ThreadLocalMap有自己的独立实现，可以简单地将它的key视作ThreadLocal，value为代码中放入的值（实际上key并不是ThreadLocal本身，而是它的一个**弱引用**）。每个线程在往ThreadLocal里放值的时候，都会往自己的ThreadLocalMap里存，读也是以ThreadLocal作为引用，在自己的map里找对应的key，从而实现了**线程隔离**。ThreadLocalMap有点类似HashMap的结构，只是HashMap是由**数组+链表**实现的，而ThreadLocalMap中并没有链表结构。我们还要注意Entry，它的key是ThreadLocal\\<?>k，继承自WeakReference，也就是我们常说的弱引用类型。\n\n#### GC之后key是否为null？\n\n回应开头的那个问题，ThreadLocal的key是弱引用，那么在ThreadLocal.get()的时候，发生GC之后，key是否是null？为了搞清楚这个问题，我们需要搞清楚Java的四种引用类型：\n\n- **强引用**：我们常常new出来的对象就是强引用类型，只要强引用存在，垃圾回收器将永远不会回收被引用的对象，哪怕内存不足的时候\n- **软引用**：使用SoftReference修饰的对象被称为软引用，软引用指向的对象在内存要溢出的时候被回收\n- **弱引用**：使用WeakReference修饰的对象被称为弱引用，只要发生垃圾回收，若这个对象只被弱引用指向，那么就会被回收\n- **虚引用**：虚引用是最弱的引用，在Java中使用PhantomReference进行定义。虚引用中唯一的作用就是用队列接收对象即将死亡的通知\n\n接着再来看下代码，我们使用反射的方式来看看GC后ThreadLocal中的数据情况：(下面代码来源自：https://blog.csdn.net/thewindkee/article/details/103726942 本地运行演示GC回收场景)\n\n```java\npublic class ThreadLocalDemo {\n\n    public static void main(String[] args) throws NoSuchFieldException, IllegalAccessException, InterruptedException {\n        Thread t = new Thread(()->test(\"abc\",false));\n        t.start();\n        t.join();\n        System.out.println(\"--gc后--\");\n        Thread t2 = new Thread(() -> test(\"def\", true));\n        t2.start();\n        t2.join();\n    }\n\n    private static void test(String s,boolean isGC)  {\n        try {\n            new ThreadLocal<>().set(s);\n            if (isGC) {\n                System.gc();\n            }\n            Thread t = Thread.currentThread();\n            Class<? extends Thread> clz = t.getClass();\n            Field field = clz.getDeclaredField(\"threadLocals\");\n            field.setAccessible(true);\n            Object ThreadLocalMap = field.get(t);\n            Class<?> tlmClass = ThreadLocalMap.getClass();\n            Field tableField = tlmClass.getDeclaredField(\"table\");\n            tableField.setAccessible(true);\n            Object[] arr = (Object[]) tableField.get(ThreadLocalMap);\n            for (Object o : arr) {\n                if (o != null) {\n                    Class<?> entryClass = o.getClass();\n                    Field valueField = entryClass.getDeclaredField(\"value\");\n                    Field referenceField = entryClass.getSuperclass().getSuperclass().getDeclaredField(\"referent\");\n                    valueField.setAccessible(true);\n                    referenceField.setAccessible(true);\n                    System.out.println(String.format(\"弱引用key:%s,值:%s\", referenceField.get(o), valueField.get(o)));\n                }\n            }\n        } catch (Exception e) {\n            e.printStackTrace();\n        }\n    }\n}\n```\n\n结果如下：\n\n\n```java\n弱引用key:java.lang.ThreadLocal@433619b6,值:abc\n弱引用key:java.lang.ThreadLocal@418a15e3,值:java.lang.ref.SoftReference@bf97a12\n--gc后--\n弱引用key:null,值:def\n```\n\n因为这里创建的ThreadLocal并没有指向任何值，也就是没有任何引用：\n\n```java\nnew ThreadLocal<>().set(s);\n```\n\n所以这里在GC之后，key就会被回收，我们看到上面debug中的referent=null。这个问题刚开始看，如果没有过多思考，弱引用，还有垃圾回收，那么肯定会觉得是null。其实是不对的，因为题目说的是在做ThreadLocal.get()操作，证明其实还是有强引用存在的，所以key并不为null。如果我们的强引用不存在的话，那么key就会被回收，也就是会出现我们value没被回收，key被回收，导致value永远存在，出现内存泄漏。\n\n#### ThreadLocal.set()方法源码详解\n\nThreadLocal中的set方法原理很简单，主要是判断ThreadLocalMap是否存在，然后使用ThreadLocal中的set方法进行数据处理。代码如下：\n\n\n```java\npublic void set(T value) {\n    Thread t = Thread.currentThread();\n    ThreadLocalMap map = getMap(t);\n    if (map != null)\n        map.set(this, value);\n    else\n        createMap(t, value);\n}\n\nvoid createMap(Thread t, T firstValue) {\n    t.threadLocals = new ThreadLocalMap(this, firstValue);\n}\n```\n\n主要的核心逻辑还是在ThreadLocalMap中的，一步步往下看，后面还有更详细的剖析。\n\n##### ThreadLocalMapHash算法\n\n既然是Map结构，那么ThreadLocalMap当然也要实现自己的hash算法来解决散列表数组冲突问题。\n\n```java\nint i = key.threadLocalHashCode & (len-1);\n```\n\nThreadLocalMap中hash算法很简单，这里i就是当前key在散列表中对应的数组下标位置。\n\n这里最关键的就是threadLocalHashCode值的计算，ThreadLocal中有一个属性为HASH_INCREMENT = 0x61c88647\n\n\n```java\npublic class ThreadLocal<T> {\n    private final int threadLocalHashCode = nextHashCode();\n\n    private static AtomicInteger nextHashCode = new AtomicInteger();\n\n    private static final int HASH_INCREMENT = 0x61c88647;\n\n    private static int nextHashCode() {\n        return nextHashCode.getAndAdd(HASH_INCREMENT);\n    }\n\n    static class ThreadLocalMap {\n        ThreadLocalMap(ThreadLocal<?> firstKey, Object firstValue) {\n            table = new Entry[INITIAL_CAPACITY];\n            int i = firstKey.threadLocalHashCode & (INITIAL_CAPACITY - 1);\n\n            table[i] = new Entry(firstKey, firstValue);\n            size = 1;\n            setThreshold(INITIAL_CAPACITY);\n        }\n    }\n}\n```\n\n每当创建一个ThreadLocal对象，这个ThreadLocal.nextHashCode这个值就会增长0x61c88647。这个值很特殊，它是斐波那契数也叫黄金分割数。hash增量为这个数字，带来的好处就是hash分布非常均匀。\n\n##### ThreadLocalMapHash冲突\n\n> **注明**：下面所有示例图中，绿色块Entry代表正常数据，灰色块代表Entry的key值为null，已被垃圾回收。白色块表示Entry为null。\n\n虽然ThreadLocalMap中使用了黄金分割数来作为hash计算因子，大大减少了Hash冲突的概率，但是仍然会存在冲突。HashMap中解决冲突的方法是在数组上构造一个链表结构，冲突的数据挂载到链表上，如果链表长度超过一定数量则会转化成红黑树。而ThreadLocalMap中并没有链表结构，所以这里不能使用HashMap解决冲突的方式了。如果我们插入一个value=27的数据，通过hash计算后应该落入槽位4中，而槽位4已经有了Entry数据。此时就会线性向后查找，一直找到Entry为null的槽位才会停止查找，将当前元素放入此槽位中。当然迭代过程中还有其他的情况，比如遇到了Entry不为null且key值相等的情况，还有Entry中的key值为null的情况等等都会有不同的处理，后面会一一详细讲解。这里还画了一个Entry中的key为null的数据（Entry=2的灰色块数据），因为key值是弱引用类型，所以会有这种数据存在。在set过程中，如果遇到了key过期的Entry数据，实际上是会进行一轮探测式清理操作的，具体操作方式后面会讲到。\n\n##### ThreadLocalMap.set()原理\n\n看完了ThreadLocalhash算法后，我们再来看set是如何实现的。往ThreadLocalMap中set数据（新增或者更新数据）分为好几种情况。\n\n**第一种情况**：通过hash计算后的槽位对应的Entry数据为空：这里直接将数据放到该槽位即可。\n\n**第二种情况**：槽位数据不为空，key值与当前ThreadLocal通过hash计算获取的key值一致：这里直接更新该槽位的数据。\n\n**第三种情况**：槽位数据不为空，往后遍历过程中，在找到Entry为null的槽位之前，没有遇到key过期的Entry，遍历散列数组，线性往后查找，如果找到Entry为null的槽位，则将数据放入该槽位中，或者往后遍历过程中，遇到了key值相等的数据，直接更新即可。\n\n**第四种情况**：槽位数据不为空，往后遍历过程中，在找到Entry为null的槽位之前，遇到key过期的Entry，如下图，往后遍历过程中，遇到了index=7的槽位数据Entry的key=null，散列数组下标为7位置对应的Entry数据key为null，表明此数据key值已经被垃圾回收掉了，此时就会执行replaceStaleEntry()方法，该方法含义是替换过期数据的逻辑，以index=7位起点开始遍历，进行探测式数据清理工作。初始化探测式清理过期数据扫描的开始位置：slotToExpunge=staleSlot=7。以当前staleSlot开始向前迭代查找，找其他过期的数据，然后更新过期数据起始扫描下标slotToExpunge。for循环迭代，直到碰到Entry为null结束。如果找到了过期的数据，继续向前迭代，直到遇到Entry=null的槽位才停止迭代，如下图所示，slotToExpunge被更新为0，以当前节点(index=7)向前迭代，检测是否有过期的Entry数据，如果有则更新slotToExpunge值。碰到null则结束探测。以上图为例slotToExpunge被更新为0。上面向前迭代的操作是为了更新探测清理过期数据的起始下标slotToExpunge的值，这个值在后面会讲解，它是用来判断当前过期槽位staleSlot之前是否还有过期元素。接着开始以staleSlot位置(index=7)向后迭代，如果找到了相同key值的Entry数据：从当前节点staleSlot向后查找key值相等的Entry元素，找到后更新Entry的值并交换staleSlot元素的位置(staleSlot位置为过期元素)，更新Entry数据，然后开始进行过期Entry的清理工作，如下图所示：\n\n![img](https://guide-blog-images.oss-cn-shenzhen.aliyuncs.com/java-guide-blog/view.png)向后遍历过程中，如果没有找到相同key值的Entry数据，从当前节点staleSlot向后查找key值相等的Entry元素，直到Entry为null则停止寻找。通过上图可知，此时table中没有key值相同的Entry。创建新的Entry，替换table[stableSlot]位置：替换完成后也是进行过期元素清理工作，清理工作主要是有两个方法：expungeStaleEntry()和cleanSomeSlots()，具体细节后面会讲到，请继续往后看。\n\n##### ThreadLocalMap.set()源码详解\n\n上面已经用图的方式解析了set()实现的原理，其实已经很清晰了，我们接着再看下源码：\n\njava.lang.ThreadLocal.ThreadLocalMap.set():\n\n```java\nprivate void set(ThreadLocal<?> key, Object value) {\n    Entry[] tab = table;\n    int len = tab.length;\n    int i = key.threadLocalHashCode & (len-1);\n\n    for (Entry e = tab[i];\n         e != null;\n         e = tab[i = nextIndex(i, len)]) {\n        ThreadLocal<?> k = e.get();\n\n        if (k == key) {\n            e.value = value;\n            return;\n        }\n\n        if (k == null) {\n            replaceStaleEntry(key, value, i);\n            return;\n        }\n    }\n\n    tab[i] = new Entry(key, value);\n    int sz = ++size;\n    if (!cleanSomeSlots(i, sz) && sz >= threshold)\n        rehash();\n}\n```\n\n这里会通过key来计算在散列表中的对应位置，然后以当前key对应的桶的位置向后查找，找到可以使用的桶。\n\n```java\nEntry[] tab = table;\nint len = tab.length;\nint i = key.threadLocalHashCode & (len-1);\n```\n\n什么情况下桶才是可以使用的呢？\n\n1. k = key说明是替换操作，可以使用\n2. 碰到一个过期的桶，执行替换逻辑，占用过期桶\n3. 查找过程中，碰到桶中Entry=null的情况，直接使用\n\n接着就是执行for循环遍历，向后查找，我们先看下nextIndex()、prevIndex()方法实现：\n\n```java\nprivate static int nextIndex(int i, int len) {\n    return ((i + 1 < len) ? i + 1 : 0);\n}\n\nprivate static int prevIndex(int i, int len) {\n    return ((i - 1 >= 0) ? i - 1 : len - 1);\n}\n```\n\n接着看剩下for循环中的逻辑：\n\n1. 遍历当前key值对应的桶中Entry数据为空，这说明散列数组这里没有数据冲突，跳出for循环，直接set数据到对应的桶中\n2. 如果key值对应的桶中Entry数据不为空\n    2.1 如果k = key，说明当前set操作是一个替换操作，做替换逻辑，直接返回\n    2.2 如果key = null，说明当前桶位置的Entry是过期数据，执行replaceStaleEntry()方法(核心方法)，然后返回\n3. for循环执行完毕，继续往下执行说明向后迭代的过程中遇到了entry为null的情况\n    3.1 在Entry为null的桶中创建一个新的Entry对象\n    3.2 执行++size操作\n4. 调用cleanSomeSlots()做一次启发式清理工作，清理散列数组中Entry的key过期的数据\n    4.1 如果清理工作完成后，未清理到任何数据，且size超过了阈值(数组长度的 2/3)，进行rehash()操作\n    4.2 rehash()中会先进行一轮探测式清理，清理过期key，清理完成后如果**size >= threshold - threshold / 4**，就会执行真正的扩容逻辑(扩容逻辑往后看)\n\n接着重点看下replaceStaleEntry()方法，replaceStaleEntry()方法提供替换过期数据的功能，我们可以对应上面第四种情况的原理图来再回顾下，具体代码如下：`java.lang.ThreadLocal.ThreadLocalMap.replaceStaleEntry()`:\n\n```java\nprivate void replaceStaleEntry(ThreadLocal<?> key, Object value,\n                                       int staleSlot) {\n    Entry[] tab = table;\n    int len = tab.length;\n    Entry e;\n\n    int slotToExpunge = staleSlot;\n    for (int i = prevIndex(staleSlot, len);\n         (e = tab[i]) != null;\n         i = prevIndex(i, len))\n\n        if (e.get() == null)\n            slotToExpunge = i;\n\n    for (int i = nextIndex(staleSlot, len);\n         (e = tab[i]) != null;\n         i = nextIndex(i, len)) {\n\n        ThreadLocal<?> k = e.get();\n\n        if (k == key) {\n            e.value = value;\n\n            tab[i] = tab[staleSlot];\n            tab[staleSlot] = e;\n\n            if (slotToExpunge == staleSlot)\n                slotToExpunge = i;\n            cleanSomeSlots(expungeStaleEntry(slotToExpunge), len);\n            return;\n        }\n\n        if (k == null && slotToExpunge == staleSlot)\n            slotToExpunge = i;\n    }\n\n    tab[staleSlot].value = null;\n    tab[staleSlot] = new Entry(key, value);\n\n    if (slotToExpunge != staleSlot)\n        cleanSomeSlots(expungeStaleEntry(slotToExpunge), len);\n}\n// slotToExpunge表示开始探测式清理过期数据的开始下标，默认从当前的staleSlot开始。以当前的staleSlot开始，向前迭代查找，找到没有过期的数据，for循环一直碰到Entry为null才会结束。如果向前找到了过期数据，更新探测清理过期数据的开始下标为i，即slotToExpunge=i\n```\n\n```java\nfor (int i = prevIndex(staleSlot, len);\n     (e = tab[i]) != null;\n     i = prevIndex(i, len)){\n\n    if (e.get() == null){\n        slotToExpunge = i;\n    }\n}\n```\n\n接着开始从staleSlot向后查找，也是碰到Entry为null的桶结束。如果迭代过程中，碰到k==key，这说明这里是替换逻辑，替换新数据并且交换当前staleSlot位置。如果slotToExpunge==staleSlot，这说明replaceStaleEntry()一开始向前查找过期数据时并未找到过期的Entry数据，接着向后查找过程中也未发现过期数据，修改开始探测式清理过期数据的下标为当前循环的index，即slotToExpunge=i。最后调用cleanSomeSlots(expungeStaleEntry(slotToExpunge),len);进行启发式过期数据清理。\n\n```java\nif (k == key) {\n    e.value = value;\n\n    tab[i] = tab[staleSlot];\n    tab[staleSlot] = e;\n\n    if (slotToExpunge == staleSlot)\n        slotToExpunge = i;\n\n    cleanSomeSlots(expungeStaleEntry(slotToExpunge), len);\n    return;\n}\n```\n\ncleanSomeSlots()和expungeStaleEntry()方法后面都会细讲，这两个是和清理相关的方法，一个是过期key相关Entry的启发式清理(Heuristicallyscan)，另一个是过期key相关Entry的探测式清理。如果k!=key则会接着往下走，k==null说明当前遍历的Entry是一个过期数据，slotToExpunge==staleSlot说明，一开始的向前查找数据并未找到过期的Entry。如果条件成立，则更新slotToExpunge为当前位置，这个前提是前驱节点扫描时未发现过期数据。\n\n```java\nif (k == null && slotToExpunge == staleSlot)\n    slotToExpunge = i;\n```\n\n往后迭代的过程中如果没有找到k == key的数据，且碰到Entry为null的数据，则结束当前的迭代操作。此时说明这里是一个添加的逻辑，将新的数据添加到table[staleSlot]对应的slot中。\n\n```java\ntab[staleSlot].value = null;\ntab[staleSlot] = new Entry(key, value);\n```\n\n最后判断除了staleSlot以外，还发现了其他过期的slot数据，就要开启清理数据的逻辑：\n\n```java\nif (slotToExpunge != staleSlot)\n    cleanSomeSlots(expungeStaleEntry(slotToExpunge), len);\n```\n\n#### ThreadLocalMap过期key的探测式清理流程\n\n上面我们有提及ThreadLocalMap的两种过期key数据清理方式：探测式清理和启发式清理。\n\n我们先讲下探测式清理，也就是expungeStaleEntry方法，遍历散列数组，从开始位置向后探测清理过期数据，将过期数据的Entry设置为null，沿途中碰到未过期的数据则将此数据rehash后重新在table数组中定位，如果定位的位置已经有了数据，则会将未过期的数据放到最靠近此位置的Entry=null的桶中，使rehash后的Entry数据距离正确的桶的位置更近一些。如果再有其他数据set到map中，就会触发探测式清理操作。如上图，执行探测式清理后，index=5的数据被清理掉，继续往后迭代，到index=7的元素时，经过rehash后发现该元素正确的index=4，而此位置已经有了数据，往后查找离index=4最近的Entry=null的节点(刚被探测式清理掉的数据：index=5)，找到后移动index=7的数据到index=5中，此时桶的位置离正确的位置index=4更近了。经过一轮探测式清理后，key过期的数据会被清理掉，没过期的数据经过rehash重定位后所处的桶位置理论上更接近i=key.hashCode & (tab.len - 1)的位置。这种优化会提高整个散列表查询性能。接着看下expungeStaleEntry()具体流程，我们假设expungeStaleEntry(3)来调用此方法，我们可以看到ThreadLocalMap中table的数据情况，接着执行清理操作：第一步是清空当前staleSlot位置的数据，index=3位置的Entry变成了null。然后接着往后探测，执行完第二步后，index=4的元素挪到index=3的槽位中。继续往后迭代检查，碰到正常数据，计算该数据位置是否偏移，如果被偏移，则重新计算slot位置，目的是让正常数据尽可能存放在正确位置或离正确位置更近的位置，在往后迭代的过程中碰到空的槽位，终止探测，这样一轮探测式清理工作就完成了，接着我们继续看看具体实现源代码：\n\n\n```java\nprivate int expungeStaleEntry(int staleSlot) {\n    Entry[] tab = table;\n    int len = tab.length;\n\n    tab[staleSlot].value = null;\n    tab[staleSlot] = null;\n    size--;\n\n    Entry e;\n    int i;\n    for (i = nextIndex(staleSlot, len);\n         (e = tab[i]) != null;\n         i = nextIndex(i, len)) {\n        ThreadLocal<?> k = e.get();\n        if (k == null) {\n            e.value = null;\n            tab[i] = null;\n            size--;\n        } else {\n            int h = k.threadLocalHashCode & (len - 1);\n            if (h != i) {\n                tab[i] = null;\n\n                while (tab[h] != null)\n                    h = nextIndex(h, len);\n                tab[h] = e;\n            }\n        }\n    }\n    return i;\n}\n```\n\n这里我们还是以staleSlot=3来做示例说明，首先是将tab[staleSlot]槽位的数据清空，然后设置size--接着以staleSlot位置往后迭代，如果遇到k==null的过期数据，也是清空该槽位数据，然后size--\n\n```java\nThreadLocal<?> k = e.get();\n\nif (k == null) {\n    e.value = null;\n    tab[i] = null;\n    size--;\n}\n```\n\n如果key没有过期，重新计算当前key的下标位置是不是当前槽位下标位置，如果不是，那么说明产生了hash冲突，此时以新计算出来正确的槽位位置往后迭代，找到最近一个可以存放entry的位置。\n\n```java\nint h = k.threadLocalHashCode & (len - 1);\nif (h != i) {\n    tab[i] = null;\n\n    while (tab[h] != null)\n        h = nextIndex(h, len);\n\n    tab[h] = e;\n}\n```\n\n这里是处理正常的产生Hash冲突的数据，经过迭代后，有过Hash冲突数据的Entry位置会更靠近正确位置，这样的话，查询的时候效率才会更高。\n\n##### ThreadLocalMap扩容机制\n\n在ThreadLocalMap.set()方法的最后，如果执行完启发式清理工作后，未清理到任何数据，且当前散列数组中Entry的数量已经达到了列表的扩容阈值(len*2/3)，就开始执行rehash()逻辑：\n\n```java\nif (!cleanSomeSlots(i, sz) && sz >= threshold)\n    rehash();\n```\n\n接着看下rehash()具体实现：\n\n```java\nprivate void rehash() {\n    expungeStaleEntries();\n\n    if (size >= threshold - threshold / 4)\n        resize();\n}\n\nprivate void expungeStaleEntries() {\n    Entry[] tab = table;\n    int len = tab.length;\n    for (int j = 0; j < len; j++) {\n        Entry e = tab[j];\n        if (e != null && e.get() == null)\n            expungeStaleEntry(j);\n    }\n}\n```\n\n这里首先是会进行探测式清理工作，从table的起始位置往后清理，上面有分析清理的详细流程。清理完成之后，table中可能有一些key为null的Entry数据被清理掉，所以此时通过判断size>=threshold-threshold/4也就是size>=threshold\\*3/4来决定是否扩容。我们还记得上面进行rehash()的阈值是size>=threshold，所以当面试官套路我们ThreadLocalMap扩容机制的时候我们一定要说清楚这两个步骤。接着看看具体的resize()方法，为了方便演示，我们以oldTab.len=8来举例：扩容后的tab的大小为oldLen\\*2，然后遍历老的散列表，重新计算hash位置，然后放到新的tab数组中，如果出现hash冲突则往后寻找最近的entry为null的槽位，遍历完成之后，oldTab中所有的entry数据都已经放入到新的tab中了。重新计算tab下次扩容的阈值，具体代码如下：\n\n```java\nprivate void resize() {\n    Entry[] oldTab = table;\n    int oldLen = oldTab.length;\n    int newLen = oldLen * 2;\n    Entry[] newTab = new Entry[newLen];\n    int count = 0;\n\n    for (int j = 0; j < oldLen; ++j) {\n        Entry e = oldTab[j];\n        if (e != null) {\n            ThreadLocal<?> k = e.get();\n            if (k == null) {\n                e.value = null;\n            } else {\n                int h = k.threadLocalHashCode & (newLen - 1);\n                while (newTab[h] != null)\n                    h = nextIndex(h, newLen);\n                newTab[h] = e;\n                count++;\n            }\n        }\n    }\n\n    setThreshold(newLen);\n    size = count;\n    table = newTab;\n}\n```\n\n##### ThreadLocalMap.get()详解\n\n上面已经看完了set()方法的源码，其中包括set数据、清理数据、优化数据桶的位置等操作，接着看看get()操作的原理。\n\n###### ThreadLocalMap.get()图解\n\n**第一种情况**:通过查找key值计算出散列表中slot位置，然后该slot位置中的Entry.key和查找的key一致，则直接返回。\n\n**第二种情况**:slot位置中的Entry.key和要查找的key不一致，我们以get(ThreadLocal1)为例，通过hash计算后，正确的slot位置应该是4，而index=4的槽位已经有了数据，且key值不等于ThreadLocal1，所以需要继续往后迭代查找。迭代到index=5的数据时，此时Entry.key=null，触发一次探测式数据回收操作，执行expungeStaleEntry()方法，执行完后，index 5,8的数据都会被回收，而index 6,7的数据都会前移。index 6,7前移之后，继续从index=5往后迭代，于是就在index=5找到了key值相等的Entry数据。\n\n###### ThreadLocalMap.get()源码详解\n\njava.lang.ThreadLocal.ThreadLocalMap.getEntry():\n\n```java\nprivate Entry getEntry(ThreadLocal<?> key) {\n    int i = key.threadLocalHashCode & (table.length - 1);\n    Entry e = table[i];\n    if (e != null && e.get() == key)\n        return e;\n    else\n        return getEntryAfterMiss(key, i, e);\n}\n\nprivate Entry getEntryAfterMiss(ThreadLocal<?> key, int i, Entry e) {\n    Entry[] tab = table;\n    int len = tab.length;\n\n    while (e != null) {\n        ThreadLocal<?> k = e.get();\n        if (k == key)\n            return e;\n        if (k == null)\n            expungeStaleEntry(i);\n        else\n            i = nextIndex(i, len);\n        e = tab[i];\n    }\n    return null;\n}\n```\n\n##### ThreadLocalMap过期key的启发式清理流程\n\n上面多次提及到ThreadLocalMap过期key的两种清理方式：探测式清理(expungeStaleEntry())、启发式清理(cleanSomeSlots())。探测式清理是以当前Entry往后清理，遇到值为null则结束清理，属于线性探测清理。而启发式清理被作者定义为：Heuristically scan some cells looking for stale entries.\n\n具体代码如下：\n\n```java\nprivate boolean cleanSomeSlots(int i, int n) {\n    boolean removed = false;\n    Entry[] tab = table;\n    int len = tab.length;\n    do {\n        i = nextIndex(i, len);\n        Entry e = tab[i];\n        if (e != null && e.get() == null) {\n            n = len;\n            removed = true;\n            i = expungeStaleEntry(i);\n        }\n    } while ( (n >>>= 1) != 0);\n    return removed;\n}\n```\n\n##### InheritableThreadLocal\n\n我们使用ThreadLocal的时候，在异步场景下是无法给子线程共享父线程中创建的线程副本数据的。为了解决这个问题，JDK中还有一个InheritableThreadLocal类，我们来看一个例子：\n\n```java\npublic class InheritableThreadLocalDemo {\n    public static void main(String[] args) {\n        ThreadLocal<String> ThreadLocal = new ThreadLocal<>();\n        ThreadLocal<String> inheritableThreadLocal = new InheritableThreadLocal<>();\n        ThreadLocal.set(\"父类数据:threadLocal\");\n        inheritableThreadLocal.set(\"父类数据:inheritableThreadLocal\");\n\n        new Thread(new Runnable() {\n            @Override\n            public void run() {\n                System.out.println(\"子线程获取父类ThreadLocal数据：\" + ThreadLocal.get());\n                System.out.println(\"子线程获取父类inheritableThreadLocal数据：\" + inheritableThreadLocal.get());\n            }\n        }).start();\n    }\n}\n```\n\n打印结果：\n\n\n```java\n子线程获取父类ThreadLocal数据：null\n子线程获取父类inheritableThreadLocal数据：父类数据:inheritableThreadLocal\n```\n\n实现原理是子线程是通过在父线程中通过调用new Thread()方法来创建子线程，Thread#init方法在Thread的构造方法中被调用。在init方法中拷贝父线程数据到子线程中：\n\n```java\nprivate void init(ThreadGroup g, Runnable target, String name,\n                      long stackSize, AccessControlContext acc,\n                      boolean inheritThreadLocals) {\n    if (name == null) {\n        throw new NullPointerException(\"name cannot be null\");\n    }\n\n    if (inheritThreadLocals && parent.inheritableThreadLocals != null)\n        this.inheritableThreadLocals =\n            ThreadLocal.createInheritedMap(parent.inheritableThreadLocals);\n    this.stackSize = stackSize;\n    tid = nextThreadID();\n}\n```\n\n但InheritableThreadLocal仍然有缺陷，一般我们做异步化处理都是使用的线程池，而InheritableThreadLocal是在new Thread中的init()方法给赋值的，而线程池是线程复用的逻辑，所以这里会存在问题。当然，有问题出现就会有解决问题的方案，阿里巴巴开源了一个TransmittableThreadLocal组件就可以解决这个问题，这里就不再延伸，感兴趣的可自行查阅资料。\n\n##### ThreadLocal项目中使用实战\n\n###### ThreadLocal使用场景\n\n我们现在项目中日志记录用的是ELK+Logstash，最后在Kibana中进行展示和检索。现在都是分布式系统统一对外提供服务，项目间调用的关系可以通过traceId来关联，但是不同项目之间如何传递traceId呢？这里我们使用org.slf4j.MDC来实现此功能，内部就是通过ThreadLocal来实现的，具体实现如下：当前端发送请求到服务A时，服务A会生成一个类似UUID的traceId字符串，将此字符串放入当前线程的ThreadLocal中，在调用服务B的时候，将traceId写入到请求的Header中，服务B在接收请求时会先判断请求的Header中是否有traceId，如果存在则写入自己线程的ThreadLocal中。\n\n###### Feign远程调用解决方案\n\n**服务发送请求：**\n\n\n```java\n@Component\n@Slf4j\npublic class FeignInvokeInterceptor implements RequestInterceptor {\n\n    @Override\n    public void apply(RequestTemplate template) {\n        String requestId = MDC.get(\"requestId\");\n        if (StringUtils.isNotBlank(requestId)) {\n            template.header(\"requestId\", requestId);\n        }\n    }\n}\n```\n\n**服务接收请求：**\n\n```java\n@Slf4j\n@Component\npublic class LogInterceptor extends HandlerInterceptorAdapter {\n\n    @Override\n    public void afterCompletion(HttpServletRequest arg0, HttpServletResponse arg1, Object arg2, Exception arg3) {\n        MDC.remove(\"requestId\");\n    }\n\n    @Override\n    public void postHandle(HttpServletRequest arg0, HttpServletResponse arg1, Object arg2, ModelAndView arg3) {\n    }\n\n    @Override\n    public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception {\n\n        String requestId = request.getHeader(BaseConstant.REQUEST_ID_KEY);\n        if (StringUtils.isBlank(requestId)) {\n            requestId = UUID.randomUUID().toString().replace(\"-\", \"\");\n        }\n        MDC.put(\"requestId\", requestId);\n        return true;\n    }\n}\n```\n\n###### 线程池异步调用，requestId传递\n\n因为MDC是基于ThreadLocal去实现的，异步过程中，子线程并没有办法获取到父线程ThreadLocal存储的数据，所以这里可以自定义线程池执行器，修改其中的run()方法：\n\n```java\npublic class MyThreadPoolTaskExecutor extends ThreadPoolTaskExecutor {\n\n    @Override\n    public void execute(Runnable runnable) {\n        Map<String, String> context = MDC.getCopyOfContextMap();\n        super.execute(() -> run(runnable, context));\n    }\n\n    @Override\n    private void run(Runnable runnable, Map<String, String> context) {\n        if (context != null) {\n            MDC.setContextMap(context);\n        }\n        try {\n            runnable.run();\n        } finally {\n            MDC.remove();\n        }\n    }\n}\n```\n\n###### 使用MQ发送消息给第三方系统\n\n在MQ发送的消息体中自定义属性requestId，接收方消费消息后，自己解析requestId使用即可\n\n> [原文链接](https://javaguide.cn/java/concurrent/threadlocal.html)\n\n\n### ThreadLocal内存泄露问题是怎么导致的？\n\nThreadLocalMap中使用的key为ThreadLocal的弱引用，而value是强引用。所以，如果ThreadLocal没有被外部强引用的情况下，在垃圾回收的时候，key会被清理掉，而value不会被清理掉。这样一来，ThreadLocalMap中就会出现key为null的Entry。假如我们不做任何措施的话，value永远无法被GC回收，这个时候就可能会产生内存泄露。ThreadLocalMap实现中已经考虑了这种情况，在调用set()、get()、remove()方法的时候，会清理掉key为null的记录。使用完ThreadLocal方法后最好手动调用remove()方法.\n\n```java\nstatic class Entry extends WeakReference<ThreadLocal<?>> {\n    /** The value associated with this ThreadLocal. */\n    Object value;\n\n    Entry(ThreadLocal<?> k, Object v) {\n        super(k);\n        value = v;\n    }\n}\n```\n\n**ThreadLocal为什么内存泄漏**\n\n因为ThreadLocal是基于ThreadLocalMap实现的，其中ThreadLocalMap的Entry继承了WeakReference，而Entry对象中的key使用了WeakReference封装，也就是说，Entry中的key是一个弱引用类型，对于弱引用来说，它只能存活到下次GC之前，如果此时一个线程调用了ThreadLocalMap的set设置变量，当前的ThreadLocalMap就会新增一条记录，但由于发生了一次垃圾回收，这样就会造成一个结果:key值被回收掉了，但是value值还在内存中，而且如果线程一直存在的话，那么它的value值就会一直存在,这样被垃圾回收掉的key就会一直存在一条引用链:Thread->ThreadLocalMap->Entry->Value:就是因为这条引用链的存在，就会导致如果Thread还在运行，那么Entry不会被回收，进而value也不会被回收掉，但是Entry里面的key值已经被回收掉了,这只是一个线程，如果再来一个线程，又来一个线程…多了之后就会造成内存泄漏\n\n> [详细解读ThreadLocal的内存泄露](https://mp.weixin.qq.com/s/gasR16pjlN3WfuFj9mQxdQ)\n> [ThreadLocal你怎么动不动就内存泄漏？](https://mp.weixin.qq.com/s/S0IwbXadRgZ86fFLSFObVQ)\n> [细数ThreadLocal三大坑，内存泄露仅是小儿科](https://mp.weixin.qq.com/s/P2eiSHcyf0xMkQmyhTAfhg)\n> [内存泄露的原因找到了，罪魁祸首居然是Java TheadLocal](https://mp.weixin.qq.com/s/0Hj4y5lO2Ha4483qluDJ0g)\n> [线上系统因为一个ThreadLocal直接内存飙升](https://mp.weixin.qq.com/s/CQA-7FG1txi1pzUgdCV6ig)\n> [ThreadLocal搭配线程池时为什么会造成内存泄漏](https://mp.weixin.qq.com/s/NaPyv6PWEFE0l5kD5PDHCA)\n> [为什么大家都说ThreadLocal存在内存泄漏的风险？](https://mp.weixin.qq.com/s/N2YBtHf1AutOJYwRaCrfRA)\n\n### ThreadLocalMap为什么使用弱引用而不是强引用？\n\n#### 强引用\n\n一直活着：类似“Object obj=new Object()”这类的引用，只要强引用还存在，垃圾收集器永远不会回收掉被引用的对象实例。\n\n#### 弱引用\n**弱引用介绍：**\n\n> 如果一个对象只具有弱引用，那就类似于可有可无的生活用品。弱引用与软引用的区别在于：只具有弱引用的对象拥有更短暂的生命周期。在垃圾回收器线程扫描它所管辖的内存区域的过程中，一旦发现了只具有弱引用的对象，不管当前内存空间足够与否，都会回收它的内存。不过，由于垃圾回收器是一个优先级很低的线程，因此不一定会很快发现那些只具有弱引用的对象。\n>\n> 弱引用可以和一个引用队列（ReferenceQueue）联合使用，如果弱引用所引用的对象被垃圾回收，Java虚拟机就会把这个弱引用加入到与之关联的引用队列中。\n\n回收就会死亡：被弱引用关联的对象实例只能生存到下一次垃圾收集发生之前。当垃圾收集器工作时，无论当前内存是否足够，都会回收掉只被弱引用关联的对象实例。在JDK1.2之后，提供了WeakReference类来实现弱引用。\n\n#### 软引用\n\n有一次活的机会：软引用关联着的对象，在系统将要发生内存溢出异常之前，将会把这些对象实例列进回收范围之中进行第二次回收。如果这次回收还没有足够的内存，才会抛出内存溢出异常。在JDK1.2之后，提供了SoftReference类来实现软引用。\n\n#### 虚引用\n\n也称为幽灵引用或者幻影引用，它是最弱的一种引用关系。一个对象实例是否有虚引用的存在，完全不会对其生存时间构成影响，也无法通过虚引用来取得一个对象实例。为一个对象设置虚引用关联的唯一目的就是能在这个对象实例被收集器回收时收到一个系统通知。在JDK1.2之后，提供了PhantomReference类来实现虚引用。\n\n#### 总结\n关于为什么ThreadLocalMap使用弱引用而不是强引用分两种情况讨论：\n\n1. key使用强引用\n引用ThreadLocal的对象被回收了，但是ThreadLocalMap还持有ThreadLocal的强引用，如果没有手动删除，ThreadLocal不会被回收，导致Entry内存泄漏。\n2. key使用弱引\n引用ThreadLocal的对象被回收了，由于ThreadLocalMap持有ThreadLocal的弱引用，即使没有手动删除，ThreadLocal也会被回收。value在下一次ThreadLocalMap调用set、get、remove的时候会被清除。\n\n比较两种情况，我们可以发现：由于ThreadLocalMap的生命周期跟Thread一样长，如果都没有手动删除对应key，都会导致内存泄漏，但是使用弱引用可以多一层保障：弱引用ThreadLocal被清理后key为null，对应的value在下一次ThreadLocalMap调用set、get、remove的时候可能会被清除。因此，ThreadLocal内存泄漏的根源是：由于ThreadLocalMap的生命周期跟Thread一样长，如果没有手动删除对应key就会导致内存泄漏，而不是因为弱引用。\n\n\n### 相关文章\n\n- [面试官：听说你看过ThreadLocal源码](https://mp.weixin.qq.com/s/7sR7okSS1_LGpUWudWtQNw)\n- [Java并发之ThreadLocal](https://mp.weixin.qq.com/s/ntjmEHIj_aINhNmtwhMecA)\n- [ThreadLocal夺命11连问](https://mp.weixin.qq.com/s/s6waqV8X7KPKip8zbOIDqQ)\n- [ThreadLocal父子线程之间该如何传递数据？](https://mp.weixin.qq.com/s/yZNBAtN9AFocQ2-FR9so6g)\n- [用这4招优雅的实现Spring Boot异步线程间数据传递](https://mp.weixin.qq.com/s/HmaGSW71lAI-9WlNlfuICw)\n","categories":["Java"]},{"title":"Java中的线程","slug":"Java中的线程","url":"/blog/posts/50718223d917/","content":"\n### 线程\n\n#### 创建线程三种方式\n\n1. 继承Thread类，重写run()方法。然后直接new这个对象的实例，再调用start()方法启动线程。其实本质上Thread是实现了Runnable接口的一个实例：`public class Thread implements Runnable`\n2. 实现Runnable接口，重写run()方法。然后调用new Thread（runnable）的方式创建一个线程，再调用start()方法启动线程。\n3. 实现Callable接口，重写call()方法。Callable是类似于Runnable的接口，是属于Executor框架中的功能类。具有返回值，并且可以对异常进行声明和抛出\n\n> [【图解】透彻Java线程状态转换](https://mp.weixin.qq.com/s/G-X82-Fp7zShTTnkWg1N5A)\n\n#### 线程相关方法\n\n##### yield()\n\n它让掉当前线程CPU的时间片，使正在运行中的线程重新变成就绪状态，并重新竞争CPU的调度权。它可能会获取到，也有可能被其他线程获取到。使当前线程从执行状态（运行状态）变为可执行态（就绪状态）。cpu会从众多的可执行态里选择。也就是说，当前也就是刚刚的那个线程还是有可能会被再次执行到的\n\n##### join()\n\n并行变串行，当前线程等待另一个调用join()方法的线程执行结束后再往下执行,哪个线程调用join()哪个线程优先执行（前提必须调用start()方法启动线程）\n\n##### setDaemon()\n\n设置是否为守护线程，线程分为用户线程和守护线程，当用户线程都退出时，无论当jvm里面的守护线程有没有执行完，jvm都会退出，使用setDaemon()必须在thread.start()之前，否则会抛出异常。守护线程服务于用户线程,当用户线程结束后守护线程也会结束,当所有线程都运行结束时，JVM退出，进程结束。例如有一种线程的目的就是无限循环\n\n```java\nclass TimerThread extends Thread {\n  @Override\n  public void run() {\n\twhile (true) {\n\t\tSystem.out.println(LocalTime.now());\n\t\ttry {\n\t\t\tThread.sleep(1000);\n\t\t} catch (InterruptedException e) {\n\t\t\tbreak\n\t\t}\n\t}\n  }\n}\n```\n如果这个线程不结束，JVM进程就无法结束。问题是，由谁负责结束这个线程？然而这类线程经常没有负责人来负责结束它们。但是，当其他线程结束时，JVM进程又必须要结束，怎么办？答案是将这个线程设置成守护线程（Daemon Thread）。守护线程是指为其他线程服务的线程。在JVM中，所有非守护线程都执行完毕后，无论有没有守护线程，虚拟机都会自动退出。因此，JVM退出时，不必关心守护线程是否已结束。在守护线程中，编写代码要注意：守护线程不能持有任何需要关闭的资源，例如打开文件等，因为虚拟机退出时，守护线程没有任何机会来关闭文件，这会导致数据丢失\n\n##### Thread.interrupted()\n\n检测当前线程是否被中断，并且中断状态会被清除（即重置为false）；它是静态方法，即使是线程对象去调用，底层使用的也是判断当前线程的中断状态，而不是被调用线程的中断状态。如果连续两次调用该方法，则第二次调用将返回false（在第一次调用已清除了其中断状态之后，且第二次调用检验完中断状态前，当前线程再次中断的情况除外）。\n\n- this.isInterrupted()\n检测调用该方法的线程是否被中断，中断状态不会被清除。线程一旦被中断，该方法返回true，而一旦sleep等方法抛出异常，它将清除中断状态，此时方法将返回false。\n\n- this.interrupt()\n\n中断调用该方法的线程,中断被阻塞的线程，会抛出一个InterruptedException，把线程从阻塞状态中解救出来，会清除中断标志位。如果当前线程没有中断它自己（这在任何情况下都是允许的），则该线程的checkAccess方法就会被调用，这可能抛出Security Exception。如果线程在调用Object类的wait()、wait(long)或wait(long,int)方法，或者该类的join()、join(long)、join(long,int)、sleep(long)或sleep(long,int)方法过程中受阻，则其中断状态将被清除，它还将收到一个Interrupted Exception。如果该线程在可中断的通道上的I/O操作中受阻，则该通道将被关闭，该线程的中断状态将被设置并且该线程将收到ClosedByInterrupt Exception。如果该线程在一个Selector中受阻，则该线程的中断状态将被设置，它将立即从选择操作返回，并可能带有一个非零值，就好像调用了选择器的wakeup方法一样。如果以前的条件都没有保存，则该线程的中断状态将被设置。中断一个不处于活动状态的线程不需要任何作用。\n\n> [如何停止一个正在运行的线程？](https://mp.weixin.qq.com/s/J8Acb1FBPhqb1Z7Vur0erQ)\n\n##### 捕获异常\n\n- Thread.setDefaultUncaughtExceptionHandler()\n相当于一个全局的捕获异常。用于记录当程序发生你未捕获的异常的时候,调用一个你默认的handler来进行某些操作\n\n- Thread.getDefaultUncaughtExceptionHandler()\n返回当线程由于未捕获的异常而突然终止时调用的默认处理程序。如果返回的值为null，则没有默认值\n\n- setUncaughtExceptionHandler\n用来获取线程中产生的异常,建议使用该方法为线程设置异常捕获方法，主线程无法捕获子线程异常，当子线程异常时，可以使用这个方法处理异常\n\n- getUncaughtExceptionHandler\n返回该线程由于未捕获的异常而突然终止时调用的处理程序。\n\n##### 线程的优先级\n\n- Thread.MAX_PRIORITY：10\n- Thread.MIN_PRIORITY：1\n- Thread.NORM_PRIORITY：5-->默认优先级\n- getPriority():获取线程的优先级\n- setPriority(int p):设置线程的优先级\n\n说明：⾼优先级的线程要抢占低优先级线程cpu的执⾏权。但是只是从概率上讲，⾼优先级的线程⾼概率的情况下被执⾏。并不意味着只当⾼优先级的线程执⾏完以后，低优先级的线程才执行\n\n##### checkAccess\n\n确定当前运行的线程是否具有修改此线程的权限\n\n##### countStackFrames\n\n计算此线程中的堆栈帧数，当前线程必须被挂起\n\n##### getThreadGroup()\n\n获取线程所在的线程组\n\n##### Thread.activeCount()\n\n返回当前线程的线程组中活动线程的数量。返回的值只是一个估计值，因为当此方法遍历内部数据结构时，线程数可能会动态更改\n\n##### Thread.dumpStack()\n\n打印当前线程的堆栈跟踪到标准错误流。此方法仅用于调试。\n\n##### Thread.enumerate(Thread[] tarray)\n\n用于将每个活动线程的线程组及其子组复制到指定的数组中。此方法使用tarray参数调用enumerate方法。此方法使用activeCount方法来估计数组应该有多大。如果数组的长度太短而无法容纳所有线程，则会以静默方式忽略额外的线程。tarray：此方法是要复制到的Thread对象数组。返回此方法返回放入数组的线程数。\n\n##### Thread.getAllStackTraces()\n\n返回所有活动线程的堆栈跟踪的一个映射\n\n##### Thread.holdsLock()\n\n当且仅当当前线程在指定的对象上保持监视器锁方法返回true\n\n> [多线程基础知识、线程相关方法](https://mp.weixin.qq.com/s?__biz=Mzg2MDYzODI5Nw==&mid=2247493938&idx=1&sn=125990919a15c7dd3c4ed4c36451d34b&source=41#wechat_redirect)\n\n\n#### 线程同步\n\n1. synchronized\n2. 使用特殊域变量(volatile)实现线程同步\n3. 使用重入锁实现线程同步（ReentrantLock）\n4. ThreadLocal与同步机制\n\n\n#### 8种保证线程安全的技术\n\n1. 无状态\n我们都知道只有多个线程访问公共资源的时候，才可能出现数据安全问题，那么如果我们没有公共资源，是不是就没有这个问题呢？\n2. 不可变（final）\n如果多个线程访问公共资源是不可变的，也不会出现数据的安全性问题\n3. 安全的发布（private）\n如果类中有公共资源，但是没有对外开放访问权限，即对外安全发布，也没有线程安全问题\n4. volatile\n如果有些公共资源只是一个开关，只要求可见性，不要求原子性，这样可以用volidate关键字定义来解决问题。\n5. synchronized\n使用JDK内部提供的同步机制，这也是使用比较多的手段，分为：方法同步和代码块同步，我们优先使用代码块同步，因为方法同步的范围更大，更消耗性能。每个对象内部都又一把锁，只有抢答那把锁的线程，才能进入代码块里，代码块执行完之后，会自动释放锁\n6. lock\n除了使用synchronized关键字实现同步功能之外，JDK还提供了lock显示锁的方式。它包含：可重入锁、读写锁等更多更强大的功能，有个小问题就是需要手动释放锁，不过在编码时提供了更多的灵活性\n7. cas\nJDK除了使用锁的机制解决多线程情况下数据安全问题之外，还提供了cas机制。这种机制是使用CPU中比较和交换指令的原子性，JDK里面是通过Unsafe类实现的。cas需要四个值：旧数据、期望数据、新数据和地址，比较旧数据和期望的数据如果一样的话，就把旧数据改成新数据，当前线程不断自旋，一直到成功为止。不过可能会出现aba问题，需要使用AtomicStampedReference增加版本号解决。其实，实际工作中很少直接使用Unsafe类的，一般用atomic包下面的类即可。\n8. threadlocal\n除了上面几种解决思路之外，JDK还提供了另外一种用空间换时间的新思路：threadlocal。它的核心思想是：共享变量在每个线程都有一个副本，每个线程操作的都是自己的副本，对另外的线程没有影响。特别注意，使用threadlocal时，使用完之后，要记得调用remove方法，不然可能会出现内存泄露问题\n\n#### 线程通信（例如：A线程操作到某一步通知B线程）\n\n1. thread.join(),\n2. object.wait(),object.notify()\n3. CountdownLatch\n4. 使用volatile关键字\n5. 使用ReentrantLock结合Condition\n6. LockSupport是一种非常灵活的实现线程间阻塞和唤醒的工具，使用它不用关注是等待线程先进行还是唤醒线程先运行，但是得知道线程的名字\n\n#### 最佳线程数\n\nQPS=每秒钟request数量\nTPS=每秒钟事务数量\nRT=一般取平均响应时间\nQPS=并发数/RT或者并发数=QPS * RT\n最佳线程数=RT/CPU Time * CPU核心数 * CPU利用率\n最大QPS=最佳线程数 * 单线程QPS=（RT/CPU Time * CPU核心数 * CPU利用率）\\*（1/RT) = CPU核心数 * CPU利用率/CPU time\n\n最佳线程经验值：\nIO密集型配置线程数经验值是：2N，其中N代表CPU核数。\nCPU密集型配置线程数经验值是：N + 1，其中N代表CPU核数。\n如果获取N的值？\n```java\nint availableProcessors = Runtime.getRuntime().availableProcessors()\n```\n最佳线程数目 = （线程等待时间+线程CPU时间）/线程CPU时间 * CPU数目\n数据库连接池连接数 = ((核心数 * 2) + 有效磁盘数)\n\n#### 线程顺序执行\n\n1. 使用线程的join方法\n2. 使用主线程的join方法\n3. 使用线程的wait方法\n4. 使用线程的线程池方法\n5. 使用线程的Condition(条件变量)方法\n6. 使用线程的CountDownLatch(倒计数)方法\n7. 使用线程的CyclicBarrier(回环栅栏)方法\n8. 使用线程的Semaphore(信号量)方法\n\n\n#### 线程相关文章\n- [Java多线程与并发高频面试题解析](https://mp.weixin.qq.com/s/DIbxSun2NcD-wbqKGQpRLg)\n- [超赞，大牛总结的多线程的问题及答案](https://mp.weixin.qq.com/s/0KmWOLNqhck85WECC9uQ-g)\n- [99道Java多线程面试题，看完我跪了！](https://mp.weixin.qq.com/s/dRqLZG7eev87hda9ohlJrA)\n- [2万字长文详解10大多线程面试题](https://mp.weixin.qq.com/s/hq5GbYBe98YsBDNA3u2s5Q)\n- [两万字！多线程硬核50问！](https://mp.weixin.qq.com/s/wGJsOWAGUhlE4QlZsNpMXg)\n- [面试官：线程池中多余的线程是如何回收的？](https://mp.weixin.qq.com/s/Ts2DGoUJ6SOhdRuDaLa8UQ)\n- [你真的了解Thread线程类吗](https://mp.weixin.qq.com/s/PNHueqqUhKPihiJm3Hqmvw)\n- [面试官提问：线程中的wait和notify方法有啥作用？](https://mp.weixin.qq.com/s/ZY65yfzxVaWn-WFNKy1v4A)\n\n### 线程池\n\n> ```java\n> ThreadPoolExecutor extends AbstractExecutorService,\n> AbstractExecutorService implements ExecutorService,\n> ExecutorService extends Executor\n> ```\n\n#### Executors\n\n```java\nExecutorService threadPool = Executors.newCachedThreadPool();\n\n// newCachedThreadPool:创建一个可缓存线程池，可以无限扩大，如果线程池长度超过处理需要，可灵活回收空闲线程，若无可回收，则新建线程。适用于服务器负载较轻，执行很多短期异步任务。\n// newFixedThreadPool:创建一个定长、固定大小的线程池，可控制线程最大并发数，超出的线程会在队列中等待，表示同一时刻只能有这么大的并发数，实际线程数量永远不会变化，适用于可以预测线程数量的业务中，或者服务器负载较重，对当前线程数量进行限制。\n// newScheduledThreadPool:创建一个定长线程池，支持定时及周期性任务执行。可以延时启动，定时启动，适用于需要多个后台线程执行周期任务的场景。\n// newSingleThreadExecutor:创建一个单线程化的线程池，它只会用唯一的工作线程来执行任务，保证所有任务按照指定顺序(FIFO,LIFO,优先级)执行。是一个单线程的线程池，适用于需要保证顺序执行各个任务，并且在任意时间点，不会有多个线程是活动的场景\n// newWorkStealingPool:创建一个拥有多个任务队列的线程池，可以减少连接数，创建当前可用cpu数量的线程来并行执行，适用于大耗时的操作，可以并行来执行\n// newSingleThreadScheduledExecutor:只有一个线程，该线程池可用于定时或周期性任务的执行，类似于Timer，但比Timer要更安全\n```\n\n\n#### 为什么阿里巴巴要禁用Executors创建线程池\n\n《阿里巴巴Java开发手册》中强制线程池不允许使用Executors去创建，而是通过ThreadPoolExecutor构造函数的方式，这样的处理方式让写的同学更加明确线程池的运行规则，规避资源耗尽的风险\n\nExecutors返回线程池对象的弊端如下：\n\n- **FixedThreadPool和SingleThreadExecutor**：使用的是无界的LinkedBlockingQueue，任务队列最大长度为Integer.MAX_VALUE,可能堆积大量的请求，从而导致OOM。\n- **CachedThreadPool**：使用的是同步队列SynchronousQueue,允许创建的线程数量为Integer.MAX_VALUE，可能会创建大量线程，从而导致OOM。\n- **ScheduledThreadPool和SingleThreadScheduledExecutor**：使用的无界的延迟阻塞队列DelayedWorkQueue，任务队列最大长度为Integer.MAX_VALUE,可能堆积大量的请求，从而导致OOM。\n\n```java\n// 无界队列LinkedBlockingQueue\npublic static ExecutorService newFixedThreadPool(int nThreads) {\n    return new ThreadPoolExecutor(nThreads, nThreads,0L, TimeUnit.MILLISECONDS,new LinkedBlockingQueue<Runnable>());\n}\n\n// 无界队列LinkedBlockingQueue\npublic static ExecutorService newSingleThreadExecutor() {\n    return new FinalizableDelegatedExecutorService (new ThreadPoolExecutor(1,1,0L,TimeUnit.MILLISECONDS,new LinkedBlockingQueue<Runnable>()));\n}\n\n// 同步队列SynchronousQueue，没有容量，最大线程数是Integer.MAX_VALUE\npublic static ExecutorService newCachedThreadPool() {\n    return new ThreadPoolExecutor(0, Integer.MAX_VALUE,60L, TimeUnit.SECONDS,new SynchronousQueue<Runnable>());\n}\n\n// DelayedWorkQueue（延迟阻塞队列）\npublic static ScheduledExecutorService newScheduledThreadPool(int corePoolSize) {\n    return new ScheduledThreadPoolExecutor(corePoolSize);\n}\n\npublic ScheduledThreadPoolExecutor(int corePoolSize) {\n    super(corePoolSize, Integer.MAX_VALUE, 0, NANOSECONDS,\n          new DelayedWorkQueue());\n}\n```\n\n\n#### ThreadPoolExecutor\n```java\nExecutorService threadPool = new ThreadPoolExecutor(\n    int corePoolSize,\n    int maximumPoolSize,\n    long keepAliveTime,\n    TimeUnit unit,\n    BlockingQueue<Runnable> workQueue,\n    ThreadFactory threadFactory,\n    RejectedExecutionHandler handler\n)\n\n// corePoolSize：线程池的核心线程数(最小线程数)，不管它们创建以后是不是空闲的。线程池需要保持corePoolSize数量的线程，除非设置了allowCoreThreadTimeOut\n\n// maximumPoolSize：线程池的最大线程数；\n\n// keepAliveTime：线程池空闲时线程的存活时长；如果经过keepAliveTime时间后，超过核心线程数的线程还没有接受到新的任务，那就销毁，超出线程池核心线程数小于线程池最大线程数的线程都是借的，没有用了,超时就销毁\n\n// unit：keepAliveTime时长单位；\n\n// workQueue：当提交的任务数超过核心线程数大小后，再提交的任务就存放在这里。它仅仅用来存放被execute方法提交的Runnable任务。存放任务的队列，上面提到的线程数超过corePoolSize存放任务的地方；\n// new ArrayBlockingQueue<Runnable>(10)：是一个基于数组结构的有界阻塞队列，此队列按FIFO（先进先出）原则对元素进行排序。\n// new LinkedBlockingQueue<Runnable>(10)：一个基于链表结构的阻塞队列，此队列按FIFO（先进先出）排序元素，也可以不传参数，默认是Integer.MAX_VALUE\n// new SynchronousQueue<Runnable>()：一个不存储元素的阻塞队列。每个插入操作必须等到另一个线程调用移除操作，否则插入操作一直处于阻塞状态，使用SynchronousQueue阻塞队列一般要求maximumPoolSizes为无界(Integer.MAX_VALUE)，避免线程拒绝执行操作。\n// PriorityBlockingQueue：一个具有优先级的无限阻塞队列。\n// DelayQueue:DelayQueue中的元素只有当其指定的延迟时间到了，才能够从队列中获取到该元素。DelayQueue是一个没有大小限制的队列，因此往队列中插入数据的操作（生产者）永远不会被阻塞，而只有获取数据的操作（消费者）才会被阻塞。\n\n// threadFactory：线程工厂，可以自己重写一下，为每个线程赋予一个名字，便于排查问题\nclass MyThreadFactory implements ThreadFactory{\n\t@Override\n\tpublic Thread newThread(Runnable r){\n\t\treturn new Thread(r,\"thread_name\");\n\t}\n}\n\n// handler：当队列里面放满了任务、最大线程数的线程都在工作时，这时继续提交的任务线程池就处理不了，应该执行怎么样的拒绝策略。\n//在队列（workQueue）和线程池达到最大线程数（maximumPoolSize）均满时仍有任务的情况下的处理方式即当任务数大于最大线程数并且队列已满时，采用的拒绝策略，分4种，\nnew ThreadPoolExecutor.AbortPolicy //丢弃任务并抛出RejectedExecutionException异常\n// AbortPolicy策略：默认策略，如果线程池队列满了丢掉这个任务并且抛出RejectedExecutionException异常。\nnew ThreadPoolExecutor.DiscardPolicy //丢弃任务，但是不抛出异常\n// DiscardPolicy策略：如果线程池队列满了，会直接丢掉这个任务并且不会有任何异常。\nnew ThreadPoolExecutor.CallerRunsPolicy//（调用者运行）:如果线程池的线程数量达到上限，该策略会把任务队列中的任务放在调用者线程当中运行由调用线程处理该任务\n// CallerRunsPolicy策略：如果添加到线程池失败，那么主线程会自己去执行该任务，不会等待线程池中的线程去执行。\nnew ThreadPoolExecutor.DiscardOldestPolicy //抛弃最旧的丢弃队列最前面的任务，然后重新尝试执行任务（重复此过程）\n// DiscardOldestPolicy策略：如果队列满了，会将最早进入队列的任务删掉腾出空间，再尝试加入队列。\n\n// demo\nThreadPoolExecutor exec = new ThreadPoolExecutor(\n10, 30, 300, TimeUnit.SECONDS, \nnew ArrayBlockingQueue<Runnable>(10000),\nExecutors.defaultThreadFactory(), \nnew ThreadPoolExecutor.CallerRunsPolicy());\n\n// 核心线程数10，最大线程数30，keepAliveTime是3秒,随着任务数量不断上升，线程池会不断的创建线程，直到到达核心线程数10，就不创建线程了，这时多余的任务通过加入阻塞队列来运行，当超出阻塞队列长度+核心线程数时，这时不得不扩大线程个数来满足当前任务的运行，这时就需要创建新的线程了（最大线程数起作用），上限是最大线程数30那么超出核心线程数10并小于最大线程数30的可能新创建的这20个线程相当于是“借”的，如果这20个线程空闲时间超过keepAliveTime，就会被退出\n\n```\n**ArrayBlockingQueue、LinkedBlockingQueue区别**\n![](/images/ArrayBlockingQueueLinkedBlockingQueue区别.png)\n\n> [ThreadPoolExecutor深入解析](https://mp.weixin.qq.com/s/QEur_4cwOSc_4AHWAPFhJQ)\n\n#### submit与execute区别\n\n1. submit在执行过程中与execute不一样，submit不会抛出异常而是把异常保存在成员变量中，在FutureTask.get阻塞获取的时候再把异常抛出来。\n2. submit有返回值Future，execute无返回值\n3. execute会抛出异常，sumbit方法不会抛出异常。除非你调用Future.get()。execute直接抛出异常之后线程就死掉了，submit保存异常线程没有死掉，因此execute的线程池可能会出现没有意义的情况，因为线程没有得到重用。而submit不会出现这种情况。\n```java\n// ①execute方法\nthreadPool.execute(new Runnable() {\n  public void run() {\n   try {\n\t\tSystem.out.println(index);Thread.sleep(2000);\n   } catch (InterruptedException e){\n\t\te.printStackTrace();\n   }\n  }\n});\n\n// ②submit方法\nFuture<String> f = threadPool.submit(new Callable<String>(){\n    @Override\n    public String call(){\n\t\treturn \"e\";\n    }\n}); \n\nString str = f.get();\nFutureTask<String> futureTask = new FutureTask<String>(new Callable(){\n    @Override\n    public String call(){\n    \treturn \"e\";\n    }\n});\n\nnew Thread(futureTask).start();// executor.submit(futureTask);\nString result = futureTask.get(2000,TimeUnit.MILLISECONDS)// 如果在指定时间内，还没获取到结果，就直接返回null\n```\n> [线程池中的线程抛出了异常，该如何处理](https://mp.weixin.qq.com/s/LVAv7PKVvg7prW9ayD1eZA)\n\n\n### 虚拟线程\n\n> JDK 19新推出的虚拟线程，或者叫协程，主要是为了解决在读书操作系统中线程需要依赖内核线程的实现，导致有很多额外开销的问题。通过在Java语言层面引入虚拟线程，通过JVM进行调度管理，从而减少上下文切换的成本。\n虚拟线程是守护线程，所以有可能会没等他执行完虚拟机就会shutdown掉。\n\n#### 虚拟线程与平台线程的区别\n1. **虚拟线程总是守护线程**。setDaemon(false)方法不能将虚拟线程更改为非守护线程。所以，需要注意的是，**当所有启动的非守护进程线程都终止时，JVM将终止。这意味着JVM不会等待虚拟线程完成后才退出**。\n2. 即使使用setPriority()方法，**虚拟线程始终具有normal的优先级**，且不能更改优先级。在虚拟线程上调用此方法没有效果。\n3. 虚拟线程是不支持stop()、suspend()或resume()等方法。这些方法在虚拟线程上调用时会抛出UnsupportedOperationException异常。\n\n#### 如何使用虚拟线程\n\n首先，通过Thread.startVirtualThread()可以运行一个虚拟线程：\n```java\nThread.startVirtualThread(() -> {\n    System.out.println(\"虚拟线程执行中...\");\n});\n```\n其次，通过Thread.Builder也可以创建虚拟线程，Thread类提供了ofPlatform()来创建一个平台线程、ofVirtual()来创建虚拟现场。\n```java\nThread.Builder platformBuilder = Thread.ofPlatform().name(\"平台线程\");\nThread.Builder virtualBuilder = Thread.ofVirtual().name(\"虚拟线程\");\n\nThread t1 = platformBuilder .start(() -> {...});\nThread t2 = virtualBuilder.start(() -> {...});\n```\n\n另外，线程池也支持了虚拟线程，可以通过Executors.newVirtualThreadPerTaskExecutor()来创建虚拟线程：\n```java\ntry (var executor = Executors.newVirtualThreadPerTaskExecutor()) {\n    IntStream.range(0, 10000).forEach(i -> {\n        executor.submit(() -> {\n            Thread.sleep(Duration.ofSeconds(1));\n            return i;\n        });\n    });\n}\n```\n但是，其实并不建议虚拟线程和线程池一起使用，因为Java线程池的设计是为了避免创建新的操作系统线程的开销，但是创建虚拟线程的开销并不大，所以其实没必要放到线程池中。\n\n#### 相关文章\n- [科技与狠活？JDK19中的虚拟线程到底什么鬼？](https://mp.weixin.qq.com/s/1AuTVrBJmONKEku403BHhQ)\n- [虚拟线程将会深刻影响大规模Java应用的并发机制](https://mp.weixin.qq.com/s/UkZjAcsYWncFBHc8DhnGcA)\n\n","categories":["Java"]},{"title":"ArrayList&CopyOnWriteArrayList","slug":"ArrayList&CopyOnWriteArrayList","url":"/blog/posts/289bdc29a0ae/","content":"\n## ArrayList\n\nArrayList的底层实现是一个Object数组,ArrayList的无参构造函数为底层的Object数组也就是elementData赋值了一个默认的空数组DEFAULTCAPACITY_EMPTY_ELEMENTDATA。也就是说，使用无参构造函数初始化ArrayList后，它当时的数组容量为0,只有当我们真正对数据进行添加操作add时，才会给数组分配一个默认的初始容量DEFAULT_CAPACITY = 10,ArrayList的有参构造函数就是按照用户传入的大小开辟数组空间。\n\nArrayList的底层是数组队列，相当于动态数组。与Java中的数组相比，它的容量能动态增长。在添加大量元素前，应用程序可以使用ensureCapacity操作来增加ArrayList实例的容量。这可以减少递增式再分配的数量。\n\nArrayList继承于AbstractList,实现了**List**,**RandomAccess**,**Cloneable**,**java.io.Serializable**这些接口。\n\n```java\npublic class ArrayList<E> extends AbstractList<E> implements List<E>, RandomAccess, Cloneable, java.io.Serializable{\n}\n```\n\n- **RandomAccess**是一个标志接口，表明实现这个接口的List集合是支持快速随机访问的。在ArrayList中，我们即可以通过元素的序号快速获取元素对象，这就是快速随机访问。\n- ArrayList实现了**Cloneable接口**，即覆盖了函数clone()，能被克隆。\n- ArrayList实现了**java.io.Serializable**接口，这意味着ArrayList支持序列化，能通过序列化去传输。\n\n### Arraylist和Vector的区别\n\n1. ArrayList是List的主要实现类，底层使用Object[]存储，适用于频繁的查找工作，线程不安全。\n2. Vector是List的古老实现类，底层使用Object[]存储，线程安全的。\n\n### Arraylist与LinkedList区别\n\n(1) **是否保证线程安全**：ArrayList和LinkedList都是不同步的，也就是ArrayList和LinkedList都不是线程安全的，以add为例,源码如下\n```java\nelementData[size++] = e;\n// 它由两步操作构成\nelementData[size] = e;\nsize = size + 1;\n```\n在单线程执行这两条代码时，那当然没有任何问题，但是当多线程环境下执行时，可能就会发生一个线程添加的值覆盖另一个线程添加的值。举个例子：假设size = 0，我们要往这个数组的末尾添加元素，线程A开始添加一个元素，值为A。此时它执行第一条操作，将A放在了数组elementData下标为0的位置上，接着线程B刚好也要开始添加一个值为B的元素，且走到了第一步操作。此时线程B获取到的size值依然为0，于是它将B也放在了elementData下标为0的位置上，线程A开始增加size的值，size = 1,线程B开始增加size的值，size = 2,这样，线程A、B 都执行完毕后，理想的情况应该是size = 2，elementData[0] = A，elementData[1] = B。而实际情况变成了size = 2，elementData[0] = B（线程B覆盖了线程A的操作），下标1的位置上什么都没有。并且后续除非我们使用set方法修改下标为1的值，否则这个位置上将一直为null，因为在末尾添加元素时将会从size = 2的位置上开始。\n\n(2) **底层数据结构**：Arraylist底层使用的是Object数组；LinkedList底层使用的是双向链表数据结构（JDK1.6之前为循环链表，JDK1.7取消了循环。注意双向链表和双向循环链表的区别）\n\n(3) **插入和删除是否受元素位置的影响**：\n\n   ①ArrayList采用数组存储，所以插入和删除元素的时间复杂度受元素位置的影响。比如：执行`add(E e)`方法的时候，ArrayList会默认在将指定的元素追加到此列表的末尾，这种情况时间复杂度就是O(1)。但是如果要在指定位置i插入和删除元素的话（add(int index, E element)）时间复杂度就为O(n-i)。因为在进行上述操作的时候集合中第i和第i个元素之后的(n-i)个元素都要执行向后位/向前移一位的操作。\n\n   ②LinkedList采用链表存储，所以对于`add(E e)`方法的插入，删除元素时间复杂度不受元素位置的影响，近似O(1)，如果是要在指定位置i插入和删除元素的话（(add(int index, E element)）时间复杂度近似为o(n))因为需要先移动到指定位置再插入。\n\n(4) **是否支持快速随机访问**：LinkedList不支持高效的随机元素访问，而ArrayList支持。快速随机访问就是通过元素的序号快速获取元素对象(对应于get(int index)方法)。\n\n(5) **内存空间占用**：ArrayList的空间浪费主要体现在在list列表的结尾会预留一定的容量空间，而LinkedList的空间花费则体现在它的每一个元素都需要消耗比ArrayList更多的空间（因为要存放直接后继和直接前驱以及数据）。\n\n### ArrayList核心源码解读\n\n```java\npackage java.util;\n\nimport java.util.function.Consumer;\nimport java.util.function.Predicate;\nimport java.util.function.UnaryOperator;\n\npublic class ArrayList<E> extends AbstractList<E>\n        implements List<E>, RandomAccess, Cloneable, java.io.Serializable{\n    \n    private static final long serialVersionUID = 8683452581122892189L;\n    /**\n     * 默认初始容量大小\n     */\n    private static final int DEFAULT_CAPACITY = 10;\n    /**\n     * 空数组（用于空实例）。\n     */\n    private static final Object[] EMPTY_ELEMENTDATA = {};\n    \n     //用于默认大小空实例的共享空数组实例。\n     //我们把它从EMPTY_ELEMENTDATA数组中区分出来，以知道在添加第一个元素时容量需要增加多少。\n    private static final Object[] DEFAULTCAPACITY_EMPTY_ELEMENTDATA = {};\n\n    /**\n     * 保存ArrayList数据的数组\n     */\n    transient Object[] elementData; // non-private to simplify nested class access\n\n    /**\n     * ArrayList所包含的元素个数\n     */\n    private int size;\n\n    /**\n     * 带初始容量参数的构造函数（用户可以在创建ArrayList对象时自己指定集合的初始大小）\n     */\n    public ArrayList(int initialCapacity) {\n        if (initialCapacity > 0) {\n            //如果传入的参数大于0，创建initialCapacity大小的数组\n            this.elementData = new Object[initialCapacity];\n        } else if (initialCapacity == 0) {\n            //如果传入的参数等于0，创建空数组\n            this.elementData = EMPTY_ELEMENTDATA;\n        } else {\n            //其他情况，抛出异常\n            throw new IllegalArgumentException(\"Illegal Capacity: \"+\n                                               initialCapacity);\n        }\n    }\n\n    /**\n     *默认无参构造函数\n     *DEFAULTCAPACITY_EMPTY_ELEMENTDATA为0.初始化为10，也就是说初始其实是空数组,当添加第一个元素的时候数组容量才变成10\n     */\n    public ArrayList() {\n        this.elementData = DEFAULTCAPACITY_EMPTY_ELEMENTDATA;\n    }\n\n    /**\n     * 构造一个包含指定集合的元素的列表，按照它们由集合的迭代器返回的顺序。\n     */\n    public ArrayList(Collection<? extends E> c) {\n        //将指定集合转换为数组\n        elementData = c.toArray();\n        //如果elementData数组的长度不为0\n        if ((size = elementData.length) != 0) {\n            // 如果elementData不是Object类型数据（c.toArray可能返回的不是Object类型的数组所以加上下面的语句用于判断）\n            if (elementData.getClass() != Object[].class)\n                //将原来不是Object类型的elementData数组的内容，赋值给新的Object类型的elementData数组\n                elementData = Arrays.copyOf(elementData, size, Object[].class);\n        } else {\n            // 其他情况，用空数组代替\n            this.elementData = EMPTY_ELEMENTDATA;\n        }\n    }\n\n    /**\n     * 修改这个ArrayList实例的容量是列表的当前大小。应用程序可以使用此操作来最小化ArrayList实例的存储。\n     */\n    public void trimToSize() {\n        modCount++;\n        if (size < elementData.length) {\n            elementData = (size == 0)\n              ? EMPTY_ELEMENTDATA\n              : Arrays.copyOf(elementData, size);\n        }\n    }\n    \n    //下面是ArrayList的扩容机制\n    //ArrayList的扩容机制提高了性能，如果每次只扩充一个，\n    //那么频繁的插入会导致频繁的拷贝，降低性能，而ArrayList的扩容机制避免了这种情况。\n    /**\n     * 如有必要，增加此ArrayList实例的容量，以确保它至少能容纳元素的数量\n     * @param   minCapacity   所需的最小容量\n     */\n    public void ensureCapacity(int minCapacity) {\n        //如果是true，minExpand的值为0，如果是false,minExpand的值为10\n        int minExpand = (elementData != DEFAULTCAPACITY_EMPTY_ELEMENTDATA)\n            // any size if not default element table\n            ? 0\n            // larger than default for default empty table. It's already\n            // supposed to be at default size.\n            : DEFAULT_CAPACITY;\n        //如果最小容量大于已有的最大容量\n        if (minCapacity > minExpand) {\n            ensureExplicitCapacity(minCapacity);\n        }\n    }\n   //1.得到最小扩容量\n   //2.通过最小容量扩容\n    private void ensureCapacityInternal(int minCapacity) {\n        if (elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA) {\n              // 获取“默认的容量”和“传入参数”两者之间的最大值\n            minCapacity = Math.max(DEFAULT_CAPACITY, minCapacity);\n        }\n\n        ensureExplicitCapacity(minCapacity);\n    }\n  //判断是否需要扩容\n    private void ensureExplicitCapacity(int minCapacity) {\n        modCount++;\n\n        // overflow-conscious code\n        if (minCapacity - elementData.length > 0)\n            //调用grow方法进行扩容，调用此方法代表已经开始扩容了\n            grow(minCapacity);\n    }\n\n    /**\n     * 要分配的最大数组大小\n     */\n    private static final int MAX_ARRAY_SIZE = Integer.MAX_VALUE - 8;\n\n    /**\n     * ArrayList扩容的核心方法。\n     */\n    private void grow(int minCapacity) {\n        // oldCapacity为旧容量，newCapacity为新容量\n        int oldCapacity = elementData.length;\n        //将oldCapacity右移一位，其效果相当于oldCapacity/2，\n        //我们知道位运算的速度远远快于整除运算，整句运算式的结果就是将新容量更新为旧容量的1.5倍，\n        int newCapacity = oldCapacity + (oldCapacity >> 1);\n        //然后检查新容量是否大于最小需要容量，若还是小于最小需要容量，那么就把最小需要容量当作数组的新容量，\n        if (newCapacity - minCapacity < 0)\n            newCapacity = minCapacity;\n        //再检查新容量是否超出了ArrayList所定义的最大容量，\n        //若超出了，则调用hugeCapacity()来比较minCapacity和MAX_ARRAY_SIZE，\n        //如果minCapacity大于MAX_ARRAY_SIZE，则新容量则为Interger.MAX_VALUE，否则，新容量大小则为MAX_ARRAY_SIZE。\n        if (newCapacity - MAX_ARRAY_SIZE > 0)\n            newCapacity = hugeCapacity(minCapacity);\n        // minCapacity is usually close to size, so this is a win:\n        elementData = Arrays.copyOf(elementData, newCapacity);\n    }\n    //比较minCapacity和MAX_ARRAY_SIZE\n    private static int hugeCapacity(int minCapacity) {\n        if (minCapacity < 0) // overflow\n            throw new OutOfMemoryError();\n        return (minCapacity > MAX_ARRAY_SIZE) ?\n            Integer.MAX_VALUE :\n            MAX_ARRAY_SIZE;\n    }\n\n    /**\n     *返回此列表中的元素数。\n     */\n    public int size() {\n        return size;\n    }\n\n    /**\n     * 如果此列表不包含元素，则返回true。\n     */\n    public boolean isEmpty() {\n        //注意=和==的区别\n        return size == 0;\n    }\n\n    /**\n     * 如果此列表包含指定的元素，则返回true。\n     */\n    public boolean contains(Object o) {\n        //indexOf()方法：返回此列表中指定元素的首次出现的索引，如果此列表不包含此元素，则为-1\n        return indexOf(o) >= 0;\n    }\n\n    /**\n     *返回此列表中指定元素的首次出现的索引，如果此列表不包含此元素，则为-1\n     */\n    public int indexOf(Object o) {\n        if (o == null) {\n            for (int i = 0; i < size; i++)\n                if (elementData[i]==null)\n                    return i;\n        } else {\n            for (int i = 0; i < size; i++)\n                //equals()方法比较\n                if (o.equals(elementData[i]))\n                    return i;\n        }\n        return -1;\n    }\n\n    /**\n     * 返回此列表中指定元素的最后一次出现的索引，如果此列表不包含元素，则返回-1。.\n     */\n    public int lastIndexOf(Object o) {\n        if (o == null) {\n            for (int i = size-1; i >= 0; i--)\n                if (elementData[i]==null)\n                    return i;\n        } else {\n            for (int i = size-1; i >= 0; i--)\n                if (o.equals(elementData[i]))\n                    return i;\n        }\n        return -1;\n    }\n\n    /**\n     * 返回此ArrayList实例的浅拷贝。（元素本身不被复制。）\n     */\n    public Object clone() {\n        try {\n            ArrayList<?> v = (ArrayList<?>) super.clone();\n            //Arrays.copyOf功能是实现数组的复制，返回复制后的数组。参数是被复制的数组和复制的长度\n            v.elementData = Arrays.copyOf(elementData, size);\n            v.modCount = 0;\n            return v;\n        } catch (CloneNotSupportedException e) {\n            // 这不应该发生，因为我们是可以克隆的\n            throw new InternalError(e);\n        }\n    }\n\n    /**\n     * 以正确的顺序（从第一个到最后一个元素）返回一个包含此列表中所有元素的数组。\n     * 返回的数组将是“安全的”，因为该列表不保留对它的引用。（换句话说，这个方法必须分配一个新的数组）。\n     * 因此，调用者可以自由地修改返回的数组。此方法充当基于阵列和基于集合的API之间的桥梁。\n     */\n    public Object[] toArray() {\n        return Arrays.copyOf(elementData, size);\n    }\n\n    /**\n     * 以正确的顺序返回一个包含此列表中所有元素的数组（从第一个到最后一个元素）;\n     * 返回的数组的运行时类型是指定数组的运行时类型。如果列表适合指定的数组，则返回其中。\n     * 否则，将为指定数组的运行时类型和此列表的大小分配一个新数组。\n     * 如果列表适用于指定的数组，其余空间（即数组的列表数量多于此元素），则紧跟在集合结束后的数组中的元素设置为null。\n     * （这仅在调用者知道列表不包含任何空元素的情况下才能确定列表的长度。）\n     */\n    @SuppressWarnings(\"unchecked\")\n    public <T> T[] toArray(T[] a) {\n        if (a.length < size)\n            // 新建一个运行时类型的数组，但是ArrayList数组的内容\n            return (T[]) Arrays.copyOf(elementData, size, a.getClass());\n            //调用System提供的arraycopy()方法实现数组之间的复制\n        System.arraycopy(elementData, 0, a, 0, size);\n        if (a.length > size)\n            a[size] = null;\n        return a;\n    }\n\n    // Positional Access Operations\n\n    @SuppressWarnings(\"unchecked\")\n    E elementData(int index) {\n        return (E) elementData[index];\n    }\n\n    /**\n     * 返回此列表中指定位置的元素。\n     */\n    public E get(int index) {\n        rangeCheck(index);\n\n        return elementData(index);\n    }\n\n    /**\n     * 用指定的元素替换此列表中指定位置的元素。\n     */\n    public E set(int index, E element) {\n        //对index进行界限检查\n        rangeCheck(index);\n\n        E oldValue = elementData(index);\n        elementData[index] = element;\n        //返回原来在这个位置的元素\n        return oldValue;\n    }\n\n    /**\n     * 将指定的元素追加到此列表的末尾。\n     */\n    public boolean add(E e) {\n        ensureCapacityInternal(size + 1);  // Increments modCount!!\n        //这里看到ArrayList添加元素的实质就相当于为数组赋值\n        elementData[size++] = e;\n        return true;\n    }\n\n    /**\n     * 在此列表中的指定位置插入指定的元素。\n     * 先调用rangeCheckForAdd对index进行界限检查；然后调用ensureCapacityInternal方法保证capacity足够大；\n     * 再将从index开始之后的所有成员后移一个位置；将element插入index位置；最后size加1。\n     */\n    public void add(int index, E element) {\n        rangeCheckForAdd(index);\n\n        ensureCapacityInternal(size + 1);  // Increments modCount!!\n        //arraycopy()这个实现数组之间复制的方法一定要看一下，下面就用到了arraycopy()方法实现数组自己复制自己\n        System.arraycopy(elementData, index, elementData, index + 1,\n                         size - index);\n        elementData[index] = element;\n        size++;\n    }\n\n    /**\n     * 删除该列表中指定位置的元素。将任何后续元素移动到左侧（从其索引中减去一个元素）。\n     */\n    public E remove(int index) {\n        rangeCheck(index);\n\n        modCount++;\n        E oldValue = elementData(index);\n\n        int numMoved = size - index - 1;\n        if (numMoved > 0)\n            System.arraycopy(elementData, index+1, elementData, index,\n                             numMoved);\n        elementData[--size] = null; // clear to let GC do its work\n      //从列表中删除的元素\n        return oldValue;\n    }\n\n    /**\n     * 从列表中删除指定元素的第一个出现（如果存在）。如果列表不包含该元素，则它不会更改。\n     * 返回true，如果此列表包含指定的元素\n     */\n    public boolean remove(Object o) {\n        if (o == null) {\n            for (int index = 0; index < size; index++)\n                if (elementData[index] == null) {\n                    fastRemove(index);\n                    return true;\n                }\n        } else {\n            for (int index = 0; index < size; index++)\n                if (o.equals(elementData[index])) {\n                    fastRemove(index);\n                    return true;\n                }\n        }\n        return false;\n    }\n\n    /*\n     * Private remove method that skips bounds checking and does not\n     * return the value removed.\n     */\n    private void fastRemove(int index) {\n        modCount++;\n        int numMoved = size - index - 1;\n        if (numMoved > 0)\n            System.arraycopy(elementData, index+1, elementData, index,\n                             numMoved);\n        elementData[--size] = null; // clear to let GC do its work\n    }\n\n    /**\n     * 从列表中删除所有元素。\n     */\n    public void clear() {\n        modCount++;\n\n        // 把数组中所有的元素的值设为null\n        for (int i = 0; i < size; i++)\n            elementData[i] = null;\n\n        size = 0;\n    }\n\n    /**\n     * 按指定集合的Iterator返回的顺序将指定集合中的所有元素追加到此列表的末尾。\n     */\n    public boolean addAll(Collection<? extends E> c) {\n        Object[] a = c.toArray();\n        int numNew = a.length;\n        ensureCapacityInternal(size + numNew);  // Increments modCount\n        System.arraycopy(a, 0, elementData, size, numNew);\n        size += numNew;\n        return numNew != 0;\n    }\n\n    /**\n     * 将指定集合中的所有元素插入到此列表中，从指定的位置开始。\n     */\n    public boolean addAll(int index, Collection<? extends E> c) {\n        rangeCheckForAdd(index);\n\n        Object[] a = c.toArray();\n        int numNew = a.length;\n        ensureCapacityInternal(size + numNew);  // Increments modCount\n\n        int numMoved = size - index;\n        if (numMoved > 0)\n            System.arraycopy(elementData, index, elementData, index + numNew,\n                             numMoved);\n\n        System.arraycopy(a, 0, elementData, index, numNew);\n        size += numNew;\n        return numNew != 0;\n    }\n\n    /**\n     * 从此列表中删除所有索引为fromIndex（含）和toIndex之间的元素。\n     * 将任何后续元素移动到左侧（减少其索引）。\n     */\n    protected void removeRange(int fromIndex, int toIndex) {\n        modCount++;\n        int numMoved = size - toIndex;\n        System.arraycopy(elementData, toIndex, elementData, fromIndex,\n                         numMoved);\n\n        // clear to let GC do its work\n        int newSize = size - (toIndex-fromIndex);\n        for (int i = newSize; i < size; i++) {\n            elementData[i] = null;\n        }\n        size = newSize;\n    }\n\n    /**\n     * 检查给定的索引是否在范围内。\n     */\n    private void rangeCheck(int index) {\n        if (index >= size)\n            throw new IndexOutOfBoundsException(outOfBoundsMsg(index));\n    }\n\n    /**\n     * add和addAll使用的rangeCheck的一个版本\n     */\n    private void rangeCheckForAdd(int index) {\n        if (index > size || index < 0)\n            throw new IndexOutOfBoundsException(outOfBoundsMsg(index));\n    }\n\n    /**\n     * 返回IndexOutOfBoundsException细节信息\n     */\n    private String outOfBoundsMsg(int index) {\n        return \"Index: \"+index+\", Size: \"+size;\n    }\n\n    /**\n     * 从此列表中删除指定集合中包含的所有元素。\n     */\n    public boolean removeAll(Collection<?> c) {\n        Objects.requireNonNull(c);\n        //如果此列表被修改则返回true\n        return batchRemove(c, false);\n    }\n\n    /**\n     * 仅保留此列表中包含在指定集合中的元素。\n     *换句话说，从此列表中删除其中不包含在指定集合中的所有元素。\n     */\n    public boolean retainAll(Collection<?> c) {\n        Objects.requireNonNull(c);\n        return batchRemove(c, true);\n    }\n\n    /**\n     * 从列表中的指定位置开始，返回列表中的元素（按正确顺序）的列表迭代器。\n     * 指定的索引表示初始调用将返回的第一个元素为next。初始调用previous将返回指定索引减1的元素。\n     * 返回的列表迭代器是fail-fast。\n     */\n    public ListIterator<E> listIterator(int index) {\n        if (index < 0 || index > size)\n            throw new IndexOutOfBoundsException(\"Index: \"+index);\n        return new ListItr(index);\n    }\n\n    /**\n     * 返回列表中的列表迭代器（按适当的顺序）。\n     * 返回的列表迭代器是fail-fast。\n     */\n    public ListIterator<E> listIterator() {\n        return new ListItr(0);\n    }\n\n    /**\n     * 以正确的顺序返回该列表中的元素的迭代器。\n     * 返回的迭代器是fail-fast。\n     */\n    public Iterator<E> iterator() {\n        return new Itr();\n    }\n}\n```\n\n### ArrayList扩容机制分析\n\n扩容后的数组长度 = 当前数组长度 + 当前数组长度 / 2,即扩容成原来的1.5倍。最后使用Arrays.copyOf方法直接把原数组中的数组copy过来，需要注意的是，Arrays.copyOf方法会创建一个新数组然后再进行拷贝。\n\n> [ArrayList的扩容机制](https://mp.weixin.qq.com/s/GY7RLE-yIF7jPAjqu5V9Cg)\n\n#### 先从ArrayList的构造函数说起\n\n**（JDK8）ArrayList有三种方式来初始化，构造方法源码如下**：\n\n```java\n/**\n  * 默认初始容量大小\n  */\nprivate static final int DEFAULT_CAPACITY = 10;\n\nprivate static final Object[] DEFAULTCAPACITY_EMPTY_ELEMENTDATA = {};\n\n/**\n  * 默认构造函数，使用初始容量10构造一个空列表(无参数构造)\n  */\npublic ArrayList() {\n    this.elementData = DEFAULTCAPACITY_EMPTY_ELEMENTDATA;\n}\n\n/**\n  * 带初始容量参数的构造函数。（用户自己指定容量）\n  */\npublic ArrayList(int initialCapacity) {\n    if (initialCapacity > 0) {//初始容量大于0\n        //创建initialCapacity大小的数组\n        this.elementData = new Object[initialCapacity];\n    } else if (initialCapacity == 0) {//初始容量等于0\n        //创建空数组\n        this.elementData = EMPTY_ELEMENTDATA;\n    } else {//初始容量小于0，抛出异常\n        throw new IllegalArgumentException(\"Illegal Capacity: \"+\n                                           initialCapacity);\n    }\n}\n/**\n  *构造包含指定collection元素的列表，这些元素利用该集合的迭代器按顺序返回\n  *如果指定的集合为null，throws NullPointerException。\n  */\npublic ArrayList(Collection<? extends E> c) {\n    elementData = c.toArray();\n    if ((size = elementData.length) != 0) {\n        // c.toArray might (incorrectly) not return Object[] (see 6260652)\n        if (elementData.getClass() != Object[].class)\n            elementData = Arrays.copyOf(elementData, size, Object[].class);\n    } else {\n        // replace with empty array.\n        this.elementData = EMPTY_ELEMENTDATA;\n    }\n}\n```\n\n细心的同学一定会发现：以无参数构造方法创建ArrayList时，实际上初始化赋值的是一个空数组。当真正对数组进行添加元素操作时，才真正分配容量。即向数组中添加第一个元素时，数组容量扩为10。\n\n> 补充：JDK6 new无参构造的ArrayList对象时，直接创建了长度是10的Object[]数组elementData。\n\n#### 一步一步分析ArrayList扩容机制\n\n这里以无参构造函数创建的ArrayList为例分析\n\n##### add()方法\n\n```java\n/**\n  * 将指定的元素追加到此列表的末尾。\n  */\npublic boolean add(E e) {\n    //添加元素之前，先调用ensureCapacityInternal方法\n    ensureCapacityInternal(size + 1);  // Increments modCount!!\n    //这里看到ArrayList添加元素的实质就相当于为数组赋值\n    elementData[size++] = e;\n    return true;\n}\n```\n\n> **注意**：JDK11移除了ensureCapacityInternal()和ensureExplicitCapacity()方法\n\n添加数据前会先判断一下是否需要扩容,先讲下add(int index, E element) 这个方法的含义，就是在指定索引index处插入元素element。比如说ArrayList.add(0, 3)，意思就是在头部插入元素3。再来看看add方法的核心System.arraycopy，这个方法有5个参数：\n```java\nelementData //源数组\nindex //从源数组中的哪个位置开始复制\nelementData //目标数组\ntoIndex //复制到目标数组中的哪个位置\nlength //要复制的源数组中数组元素的数量\n```\n举个例子，我们想要在index = 5的位置插入元素(数组大小为10)，首先，我们会复制一遍源数组elementData，然后把源数组中从index = 5的位置开始到数组末尾的元素，放到新数组的index + 1 = 6的位置上,于是，这就给我们要新增的元素腾出了位置，然后在新数组index = 5的位置放入元素element就完成了添加的操作,显然ArrayList的将数据插入到指定位置的操作性能非常低下，因为要开辟新数组复制元素，要是涉及到扩容那就更慢了。另外，ArrayList还内置了一个直接在末尾添加元素的add方法，不用复制数组，直接size++就好，这个方法应该是我们最常使用的，即直接add(element);\n\n##### ensureCapacityInternal()方法\n\n（JDK7）可以看到add方法首先调用了ensureCapacityInternal(size + 1)\n\n```java\n// 得到最小扩容量\nprivate void ensureCapacityInternal(int minCapacity) {\n    if (elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA) {\n        // 获取默认的容量和传入参数的较大值\n        minCapacity = Math.max(DEFAULT_CAPACITY, minCapacity);\n    }\n    ensureExplicitCapacity(minCapacity);\n}\n```\n\n**当要add进第1个元素时，minCapacity为1，在Math.max()方法比较后，minCapacity为10。**\n\n> 此处和后续JDK8代码格式化略有不同，核心代码基本一样。\n\n##### ensureExplicitCapacity()方法\n\n如果调用ensureCapacityInternal()方法就一定会进入（执行）这个方法，下面我们来研究一下这个方法的源码！\n\n```java\n// 判断是否需要扩容\nprivate void ensureExplicitCapacity(int minCapacity) {\n    modCount++;\n\n    // overflow-conscious code\n    if (minCapacity - elementData.length > 0)\n        //调用grow方法进行扩容，调用此方法代表已经开始扩容了\n        grow(minCapacity);\n}\n```\n\n我们来仔细分析一下：\n\n- 当我们要add进第1个元素到ArrayList时，elementData.length为0（因为还是一个空的list），因为执行了ensureCapacityInternal()方法，所以minCapacity此时为10。此时minCapacity - elementData.length > 0成立，所以会进入grow(minCapacity)方法。\n- 当add第2个元素时，minCapacity为2，此时elementData.length(容量)在添加第一个元素后扩容成10了。此时，minCapacity - elementData.length > 0不成立，所以不会进入（执行）grow(minCapacity)方法。\n- 添加第3、4···到第10个元素时，依然不会执行grow方法，数组容量都为10。\n\n直到添加第11个元素，minCapacity(为11)比elementData.length（为10）要大。进入grow方法进行扩容。\n\n##### grow()方法\n\n```java\n/**\n  * 要分配的最大数组大小\n  */\nprivate static final int MAX_ARRAY_SIZE = Integer.MAX_VALUE - 8;\n\n/**\n  * ArrayList扩容的核心方法。\n  */\nprivate void grow(int minCapacity) {\n    // oldCapacity为旧容量，newCapacity为新容量\n    int oldCapacity = elementData.length;\n    //将oldCapacity 右移一位，其效果相当于oldCapacity /2，\n    //我们知道位运算的速度远远快于整除运算，整句运算式的结果就是将新容量更新为旧容量的1.5倍，\n    int newCapacity = oldCapacity + (oldCapacity >> 1);\n    //然后检查新容量是否大于最小需要容量，若还是小于最小需要容量，那么就把最小需要容量当作数组的新容量，\n    if (newCapacity - minCapacity < 0)\n        newCapacity = minCapacity;\n    // 如果新容量大于MAX_ARRAY_SIZE,进入(执行)`hugeCapacity()`方法来比较minCapacity和MAX_ARRAY_SIZE，\n    //如果minCapacity大于最大容量，则新容量则为`Integer.MAX_VALUE`，否则，新容量大小则为MAX_ARRAY_SIZE即为`Integer.MAX_VALUE - 8`。\n    if (newCapacity - MAX_ARRAY_SIZE > 0)\n        newCapacity = hugeCapacity(minCapacity);\n    // minCapacity is usually close to size, so this is a win:\n    elementData = Arrays.copyOf(elementData, newCapacity);\n}\n```\n\n**int newCapacity = oldCapacity + (oldCapacity >> 1),所以ArrayList每次扩容之后容量都会变为原来的1.5倍左右（oldCapacity为偶数就是1.5倍，否则是1.5倍左右）**。奇偶不同，比如：10+10/2 = 15,33+33/2=49。如果是奇数的话会丢掉小数.\n\n> \">>\"（移位运算符）：>>1右移一位相当于除2，右移n位相当于除以2的n次方。这里oldCapacity明显右移了1位所以相当于oldCapacity/2。对于大数据的2进制运算,位移运算符比那些普通运算符的运算要快很多,因为程序仅仅移动一下而已,不去计算,这样提高了效率,节省了资源\n\n**我们再来通过例子探究一下grow()方法**：\n\n- 当add第1个元素时，oldCapacity为0，经比较后第一个if判断成立，newCapacity = minCapacity(为10)。但是第二个if判断不会成立，即newCapacity不比MAX_ARRAY_SIZE大，则不会进入hugeCapacity方法。数组容量为10，add方法中return true,size增为1。\n- 当add第11个元素进入grow方法时，newCapacity为15，比minCapacity（为11）大，第一个if判断不成立。新容量没有大于数组最大size，不会进入hugeCapacity方法。数组容量扩为15，add方法中return true,size增为11。\n- 以此类推······\n\n**这里补充一点比较重要，但是容易被忽视掉的知识点**：\n\n- java中的length属性是针对数组说的,比如说你声明了一个数组,想知道这个数组的长度则用到了length这个属性。\n- java中的length()方法是针对字符串说的,如果想看这个字符串的长度则用到length()这个方法。\n- java中的size()方法是针对泛型集合说的,如果想看这个泛型有多少个元素,就调用此方法来查看。\n\n##### hugeCapacity()方法。\n\n从上面grow()方法源码我们知道：如果新容量大于MAX_ARRAY_SIZE,进入(执行)hugeCapacity()方法来比较minCapacity和MAX_ARRAY_SIZE，如果minCapacity大于最大容量，则新容量则为Integer.MAX_VALUE，否则，新容量大小则为MAX_ARRAY_SIZE即为Integer.MAX_VALUE - 8。\n\n```java\nprivate static int hugeCapacity(int minCapacity) {\n    if (minCapacity < 0) // overflow\n        throw new OutOfMemoryError();\n    //对minCapacity和MAX_ARRAY_SIZE进行比较\n    //若minCapacity大，将Integer.MAX_VALUE作为新数组的大小\n    //若MAX_ARRAY_SIZE大，将MAX_ARRAY_SIZE作为新数组的大小\n    //MAX_ARRAY_SIZE = Integer.MAX_VALUE - 8;\n    return (minCapacity > MAX_ARRAY_SIZE) ?\n        Integer.MAX_VALUE :\n    MAX_ARRAY_SIZE;\n}\n```\n\n#### System.arraycopy()和Arrays.copyOf()方法\n\n阅读源码的话，我们就会发现ArrayList中大量调用了这两个方法。比如：我们上面讲的扩容操作以及add(int index, E element)、toArray()等方法中都用到了该方法\n\n##### System.arraycopy()方法\n\n源码：\n\n```java\n// 我们发现arraycopy是一个native方法,接下来我们解释一下各个参数的具体意义\n/**\n  * 复制数组\n  * @param src 源数组\n  * @param srcPos 源数组中的起始位置\n  * @param dest 目标数组\n  * @param destPos 目标数组中的起始位置\n  * @param length 要复制的数组元素的数量\n  */\npublic static native void arraycopy(Object src,  int  srcPos,\n                                    Object dest, int destPos,\n                                    int length);\n```\n\n场景：\n\n```java\n/**\n   * 在此列表中的指定位置插入指定的元素。\n   * 先调用rangeCheckForAdd对index进行界限检查；然后调用ensureCapacityInternal方法保证capacity足够大；\n   * 再将从index开始之后的所有成员后移一个位置；将element插入index位置；最后size加1。\n   */\npublic void add(int index, E element) {\n    rangeCheckForAdd(index);\n    \n    ensureCapacityInternal(size + 1);  // Increments modCount!!\n    //arraycopy()方法实现数组自己复制自己\n    //elementData:源数组;index:源数组中的起始位置;elementData：目标数组；index + 1：目标数组中的起始位置；size - index：要复制的数组元素的数量；\n    System.arraycopy(elementData, index, elementData, index + 1, size - index);\n    elementData[index] = element;\n    size++;\n}\n```\n\n我们写一个简单的方法测试以下：\n\n```java\npublic class ArraycopyTest {\n\tpublic static void main(String[] args) {\n\t\t// TODO Auto-generated method stub\n\t\tint[] a = new int[10];\n\t\ta[0] = 0;\n\t\ta[1] = 1;\n\t\ta[2] = 2;\n\t\ta[3] = 3;\n\t\tSystem.arraycopy(a, 2, a, 3, 3);\n\t\ta[2]=99;\n\t\tfor (int i = 0; i < a.length; i++) {\n\t\t\tSystem.out.print(a[i] + \" \");\n\t\t}\n\t}\n}\n```\n\n结果：\n```text\n0 1 99 2 3 0 0 0 0 0\n```\n\n##### Arrays.copyOf()方法\n\n源码：\n```java\npublic static int[] copyOf(int[] original, int newLength) {\n    // 申请一个新的数组\n    int[] copy = new int[newLength];\n    // 调用System.arraycopy,将源数组中的数据进行拷贝,并返回新的数组\n    System.arraycopy(original, 0, copy, 0,\n                     Math.min(original.length, newLength));\n    return copy;\n}\n```\n\n场景：\n```java\n/**\n  * 以正确的顺序返回一个包含此列表中所有元素的数组（从第一个到最后一个元素）;返回的数组的运行时类型是指定数组的运行时类型。\n  */\npublic Object[] toArray() {\n    //elementData：要复制的数组；size：要复制的长度\n    return Arrays.copyOf(elementData, size);\n}\n```\n\n个人觉得使用Arrays.copyOf()方法主要是为了给原有数组扩容，测试代码如下：\n\n```java\npublic class ArrayscopyOfTest {\n\tpublic static void main(String[] args) {\n\t\tint[] a = new int[3];\n\t\ta[0] = 0;\n\t\ta[1] = 1;\n\t\ta[2] = 2;\n\t\tint[] b = Arrays.copyOf(a, 10);\n\t\tSystem.out.println(\"b.length\"+b.length);\n\t}\n}\n```\n\n结果：\n\n```text\n10\n```\n\n##### 两者联系和区别\n\n**联系**：\n看两者源代码可以发现`copyOf()`内部实际调用了`System.arraycopy()`方法\n\n**区别**：\n`arraycopy()`需要目标数组，将原数组拷贝到你自己定义的数组里或者原数组，而且可以选择拷贝的起点和长度以及放入新数组中的位置,`copyOf()`是系统自动在内部新建一个数组，并返回该数组。\n\n#### ensureCapacity()方法\n\nArrayList源码中有一个ensureCapacity方法，这个方法ArrayList内部没有被调用过，所以很显然是提供给用户调用的，那么这个方法有什么作用呢？\n\n```java\n/**\n  * 如有必要，增加此ArrayList实例的容量，以确保它至少可以容纳由minimum capacity参数指定的元素数。\n  * @param   minCapacity   所需的最小容量\n  */\npublic void ensureCapacity(int minCapacity) {\n    int minExpand = (elementData != DEFAULTCAPACITY_EMPTY_ELEMENTDATA)\n        // any size if not default element table\n        ? 0\n        // larger than default for default empty table. It's already\n        // supposed to be at default size.\n        : DEFAULT_CAPACITY;\n\n    if (minCapacity > minExpand) {\n        ensureExplicitCapacity(minCapacity);\n    }\n}\n```\n理论上来说，最好在向ArrayList添加大量元素之前用ensureCapacity方法，以减少增量重新分配的次数。我们通过下面的代码实际测试以下这个方法的效果：\n\n```java\npublic class EnsureCapacityTest {\n\tpublic static void main(String[] args) {\n\t\tArrayList<Object> list = new ArrayList<Object>();\n\t\tfinal int N = 10000000;\n\t\tlong startTime = System.currentTimeMillis();\n\t\tfor (int i = 0; i < N; i++) {\n\t\t\tlist.add(i);\n\t\t}\n\t\tlong endTime = System.currentTimeMillis();\n\t\tSystem.out.println(\"使用ensureCapacity方法前：\"+(endTime - startTime));\n\n\t}\n}\n```\n运行结果：\n```text\n使用ensureCapacity方法前：2158\n```\n\n```java\npublic class EnsureCapacityTest {\n    public static void main(String[] args) {\n        ArrayList<Object> list = new ArrayList<Object>();\n        final int N = 10000000;\n        long startTime1 = System.currentTimeMillis();\n        list.ensureCapacity(N);\n        for (int i = 0; i < N; i++) {\n            list.add(i);\n        }\n        long endTime1 = System.currentTimeMillis();\n        System.out.println(\"使用ensureCapacity方法后：\"+(endTime1 - startTime1));\n    }\n}\n```\n\n运行结果：\n```text\n使用ensureCapacity方法后：1773\n```\n通过运行结果，我们可以看出向ArrayList添加大量元素之前使用ensureCapacity方法可以提升性能。不过，这个性能差距几乎可以忽略不计。而且，实际项目根本也不可能往ArrayList里面添加这么多元素。\n\n#### remove()方法\n\n假设我们要删除数组的index = 5的元素，首先，我们会复制一遍源数组，然后把源数组中从 index + 1 = 6的位置开始到数组末尾的元素，放到新数组的index = 5的位置上,也就是说 index = 5的元素直接被覆盖掉了，给了你被删除的感觉。同样的，它的效率自然也是十分低下的\n\n> [原文链接](https://javaguide.cn/java/collection/arraylist-source-code.html)\n\n## CopyOnWriteArrayList\n\n### CopyOnWriteArrayList简介\n\nCopyOnWriteArrayList实际上是ArrayList一个线程安全的操作类,是一个典型的读写分离的动态数组操作类.从它的名字可以看出，CopyOnWrite是在写入的时候，不修改原内容，而是将原来的内容复制一份到新的数组，然后向新数组写完数据之后，再移动内存指针，将目标指向最新的位置。CopyOnWriteArraySet主要针对集，CopyOnWriteArraySet可以理解为HashSet线程安全的操作类，我们都知道HashSet基于散列表HashMap实现，但是CopyOnWriteArraySet并不是基于散列表实现，而是基于CopyOnWriteArrayList动态数组实现。两者最大的不同点是，CopyOnWriteArrayList可以允许元素重复，而CopyOnWriteArraySet不允许有重复的元素。CopyOnWriteArrayList在写入数据的时候，将旧数组内容复制一份出来，然后向新的数组写入数据，最后将新的数组内存地址返回给数组变量；移除操作也类似，只是方式是移除元素而不是添加元素；而查询方法，因为不涉及线程操作，所以并没有加锁出来！因为CopyOnWriteArrayList读取内容没有加锁，在写入数据的时候同时也可以进行读取数据操作，因此性能得到很大的提升，但是也有缺陷，**对于边读边写的情况，不一定能实时的读到最新的数据,因此CopyOnWriteArrayList很适合读多写少的应用场景**\n\n\n```java\npublic class CopyOnWriteArrayList<E> extends Object implements List<E>, RandomAccess, Cloneable, Serializable\n```\n\n在很多应用场景中，读操作可能会远远大于写操作。由于读操作根本不会修改原有的数据，因此对于每次读取都进行加锁其实是一种资源浪费。我们应该允许多个线程同时访问List的内部数据，毕竟读取操作是安全的。这和我们之前提到过的ReentrantReadWriteLock读写锁的思想非常类似，也就是读读共享、写写互斥、读写互斥、写读互斥。JDK中提供了CopyOnWriteArrayList类比相比于在读写锁的思想又更进一步。为了将读取的性能发挥到极致，CopyOnWriteArrayList读取是完全不用加锁的，并且更厉害的是：写入也不会阻塞读取操作。只有写入和写入之间需要进行同步等待。这样一来，读操作的性能就会大幅度提升。\n\n### CopyOnWriteArrayList是如何做到的？\n\nCopyOnWriteArrayList类的所有可变操作（add，set等等）都是通过创建底层数组的新副本来实现的。当List需要被修改的时候，我并不修改原有内容，而是对原有数据进行一次复制，将修改的内容写入副本。写完之后，再将修改完的副本替换原来的数据，这样就可以保证写操作不会影响读操作了。从CopyOnWriteArrayList的名字就能看出CopyOnWriteArrayList是满足CopyOnWrite的。所谓CopyOnWrite也就是说：在计算机，如果你想要对一块内存进行修改时，我们不在原有内存块中进行写操作，而是将内存拷贝一份，在新的内存中进行写操作，写完之后呢，就将指向原来内存指针指向新的内存，原来的内存就可以被回收掉了。\n\n### CopyOnWriteArrayList读取和写入源码简单分析\n\n#### CopyOnWriteArrayList读取操作的实现\n\n读取操作没有任何同步控制和锁操作，理由就是内部数组array不会发生修改，只会被另外一个array替换，因此可以保证数据安全。\n\n\n```java\n/** The array, accessed only via getArray/setArray. */\nprivate transient volatile Object[] array;\npublic E get(int index) {\n    return get(getArray(), index);\n}\n@SuppressWarnings(\"unchecked\")\nprivate E get(Object[] a, int index) {\n    return (E) a[index];\n}\nfinal Object[] getArray() {\n    return array;\n}\n```\n\n#### CopyOnWriteArrayList写入操作的实现\n\nCopyOnWriteArrayList写入操作add()方法在添加集合的时候加了锁，保证了同步，避免了多线程写的时候会copy出多个副本出来。\n\n\n```java\n/**\n  * Appends the specified element to the end of this list.\n  * @param e element to be appended to this list\n  * @return {@code true} (as specified by {@link Collection#add})\n  */\npublic boolean add(E e) {\n    final ReentrantLock lock = this.lock;\n    lock.lock();//加锁\n    try {\n        Object[] elements = getArray();\n        int len = elements.length;\n        Object[] newElements = Arrays.copyOf(elements, len + 1);//拷贝新数组\n        newElements[len] = e;\n        setArray(newElements);\n        return true;\n    } finally {\n        lock.unlock();//释放锁\n    }\n}\n```\n\n### 常用方法\n\n#### add()\n\nadd()方法是CopyOnWriteArrayList的添加元素的入口,CopyOnWriteArrayList之所以能保证多线程下安全操作，add()方法功不可没,操作步骤如下：\n- 获得对象锁\n- 获取数组内容\n- 将原数组内容复制到新数组\n- 写入数据\n- 将array数组变量地址指向新数组\n- 释放对象锁\nCopyOnWriteArrayList使用了ReentrantLock这种可重入锁，保证了线程操作安全，同时数组变量array使用volatile保证多线程下数据的可见性\n\n#### remove()\n\nremove()方法是CopyOnWriteArrayList的移除元素的入口\n操作类似添加方法，步骤如下：\n- 获得对象锁\n- 获取数组内容\n- 判断移除的元素是否为数组最后的元素，如果是最后的元素，直接将旧元素内容复制到新数组，并重新设置array值\n- 如果是中间元素，以index为分界点，分两节复制\n- 将array数组变量地址指向新数组\n- 释放对象锁\n当然，移除的方法还有基于对象的remove(Object o)，原理也是一样的，先找到元素的下标，然后执行移除操作。\n\n#### get()\n\nget()方法是CopyOnWriteArrayList的查询元素的入口,源码如下：\n\n```java\npublic E get(int index) {\n    //获取数组内容，通过下标直接获取\n    return get(getArray(), index);\n}\n```\n查询因为不涉及到数据操作，所以无需使用锁进行处理！\n\n#### iterator()\n\nCopyOnWriteArrayList在使用迭代器遍历的时候，操作的都是原数组，没有像上面那样进行修改次数判断，所以不会抛异常,当然，从源码上也可以得出，使用CopyOnWriteArrayList的迭代器进行遍历元素的时候，不能调用remove()方法移除元素，因为不支持此操作,如果想要移除元素，只能使用CopyOnWriteArrayList提供的remove()方法，而不是迭代器的remove()方法，这个需要注意一下\n\n","categories":["Java"]},{"title":"HashMap&ConcurrentHashMap","slug":"HashMap&ConcurrentHashMap","url":"/blog/posts/366519dedccc/","content":"\n## HashMap\n\n### HashMap简介\n\nHashMap主要用来存放键值对，它基于哈希表的Map接口实现，是常用的Java集合之一，是非线程安全的。HashMap可以存储null的key和value，但null作为键只能有一个，null作为值可以有多个。\n\nJDK1.8之前HashMap由数组+链表组成的，数组是HashMap的主体，链表则是主要为了解决哈希冲突而存在的（“拉链法”解决冲突）。JDK1.8以后的HashMap在解决哈希冲突时有了较大的变化，当链表长度大于阈值（默认为8）（将链表转换成红黑树前会判断，如果当前数组的长度小于64，那么会选择先进行数组扩容，而不是转换为红黑树）时，将链表转化为红黑树，以减少搜索时间。\n\nHashMap默认的初始化大小为16。之后每次扩充，容量变为原来的2倍。并且，HashMap总是使用2的幂作为哈希表的大小。\n\n### HashMap的底层实现\n\n#### JDK1.8之前\n\nJDK1.8之前HashMap底层是**数组和链表**结合在一起使用也就是**链表散列**。HashMap通过key的hashcode经过扰动函数处理过后得到hash值，然后通过(n-1)&hash判断当前元素存放的位置（这里的n指的是数组的长度），如果当前位置存在元素的话，就判断该元素与要存入的元素的hash值以及key是否相同，如果相同的话，直接覆盖，不相同就通过拉链法解决冲突。所谓扰动函数指的就是HashMap的hash方法。使用hash方法也就是扰动函数是为了防止一些实现比较差的hashCode()方法,换句话说使用扰动函数之后可以减少碰撞。\n\n**JDK1.8HashMap的hash方法源码**：\n\nJDK1.8的hash方法相比于JDK1.7hash方法更加简化，但是原理不变。\n```java\nstatic final int hash(Object key) {\n    int h;\n    // key.hashCode()：返回散列值也就是hashcode\n    // ^ ：按位异或\n    // >>>:无符号右移，忽略符号位，空位都以0补齐\n    return (key == null) ? 0 : (h = key.hashCode()) ^ (h >>> 16);\n}\n```\n\n对比一下JDK1.7的HashMap的hash方法源码.\n\n```java\nstatic int hash(int h) {\n    // This function ensures that hashCodes that differ only by\n    // constant multiples at each bit position have a bounded\n    // number of collisions (approximately 8 at default load factor).\n\n    h ^= (h >>> 20) ^ (h >>> 12);\n    return h ^ (h >>> 7) ^ (h >>> 4);\n}\n```\n\n相比于JDK1.8的hash方法，JDK1.7的hash方法的性能会稍差一点点，因为毕竟扰动了4次。\n所谓**拉链法**就是：将链表和数组相结合。也就是说创建一个链表数组，数组中每一格就是一个链表。若遇到哈希冲突，则将冲突的值加到链表中即可。\n\n![jdk1.8之前的内部结构-HashMap](https://guide-blog-images.oss-cn-shenzhen.aliyuncs.com/github/javaguide/java/collection/jdk1.7_hashmap.png)\n\n#### JDK1.8之后\n\n相比于之前的版本，JDK1.8之后在解决哈希冲突时有了较大的变化，当链表长度大于阈值（默认为8）（将链表转换成红黑树前会判断，如果当前数组的长度小于64，那么会选择先进行数组扩容，而不是转换为红黑树）时，将链表转化为红黑树，以减少搜索时间。\n\n![jdk1.8之后的内部结构-HashMap](https://guide-blog-images.oss-cn-shenzhen.aliyuncs.com/github/javaguide/java/collection/jdk1.8_hashmap.png)\n\n> TreeMap、TreeSet以及JDK1.8之后的HashMap底层都用到了红黑树。红黑树就是为了解决二叉查找树的缺陷，因为二叉查找树在某些情况下会退化成一个线性结构。\n\n我们来结合源码分析一下HashMap链表到红黑树的转换。\n\n**1、putVal方法中执行链表转红黑树的判断逻辑。**\n\n链表的长度大于8的时候，就执行treeifyBin（转换红黑树）的逻辑。\n\n```java\n// 遍历链表\nfor (int binCount = 0; ; ++binCount) {\n    // 遍历到链表最后一个节点\n    if ((e = p.next) == null) {\n        p.next = newNode(hash, key, value, null);\n        // 如果链表元素个数大于等于TREEIFY_THRESHOLD（8）\n        if (binCount >= TREEIFY_THRESHOLD - 1) // -1 for 1st\n            // 红黑树转换（并不会直接转换成红黑树）\n            treeifyBin(tab, hash);\n        break;\n    }\n    if (e.hash == hash &&\n        ((k = e.key) == key || (key != null && key.equals(k))))\n        break;\n    p = e;\n}\n```\n\n**2、treeifyBin方法中判断是否真的转换为红黑树。**\n\n```java\nfinal void treeifyBin(Node<K,V>[] tab, int hash) {\n    int n, index; Node<K,V> e;\n    // 判断当前数组的长度是否小于 64\n    if (tab == null || (n = tab.length) < MIN_TREEIFY_CAPACITY)\n        // 如果当前数组的长度小于 64，那么会选择先进行数组扩容\n        resize();\n    else if ((e = tab[index = (n - 1) & hash]) != null) {\n        // 否则才将列表转换为红黑树\n\n        TreeNode<K,V> hd = null, tl = null;\n        do {\n            TreeNode<K,V> p = replacementTreeNode(e, null);\n            if (tl == null)\n                hd = p;\n            else {\n                p.prev = tl;\n                tl.next = p;\n            }\n            tl = p;\n        } while ((e = e.next) != null);\n        if ((tab[index] = hd) != null)\n            hd.treeify(tab);\n    }\n}\n```\n\n将链表转换成红黑树前会判断，如果当前数组的长度小于64，那么会选择先进行数组扩容，而不是转换为红黑树。当链表长度大于阈值（默认为8）时，会首先调用treeifyBin()方法。这个方法会根据HashMap数组来决定是否转换为红黑树。只有当数组长度大于或者等于64的情况下，才会执行转换红黑树操作，以减少搜索时间。否则，就是只是执行resize()方法对数组扩容。相关源码这里就不贴了，重点关注treeifyBin()方法即可！\n\n![img](https://oscimg.oschina.net/oscnet/up-bba283228693dae74e78da1ef7a9a04c684.png)\n\n**类的属性**：\n\n```java\npublic class HashMap<K,V> extends AbstractMap<K,V> implements Map<K,V>, Cloneable, Serializable {\n    // 序列号\n    private static final long serialVersionUID = 362498820763181265L;\n    // 默认的初始容量是16\n    static final int DEFAULT_INITIAL_CAPACITY = 1 << 4;\n    // 最大容量\n    static final int MAXIMUM_CAPACITY = 1 << 30;\n    // 默认的填充因子\n    static final float DEFAULT_LOAD_FACTOR = 0.75f;\n    // 当桶(bucket)上的结点数大于这个值时会转成红黑树\n    static final int TREEIFY_THRESHOLD = 8;\n    // 当桶(bucket)上的结点数小于这个值时树转链表\n    static final int UNTREEIFY_THRESHOLD = 6;\n    // 桶中结构转化为红黑树对应的table的最小容量\n    static final int MIN_TREEIFY_CAPACITY = 64;\n    // 存储元素的数组，总是2的幂次倍\n    transient Node<k,v>[] table;\n    // 存放具体元素的集\n    transient Set<map.entry<k,v>> entrySet;\n    // 存放元素的个数，注意这个不等于数组的长度。\n    transient int size;\n    // 每次扩容和更改map结构的计数器\n    transient int modCount;\n    // 临界值(容量*填充因子) 当实际大小超过临界值时，会进行扩容\n    int threshold;\n    // 加载因子\n    final float loadFactor;\n}\n```\n\n- **loadFactor加载因子**\nloadFactor加载因子是控制数组存放数据的疏密程度，loadFactor越趋近于1，那么数组中存放的数据(entry)也就越多，也就越密，也就是会让链表的长度增加，loadFactor越小，也就是趋近于0，数组中存放的数据(entry)也就越少，也就越稀疏。**loadFactor太大导致查找元素效率低，太小导致数组的利用率低，存放的数据会很分散。loadFactor的默认值为0.75f是官方给出的一个比较好的临界值**。给定的默认容量为16，负载因子为0.75。Map在使用过程中不断的往里面存放数据，当数量达到了16*0.75=12就需要将当前16的容量进行扩容，而扩容这个过程涉及到rehash、复制数据等操作，所以非常消耗性能。\n\n- **threshold**\n**threshold=capacity\\*loadFactor**，**当Size>=threshold**的时候，那么就要考虑对数组的扩增了，也就是说，这个的意思就是衡量数组是否需要扩增的一个标准。\n\n**Node节点类源码:**\n\n```java\n// 继承自Map.Entry<K,V>\nstatic class Node<K,V> implements Map.Entry<K,V> {\n    final int hash;// 哈希值，存放元素到hashmap中时用来与其他元素hash值比较\n    final K key;//键\n    V value;//值\n    // 指向下一个节点\n    Node<K,V> next;\n    Node(int hash, K key, V value, Node<K,V> next) {\n        this.hash = hash;\n        this.key = key;\n        this.value = value;\n        this.next = next;\n    }\n    public final K getKey()        { return key; }\n    public final V getValue()      { return value; }\n    public final String toString() { return key + \"=\" + value; }\n    // 重写hashCode()方法\n    public final int hashCode() {\n        return Objects.hashCode(key) ^ Objects.hashCode(value);\n    }\n\n    public final V setValue(V newValue) {\n        V oldValue = value;\n        value = newValue;\n        return oldValue;\n    }\n    // 重写equals()方法\n    public final boolean equals(Object o) {\n        if (o == this)\n            return true;\n        if (o instanceof Map.Entry) {\n            Map.Entry<?,?> e = (Map.Entry<?,?>)o;\n            if (Objects.equals(key, e.getKey()) &&\n                Objects.equals(value, e.getValue()))\n                return true;\n        }\n        return false;\n    }\n}\n```\n\n**树节点类源码:**\n\n```java\nstatic final class TreeNode<K,V> extends LinkedHashMap.Entry<K,V> {\n    TreeNode<K,V> parent;  // 父\n    TreeNode<K,V> left;    // 左\n    TreeNode<K,V> right;   // 右\n    TreeNode<K,V> prev;    // needed to unlink next upon deletion\n    boolean red;           // 判断颜色\n    TreeNode(int hash, K key, V val, Node<K,V> next) {\n        super(hash, key, val, next);\n    }\n    // 返回根节点\n    final TreeNode<K,V> root() {\n        for (TreeNode<K,V> r = this, p;;) {\n            if ((p = r.parent) == null)\n                return r;\n            r = p;\n        }\n    }\n}\n```\n\n### HashMap源码分析\n\n#### 构造方法\n\nHashMap中有四个构造方法，它们分别如下：\n```java\n// 默认构造函数。\npublic HashMap() {\n    this.loadFactor = DEFAULT_LOAD_FACTOR; // all   other fields defaulted\n}\n\n// 包含另一个“Map”的构造函数\npublic HashMap(Map<? extends K, ? extends V> m) {\n    this.loadFactor = DEFAULT_LOAD_FACTOR;\n    putMapEntries(m, false);//下面会分析到这个方法\n}\n\n// 指定“容量大小”的构造函数\npublic HashMap(int initialCapacity) {\n    this(initialCapacity, DEFAULT_LOAD_FACTOR);\n}\n\n// 指定“容量大小”和“加载因子”的构造函数\npublic HashMap(int initialCapacity, float loadFactor) {\n    if (initialCapacity < 0)\n        throw new IllegalArgumentException(\"Illegal initial capacity: \" + initialCapacity);\n    if (initialCapacity > MAXIMUM_CAPACITY)\n        initialCapacity = MAXIMUM_CAPACITY;\n    if (loadFactor <= 0 || Float.isNaN(loadFactor))\n        throw new IllegalArgumentException(\"Illegal load factor: \" + loadFactor);\n    this.loadFactor = loadFactor;\n    this.threshold = tableSizeFor(initialCapacity);\n}\n```\n\n**putMapEntries方法：**\n```java\nfinal void putMapEntries(Map<? extends K, ? extends V> m, boolean evict) {\n    int s = m.size();\n    if (s > 0) {\n        // 判断table是否已经初始化\n        if (table == null) { // pre-size\n            // 未初始化，s为m的实际元素个数\n            float ft = ((float)s / loadFactor) + 1.0F;\n            int t = ((ft < (float)MAXIMUM_CAPACITY) ?\n                    (int)ft : MAXIMUM_CAPACITY);\n            // 计算得到的t大于阈值，则初始化阈值\n            if (t > threshold)\n                threshold = tableSizeFor(t);\n        }\n        // 已初始化，并且m元素个数大于阈值，进行扩容处理\n        else if (s > threshold)\n            resize();\n        // 将m中的所有元素添加至HashMap中\n        for (Map.Entry<? extends K, ? extends V> e : m.entrySet()) {\n            K key = e.getKey();\n            V value = e.getValue();\n            putVal(hash(key), key, value, false, evict);\n        }\n    }\n}\n```\n\n#### put方法\n\nHashMap只提供了put用于添加元素，putVal方法只是给put方法调用的一个方法，并没有提供给用户使用。\n\n**对putVal方法添加元素的分析如下**：\n\n1. 如果定位到的数组位置没有元素就直接插入。\n2. 如果定位到的数组位置有元素就和要插入的key比较，如果key相同就直接覆盖，如果key不相同，就判断p是否是一个树节点，如果是就调用`e = ((TreeNode<K,V>)p).putTreeVal(this,tab,hash,key,value)`将元素添加进入。如果不是就遍历链表插入(插入的是链表尾部)。\n\n![](https://my-blog-to-use.oss-cn-beijing.aliyuncs.com/2019-7/put方法.png)\n\n说明:上图有两个小问题：\n\n- 直接覆盖之后应该就会return，不会有后续操作。参考JDK8 HashMap.java658行（[issue#608](https://github.com/Snailclimb/JavaGuide/issues/608)）。\n- 当链表长度大于阈值（默认为8）并且HashMap数组长度超过64的时候才会执行链表转红黑树的操作，否则就只是对数组扩容。参考HashMap的treeifyBin()方法（[issue#1087](https://github.com/Snailclimb/JavaGuide/issues/1087)）。\n\n```java\npublic V put(K key, V value) {\n    return putVal(hash(key), key, value, false, true);\n}\n\nfinal V putVal(int hash, K key, V value, boolean onlyIfAbsent,boolean evict) {\n    Node<K,V>[] tab; Node<K,V> p; int n, i;\n    // table未初始化或者长度为0，进行扩容\n    if ((tab = table) == null || (n = tab.length) == 0)\n        n = (tab = resize()).length;\n    // (n - 1) & hash 确定元素存放在哪个桶中，桶为空，新生成结点放入桶中(此时，这个结点是放在数组中)\n    if ((p = tab[i = (n - 1) & hash]) == null)\n        tab[i] = newNode(hash, key, value, null);\n    // 桶中已经存在元素（处理hash冲突）\n    else {\n        Node<K,V> e; K k;\n        // 判断table[i]中的元素是否与插入的key一样，若相同那就直接使用插入的值p替换掉旧的值e。\n        if (p.hash == hash &&\n            ((k = p.key) == key || (key != null && key.equals(k))))\n                e = p;\n        // 判断插入的是否是红黑树节点\n        else if (p instanceof TreeNode)\n            // 放入树中\n            e = ((TreeNode<K,V>)p).putTreeVal(this, tab, hash, key, value);\n        // 不是红黑树节点则说明为链表结点\n        else {\n            // 在链表最末插入结点\n            for (int binCount = 0; ; ++binCount) {\n                // 到达链表的尾部\n                if ((e = p.next) == null) {\n                    // 在尾部插入新结点\n                    p.next = newNode(hash, key, value, null);\n                    // 结点数量达到阈值(默认为 8 )，执行 treeifyBin 方法\n                    // 这个方法会根据 HashMap 数组来决定是否转换为红黑树。\n                    // 只有当数组长度大于或者等于 64 的情况下，才会执行转换红黑树操作，以减少搜索时间。否则，就是只是对数组扩容。\n                    if (binCount >= TREEIFY_THRESHOLD - 1) // -1 for 1st\n                        treeifyBin(tab, hash);\n                    // 跳出循环\n                    break;\n                }\n                // 判断链表中结点的key值与插入的元素的key值是否相等\n                if (e.hash == hash &&\n                    ((k = e.key) == key || (key != null && key.equals(k))))\n                    // 相等，跳出循环\n                    break;\n                // 用于遍历桶中的链表，与前面的e = p.next组合，可以遍历链表\n                p = e;\n            }\n        }\n        // 表示在桶中找到key值、hash值与插入元素相等的结点\n        if (e != null) {\n            // 记录e的value\n            V oldValue = e.value;\n            // onlyIfAbsent为false或者旧值为null\n            if (!onlyIfAbsent || oldValue == null)\n                //用新值替换旧值\n                e.value = value;\n            // 访问后回调\n            afterNodeAccess(e);\n            // 返回旧值\n            return oldValue;\n        }\n    }\n    // 结构性修改\n    ++modCount;\n    // 实际大小大于阈值则扩容\n    if (++size > threshold)\n        resize();\n    // 插入后回调\n    afterNodeInsertion(evict);\n    return null;\n}\n```\n\n**我们再来对比一下JDK1.7put方法的代码，对于put方法的分析如下**：\n\n①如果定位到的数组位置没有元素就直接插入。\n②如果定位到的数组位置有元素，遍历以这个元素为头结点的链表，依次和插入的key比较，如果key相同就直接覆盖，不同就采用头插法插入元素。\n\n```java\npublic V put(K key, V value)\n    if (table == EMPTY_TABLE) {\n    \tinflateTable(threshold);\n\t}\n    if (key == null)\n        return putForNullKey(value);\n    int hash = hash(key);\n    int i = indexFor(hash, table.length);\n    for (Entry<K,V> e = table[i]; e != null; e = e.next) { // 先遍历\n        Object k;\n        if (e.hash == hash && ((k = e.key) == key || key.equals(k))) {\n            V oldValue = e.value;\n            e.value = value;\n            e.recordAccess(this);\n            return oldValue;\n        }\n    }\n\n    modCount++;\n    addEntry(hash, key, value, i);  // 再插入\n    return null;\n}\n```\n\n#### get方法\n\n```java\npublic V get(Object key) {\n    Node<K,V> e;\n    return (e = getNode(hash(key), key)) == null ? null : e.value;\n}\n\nfinal Node<K,V> getNode(int hash, Object key) {\n    Node<K,V>[] tab; Node<K,V> first, e; int n; K k;\n    if ((tab = table) != null && (n = tab.length) > 0 &&\n        (first = tab[(n - 1) & hash]) != null) {\n        // 数组元素相等\n        if (first.hash == hash && // always check first node\n            ((k = first.key) == key || (key != null && key.equals(k))))\n            return first;\n        // 桶中不止一个节点\n        if ((e = first.next) != null) {\n            // 在树中get\n            if (first instanceof TreeNode)\n                return ((TreeNode<K,V>)first).getTreeNode(hash, key);\n            // 在链表中get\n            do {\n                if (e.hash == hash &&\n                    ((k = e.key) == key || (key != null && key.equals(k))))\n                    return e;\n            } while ((e = e.next) != null);\n        }\n    }\n    return null;\n}\n```\n\n#### resize方法\n\n进行扩容，会伴随着一次重新hash分配，并且会遍历hash表中所有的元素，是非常耗时的。在编写程序中，要尽量避免resize。\n\n```java\nfinal Node<K,V>[] resize() {\n    Node<K,V>[] oldTab = table;\n    int oldCap = (oldTab == null) ? 0 : oldTab.length;\n    int oldThr = threshold;\n    int newCap, newThr = 0;\n    if (oldCap > 0) {\n        // 超过最大值就不再扩充了，就只好随你碰撞去吧\n        if (oldCap >= MAXIMUM_CAPACITY) {\n            threshold = Integer.MAX_VALUE;\n            return oldTab;\n        }\n        // 没超过最大值，就扩充为原来的2倍\n        else if ((newCap = oldCap << 1) < MAXIMUM_CAPACITY && oldCap >= DEFAULT_INITIAL_CAPACITY)\n            newThr = oldThr << 1; // double threshold\n    }\n    else if (oldThr > 0) // initial capacity was placed in threshold\n        newCap = oldThr;\n    else {\n        // signifies using defaults\n        newCap = DEFAULT_INITIAL_CAPACITY;\n        newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY);\n    }\n    // 计算新的resize上限\n    if (newThr == 0) {\n        float ft = (float)newCap * loadFactor;\n        newThr = (newCap < MAXIMUM_CAPACITY && ft < (float)MAXIMUM_CAPACITY ? (int)ft : Integer.MAX_VALUE);\n    }\n    threshold = newThr;\n    @SuppressWarnings({\"rawtypes\",\"unchecked\"})\n        Node<K,V>[] newTab = (Node<K,V>[])new Node[newCap];\n    table = newTab;\n    if (oldTab != null) {\n        // 把每个bucket都移动到新的buckets中\n        for (int j = 0; j < oldCap; ++j) {\n            Node<K,V> e;\n            if ((e = oldTab[j]) != null) {\n                oldTab[j] = null;\n                if (e.next == null)\n                    newTab[e.hash & (newCap - 1)] = e;\n                else if (e instanceof TreeNode)\n                    ((TreeNode<K,V>)e).split(this, newTab, j, oldCap);\n                else {\n                    Node<K,V> loHead = null, loTail = null;\n                    Node<K,V> hiHead = null, hiTail = null;\n                    Node<K,V> next;\n                    do {\n                        next = e.next;\n                        // 原索引\n                        if ((e.hash & oldCap) == 0) {\n                            if (loTail == null)\n                                loHead = e;\n                            else\n                                loTail.next = e;\n                            loTail = e;\n                        }\n                        // 原索引+oldCap\n                        else {\n                            if (hiTail == null)\n                                hiHead = e;\n                            else\n                                hiTail.next = e;\n                            hiTail = e;\n                        }\n                    } while ((e = next) != null);\n                    // 原索引放到bucket里\n                    if (loTail != null) {\n                        loTail.next = null;\n                        newTab[j] = loHead;\n                    }\n                    // 原索引+oldCap放到bucket里\n                    if (hiTail != null) {\n                        hiTail.next = null;\n                        newTab[j + oldCap] = hiHead;\n                    }\n                }\n            }\n        }\n    }\n    return newTab;\n}\n```\n\n### HashMap常用方法测试\n\n```java\npackage map;\n\nimport java.util.Collection;\nimport java.util.HashMap;\nimport java.util.Set;\n\npublic class HashMapDemo {\n    public static void main(String[] args) {\n        HashMap<String, String> map = new HashMap<String, String>();\n        // 键不能重复，值可以重复\n        map.put(\"san\", \"张三\");\n        map.put(\"si\", \"李四\");\n        map.put(\"wu\", \"王五\");\n        map.put(\"wang\", \"老王\");\n        map.put(\"wang\", \"老王2\");// 老王被覆盖\n        map.put(\"lao\", \"老王\");\n        System.out.println(\"-------直接输出hashmap:-------\");\n        System.out.println(map);\n        /**\n         * 遍历HashMap\n         */\n        // 1.获取Map中的所有键\n        System.out.println(\"-------foreach获取Map中所有的键:------\");\n        Set<String> keys = map.keySet();\n        for (String key : keys) {\n            System.out.print(key+\"  \");\n        }\n        System.out.println();//换行\n        // 2.获取Map中所有值\n        System.out.println(\"-------foreach获取Map中所有的值:------\");\n        Collection<String> values = map.values();\n        for (String value : values) {\n            System.out.print(value+\"  \");\n        }\n        System.out.println();//换行\n        // 3.得到key的值的同时得到key所对应的值\n        System.out.println(\"-------得到key的值的同时得到key所对应的值:-------\");\n        Set<String> keys2 = map.keySet();\n        for (String key : keys2) {\n            System.out.print(key + \"：\" + map.get(key)+\"   \");\n\n        }\n        /**\n         * 如果既要遍历key又要value，那么建议这种方式，因为如果先获取keySet然后再执行map.get(key)，map内部会执行两次遍历。\n         * 一次是在获取keySet的时候，一次是在遍历所有key的时候。\n         */\n        // 当我调用put(key,value)方法的时候，首先会把key和value封装到\n        // Entry这个静态内部类对象中，把Entry对象再添加到数组中，所以我们想获取\n        // map中的所有键值对，我们只要获取数组中的所有Entry对象，接下来\n        // 调用Entry对象中的getKey()和getValue()方法就能获取键值对了\n        Set<java.util.Map.Entry<String, String>> entrys = map.entrySet();\n        for (java.util.Map.Entry<String, String> entry : entrys) {\n            System.out.println(entry.getKey() + \"--\" + entry.getValue());\n        }\n\n        /**\n         * HashMap其他常用方法\n         */\n        System.out.println(\"after map.size()：\"+map.size());\n        System.out.println(\"after map.isEmpty()：\"+map.isEmpty());\n        System.out.println(map.remove(\"san\"));\n        System.out.println(\"after map.remove()：\"+map);\n        System.out.println(\"after map.get(si)：\"+map.get(\"si\"));\n        System.out.println(\"after map.containsKey(si)：\"+map.containsKey(\"si\"));\n        System.out.println(\"after containsValue(李四)：\"+map.containsValue(\"李四\"));\n        System.out.println(map.replace(\"si\", \"李四2\"));\n        System.out.println(\"after map.replace(si, 李四2):\"+map);\n    }\n\n}\n```\n\n### HashMap常见问题\n\n#### HashMap的长度为什么是2的幂次方\n\n答：HashMap的默认长度为16和规定数组长度为2的幂,是为了降低hash碰撞的几率\n\n为了能让HashMap存取高效，尽量较少碰撞，也就是要尽量把数据分配均匀。我们上面也讲到了过了，Hash值的范围值-2147483648到2147483647，前后加起来大概40亿的映射空间，只要哈希函数映射得比较均匀松散，一般应用是很难出现碰撞的。但问题是一个40亿长度的数组，内存是放不下的。所以这个散列值是不能直接拿来用的。用之前还要先做对数组的长度取模运算，得到的余数才能用来要存放的位置也就是对应的数组下标。这个数组下标的计算方法是(n - 1) & hash。（n代表数组长度）。这也就解释了HashMap的长度为什么是2的幂次方。\n\n**这个算法应该如何设计呢？**\n\n我们首先可能会想到采用%取余的操作来实现。但是，重点来了：\n\n> “取余(%)操作中如果除数是2的幂次则等价于与其除数减一的与(&)操作（也就是说hash%length==hash&(length-1)的前提是length是2的n次方；）”。并且采用二进制位操作&，相对于%能够提高运算效率，这就解释了HashMap的长度为什么是2的幂次方。\n\n#### 加载因子为什么0.75，而不是其他值？\n答：可以说是一个经过考量的经验值。加载因子涉及扩容，下次扩容的阈值=数组桶的大小\\*加载因子，如果加载因子太小，这就会导致阈值太小，这就会导致比较容易发生扩容。如果加载因子太大，那就会导致阈值太大，可能冲突会很多，导致查找效率下降。负载因子为什么是0.75，如果负载因子为0.5甚至更低的可能的话，最后得到的临时阈值明显会很小，这样的情况就会造成分配的内存的浪费，存在多余的没用的内存空间，也不满足了哈希表均匀分布的情况。如果负载因子达到了1的情况，也就是Entry数组存满了才发生扩容，这样会出现大量的哈希冲突的情况，出现链表过长，因此造成get查询数据的效率。因此选择了0.5~1的折中数也就是0.75，均衡解决了上面出现的情况。\n> [面试官竟然问我为啥HashMap的负载因子不设置成1！](https://mp.weixin.qq.com/s/kbLASf0lcF4PDJ3qBsFyUg)\n> [面试官：为什么HashMap的加载因子是0.75？](https://mp.weixin.qq.com/s/ZxwU2qSXvdEZVAIbY_5EPQ)\n\n#### 为什么不能将实数作为HashMap的key？\n\n答：java中浮点数的表示比较复杂，特别是牵涉到-0.0,NaN,正负无穷这种，所以不适宜用来作为Map的key。\n\n#### HashMap多线程操作导致死循环问题\n\n答：准确的讲应该是JDK1.7的HashMap链表会有死循环的可能，因为JDK1.7是采用的头插法，在多线程环境下有可能会使链表形成环状，从而导致死循环。JDK1.8做了改进，用的是尾插法，不会产生死循环。\n我们从`put()`方法开始，最终找到线程不安全的那个方法。这里省略中间不重要的过程，我只把方法的跳转流程贴出来：\n> //添加元素方法->添加新节点方法->扩容方法->把原数组元素重新分配到新数组中\n> put()-->addEntry()-->resize()-->transfer()\n\n现在，有两个线程都执行transfer方法。每个线程都会在它们自己的工作内存生成一个newTable的数组，用于存储变化后的链表，它们互不影响（这里互不影响，指的是两个新数组本身互不影响）。但是，需要注意的是，它们操作的数据却是同一份。一番扩容操作后出现环形链表，这时，有的同学可能就会问了，就算他们成环了，又怎样，跟死循环有什么关系？我们看下get()方法（最终调用getEntry方法），可以看到查找元素时，只要e不为空，就会一直循环查找下去。若有某个元素C的hash值也落在了和A，B元素同一个桶中，则会由于，A，B互相指向，e.next永远不为空，就会形成死循环。主要原因在于并发下的Rehash会造成元素之间会形成一个循环链表。不过，jdk1.8后解决了这个问题，但是还是不建议在多线程下使用HashMap,因为多线程下使用HashMap还是会存在其他问题比如数据丢失。并发环境下推荐使用ConcurrentHashMap。\n\n> [JAVA HASHMAP的死循环](https://coolshell.cn/articles/9606.html)\n> [美团面试题：HashMap是如何形成死循环的？（最完整的配图讲解）](https://mp.weixin.qq.com/s/5FdDjDo5H-nDZhFxo7H6QQ)\n> [多线程环境下，HashMap为什么会出现死循环？](https://mp.weixin.qq.com/s/gAw9K6yd-w9ZyP90xyvTwg)\n\n#### HashMap有哪几种常见的遍历方式?\n\n- [HashMap的7种遍历方式与性能分析](https://mp.weixin.qq.com/s/zQBN3UvJDhRTKP6SzcZFKw)\n\n> [原文链接](https://javaguide.cn/java/collection/java-collection-questions-02.html)\n\n### HashMap相关文章\n\n- [面试官再问你HashMap底层原理，就把这篇文章甩给他看](https://mp.weixin.qq.com/s/8Nl9dv_ywubW7Wc45--pgw)\n- [HashMap面试21问，这次要跪了！](https://mp.weixin.qq.com/s/WyPnPAKZfA58eX7qSBcP8Q)\n- [为什么HashMap不能一边遍历一边删除](https://mp.weixin.qq.com/s/JZiSfe7IIIJCstTucH5tlw)\n\n\n## ConcurrentHashMap\n\njdk1.7 ConcurrentHashMap类所采用的是分段锁的思想，将HashMap进行切割，把HashMap中的哈希数组切分成小数组，这个小数组名叫Segment,每个Segment有n个HashEntry组成，其中Segment继承自ReentrantLock（可重入锁）,在操作的时候给Segment赋予了一个对象锁，从而保证多线程环境下并发操作安全。\n\nJDK1.8对HashMap做了改造，当冲突链表长度大于8时，会将链表转变成红黑树结构,JDK1.8中ConcurrentHashMap类取消了Segment分段锁，采用CAS+synchronized来保证并发安全，数据结构跟jdk1.8中HashMap结构类似，都是数组+链表（当链表长度大于8时，链表结构转为红黑二叉树）结构。ConcurrentHashMap中synchronized只锁定当前链表或红黑二叉树的首节点，只要节点hash不冲突，就不会产生并发，相比JDK1.7的ConcurrentHashMap效率又提升了N倍！\n\n### ConcurrentHashMap源码&底层数据结构分析\n\n#### ConcurrentHashMap 1.7\n\n##### 1.存储结构\n\n![Java7 ConcurrentHashMap存储结构](https://guide-blog-images.oss-cn-shenzhen.aliyuncs.com/github/javaguide/java/collection/java7_concurrenthashmap.png)\n\nJava7中ConcurrentHashMap的存储结构如上图，ConcurrnetHashMap由很多个Segment组合，而每一个Segment是一个类似于HashMap的结构，所以每一个HashMap的内部可以进行扩容。但是Segment的个数一旦**初始化就不能改变**，默认Segment的个数是16个，你也可以认为ConcurrentHashMap默认支持最多16个线程并发。\n\n##### 2.初始化\n\n通过ConcurrentHashMap的无参构造探寻ConcurrentHashMap的初始化流程。\n\n```java\n/**\n  * Creates a new, empty map with a default initial capacity (16),\n  * load factor (0.75) and concurrencyLevel (16).\n  */\npublic ConcurrentHashMap() {\n    this(DEFAULT_INITIAL_CAPACITY, DEFAULT_LOAD_FACTOR, DEFAULT_CONCURRENCY_LEVEL);\n}\n```\n\n无参构造中调用了有参构造，传入了三个参数的默认值，他们的值是。\n\n```java\n/**\n  * 默认初始化容量\n  */\nstatic final int DEFAULT_INITIAL_CAPACITY = 16;\n\n/**\n  * 默认负载因子\n  */\nstatic final float DEFAULT_LOAD_FACTOR = 0.75f;\n\n/**\n  * 默认并发级别\n  */\nstatic final int DEFAULT_CONCURRENCY_LEVEL = 16;\n```\n\n接着看下这个有参构造函数的内部实现逻辑。\n\n```java\n@SuppressWarnings(\"unchecked\")\npublic ConcurrentHashMap(int initialCapacity,float loadFactor, int concurrencyLevel) {\n    // 参数校验\n    if (!(loadFactor > 0) || initialCapacity < 0 || concurrencyLevel <= 0)\n        throw new IllegalArgumentException();\n    // 校验并发级别大小，大于1<<16，重置为65536\n    if (concurrencyLevel > MAX_SEGMENTS)\n        concurrencyLevel = MAX_SEGMENTS;\n    // Find power-of-two sizes best matching arguments\n    // 2的多少次方\n    int sshift = 0;\n    int ssize = 1;\n    // 这个循环可以找到concurrencyLevel之上最近的2的次方值\n    while (ssize < concurrencyLevel) {\n        ++sshift;\n        ssize <<= 1;\n    }\n    // 记录段偏移量\n    this.segmentShift = 32 - sshift;\n    // 记录段掩码\n    this.segmentMask = ssize - 1;\n    // 设置容量\n    if (initialCapacity > MAXIMUM_CAPACITY)\n        initialCapacity = MAXIMUM_CAPACITY;\n    // c=容量/ssize，默认16/16=1，这里是计算每个Segment中的类似于HashMap的容量\n    int c = initialCapacity / ssize;\n    if (c * ssize < initialCapacity)\n        ++c;\n    int cap = MIN_SEGMENT_TABLE_CAPACITY;\n    //Segment中的类似于HashMap的容量至少是2或者2的倍数\n    while (cap < c)\n        cap <<= 1;\n    // create segments and segments[0]\n    // 创建Segment数组，设置segments[0]\n    Segment<K,V> s0 = new Segment<K,V>(loadFactor, (int)(cap * loadFactor),\n                         (HashEntry<K,V>[])new HashEntry[cap]);\n    Segment<K,V>[] ss = (Segment<K,V>[])new Segment[ssize];\n    UNSAFE.putOrderedObject(ss, SBASE, s0); // ordered write of segments[0]\n    this.segments = ss;\n}\n```\n\n总结一下在Java7中ConcurrnetHashMap的初始化逻辑。\n\n1. 必要参数校验。\n2. 校验并发级别concurrencyLevel大小，如果大于最大值，重置为最大值。无参构造**默认值是16**。\n3. 寻找并发级别concurrencyLevel之上最近的**2的幂次方**值，作为初始化容量大小，**默认是16**。\n4. 记录segmentShift偏移量，这个值为【容量=2的N次方】中的N，在后面Put时计算位置时会用到。**默认是32-sshift=28**。\n5. 记录segmentMask，默认是ssize-1=16-1=15.\n6. **初始化segments[0]**，**默认大小为2**，**负载因子0.75**，**扩容阀值是2\\*0.75=1.5**，插入第二个值时才会进行扩容。\n\n##### 3.put\n\n接着上面的初始化参数继续查看put方法源码。\n\n```java\n/**\n * Maps the specified key to the specified value in this table.\n * Neither the key nor the value can be null.\n *\n * <p> The value can be retrieved by calling the <tt>get</tt> method\n * with a key that is equal to the original key.\n *\n * @param key key with which the specified value is to be associated\n * @param value value to be associated with the specified key\n * @return the previous value associated with <tt>key</tt>, or\n *         <tt>null</tt> if there was no mapping for <tt>key</tt>\n * @throws NullPointerException if the specified key or value is null\n */\npublic V put(K key, V value) {\n    Segment<K,V> s;\n    if (value == null)\n        throw new NullPointerException();\n    int hash = hash(key);\n    // hash值无符号右移28位（初始化时获得），然后与segmentMask=15做与运算\n    // 其实也就是把高4位与segmentMask（1111）做与运算\n    int j = (hash >>> segmentShift) & segmentMask;\n    if ((s = (Segment<K,V>)UNSAFE.getObject          // nonvolatile; recheck\n         (segments, (j << SSHIFT) + SBASE)) == null) //  in ensureSegment\n        // 如果查找到的Segment为空，初始化\n        s = ensureSegment(j);\n    return s.put(key, hash, value, false);\n}\n\n/**\n * Returns the segment for the given index, creating it and\n * recording in segment table (via CAS) if not already present.\n *\n * @param k the index\n * @return the segment\n */\n@SuppressWarnings(\"unchecked\")\nprivate Segment<K,V> ensureSegment(int k) {\n    final Segment<K,V>[] ss = this.segments;\n    long u = (k << SSHIFT) + SBASE; // raw offset\n    Segment<K,V> seg;\n    // 判断u位置的Segment是否为null\n    if ((seg = (Segment<K,V>)UNSAFE.getObjectVolatile(ss, u)) == null) {\n        Segment<K,V> proto = ss[0]; // use segment 0 as prototype\n        // 获取0号segment里的HashEntry<K,V>初始化长度\n        int cap = proto.table.length;\n        // 获取0号segment里的hash表里的扩容负载因子，所有的segment的loadFactor是相同的\n        float lf = proto.loadFactor;\n        // 计算扩容阀值\n        int threshold = (int)(cap * lf);\n        // 创建一个cap容量的HashEntry数组\n        HashEntry<K,V>[] tab = (HashEntry<K,V>[])new HashEntry[cap];\n        if ((seg = (Segment<K,V>)UNSAFE.getObjectVolatile(ss, u)) == null) { // recheck\n            // 再次检查u位置的Segment是否为null，因为这时可能有其他线程进行了操作\n            Segment<K,V> s = new Segment<K,V>(lf, threshold, tab);\n            // 自旋检查u位置的Segment是否为null\n            while ((seg = (Segment<K,V>)UNSAFE.getObjectVolatile(ss, u))\n                   == null) {\n                // 使用CAS赋值，只会成功一次\n                if (UNSAFE.compareAndSwapObject(ss, u, null, seg = s))\n                    break;\n            }\n        }\n    }\n    return seg;\n}\n```\n\n上面的源码分析了ConcurrentHashMap在put一个数据时的处理流程，下面梳理下具体流程。\n\n1. 计算要put的key的位置，获取指定位置的Segment。\n\n2. 如果指定位置的Segment为空，则初始化这个Segment。**初始化Segment流程**：\n\n   1. 检查计算得到的位置的Segment是否为null.\n   2. 为null继续初始化，使用Segment[0]的容量和负载因子创建一个HashEntry数组。\n   3. 再次检查计算得到的指定位置的Segment是否为null.\n   4. 使用创建的HashEntry数组初始化这个Segment.\n   5. 自旋判断计算得到的指定位置的Segment是否为null，使用CAS在这个位置赋值为Segment.\n   \n3. Segment.put插入key,value值。\n\n上面探究了获取Segment段和初始化Segment段的操作。最后一行的Segment的put方法还没有查看，继续分析。\n\n```java\nfinal V put(K key, int hash, V value, boolean onlyIfAbsent) {\n    // 获取ReentrantLock独占锁，获取不到，scanAndLockForPut获取。\n    HashEntry<K,V> node = tryLock() ? null : scanAndLockForPut(key, hash, value);\n    V oldValue;\n    try {\n        HashEntry<K,V>[] tab = table;\n        // 计算要put的数据位置\n        int index = (tab.length - 1) & hash;\n        // CAS获取index坐标的值\n        HashEntry<K,V> first = entryAt(tab, index);\n        for (HashEntry<K,V> e = first;;) {\n            if (e != null) {\n                // 检查是否key已经存在，如果存在，则遍历链表寻找位置，找到后替换value\n                K k;\n                if ((k = e.key) == key ||\n                    (e.hash == hash && key.equals(k))) {\n                    oldValue = e.value;\n                    if (!onlyIfAbsent) {\n                        e.value = value;\n                        ++modCount;\n                    }\n                    break;\n                }\n                e = e.next;\n            }\n            else {\n                // first有值没说明index位置已经有值了，有冲突，链表头插法。\n                if (node != null)\n                    node.setNext(first);\n                else\n                    node = new HashEntry<K,V>(hash, key, value, first);\n                int c = count + 1;\n                // 容量大于扩容阀值，小于最大容量，进行扩容\n                if (c > threshold && tab.length < MAXIMUM_CAPACITY)\n                    rehash(node);\n                else\n                    // index位置赋值node，node可能是一个元素，也可能是一个链表的表头\n                    setEntryAt(tab, index, node);\n                ++modCount;\n                count = c;\n                oldValue = null;\n                break;\n            }\n        }\n    } finally {\n        unlock();\n    }\n    return oldValue;\n}\n```\n\n由于Segment继承了ReentrantLock，所以Segment内部可以很方便的获取锁，put流程就用到了这个功能。\n\n1. tryLock()获取锁，获取不到使用**scanAndLockForPut**方法继续获取。\n2. 计算put的数据要放入的index位置，然后获取这个位置上的HashEntry。\n3. 遍历put新元素，为什么要遍历？因为这里获取的HashEntry可能是一个空元素，也可能是链表已存在，所以要区别对待。如果这个位置上的**HashEntry不存在**：\n\n   1. 如果当前容量大于扩容阀值，小于最大容量，**进行扩容**。\n   2. 直接头插法插入。\n   \n   如果这个位置上的**HashEntry存在**：\n\n   1. 判断链表当前元素key和hash值是否和要put的key和hash值一致。一致则替换值\n   2. 不一致，获取链表下一个节点，直到发现相同进行值替换，或者链表表里完毕没有相同的。\n      1. 如果当前容量大于扩容阀值，小于最大容量，**进行扩容**。\n      2. 直接链表头插法插入。\n   \n4. 如果要插入的位置之前已经存在，替换后返回旧值，否则返回null。\n\n这里面的第一步中的scanAndLockForPut操作这里没有介绍，这个方法做的操作就是不断的自旋tryLock()获取锁。当自旋次数大于指定次数时，使用lock()阻塞获取锁。在自旋时顺表获取下hash位置的HashEntry。\n\n```java\nprivate HashEntry<K,V> scanAndLockForPut(K key, int hash, V value) {\n    HashEntry<K,V> first = entryForHash(this, hash);\n    HashEntry<K,V> e = first;\n    HashEntry<K,V> node = null;\n    int retries = -1; // negative while locating node\n    // 自旋获取锁\n    while (!tryLock()) {\n        HashEntry<K,V> f; // to recheck first below\n        if (retries < 0) {\n            if (e == null) {\n                if (node == null) // speculatively create node\n                    node = new HashEntry<K,V>(hash, key, value, null);\n                retries = 0;\n            }\n            else if (key.equals(e.key))\n                retries = 0;\n            else\n                e = e.next;\n        }\n        else if (++retries > MAX_SCAN_RETRIES) {\n            // 自旋达到指定次数后，阻塞等到只到获取到锁\n            lock();\n            break;\n        }\n        else if ((retries & 1) == 0 &&\n                 (f = entryForHash(this, hash)) != first) {\n            e = first = f; // re-traverse if entry changed\n            retries = -1;\n        }\n    }\n    return node;\n}\n```\n\n##### 4.扩容rehash\n\nConcurrentHashMap的扩容只会扩容到原来的两倍。老数组里的数据移动到新的数组时，位置要么不变，要么变为index+oldSize，参数里的node会在扩容之后使用链表**头插法**插入到指定位置。\n\n```java\nprivate void rehash(HashEntry<K,V> node) {\n    HashEntry<K,V>[] oldTable = table;\n    // 老容量\n    int oldCapacity = oldTable.length;\n    // 新容量，扩大两倍\n    int newCapacity = oldCapacity << 1;\n    // 新的扩容阀值 \n    threshold = (int)(newCapacity * loadFactor);\n    // 创建新的数组\n    HashEntry<K,V>[] newTable = (HashEntry<K,V>[]) new HashEntry[newCapacity];\n    // 新的掩码，默认2扩容后是4，-1是3，二进制就是11。\n    int sizeMask = newCapacity - 1;\n    for (int i = 0; i < oldCapacity ; i++) {\n        // 遍历老数组\n        HashEntry<K,V> e = oldTable[i];\n        if (e != null) {\n            HashEntry<K,V> next = e.next;\n            // 计算新的位置，新的位置只可能是不便或者是老的位置+老的容量。\n            int idx = e.hash & sizeMask;\n            if (next == null)   //  Single node on list\n                // 如果当前位置还不是链表，只是一个元素，直接赋值\n                newTable[idx] = e;\n            else { // Reuse consecutive sequence at same slot\n                // 如果是链表了\n                HashEntry<K,V> lastRun = e;\n                int lastIdx = idx;\n                // 新的位置只可能是不便或者是老的位置+老的容量。\n                // 遍历结束后，lastRun后面的元素位置都是相同的\n                for (HashEntry<K,V> last = next; last != null; last = last.next) {\n                    int k = last.hash & sizeMask;\n                    if (k != lastIdx) {\n                        lastIdx = k;\n                        lastRun = last;\n                    }\n                }\n                // lastRun后面的元素位置都是相同的，直接作为链表赋值到新位置。\n                newTable[lastIdx] = lastRun;\n                // Clone remaining nodes\n                for (HashEntry<K,V> p = e; p != lastRun; p = p.next) {\n                    // 遍历剩余元素，头插法到指定k位置。\n                    V v = p.value;\n                    int h = p.hash;\n                    int k = h & sizeMask;\n                    HashEntry<K,V> n = newTable[k];\n                    newTable[k] = new HashEntry<K,V>(h, p.key, v, n);\n                }\n            }\n        }\n    }\n    // 头插法插入新的节点\n    int nodeIndex = node.hash & sizeMask; // add the new node\n    node.setNext(newTable[nodeIndex]);\n    newTable[nodeIndex] = node;\n    table = newTable;\n}\n```\n\n有些同学可能会对最后的两个for循环有疑惑，这里第一个for是为了寻找这样一个节点，这个节点后面的所有next节点的新位置都是相同的。然后把这个作为一个链表赋值到新位置。第二个for循环是为了把剩余的元素通过头插法插入到指定位置链表。这样实现的原因可能是基于概率统计，有深入研究的同学可以发表下意见。\n\n##### 5.get\n\n到这里就很简单了，get方法只需要两步即可。\n\n1. 计算得到key的存放位置。\n2. 遍历指定位置查找相同key的value值。\n\n```java\npublic V get(Object key) {\n    Segment<K,V> s; // manually integrate access methods to reduce overhead\n    HashEntry<K,V>[] tab;\n    int h = hash(key);\n    long u = (((h >>> segmentShift) & segmentMask) << SSHIFT) + SBASE;\n    // 计算得到key的存放位置\n    if ((s = (Segment<K,V>)UNSAFE.getObjectVolatile(segments, u)) != null &&\n        (tab = s.table) != null) {\n        for (HashEntry<K,V> e = (HashEntry<K,V>) UNSAFE.getObjectVolatile\n                 (tab, ((long)(((tab.length - 1) & h)) << TSHIFT) + TBASE);\n             e != null; e = e.next) {\n            // 如果是链表，遍历查找到相同key的value。\n            K k;\n            if ((k = e.key) == key || (e.hash == h && key.equals(k)))\n                return e.value;\n        }\n    }\n    return null;\n}\n```\n\n#### ConcurrentHashMap 1.8\n\n##### 1.存储结构\n\n![Java8 ConcurrentHashMap存储结构（图片来自javadoop）](https://guide-blog-images.oss-cn-shenzhen.aliyuncs.com/github/javaguide/java/collection/java8_concurrenthashmap.png)\n\n可以发现Java8的ConcurrentHashMap相对于Java7来说变化比较大，不再是之前的**Segment数组+HashEntry数组+链表**，而是**Node数组+链表/红黑树**。当冲突链表达到一定长度时，链表会转换成红黑树。\n\n##### 2.初始化initTable\n\n\n```java\n/**\n * Initializes table, using the size recorded in sizeCtl.\n */\nprivate final Node<K,V>[] initTable() {\n    Node<K,V>[] tab; int sc;\n    while ((tab = table) == null || tab.length == 0) {\n        //　如果sizeCtl < 0,说明另外的线程执行CAS成功，正在进行初始化。\n        if ((sc = sizeCtl) < 0)\n            // 让出CPU使用权\n            Thread.yield(); // lost initialization race; just spin\n        else if (U.compareAndSwapInt(this, SIZECTL, sc, -1)) {\n            try {\n                if ((tab = table) == null || tab.length == 0) {\n                    int n = (sc > 0) ? sc : DEFAULT_CAPACITY;\n                    @SuppressWarnings(\"unchecked\")\n                    Node<K,V>[] nt = (Node<K,V>[])new Node<?,?>[n];\n                    table = tab = nt;\n                    sc = n - (n >>> 2);\n                }\n            } finally {\n                sizeCtl = sc;\n            }\n            break;\n        }\n    }\n    return tab;\n}\n```\n\n从源码中可以发现ConcurrentHashMap的初始化是通过**自旋和CAS**操作完成的。里面需要注意的是变量sizeCtl，它的值决定着当前的初始化状态。\n\n1. -1说明正在初始化\n2. -N说明有N-1个线程正在进行扩容\n3. 0表示table初始化大小，如果table没有初始化\n4. \\>0表示table扩容的阈值，如果table已经初始化。\n\n##### 3.put\n\n直接过一遍put源码。\n\n```java\npublic V put(K key, V value) {\n    return putVal(key, value, false);\n}\n\n/** Implementation for put and putIfAbsent */\nfinal V putVal(K key, V value, boolean onlyIfAbsent) {\n    // key和value不能为空\n    if (key == null || value == null) throw new NullPointerException();\n    int hash = spread(key.hashCode());\n    int binCount = 0;\n    for (Node<K,V>[] tab = table;;) {\n        // f=目标位置元素\n        Node<K,V> f; int n, i, fh;// fh后面存放目标位置的元素hash值\n        if (tab == null || (n = tab.length) == 0)\n            // 数组桶为空，初始化数组桶（自旋+CAS)\n            tab = initTable();\n        else if ((f = tabAt(tab, i = (n - 1) & hash)) == null) {\n            // 桶内为空，CAS放入，不加锁，成功了就直接break跳出\n            if (casTabAt(tab, i, null,new Node<K,V>(hash, key, value, null)))\n                break;  // no lock when adding to empty bin\n        }\n        else if ((fh = f.hash) == MOVED)\n            tab = helpTransfer(tab, f);\n        else {\n            V oldVal = null;\n            // 使用synchronized加锁加入节点\n            synchronized (f) {\n                if (tabAt(tab, i) == f) {\n                    // 说明是链表\n                    if (fh >= 0) {\n                        binCount = 1;\n                        // 循环加入新的或者覆盖节点\n                        for (Node<K,V> e = f;; ++binCount) {\n                            K ek;\n                            if (e.hash == hash &&\n                                ((ek = e.key) == key ||\n                                 (ek != null && key.equals(ek)))) {\n                                oldVal = e.val;\n                                if (!onlyIfAbsent)\n                                    e.val = value;\n                                break;\n                            }\n                            Node<K,V> pred = e;\n                            if ((e = e.next) == null) {\n                                pred.next = new Node<K,V>(hash, key,\n                                                          value, null);\n                                break;\n                            }\n                        }\n                    }\n                    else if (f instanceof TreeBin) {\n                        // 红黑树\n                        Node<K,V> p;\n                        binCount = 2;\n                        if ((p = ((TreeBin<K,V>)f).putTreeVal(hash, key,\n                                                       value)) != null) {\n                            oldVal = p.val;\n                            if (!onlyIfAbsent)\n                                p.val = value;\n                        }\n                    }\n                }\n            }\n            if (binCount != 0) {\n                if (binCount >= TREEIFY_THRESHOLD)\n                    treeifyBin(tab, i);\n                if (oldVal != null)\n                    return oldVal;\n                break;\n            }\n        }\n    }\n    addCount(1L, binCount);\n    return null;\n}\n```\n\n1. 根据key计算出hashcode。\n2. 判断是否需要进行初始化。\n3. 即为当前key定位出的Node，如果为空表示当前位置可以写入数据，利用CAS尝试写入，失败则自旋保证成功。\n4. 如果当前位置的hashcode==MOVED==-1,则需要进行扩容。\n5. 如果都不满足，则利用synchronized锁写入数据。\n6. 如果数量大于TREEIFY_THRESHOLD则要执行树化方法，在treeifyBin中会首先判断当前数组长度≥64时才会将链表转换为红黑树。\n\n##### 4.get\n\nget流程比较简单，直接过一遍源码。\n\n```java\npublic V get(Object key) {\n    Node<K,V>[] tab; Node<K,V> e, p; int n, eh; K ek;\n    // key所在的hash位置\n    int h = spread(key.hashCode());\n    if ((tab = table) != null && (n = tab.length) > 0 &&\n        (e = tabAt(tab, (n - 1) & h)) != null) {\n        // 如果指定位置元素存在，头结点hash值相同\n        if ((eh = e.hash) == h) {\n            if ((ek = e.key) == key || (ek != null && key.equals(ek)))\n                // key hash值相等，key值相同，直接返回元素value\n                return e.val;\n        }\n        else if (eh < 0)\n            // 头结点hash值小于0，说明正在扩容或者是红黑树，find查找\n            return (p = e.find(h, key)) != null ? p.val : null;\n        while ((e = e.next) != null) {\n            // 是链表，遍历查找\n            if (e.hash == h &&\n                ((ek = e.key) == key || (ek != null && key.equals(ek))))\n                return e.val;\n        }\n    }\n    return null;\n}\n```\n\n总结一下get过程：\n\n1. 根据hash值计算位置。\n2. 查找到指定位置，如果头节点就是要找的，直接返回它的value.\n3. 如果头节点hash值小于0，说明正在扩容或者是红黑树，查找之。\n4. 如果是链表，遍历查找之。\n\n总的来说ConcurrentHashMap在Java8中相对于Java7来说变化还是挺大的。\n\n#### 总结\n\nJava7中ConcurrentHashMap使用的分段锁，也就是每一个Segment上同时只有一个线程可以操作，每一个Segment都是一个类似HashMap数组的结构，它可以扩容，它的冲突会转化为链表。但是Segment的个数一但初始化就不能改变。\n\nJava8中的ConcurrentHashMap使用的Synchronized锁加CAS的机制。结构也由Java7中的**Segment数组+HashEntry数组+链表**进化成了**Node数组+链表/红黑树**，Node是类似于一个HashEntry的结构。它的冲突再达到一定大小时会转化成红黑树，在冲突小于一定数量时又退回链表。\n\n有些同学可能对Synchronized的性能存在疑问，其实Synchronized锁自从引入锁升级策略后，性能不再是问题，有兴趣的同学可以自己了解下Synchronized的锁升级。\n\n\n### ConcurrentHashMap和Hashtable的区别\n\nConcurrentHashMap和Hashtable的区别主要体现在实现线程安全的方式上不同。\n\n- **底层数据结构**：JDK1.7的ConcurrentHashMap底层采用**分段的数组+链表**实现，JDK1.8采用的数据结构跟HashMap1.8的结构一样，数组+链表/红黑二叉树。Hashtable和JDK1.8之前的HashMap的底层数据结构类似都是采用**数组+链表**的形式，数组是HashMap的主体，链表则是主要为了解决哈希冲突而存在的；\n- **实现线程安全的方式（重要）**：\n  - 在JDK1.7的时候，ConcurrentHashMap对整个桶数组进行了分割分段(Segment，分段锁)，每一把锁只锁容器其中一部分数据（下面有示意图），多线程访问容器里不同数据段的数据，就不会存在锁竞争，提高并发访问率。\n  - 到了JDK1.8的时候，ConcurrentHashMap已经摒弃了Segment的概念，而是直接用Node数组+链表+红黑树的数据结构来实现，并发控制使用synchronized和CAS来操作。（JDK1.6以后synchronized锁做了很多优化）整个看起来就像是优化过且线程安全的HashMap，虽然在JDK1.8中还能看到Segment的数据结构，但是已经简化了属性，只是为了兼容旧版本；\n  - Hashtable(同一把锁):使用synchronized来保证线程安全，效率非常低下。当一个线程访问同步方法时，其他线程也访问同步方法，可能会进入阻塞或轮询状态，如使用put添加元素，另一个线程不能使用put添加元素，也不能使用get，竞争会越来越激烈效率越低。\n\n下面，我们再来看看两者底层数据结构的对比图。\n\n**Hashtable**:\n\n![Hashtable的内部结构](https://guide-blog-images.oss-cn-shenzhen.aliyuncs.com/github/javaguide/java/collection/jdk1.7_hashmap.png)\n\n**JDK1.7的ConcurrentHashMap**：\n\n![Java7 ConcurrentHashMap存储结构](https://guide-blog-images.oss-cn-shenzhen.aliyuncs.com/github/javaguide/java/collection/java7_concurrenthashmap.png)\n\nConcurrentHashMap是由Segment数组结构和HashEntry数组结构组成。\n\nSegment数组中的每个元素包含一个HashEntry数组，每个HashEntry数组属于链表结构。\n\n**JDK1.8的ConcurrentHashMap**：\n\n![Java8 ConcurrentHashMap存储结构](https://guide-blog-images.oss-cn-shenzhen.aliyuncs.com/github/javaguide/java/collection/java8_concurrenthashmap.png)\n\nJDK1.8的ConcurrentHashMap不再是**Segment数组+HashEntry数组+链表**，而是**Node数组+链表/红黑树**。不过，Node只能用于链表的情况，红黑树的情况需要使用**TreeNode**。当冲突链表达到一定长度时，链表会转换成红黑树。\n\nTreeNode是存储红黑树节点，被TreeBin包装。TreeBin通过root属性维护红黑树的根结点，因为红黑树在旋转的时候，根结点可能会被它原来的子节点替换掉，在这个时间点，如果有其他线程要写这棵红黑树就会发生线程不安全问题，所以在ConcurrentHashMap中TreeBin通过waiter属性维护当前使用这棵红黑树的线程，来防止其他线程的进入。\n\n```java\nstatic final class TreeBin<K,V> extends Node<K,V> {\n    TreeNode<K,V> root;\n    volatile TreeNode<K,V> first;\n    volatile Thread waiter;\n    volatile int lockState;\n    // values for lockState\n    static final int WRITER = 1; // set while holding write lock\n    static final int WAITER = 2; // set when waiting for write lock\n    static final int READER = 4; // increment value for setting read lock\n...\n}\n```\n\n### ConcurrentHashMap线程安全的具体实现方式/底层具体实现\n\n#### JDK1.8之前\n\n![Java7 ConcurrentHashMap存储结构](https://guide-blog-images.oss-cn-shenzhen.aliyuncs.com/github/javaguide/java/collection/java7_concurrenthashmap.png)\n\n首先将数据分为一段一段（这个“段”就是Segment）的存储，然后给每一段数据配一把锁，当一个线程占用锁访问其中一个段数据时，其他段的数据也能被其他线程访问。\n\n**ConcurrentHashMap是由Segment数组结构和HashEntry数组结构组成**。\n\nSegment继承了ReentrantLock,所以Segment是一种可重入锁，扮演锁的角色。HashEntry用于存储键值对数据。\n\n```java\nstatic class Segment<K,V> extends ReentrantLock implements Serializable {\n}\n```\n\n一个ConcurrentHashMap里包含一个Segment数组，Segment的个数一旦**初始化就不能改变**。Segment数组的大小默认是16，也就是说默认可以同时支持16个线程并发写。\n\nSegment的结构和HashMap类似，是一种数组和链表结构，一个Segment包含一个HashEntry数组，每个HashEntry是一个链表结构的元素，每个Segment守护着一个HashEntry数组里的元素，当对HashEntry数组的数据进行修改时，必须首先获得对应的Segment的锁。也就是说，对同一Segment的并发写入会被阻塞，不同Segment的写入是可以并发执行的。\n\n#### JDK1.8之后\n\n![Java8 ConcurrentHashMap存储结构](https://guide-blog-images.oss-cn-shenzhen.aliyuncs.com/github/javaguide/java/collection/java8_concurrenthashmap.png)\n\nJava8几乎完全重写了ConcurrentHashMap，代码量从原来Java7中的1000多行，变成了现在的6000多行。\n\n**ConcurrentHashMap取消了Segment分段锁，采用Node+CAS+synchronized来保证并发安全**。数据结构跟HashMap1.8的结构类似，数组+链表/红黑二叉树。Java8在链表长度超过一定阈值（8）时将链表（寻址时间复杂度为O(N)）转换为红黑树（寻址时间复杂度为O(log(N))）。\n\nJava8中，锁粒度更细，synchronized只锁定当前链表或红黑二叉树的首节点，这样只要hash不冲突，就不会产生并发，就不会影响其他Node的读写，效率大幅提升。\n\n### JDK1.7和JDK1.8的ConcurrentHashMap实现有什么不同？\n\n- **线程安全实现方式**：JDK1.7采用Segment分段锁来保证安全，Segment是继承自ReentrantLock。JDK1.8放弃了Segment分段锁的设计，采用Node+CAS+synchronized保证线程安全，锁粒度更细，synchronized只锁定当前链表或红黑二叉树的首节点。\n- **Hash碰撞解决方法**：JDK1.7采用拉链法，JDK1.8采用拉链法结合红黑树（链表长度超过一定阈值时，将链表转换为红黑树）。\n- **并发度**：JDK1.7最大并发度是Segment的个数，默认是16。JDK1.8最大并发度是Node数组的大小，并发度更大。\n\n\n### ConcurrentHashMap的哪些操作需要加锁？\n\n答：只有写入操作才需要加锁，读取操作不需要加锁\n\n### ConcurrentHashMap的无锁读是如何实现的？\n\n答：首先HashEntry中的value和next都是有volatile修饰的，其次在写入操作的时候通过调用UNSAFE库延迟同步了主存，保证了数据的一致性\n\n### 在多线程的场景下调用size()方法获取ConcurrentHashMap的大小有什么挑战？ConcurrentHashMap是怎么解决的？\n\n答：size()具有全局的语义，如何能保证在不加全局锁的情况下读取到全局状态的值是一个很大的挑战，ConcurrentHashMap通过查看两次无锁读中间是否发生了写入操作来决定读取到的size()是否可信，如果写入操作频繁，则再退化为全局加锁读取。\n\n### 在有Segment存在的前提下，是如何扩容的？\n\n答：segment数组的大小在一开始初始化的时候就已经决定了，扩容主要扩的是HashEntry数组，基本的思路与HashTable一致，但这是一个线程不安全方法，调用之前需要加锁。\n\n### 为什么JDK8舍弃掉了分段锁呢？\n\n1. 每个锁控制的是一段，当分段很多，并且加锁的分段不连续的时候，内存空间的浪费比较严重。\n2. 如果某个分段特别的大，那么就会影响效率，耽误时间。\n\n### HashMap可以存null，ConcurrentHashMap不可以，为什么？\n\n关于这个问题，其实最有发言权的就是ConcurrentHashMap的作者——Doug Lea\nConcurrentMap(如ConcurrentHashMap、ConcurrentSkipListMap)不允许使用null值的主要原因是在非并发的Map中(如HashMap)是可以容忍模糊性(二义性)的，而在并发Map中是无法容忍的。假如说，所有的Map都支持null的话，那么map.get(key)就可以返回null，但是这时候就会存在一个不确定性，当你拿到null的时候，你是不知道他是因为本来就存了一个null进去还是说就是因为没找到而返回了null。在HashMap中，因为它的设计就是给单线程用的，所以当我们map.get(key)返回null的时候，我们是可以通过map.contains(key)检查来进行检测的，如果它返回true，则认为是存了一个null，否则就是因为没找到而返回了null。但是像ConcurrentHashMap，它是为并发而生的，它是要用在并发场景中的，当我们map.get(key)返回null的时候，是没办法通过通过map.contains(key)检查来准确的检测，因为在检测过程中可能会被其他线程锁修改，而导致检测结果并不可靠。所以，为了让ConcurrentHashMap的语义更加准确，不存在二义性的问题，他就不支持null。\n\n### ConcurrentHashMap相关文章\n- [那些年你啃过的ConcurrentHashMap](https://mp.weixin.qq.com/s/ufoKhs4VRXhE8_PT2rXoeg)\n- [面试必问之ConcurrentHashMap线程安全的具体实现方式](https://mp.weixin.qq.com/s?__biz=MzkzODE3OTI0Ng==&mid=2247491116&idx=1&sn=30ee6196266dab2cbf46cf7f98d99120&source=41#wechat_redirect)\n- [ConcurrentHashMap是如何保证线程安全的](https://mp.weixin.qq.com/s/Hb7JCkMrri0VDiPcl8HnBQ)\n\n\n## 面试题\n\n### HashMap和Hashtable的区别\n\n- **线程是否安全**：HashMap是非线程安全的，Hashtable是线程安全的,因为Hashtable内部的方法基本都经过synchronized修饰。（如果你要保证线程安全的话就使用ConcurrentHashMap）；\n- **效率**：因为线程安全的问题，HashMap要比Hashtable效率高一点。另外，Hashtable基本被淘汰，不要在代码中使用它；\n- **对Null key和Null value的支持**：HashMap可以存储null的key和value，但null作为键只能有一个，null作为值可以有多个；Hashtable不允许有null键和null值，否则会抛出NullPointerException。\n- **初始容量大小和每次扩充容量大小的不同**：①创建时如果不指定容量初始值，Hashtable默认的初始大小为11，之后每次扩充，容量变为原来的2n+1。HashMap默认的初始化大小为16。之后每次扩充，容量变为原来的2倍。②创建时如果给定了容量初始值，那么Hashtable会直接使用你给定的大小，而HashMap会将其扩充为2的幂次方大小（HashMap中的tableSizeFor()方法保证，下面给出了源代码）。也就是说HashMap总是使用2的幂作为哈希表的大小,后面会介绍到为什么是2的幂次方。\n- **底层数据结构**：JDK1.8以后的HashMap在解决哈希冲突时有了较大的变化，当链表长度大于阈值（默认为8）时，将链表转化为红黑树（将链表转换成红黑树前会判断，如果当前数组的长度小于64，那么会选择先进行数组扩容，而不是转换为红黑树），以减少搜索时间（后文中我会结合源码对这一过程进行分析）。Hashtable没有这样的机制。\n\n**HashMap中带有初始容量的构造函数**：\n\n```java\npublic HashMap(int initialCapacity, float loadFactor) {\n    if (initialCapacity < 0)\n        throw new IllegalArgumentException(\"Illegal initial capacity: \" +\n                                           initialCapacity);\n    if (initialCapacity > MAXIMUM_CAPACITY)\n        initialCapacity = MAXIMUM_CAPACITY;\n    if (loadFactor <= 0 || Float.isNaN(loadFactor))\n        throw new IllegalArgumentException(\"Illegal load factor: \" +\n                                           loadFactor);\n    this.loadFactor = loadFactor;\n    this.threshold = tableSizeFor(initialCapacity);\n}\npublic HashMap(int initialCapacity) {\n    this(initialCapacity, DEFAULT_LOAD_FACTOR);\n}\n```\n下面这个方法保证了HashMap总是使用2的幂作为哈希表的大小。\n\n```java\n/**\n  * Returns a power of two size for the given target capacity.\n  */\nstatic final int tableSizeFor(int cap) {\n    int n = cap - 1;\n    n |= n >>> 1;\n    n |= n >>> 2;\n    n |= n >>> 4;\n    n |= n >>> 8;\n    n |= n >>> 16;\n    return (n < 0) ? 1 : (n >= MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : n + 1;\n}\n```\n\n### HashMap和HashSet区别\n\n如果你看过HashSet源码的话就应该知道：HashSet底层就是基于HashMap实现的。（HashSet的源码非常非常少，因为除了clone()、writeObject()、readObject()是HashSet自己不得不实现之外，其他方法都是直接调用HashMap中的方法。\n\n|               HashMap                |                          HashSet                           |\n| :------------------------------------: | :----------------------------------------------------------: |\n|           实现了Map接口            |                       实现Set接口                        |\n|               存储键值对               |                          仅存储对象                          |\n|     调用put()向map中添加元素      |             调用add()方法向Set中添加元素              |\n| HashMap使用键（Key）计算hashcode | HashSet使用成员对象来计算hashcode值，对于两个对象来说hashcode可能相同，所以equals()方法用来判断对象的相等性 |\n\n### HashMap和TreeMap区别\n\nTreeMap和HashMap都继承自**AbstractMap**，但是需要注意的是TreeMap它还实现了**NavigableMap**接口和**SortedMap**接口。\n\n![TreeMap继承关系图](https://guide-blog-images.oss-cn-shenzhen.aliyuncs.com/github/javaguide/java/collection/treemap_hierarchy.png)\n\n实现**NavigableMap**接口让TreeMap有了对集合内元素的搜索的能力。\n\n实现**SortedMap**接口让TreeMap有了对集合中的元素根据键排序的能力。默认是按key的升序排序，不过我们也可以指定排序的比较器。示例代码如下：\n```java\n/**\n * @author shuang.kou\n * @createTime 2020年06月15日 17:02:00\n */\npublic class Person {\n    private Integer age;\n\n    public Person(Integer age) {\n        this.age = age;\n    }\n\n    public Integer getAge() {\n        return age;\n    }\n\n\n    public static void main(String[] args) {\n        TreeMap<Person, String> treeMap = new TreeMap<>(new Comparator<Person>() {\n            @Override\n            public int compare(Person person1, Person person2) {\n                int num = person1.getAge() - person2.getAge();\n                return Integer.compare(num, 0);\n            }\n        });\n        treeMap.put(new Person(3), \"person1\");\n        treeMap.put(new Person(18), \"person2\");\n        treeMap.put(new Person(35), \"person3\");\n        treeMap.put(new Person(16), \"person4\");\n        treeMap.entrySet().stream().forEach(personStringEntry -> {\n            System.out.println(personStringEntry.getValue());\n        });\n    }\n}\n```\n\n输出:\n```text\nperson1\nperson4\nperson2\nperson3\n```\n\n可以看出，TreeMap中的元素已经是按照Person的age字段的升序来排列了。\n\n上面，我们是通过传入匿名内部类的方式实现的，你可以将代码替换成Lambda表达式实现的方式：\n\n```java\nTreeMap<Person, String> treeMap = new TreeMap<>((person1, person2) -> {\n  int num = person1.getAge() - person2.getAge();\n  return Integer.compare(num, 0);\n});\n```\n\n**综上，相比于HashMap来说TreeMap主要多了对集合中的元素根据键排序的能力以及对集合内元素的搜索的能力**。\n\n### HashSet如何检查重复?\n\n> 当你把对象加入HashSet时，HashSet会先计算对象的hashcode值来判断对象加入的位置，同时也会与其他加入的对象的hashcode值作比较，如果没有相符的hashcode，HashSet会假设对象没有重复出现。但是如果发现有相同hashcode值的对象，这时会调用equals()方法来检查hashcode相等的对象是否真的相同。如果两者相同，HashSet就不会让加入操作成功。\n\n在JDK1.8中，HashSet的add()方法只是简单的调用了HashMap的put()方法，并且判断了一下返回值以确保是否有重复元素。直接看一下HashSet中的源码：\n```java\n// Returns: true if this set did not already contain the specified element\n// 返回值：当set中没有包含add的元素时返回真\npublic boolean add(E e) {\n    return map.put(e, PRESENT)==null;\n}\n```\n\n而在HashMap的putVal()方法中也能看到如下说明：\n\n```java\n// Returns : previous value, or null if none\n// 返回值：如果插入位置没有元素返回null，否则返回上一个元素\nfinal V putVal(int hash, K key, V value, boolean onlyIfAbsent,\n                   boolean evict) {\n...\n}\n```\n\n也就是说，在JDK1.8中，实际上无论HashSet中是否已经存在了某元素，HashSet都会直接插入，只是会在add()方法的返回值处告诉我们插入前是否存在相同元素。","categories":["Java"]},{"title":"事务","slug":"事务","url":"/blog/posts/5894be866015/","content":"\n## 什么是事务？\n\n**事务是逻辑上的一组操作，要么都执行，要么都不执行**。我们系统的每个业务方法可能包括了多个原子性的数据库操作，比如下面的savePerson()方法中就有两个原子性的数据库操作。这些原子性的数据库操作是有依赖的，它们要么都执行，要不就都不执行。\n\n```java\npublic void savePerson() {\n    personDao.save(person);\n    personDetailDao.save(personDetail);\n}\n```\n\n另外，需要格外注意的是：**事务能否生效数据库引擎是否支持事务是关键。比如常用的MySQL数据库默认使用支持事务的innodb引擎。但是，如果把数据库引擎变为myisam，那么程序也就不再支持事务了！**\n\n事务最经典也经常被拿出来说例子就是转账了。假如小明要给小红转账1000元，这个转账会涉及到两个关键操作就是：\n\n> 1. 将小明的余额减少1000元。\n> 2. 将小红的余额增加1000元。\n\n万一在这两个操作之间突然出现错误比如银行系统崩溃或者网络故障，导致小明余额减少而小红的余额没有增加，这样就不对了。事务就是保证这两个关键操作要么都成功，要么都要失败。\n\n```java\npublic class OrdersService {\n\tprivate AccountDao accountDao;\n\n\tpublic void setOrdersDao(AccountDao accountDao) {\n\t\tthis.accountDao = accountDao;\n\t}\n\n  @Transactional(propagation = Propagation.REQUIRED,\n                isolation = Isolation.DEFAULT, readOnly = false, timeout = -1)\n\tpublic void accountMoney() {\n    \t//小红账户多1000\n\t\taccountDao.addMoney(1000,xiaohong);\n\t\t//模拟突然出现的异常，比如银行中可能为突然停电等等\n    \t//如果没有配置事务管理的话会造成，小红账户多了1000而小明账户没有少钱\n\t\tint i = 10 / 0;\n\t\t//小王账户少1000\n\t\taccountDao.reduceMoney(1000,xiaoming);\n\t}\n}\n```\n\n另外，数据库事务的ACID四大特性是事务的基础，下面简单来了解一下。\n\n## 事务的特性(ACID)了解么?\n\n- **原子性（Atomicity）**：一个事务（transaction）中的所有操作，或者全部完成，或者全部不完成，不会结束在中间某个环节。事务在执行过程中发生错误，会被回滚（Rollback）到事务开始前的状态，就像这个事务从来没有执行过一样。即，事务不可分割、不可约简。\n- **一致性（Consistency）**：在事务开始之前和事务结束以后，数据库的完整性没有被破坏。这表示写入的资料必须完全符合所有的预设约束、触发器、级联回滚等。\n- **隔离性（Isolation）**：数据库允许多个并发事务同时对其数据进行读写和修改的能力，隔离性可以防止多个事务并发执行时由于交叉执行而导致数据的不一致。事务隔离分为不同级别，包括未提交读（Read uncommitted）、提交读（read committed）、可重复读（repeatable read）和串行化（Serializable）。\n- **持久性（Durability）**：事务处理结束后，对数据的修改就是永久的，即便系统故障也不会丢失。\n\n> 参考：[https://zh.wikipedia.org/wiki/ACID](https://zh.wikipedia.org/wiki/ACID)。\n\n## 详谈Spring对事务的支持\n\n> ⚠️再提醒一次：你的程序是否支持事务首先取决于数据库，比如使用MySQL的话，如果你选择的是innodb引擎，那么恭喜你，是可以支持事务的。但是，如果你的MySQL数据库使用的是myisam引擎的话，那不好意思，从根上就是不支持事务的。\n\n这里再提一下一个非常重要的知识点：**MySQL怎么保证原子性的？**\n\n我们知道如果想要保证事务的原子性，就需要在异常发生时，对已经执行的操作进行回滚，在MySQL中，恢复机制是通过**回滚日志（undo log）** 实现的，所有事务进行的修改都会先记录到这个回滚日志中，然后再执行相关的操作。如果执行过程中遇到异常的话，我们直接利用回滚日志中的信息将数据回滚到修改之前的样子即可！并且，回滚日志会先于数据持久化到磁盘上。这样就保证了即使遇到数据库突然宕机等情况，当用户再次启动数据库的时候，数据库还能够通过查询回滚日志来回滚之前未完成的事务。\n\n### Spring支持两种方式的事务管理\n\n#### 编程式事务管理\n\n通过TransactionTemplate或者TransactionManager手动管理事务，实际应用中很少使用，但是对于你理解Spring事务管理原理有帮助。使用TransactionTemplate进行编程式事务管理的示例代码如下：\n\n```java\n@Autowired\nprivate TransactionTemplate transactionTemplate;\npublic void testTransaction() {\n    transactionTemplate.execute(new TransactionCallbackWithoutResult() {\n        @Override\n        protected void doInTransactionWithoutResult(TransactionStatus transactionStatus) {\n            try {\n                // ....业务代码\n            } catch (Exception e){\n                //回滚\n                transactionStatus.setRollbackOnly();\n            }\n        }\n    });\n}\n```\n\n使用TransactionManager进行编程式事务管理的示例代码如下：\n\n```java\n@Autowired\nprivate PlatformTransactionManager transactionManager;\n\npublic void testTransaction() {\n  TransactionStatus status = transactionManager.getTransaction(new DefaultTransactionDefinition());\n          try {\n               // ....业务代码\n              transactionManager.commit(status);\n          } catch (Exception e) {\n              transactionManager.rollback(status);\n          }\n}\n```\n\n#### 声明式事务管理\n\n推荐使用（代码侵入性最小），实际是通过AOP实现（基于@Transactional的全注解方式使用最多）。使用@Transactional注解进行事务管理的示例代码如下：\n\n```java\n@Transactional(propagation = Propagation.REQUIRED)\npublic void aMethod {\n    //do something\n    B b = new B();\n    C c = new C();\n    b.bMethod();\n    c.cMethod();\n}\n```\n\n### Spring事务管理接口介绍\n\nSpring框架中，事务管理相关最重要的3个接口如下：\n\n- **PlatformTransactionManager**：（平台）事务管理器，Spring事务策略的核心。\n- **TransactionDefinition**：事务定义信息(事务隔离级别、传播行为、超时、只读、回滚规则)。\n- **TransactionStatus**：事务运行状态。\n\n我们可以把PlatformTransactionManager接口可以被看作是事务上层的管理者，而TransactionDefinition和TransactionStatus这两个接口可以看作是事务的描述。PlatformTransactionManager会根据TransactionDefinition的定义比如事务超时时间、隔离级别、传播行为等来进行事务管理，而TransactionStatus接口则提供了一些方法来获取事务相应的状态比如是否新事务、是否可以回滚等等。\n\n#### PlatformTransactionManager:事务管理接口\n\n**Spring并不直接管理事务，而是提供了多种事务管理器。Spring事务管理器的接口是：PlatformTransactionManager**。通过这个接口，Spring为各个平台如：JDBC(DataSourceTransactionManager)、Hibernate(HibernateTransactionManager)、JPA(JpaTransactionManager)等都提供了对应的事务管理器，但是具体的实现就是各个平台自己的事情了。**PlatformTransactionManager接口的具体实现如下**：PlatformTransactionManager接口中定义了三个方法：\n\n```java\npackage org.springframework.transaction;\n\nimport org.springframework.lang.Nullable;\n\npublic interface PlatformTransactionManager {\n    //获得事务\n    TransactionStatus getTransaction(@Nullable TransactionDefinition var1) throws TransactionException;\n    //提交事务\n    void commit(TransactionStatus var1) throws TransactionException;\n    //回滚事务\n    void rollback(TransactionStatus var1) throws TransactionException;\n}\n```\n\n**为什么要定义或者说抽象出来PlatformTransactionManager这个接口呢**？主要是因为要将事务管理行为抽象出来，然后不同的平台去实现它，这样我们可以保证提供给外部的行为不变，方便我们扩展。\n\n#### TransactionDefinition:事务属性\n\n事务管理器接口PlatformTransactionManager通过getTransaction(TransactionDefinition definition)方法来得到一个事务，这个方法里面的参数是TransactionDefinition类，这个类就定义了一些基本的事务属性。什么是事务属性呢？事务属性可以理解成事务的一些基本配置，描述了事务策略如何应用到方法上。事务属性包含了5个方面：\n\n- 隔离级别\n- 传播行为\n- 回滚规则\n- 是否只读\n- 事务超时\n\nTransactionDefinition接口中定义了5个方法以及一些表示事务属性的常量比如隔离级别、传播行为等等。\n\n```java\npackage org.springframework.transaction;\nimport org.springframework.lang.Nullable;\npublic interface TransactionDefinition {\n    int PROPAGATION_REQUIRED = 0;\n    int PROPAGATION_SUPPORTS = 1;\n    int PROPAGATION_MANDATORY = 2;\n    int PROPAGATION_REQUIRES_NEW = 3;\n    int PROPAGATION_NOT_SUPPORTED = 4;\n    int PROPAGATION_NEVER = 5;\n    int PROPAGATION_NESTED = 6;\n    int ISOLATION_DEFAULT = -1;\n    int ISOLATION_READ_UNCOMMITTED = 1;\n    int ISOLATION_READ_COMMITTED = 2;\n    int ISOLATION_REPEATABLE_READ = 4;\n    int ISOLATION_SERIALIZABLE = 8;\n    int TIMEOUT_DEFAULT = -1;\n    // 返回事务的传播行为，默认值为REQUIRED。\n    int getPropagationBehavior();\n    // 返回事务的隔离级别，默认值是DEFAULT\n    int getIsolationLevel();\n    // 返回事务的超时时间，默认值为-1。如果超过该时间限制但事务还没有完成，则自动回滚事务。\n    int getTimeout();\n    // 返回是否为只读事务，默认值为false\n    boolean isReadOnly();\n\n    @Nullable\n    String getName();\n}\n```\n\n#### TransactionStatus:事务状态\n\nTransactionStatus接口用来记录事务的状态该接口定义了一组方法,用来获取或判断事务的相应状态信息。PlatformTransactionManager.getTransaction(…)方法返回一个TransactionStatus对象。\n\n**TransactionStatus接口内容如下：**\n\n```java\npublic interface TransactionStatus{\n    boolean isNewTransaction(); // 是否是新的事务\n    boolean hasSavepoint(); // 是否有恢复点\n    void setRollbackOnly();  // 设置为只回滚\n    boolean isRollbackOnly(); // 是否为只回滚\n    boolean isCompleted; // 是否已完成\n}\n```\n\n### 事务属性详解\n\n实际业务开发中，大家一般都是使用@Transactional注解来开启事务，但很多人并不清楚这个注解里面的参数是什么意思，有什么用。为了更好的在项目中使用事务管理，强烈推荐好好阅读一下下面的内容。\n\n#### 事务传播行为\n\n**事务传播行为是为了解决业务层方法之间互相调用的事务问题**。\n\n当事务方法被另一个事务方法调用时，必须指定事务应该如何传播。例如：方法可能继续在现有事务中运行，也可能开启一个新事务，并在自己的事务中运行。举个例子：我们在A类的aMethod()方法中调用了B类的bMethod()方法。这个时候就涉及到业务层方法之间互相调用的事务问题。如果我们的bMethod()如果发生异常需要回滚，如何配置事务传播行为才能让aMethod()也跟着回滚呢？这个时候就需要事务传播行为的知识了，如果你不知道的话一定要好好看一下。\n\n```java\n@Service\nclass A {\n    @Autowired\n    B b;\n    @Transactional(propagation = Propagation.xxx)\n    public void aMethod {\n        //do something\n        b.bMethod();\n    }\n}\n\n@Service\nclass B {\n    @Transactional(propagation = Propagation.xxx)\n    public void bMethod {\n       //do something\n    }\n}\n```\n\n在TransactionDefinition定义中包括了如下几个表示传播行为的常量：\n\n```java\npublic interface TransactionDefinition {\n    int PROPAGATION_REQUIRED = 0;\n    int PROPAGATION_SUPPORTS = 1;\n    int PROPAGATION_MANDATORY = 2;\n    int PROPAGATION_REQUIRES_NEW = 3;\n    int PROPAGATION_NOT_SUPPORTED = 4;\n    int PROPAGATION_NEVER = 5;\n    int PROPAGATION_NESTED = 6;\n    ......\n}\n```\n\n不过，为了方便使用，Spring相应地定义了一个枚举类：Propagation\n\n```java\npackage org.springframework.transaction.annotation;\nimport org.springframework.transaction.TransactionDefinition;\npublic enum Propagation {\n\n    REQUIRED(TransactionDefinition.PROPAGATION_REQUIRED),\n\n    SUPPORTS(TransactionDefinition.PROPAGATION_SUPPORTS),\n\n    MANDATORY(TransactionDefinition.PROPAGATION_MANDATORY),\n\n    REQUIRES_NEW(TransactionDefinition.PROPAGATION_REQUIRES_NEW),\n\n    NOT_SUPPORTED(TransactionDefinition.PROPAGATION_NOT_SUPPORTED),\n\n    NEVER(TransactionDefinition.PROPAGATION_NEVER),\n\n    NESTED(TransactionDefinition.PROPAGATION_NESTED);\n\n    private final int value;\n\n    Propagation(int value) {\n        this.value = value;\n    }\n\n    public int value() {\n        return this.value;\n    }\n\n}\n```\n\n**正确的事务传播行为可能的值如下**：\n\n##### TransactionDefinition.PROPAGATION_REQUIRED\n\n使用的最多的一个事务传播行为，我们平时经常使用的@Transactional注解默认使用就是这个事务传播行为。如果当前存在事务，则加入该事务；如果当前没有事务，则创建一个新的事务。也就是说：\n\n- 如果外部方法没有开启事务的话，Propagation.REQUIRED修饰的内部方法会新开启自己的事务，且开启的事务相互独立，互不干扰。\n- 如果外部方法开启事务并且被Propagation.REQUIRED的话，所有Propagation.REQUIRED修饰的内部方法和外部方法均属于同一事务，只要一个方法回滚，整个事务均回滚。\n\n举个例子：如果我们上面的aMethod()和bMethod()使用的都是PROPAGATION_REQUIRED传播行为的话，两者使用的就是同一个事务，只要其中一个方法回滚，整个事务均回滚。\n\n```java\n@Service\nclass A {\n    @Autowired\n    B b;\n    @Transactional(propagation = Propagation.REQUIRED)\n    public void aMethod {\n        //do something\n        b.bMethod();\n    }\n}\n@Service\nclass B {\n    @Transactional(propagation = Propagation.REQUIRED)\n    public void bMethod {\n       //do something\n    }\n}\n```\n\n##### TransactionDefinition.PROPAGATION_REQUIRES_NEW\n\n创建一个新的事务，如果当前存在事务，则把当前事务挂起。也就是说不管外部方法是否开启事务，Propagation.REQUIRES_NEW修饰的内部方法会新开启自己的事务，且开启的事务相互独立，互不干扰。\n\n举个例子：如果我们上面的bMethod()使用PROPAGATION_REQUIRES_NEW事务传播行为修饰，aMethod还是用PROPAGATION_REQUIRED修饰的话。如果aMethod()发生异常回滚，bMethod()不会跟着回滚，因为bMethod()开启了独立的事务。但是，如果bMethod()抛出了未被捕获的异常并且这个异常满足事务回滚规则的话,aMethod()同样也会回滚，因为这个异常被aMethod()的事务管理机制检测到了。\n\n```java\n@Service\nclass A {\n    @Autowired\n    B b;\n    @Transactional(propagation = Propagation.REQUIRED)\n    public void aMethod {\n        //do something\n        b.bMethod();\n    }\n}\n\n@Service\nclass B {\n    @Transactional(propagation = Propagation.REQUIRES_NEW)\n    public void bMethod {\n       //do something\n    }\n}\n```\n\n##### TransactionDefinition.PROPAGATION_NESTED\n\n如果当前存在事务，就在嵌套事务内执行；如果当前没有事务，就执行与TransactionDefinition.PROPAGATION_REQUIRED类似的操作。也就是说：\n\n- 在外部方法开启事务的情况下,在内部开启一个新的事务，作为嵌套事务存在。\n- 如果外部方法无事务，则单独开启一个事务，与PROPAGATION_REQUIRED类似。\n\n这里还是简单举个例子：如果bMethod()回滚的话，aMethod()也会回滚。\n\n```java\n@Service\nclass A {\n    @Autowired\n    B b;\n    @Transactional(propagation = Propagation.REQUIRED)\n    public void aMethod {\n        //do something\n        b.bMethod();\n    }\n}\n\n@Service\nclass B {\n    @Transactional(propagation = Propagation.NESTED)\n    public void bMethod {\n       //do something\n    }\n}\n```\n\n##### TransactionDefinition.PROPAGATION_MANDATORY\n\n如果当前存在事务，则加入该事务；如果当前没有事务，则抛出异常。（mandatory：强制性）。若是错误的配置以下3种事务传播行为，事务将不会发生回滚。\n\n##### TransactionDefinition.PROPAGATION_SUPPORTS\n\n如果当前存在事务，则加入该事务；如果当前没有事务，则以非事务的方式继续运行。\n\n##### TransactionDefinition.PROPAGATION_NOT_SUPPORTED\n\n以非事务方式运行，如果当前存在事务，则把当前事务挂起。\n\n##### TransactionDefinition.PROPAGATION_NEVER\n\n以非事务方式运行，如果当前存在事务，则抛出异常。\n\n> 更多关于事务传播行为的内容请看这篇文章：[《太难了~面试官让我结合案例讲讲自己对Spring事务传播行为的理解。》](https://mp.weixin.qq.com/s?__biz=Mzg2OTA0Njk0OA==&mid=2247486668&idx=2&sn=0381e8c836442f46bdc5367170234abb&chksm=cea24307f9d5ca11c96943b3ccfa1fc70dc97dd87d9c540388581f8fe6d805ff548dff5f6b5b&token=1776990505&lang=zh_CN#rd)\n\n##### 总结下Spring中的事务传播机制\n\nSpring事务传播机制共7中,可以分为3组+1个特殊来分析或者记忆\n\n- REQUIRE组\n1. REQUIRED:当前存在事务则使用当前的事务,当前不存在事务则创建一个新的事务\n2. REQUIRES_NEW:创建新事务,如果已经存在事务,则把已存在的事务挂起\n- SUPPORT组\n1. SUPPORTS:支持事务.如果当前存在事务则加入该事务,如果不存在事务则以无事务状态执行\n2. NOT_SUPPORTED:不支持事务.在无事务状态下执行,如果已经存在事务则挂起已存在的事务\n- Exception组\n1. MANDATORY:必须在事务中执行,如果当前不存在事务,则抛出异常\n2. NEVER:不可在事务中执行,如果当前存在事务,则抛出异常\n- NESTED:嵌套事务.如果当前存在事务,则嵌套执行,如果当前不存在事务,则开启新事务\n\n> [Spring事务传播行为详解](https://mp.weixin.qq.com/s/S8RXcISJA0TdJuBsOXWPcQ)\n\n\n#### 事务隔离级别\n\nTransactionDefinition接口中定义了五个表示隔离级别的常量：\n\n```java\npublic interface TransactionDefinition {\n    ......\n    int ISOLATION_DEFAULT = -1;\n    int ISOLATION_READ_UNCOMMITTED = 1;\n    int ISOLATION_READ_COMMITTED = 2;\n    int ISOLATION_REPEATABLE_READ = 4;\n    int ISOLATION_SERIALIZABLE = 8;\n    ......\n}\n```\n\n和事务传播行为那块一样，为了方便使用，Spring也相应地定义了一个枚举类：Isolation\n\n```java\npublic enum Isolation {\n\n  DEFAULT(TransactionDefinition.ISOLATION_DEFAULT),\n\n  READ_UNCOMMITTED(TransactionDefinition.ISOLATION_READ_UNCOMMITTED),\n\n  READ_COMMITTED(TransactionDefinition.ISOLATION_READ_COMMITTED),\n\n  REPEATABLE_READ(TransactionDefinition.ISOLATION_REPEATABLE_READ),\n\n  SERIALIZABLE(TransactionDefinition.ISOLATION_SERIALIZABLE);\n\n  private final int value;\n\n  Isolation(int value) {\n    this.value = value;\n  }\n\n  public int value() {\n    return this.value;\n  }\n\n}\n```\n\n下面我依次对每一种事务隔离级别进行介绍：\n\n- **TransactionDefinition.ISOLATION_DEFAULT**:使用后端数据库默认的隔离级别，MySQL默认采用的**REPEATABLE_READ**隔离级别,Oracle默认采用的**READ_COMMITTED**隔离级别.\n- **TransactionDefinition.ISOLATION_READ_UNCOMMITTED**:最低的隔离级别，使用这个隔离级别很少，因为它允许读取尚未提交的数据变更，**可能会导致脏读、幻读或不可重复读**。\n- **TransactionDefinition.ISOLATION_READ_COMMITTED**:允许读取并发事务已经提交的数据，**可以阻止脏读，但是幻读或不可重复读仍有可能发生**。\n- **TransactionDefinition.ISOLATION_REPEATABLE_READ**:对同一字段的多次读取结果都是一致的，除非数据是被本身事务自己所修改，**可以阻止脏读和不可重复读，但幻读仍有可能发生**。\n- **TransactionDefinition.ISOLATION_SERIALIZABLE**:最高的隔离级别，完全服从ACID的隔离级别。所有的事务依次逐个执行，这样事务之间就完全不可能产生干扰，也就是说，**该级别可以防止脏读、不可重复读以及幻读**。但是这将严重影响程序的性能。通常情况下也不会用到该级别。\n\n> 相关阅读：[MySQL事务隔离级别详解](https://javaguide.cn/database/mysql/transaction-isolation-level.html)。\n\n##### 总结5种隔离级别\n\n1. DEFAULT\t\n使用数据库本身使用的隔离级别(oracle默认读已提交,mysql默认可重复读)\n\n2. READ_UNCOMITTED\t\n读未提交：事务即使未提交，却可以被别的事务读取到的，这级别的事务隔离有脏读、重复读、幻读的问题。\n\n3. READ_COMMITED\n读已提交：当前事务只能读取到其他事务提交的数据，这种事务的隔离级别解决了脏读问题，但还是会存在不可重复读、幻读问题。\n\n4. REPEATABLE_READ\n可重复读：限制了读取数据的时候，不可以进行修改，所以解决了不可重复读的问题，但是读取范围数据的时候，是可以插入数据，所以还会存在幻读问题。\n\n5. SERLALIZABLE\t\n串行化：事务最高的隔离级别，在该级别下，所有事务都是进行串行化顺序执行的。可以避免脏读、不可重复读与幻读所有并发问题。但是这种事务隔离级别下，事务执行很耗性能。\n\n> [MySQL事务隔离级别](https://blog.csdn.net/l1394049664/article/details/81814090)\n> [四个案例看懂MySQL事务隔离级别](https://mp.weixin.qq.com/s/eTbWDz8NiM8L8BbIrNiLHQ)\n> [长文捋明白Spring事务！隔离性？传播性？一网打尽！](https://mp.weixin.qq.com/s/6tRPXwXnWUW4mVfCdBlkog)\n> [一文详解幻读、脏读和不可重复读](https://mp.weixin.qq.com/s/H1VuZxC1JRUh2gAM-KZn6w)\n> [关于幻读，该捋清楚了！](https://mp.weixin.qq.com/s/UcQdZYUP3Eb_V3XNk2dk6A)\n\n#### 事务超时属性\n\n所谓事务超时，就是指一个事务所允许执行的最长时间，如果超过该时间限制但事务还没有完成，则自动回滚事务。在TransactionDefinition中以int的值来表示超时时间，其单位是秒，默认值为-1，这表示事务的超时时间取决于底层事务系统或者没有超时时间。\n\n#### 事务只读属性\n\n```java\npackage org.springframework.transaction;\nimport org.springframework.lang.Nullable;\npublic interface TransactionDefinition {\n    ......\n    // 返回是否为只读事务，默认值为false\n    boolean isReadOnly();\n\n}\n```\n\n对于只有读取数据查询的事务，可以指定事务类型为readonly，即只读事务。只读事务不涉及数据的修改，数据库会提供一些优化手段，适合用在有多条数据库查询操作的方法中。很多人就会疑问了，为什么我一个数据查询操作还要启用事务支持呢？拿MySQL的innodb举例子，根据[官网](https://dev.mysql.com/doc/refman/5.7/en/innodb-autocommit-commit-rollback.html)描述：\n\n> MySQL默认对每一个新建立的连接都启用了autocommit模式。在该模式下，每一个发送到MySQL服务器的sql语句都会在一个单独的事务中进行处理，执行结束后会自动提交事务，并开启一个新的事务。\n\n但是，如果你给方法加上了Transactional注解的话，这个方法执行的所有sql会被放在一个事务中。如果声明了只读事务的话，数据库就会去优化它的执行，并不会带来其他的什么收益。如果不加Transactional，每条sql会开启一个单独的事务，中间被其它事务改了数据，都会实时读取到最新值。分享一下关于事务只读属性，其他人的解答：\n\n- 如果你一次执行单条查询语句，则没有必要启用事务支持，数据库默认支持SQL执行期间的读一致性；\n- 如果你一次执行多条查询语句，例如统计查询，报表查询，在这种场景下，多条查询SQL必须保证整体的读一致性，否则，在前条SQL查询之后，后条SQL查询之前，数据被其他用户改变，则该次整体的统计查询将会出现读数据不一致的状态，此时，应该启用事务支持\n\n#### 事务回滚规则\n\n这些规则定义了哪些异常会导致事务回滚而哪些不会。默认情况下，事务只有遇到运行期异常（RuntimeException的子类）时才会回滚，Error也会导致事务回滚，但是，在遇到检查型（Checked）异常时不会回滚。如果你想要回滚你定义的特定的异常类型的话，可以这样：\n\n```java\n@Transactional(rollbackFor= MyException.class)\n```\n\n### @Transactional注解使用详解\n\n#### @Transactional的作用范围\n\n1. **方法**：推荐将注解使用于方法上，不过需要注意的是：**该注解只能应用到public方法上，否则不生效。**\n2. **类**：如果这个注解使用在类上的话，表明该注解对该类中所有的public方法都生效。\n3. **接口**：不推荐在接口上使用。\n\n#### @Transactional的常用配置参数\n\n@Transactional注解源码如下，里面包含了基本事务属性的配置：\n\n```java\n@Target({ElementType.TYPE, ElementType.METHOD})\n@Retention(RetentionPolicy.RUNTIME)\n@Inherited\n@Documented\npublic @interface Transactional {\n\n\t@AliasFor(\"transactionManager\")\n\tString value() default \"\";\n\n\t@AliasFor(\"value\")\n\tString transactionManager() default \"\";\n\n\tPropagation propagation() default Propagation.REQUIRED;\n\n\tIsolation isolation() default Isolation.DEFAULT;\n\n\tint timeout() default TransactionDefinition.TIMEOUT_DEFAULT;\n\n\tboolean readOnly() default false;\n\n\tClass<? extends Throwable>[] rollbackFor() default {};\n\n\tString[] rollbackForClassName() default {};\n\n\tClass<? extends Throwable>[] noRollbackFor() default {};\n\n\tString[] noRollbackForClassName() default {};\n\n}\n```\n\n**@Transactional的常用配置参数总结（只列出了5个我平时比较常用的）：**\n\n| 属性名      | 说明                                                         |\n| :---------- | :----------------------------------------------------------- |\n| propagation | 事务的传播行为，默认值为REQUIRED，可选的值在上面介绍过      |\n| isolation   | 事务的隔离级别，默认值采用DEFAULT，可选的值在上面介绍过     |\n| timeout     | 事务的超时时间，默认值为-1（不会超时）。如果超过该时间限制但事务还没有完成，则自动回滚事务。 |\n| readOnly    | 指定事务是否为只读事务，默认值为false。                     |\n| rollbackFor | 用于指定能够触发事务回滚的异常类型，并且可以指定多个异常类型。 |\n\n#### @Transactional事务注解原理\n\n我们知道，**@Transactional的工作机制是基于AOP实现的，AOP又是使用动态代理实现的。如果目标对象实现了接口，默认情况下会采用JDK的动态代理，如果目标对象没有实现了接口,会使用CGLIB动态代理**。`createAopProxy()`方法决定了是使用JDK还是Cglib来做动态代理，源码如下：\n\n```java\npublic class DefaultAopProxyFactory implements AopProxyFactory, Serializable {\n\n\t@Override\n\tpublic AopProxy createAopProxy(AdvisedSupport config) throws AopConfigException {\n\t\tif (config.isOptimize() || config.isProxyTargetClass() || hasNoUserSuppliedProxyInterfaces(config)) {\n\t\t\tClass<?> targetClass = config.getTargetClass();\n\t\t\tif (targetClass == null) {\n\t\t\t\tthrow new AopConfigException(\"TargetSource cannot determine target class: \" +\n\t\t\t\t\t\t\"Either an interface or a target is required for proxy creation.\");\n\t\t\t}\n\t\t\tif (targetClass.isInterface() || Proxy.isProxyClass(targetClass)) {\n\t\t\t\treturn new JdkDynamicAopProxy(config);\n\t\t\t}\n\t\t\treturn new ObjenesisCglibAopProxy(config);\n\t\t}\n\t\telse {\n\t\t\treturn new JdkDynamicAopProxy(config);\n\t\t}\n\t}\n  .......\n}\n```\n\n如果一个类或者一个类中的public方法上被标注@Transactional注解的话，Spring容器就会在启动的时候为其创建一个代理类，在调用被@Transactional注解的public方法的时候，实际调用的是，TransactionInterceptor类中的invoke()方法。这个方法的作用就是在目标方法之前开启事务，方法执行过程中如果遇到异常的时候回滚事务，方法调用完成之后提交事务。\n\n> TransactionInterceptor类中的invoke()方法内部实际调用的是TransactionAspectSupport类的invokeWithinTransaction()方法。由于新版本的Spring对这部分重写很大，而且用到了很多响应式编程的知识，这里就不列源码了。\n\n#### SpringAOP自调用问题\n\n若同一类中的其他没有@Transactional注解的方法内部调用有@Transactional注解的方法，有@Transactional注解的方法的事务会失效。这是由于SpringAOP代理的原因造成的，因为只有当@Transactional注解的方法在类以外被调用的时候，Spring事务管理才生效。MyService类中的method1()调用method2()就会导致method2()的事务失效。\n\n```java\n@Service\npublic class MyService {\n\nprivate void method1() {\n     method2();\n     //......\n}\n@Transactional\n public void method2() {\n     //......\n  }\n}\n```\n\n**解决办法就是避免同一类中自调用或者使用AspectJ取代Spring AOP代理**。\n\n#### @Transactional的使用注意事项总结\n\n- @Transactional注解只有作用到public方法上事务才生效，不推荐在接口上使用；\n- 避免同一个类中调用@Transactional注解的方法，这样会导致事务失效；\n- 正确的设置@Transactional的rollbackFor和propagation属性，否则事务可能会回滚失败;\n- 被@Transactional注解的方法所在的类必须被Spring管理，否则不生效；\n- 底层使用的数据库必须支持事务机制，否则不生效；\n\n#### 总结事务失效的集中原因\n\n- 数据库引擎不支持事务\n- 事务方法未被Spring管理\n- 方法使用final类型修饰\n- 非public修饰的方法\n- 同一个类中的方法相互调用\n- 未配置开启事务,数据源没有配置事务管理器\n- 方法的事务传播类型不支持事务(Propagation.NOT_SUPPORTED表示不以事务运行)\n- 异常被内部catch\n- 异常类型错误\n```java\n @Transactional\n    public void updateOrder(Order order) {\n        try {\n            // update order\n        } catch {\n            throw new Exception(\"更新错误\");\n        }\n    }\n/**\n * 这样事务也是不生效的，因为默认回滚的是：RuntimeException，如果你想触发其他异常的回滚，需要在注解上配置一下，如：\n * @Transactional(rollbackFor = Exception.class)\n * 这个配置仅限于Throwable异常类及其子类。\n */\n```\n- 多线程调用,会导致两个方法不在同一个线程中，从而是两个不同的事务\n\n> [原文链接](https://javaguide.cn/system-design/framework/spring/spring-transaction.html)\n\n\n### 事务相关文章\n\n- [三问Spring事务：解决什么问题？如何解决？存在什么问题?](https://mp.weixin.qq.com/s/ZCwiz5FwUuslf6ireP8o_g)\n- [Spring事务失效了，怎么办？(介绍的很好，包括代理相关内容,跨方法调用导致事务失效的原因)](https://mp.weixin.qq.com/s/J7A0rwnwpCc92IbaQ6Q6Yw)\n- [Spring事务(介绍的很详细)](https://mp.weixin.qq.com/s/zMAFhZi9wYGwofasv-0aMA)\n\n\n## 分布式事务\n\nCAP理论告诉我们，一个分布式系统不可能同时满足一致(C:Consistency)，可用性(A:Availability)和分区容错性(P:Partition tolerance)这三个基本需求，最多只能同时满足其中的2个。\n\nBASE：全称：Basically Available(基本可用)，Soft state（软状态）,和Eventually consistent（最终一致性）。Base理论是对CAP中一致性和可用性权衡的结果，其来源于对大型互联网分布式实践的总结，是基于CAP定理逐步演化而来的。其核心思想是：既是无法做到强一致性（Strong consistency），但每个应用都可以根据自身的业务特点，采用适当的方式来使系统达到最终一致性(Eventual consistency)\n\n### 2PC\n\n**阶段一(准备阶段)**:协调者向所有的参与者询问，是否准备好了执行事务，并开始等待各参与者的响应。执行事务各参与者节点执行事务操作，并将Undo和Redo信息记入事务日志中，各参与者向协调者反馈事务询问的响应，如果参与者成功执行了事务操作，那么就反馈给协调者Yes响应，表示事务可以执行；如果参与者没有成功执行事务，就返回No给协调者，表示事务不可以执行。\n\n**阶段二：在阶段二中，会根据阶段一的投票结果执行2种操作：执行事务提交，中断事务**。\n\n执行事务提交步骤如下：\n**发送提交请求**：协调者向所有参与者发出commit请求。参与者收到commit请求后，会正式执行事务提交操作，并在完成提交之后释放整个事务执行期间占用的事务资源。参与者在完成事务提交之后，向协调者发送Ack信息。协调者接收到所有参与者反馈的Ack信息后，完成事务。\n\n中断事务步骤如下：\n**发送回滚请求**：协调者向所有参与者发出Rollback请求。参与者接收到Rollback请求后，会利用其在阶段一种记录的Undo信息来执行事务回滚操作，并在完成回滚之后释放在整个事务执行期间占用的资源。参与者在完成事务回滚之后，想协调者发送Ack信息。协调者接收到所有参与者反馈的Ack信息后，完成事务中断。\n\n**二阶段提交缺点：**\n\n1. 同步阻塞问题。执行过程中，所有参与节点都是事务阻塞型的。当参与者占有公共资源时，其他第三方节点访问公共资源不得不处于阻塞状态。\n2. 单点故障。由于协调者的重要性，一旦协调者发生故障。参与者会一直阻塞下去。尤其在第二阶段，协调者发生故障，那么所有的参与者还都处于锁定事务资源的状态中，而无法继续完成事务操作。（如果是协调者挂掉，可以重新选举一个协调者，但是无法解决因为协调者宕机导致的参与者处于阻塞状态的问题）\n3. 数据不一致。在二阶段提交的阶段二中，当协调者向参与者发送commit请求之后，发生了局部网络异常或者在发送commit请求过程中协调者发生了故障，这回导致只有一部分参与者接受到了commit请求。而在这部分参与者接到commit请求之后就会执行commit操作。但是其他部分未接到commit请求的机器则无法执行事务提交。于是整个分布式系统便出现了数据不一致性的现象。\n4. 二阶段无法解决的问题：协调者再发出commit消息之后宕机，而唯一接收到这条消息的参与者同时也宕机了。那么即使协调者通过选举协议产生了新的协调者，这条事务的状态也是不确定的，没人知道事务是否被已经提交。\n由于二阶段提交存在着诸如同步阻塞、单点问题、脑裂等缺陷，所以，研究者们在二阶段提交的基础上做了改进，提出了三阶段提交。\n\n### 3PC\n与两阶段提交不同的是，三阶段提交有两个改动点。\n\n1. **引入超时机制**。同时在协调者和参与者中都引入超时机制。\n2. 在第一阶段和第二阶段中插入一个**准备阶段**。保证了在最后提交阶段之前各参与节点的状态是一致的.如果段时间内没有收到协调者的commit请求，那么就会自动进行commit，解决了2PC单点故障的问题。\n\n### TCC\n2PC要求参与者实现了XA协议，通常用来解决多个数据库之间的事务问题，比较局限。在多个系统服务利用api接口相互调用的时候，就不遵守XA协议了，这时候2PC就不适用了。现代企业多采用分布式的微服务，因此更多的是要解决多个微服务之间的分布式事务问题。\nTCC就是一种解决多个微服务之间的分布式事务问题的方案。TCC是Try、Confirm、Cancel三个词的缩写，其本质是一个应用层面上的2PC，同样分为两个阶段：\n**准备阶段**：协调者调用所有的每个微服务提供的try接口，将整个全局事务涉及到的资源锁定住，若锁定成功try接口向协调者返回yes。\n**提交阶段**：若所有的服务的try接口在阶段一都返回yes，则进入提交阶段，协调者调用所有服务的confirm接口，各个服务进行事务提交。如果有任何一个服务的try接口在阶段一返回no或者超时，则协调者调用所有服务的cancel接口\n这里有个关键问题，既然TCC是一种服务层面上的2PC。它是如何解决2PC无法应对宕机问题的缺陷的呢？答案是不断重试。\n\n### 相关文章\n- [seata官网](http://seata.io/zh-cn/)\n- [一致性协议算法-2PC、3PC、Paxos、Raft、ZAB、NWR详解](https://mp.weixin.qq.com/s/YxlAtPmCZ8h_oyPStXJP_A)\n- [阿里终面：分布式事务原理](https://mp.weixin.qq.com/s/JZnLbBrRx_fDtnsYsRs4Aw)\n- [分布式事务，阿里为什么钟爱TCC](https://mp.weixin.qq.com/s/eczKVv7Jgt4f0Mhwaq1JXw)\n- [七种分布式事务的解决方案，一次讲给你听！](https://mp.weixin.qq.com/s/VIuJ5ywyjfGjAWd3Fb-XWg)\n- [对比7种分布式事务方案，还是偏爱阿里开源的Seata，真香](https://mp.weixin.qq.com/s/J3BMnwRD-Ag8BAlJQiYuhg)\n- [实战！阿里神器Seata实现TCC模式解决分布式事务](https://mp.weixin.qq.com/s/hBSY7VwHu9kM_3OrJ8WDwA)\n- [分布式事务的6种解决方案，写得非常好！](https://mp.weixin.qq.com/s/Aj_BECgTZWkxX-dy5sayOw)\n- [我还不懂什么是分布式事务](https://mp.weixin.qq.com/s/MbPRpBudXtdfl8o4hlqNlQ)\n- [一文看懂分布式事务](https://mp.weixin.qq.com/s?__biz=Mzg2MDYzODI5Nw==&amp;mid=2247494401&amp;idx=1&amp;sn=915f97e20b4cb58bea2f638389ff60e5&amp;source=41#wechat_redirect)\n- [面试官：聊聊分布式事务，再说说解决方案！](https://mp.weixin.qq.com/s/QpOwudYMY1HMRpU6SIXjzA)\n- [看了那么多博客，还是不懂TCC，不妨看看这个案例！](https://mp.weixin.qq.com/s/83-I7hPDuWRTTfrldHJ0VA)\n- [听说TCC不支持OpenFeign](https://mp.weixin.qq.com/s/EQuVJGFi6SEj3Qj2FS-uSg)\n- [五分钟带你体验一把分布式事务](https://mp.weixin.qq.com/s/47efAPrm10l1Bxn1zECwvA)\n- [XA事务水很深，小伙子我怕你把握不住](https://mp.weixin.qq.com/s/BJHmVkNrvNL87hBT8DM8vg)\n- [你这Saga事务保“隔离性”吗？](https://mp.weixin.qq.com/s/cZabAt7JF4QrQHERHHAWjA)\n- [哪种分布式事务处理方案效率最高](https://mp.weixin.qq.com/s/jcavJfjseBvaETAuTPnRqw)\n- [一文搞明白分布式事务解决方案](https://mp.weixin.qq.com/s/6DOtO5OQyCL8bR03Z-3q9A)\n- [手把手带领小伙伴们写一个分布式事务案例](https://mp.weixin.qq.com/s/fzlr-6pDPWKbwVuJlXe8sA)\n- [Spring Boot多数据源如何处理事务](https://mp.weixin.qq.com/s/NbnCiRwRFUZGym5vDxOoPQ)\n- [亿级流量架构分布式事务如何实现？](https://mp.weixin.qq.com/s/lwCNNCyG9wwtRHsni5pD8g)\n- [SpringBoot分布式事务的解决方案（JTA+Atomic+多数据源）](https://mp.weixin.qq.com/s/ic57T3Yj2C_5tpdnM39IrQ)\n- [分布式事务，原理简单，写起来全是坑！](https://mp.weixin.qq.com/s/29PmqK_bzDgh8bl9SBY3Uw)\n- [分布式事务处理方案大PK！](https://mp.weixin.qq.com/s/kiRD3Hmdx2b__cBWeQOTWQ)\n- [如何用RabbitMQ解决分布式事务](https://mp.weixin.qq.com/s/wTF3LlUKtH3lzsVgCLdCpQ)\n- [MySQL为什么需要两阶段提交？](https://mp.weixin.qq.com/s/XRGIO7S9q9XqAfwqWr0OsQ)\n- [阿里Seata新版本终于解决了TCC模式的幂等、悬挂和空回滚问题](https://mp.weixin.qq.com/s/nM81BRyQRTWab78a6KTD-g)\n\n## MySQL事务\n\n### 并发事务带来了哪些问题?\n\n在典型的应用程序中，多个事务并发运行，经常会操作相同的数据来完成各自的任务（多个用户对同一数据进行操作）。并发虽然是必须的，但可能会导致以下的问题。\n\n#### 脏读（Dirtyread）\n\n一个事务读取数据并且对数据进行了修改，这个修改对其他事务来说是可见的，即使当前事务没有提交。这时另外一个事务读取了这个还未提交的数据，但第一个事务突然回滚，导致数据并没有被提交到数据库，那第二个事务读取到的就是脏数据，这也就是脏读的由来。\n\n例如：事务1读取某表中的数据A=20，事务1修改A=A-1，事务2读取到A=19,事务1回滚导致对A的修改并为提交到数据库，A的值还是20。\n\n#### 丢失修改（Losttomodify）\n\n在一个事务读取一个数据时，另外一个事务也访问了该数据，那么在第一个事务中修改了这个数据后，第二个事务也修改了这个数据。这样第一个事务内的修改结果就被丢失，因此称为丢失修改。\n\n例如：事务1读取某表中的数据A=20，事务2也读取A=20，事务1先修改A=A-1，事务2后来也修改A=A-1，最终结果A=19，事务1的修改被丢失。\n\n#### 不可重复读（Unrepeatableread）\n\n指在一个事务内多次读同一数据。在这个事务还没有结束时，另一个事务也访问该数据。那么，在第一个事务中的两次读数据之间，由于第二个事务的修改导致第一个事务两次读取的数据可能不太一样。这就发生了在一个事务内两次读到的数据是不一样的情况，因此称为不可重复读。\n\n例如：事务1读取某表中的数据A=20，事务2也读取A=20，事务1修改A=A-1，事务2再次读取A=19，此时读取的结果和第一次读取的结果不同。\n\n#### 幻读（Phantomread）\n\n幻读与不可重复读类似。它发生在一个事务读取了几行数据，接着另一个并发事务插入了一些数据时。在随后的查询中，第一个事务就会发现多了一些原本不存在的记录，就好像发生了幻觉一样，所以称为幻读。\n\n例如：事务2读取某个范围的数据，事务1在这个范围插入了新的数据，事务2再次读取这个范围的数据发现相比于第一次读取的结果多了新的数据。\n\n### 不可重复读和幻读有什么区别？\n\n- 不可重复读的重点是内容修改或者记录减少比如多次读取一条记录发现其中某些记录的值被修改；\n- 幻读的重点在于记录新增比如多次执行同一条查询语句（DQL）时，发现查到的记录增加了。\n\n幻读其实可以看作是不可重复读的一种特殊情况，单独区分幻读的原因主要是解决幻读和不可重复读的方案不一样。\n\n举个例子：执行`delete`和`update`操作的时候，可以直接对记录加锁，保证事务安全。而执行`insert`操作的时候，由于记录锁（RecordLock）只能锁住已经存在的记录，为了避免插入新记录，需要依赖间隙锁（GapLock）。也就是说执行`insert`操作的时候需要依赖Next-KeyLock（RecordLock+GapLock）进行加锁来保证不出现幻读。\n\n### 并发事务的控制方式有哪些？\n\nMySQL中并发事务的控制方式无非就两种：**锁和MVCC**。锁可以看作是悲观控制的模式，多版本并发控制（MVCC，Multiversionconcurrencycontrol）可以看作是乐观控制的模式。\n\n锁控制方式下会通过锁来显示控制共享资源而不是通过调度手段，MySQL中主要是通过**读写锁**来实现并发控制。\n\n- **共享锁（S锁）**：又称读锁，事务在读取记录的时候获取共享锁，允许多个事务同时获取（锁兼容）。\n- **排他锁（X锁）**：又称写锁/独占锁，事务在修改记录的时候获取排他锁，不允许多个事务同时获取。如果一个记录已经被加了排他锁，那其他事务不能再对这条记录加任何类型的锁（锁不兼容）。\n\n读写锁可以做到读读并行，但是无法做到写读、写写并行。另外，根据根据锁粒度的不同，又被分为**表级锁(table-levellocking)和行级锁(row-levellocking)**。InnoDB不光支持表级锁，还支持行级锁，默认为行级锁。行级锁的粒度更小，仅对相关的记录上锁即可（对一行或者多行记录加锁），所以对于并发写入操作来说，InnoDB的性能更高。不论是表级锁还是行级锁，都存在共享锁（ShareLock，S锁）和排他锁（ExclusiveLock，X锁）这两类。\n\n**MVCC**是多版本并发控制方法，即对一份数据会存储多个版本，通过事务的可见性来保证事务能看到自己应该看到的版本。通常会有一个全局的版本分配器来为每一行数据设置版本号，版本号是唯一的。\n\nMVCC在MySQL中实现所依赖的手段主要是:**隐藏字段、readview、undolog**。\n\n- undolog:undolog用于记录某行数据的多个版本的数据。\n- readview和隐藏字段:用来判断当前版本数据的可见性。\n\n> 关于InnoDB对MVCC的具体实现可以看这篇文章：[InnoDB存储引擎对MVCC的实现](https://javaguide.cn/database/mysql/innodb-implementation-of-mvcc.html)。\n\n### SQL标准定义了哪些事务隔离级别?\n\nSQL标准定义了四个隔离级别：\n\n- **READ-UNCOMMITTED（读取未提交）**：最低的隔离级别，允许读取尚未提交的数据变更，可能会导致脏读、幻读或不可重复读。\n- **READ-COMMITTED（读取已提交）**：允许读取并发事务已经提交的数据，可以阻止脏读，但是幻读或不可重复读仍有可能发生。\n- **REPEATABLE-READ（可重复读）**：对同一字段的多次读取结果都是一致的，除非数据是被本身事务自己所修改，可以阻止脏读和不可重复读，但幻读仍有可能发生。\n- **SERIALIZABLE（可串行化）**：最高的隔离级别，完全服从ACID的隔离级别。所有的事务依次逐个执行，这样事务之间就完全不可能产生干扰，也就是说，该级别可以防止脏读、不可重复读以及幻读。\n\n------\n\n|     隔离级别     | 脏读 | 不可重复读 | 幻读 |\n| :--------------: | :--: | :--------: | :--: |\n| READ-UNCOMMITTED |  √   |     √      |  √   |\n|  READ-COMMITTED  |  ×   |     √      |  √   |\n| REPEATABLE-READ  |  ×   |     ×      |  √   |\n|   SERIALIZABLE   |  ×   |     ×      |  ×   |\n\n### MySQL的隔离级别是基于锁实现的吗？\n\nMySQL的隔离级别基于锁和MVCC机制共同实现的。SERIALIZABLE隔离级别是通过锁来实现的，READ-COMMITTED和REPEATABLE-READ隔离级别是基于MVCC实现的。不过，SERIALIZABLE之外的其他隔离级别可能也需要用到锁机制，就比如REPEATABLE-READ在当前读情况下需要使用加锁读来保证不会出现幻读。\n\n### MySQL的默认隔离级别是什么?\n\nMySQLInnoDB存储引擎的默认支持的隔离级别是**REPEATABLE-READ（可重读）**。我们可以通过`SELECT@@tx_isolation;`命令来查看，MySQL8.0该命令改为`SELECT@@transaction_isolation;`\n\n```sql\nmysql> SELECT @@tx_isolation;\n+-----------------+\n| @@tx_isolation  |\n+-----------------+\n| REPEATABLE-READ |\n+-----------------+\n```\n\n### 相关文章\n\n\n- [面试官灵魂的一击：你懂MySQL事务吗](https://mp.weixin.qq.com/s/TWBztovuzR3jXm9TZibTIA)\n- [MySQL事务的实现原理，写得太好了！](https://mp.weixin.qq.com/s/BLmcN_hhen-lbbSlGIQcCQ)\n- [一文讲清，MySQL如何解决多事务并发问题](https://mp.weixin.qq.com/s/1x0GDRowziRtgNr2Bwz2Vw)\n","categories":["数据库","Spring"]},{"title":"Docker","slug":"Docker","url":"/blog/posts/2eaa11f9c3b9/","content":"\n## 安装\n\n### 在线安装\n\n(1) 安装docker需要关闭selinux,由于selinux和LXC（Docker实现虚拟化的方式）有冲突，所以需要禁用selinux。编辑/etc/selinux/config，设置两个关键变量。\n\n```shell\nSELINUX=disabled\n\nSELINUXTYPE=targeted\n```\n\n(2) 关闭防火墙\n\n```shell\nsystemctl stop firewalld\n```\n\n(3) 安装容器\n\n```shell\nyum -y install docker-ce\n```\n\n(4) 启动服务\n\n```shell\nsystemctl start docker\n```\n\n(5) 测试容器\n\n```shell\ndocker run hello-world\n# PS: centos7安装命令 yum -y install docker-ce | ubuntu安装命令 apt install docker-ce\n```\n\n### 离线安装\n\n(1) [下载离线包](https://download.docker.com/linux/static/stable/x86_64/)\n\n(2) 解压\n\n```shell\ntar -xvf docker-18.06.1-ce.tgz\n```\n\n(3) 将解压出来的docker文件内容移动到/usr/bin/目录下\n\n```shell\ncp docker/* /usr/bin/\n```\n\n(4) 将docker注册为service\n\n```shell\n# vim /etc/systemd/system/docker.service\n\n# lib/systemd/system、/usr/lib/systemd/system、/etc/systemd/system都可以，lib/systemd/system真实地址是/usr/lib/system/system地址，\n# /usr/lib/systemd/system/ 软件包安装的单元\n# /etc/systemd/system/ 系统管理员安装的单元,优先级更高\n# 优先级为 /etc/systemd/system，/run/systemd/system，/lib/systemd/system\n# 如果同一选项三个地方都配置了，优先级高的会覆盖优先级低的。\n\n[Unit]\nDescription=Docker Application Container Engine\nDocumentation=https://docs.docker.com\nAfter=network-online.target firewalld.service\nWants=network-online.target\n\n[Service]\n\nType=notify\n# the default is not to use systemd for cgroups because the delegate issues still\n# exists and systemd currently does not support the cgroup feature set required\n# for containers run by docker\n\nExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2375 -H unix://var/run/docker.sock\n\nExecReload=/bin/kill -s HUP $MAINPID\n\n# Having non-zero Limit*s causes performance problems due to accounting overhead\n# in the kernel. We recommend using cgroups to do container-local accounting.\n\nLimitNOFILE=infinity\n\nLimitNPROC=infinity\n\nLimitCORE=infinity\n\n# Uncomment TasksMax if your systemd version supports it.\n# Only systemd 226 and above support this version.\n# TasksMax=infinity\n\nTimeoutStartSec=0\n\n# set delegate yes so that systemd does not reset the cgroups of docker containers\n\nDelegate=yes\n\n# kill only the docker process, not all processes in the cgroup\n\nKillMode=process\n\n# restart the docker process if it exits prematurely\n\nRestart=on-failure\n\nStartLimitBurst=3\n\nStartLimitInterval=60s\n\n\n[Install]\n\nWantedBy=multi-user.target\n```\n\n(5) 添加文件权限并启动docker\n\n```shell\nchmod +x /etc/systemd/system/docker.service\n```\n\n(6) 重载unit配置文件\n\n```shell\nsystemctl daemon-reload\n```\n\n(7) 启动Docker\n\n```shell\nsystemctl start docker\n```\n\n(8) 设置开机自启\n\n```shell\nsystemctl enable docker.service\n```\n\n(9) 查看Docker状态\n\n```shell\nsystemctl status docker\n```\n\n(10) 查看Docker版本\n\n```shell\ndocker -v\n```\n\n## 常用命令\n\n### 镜像\n\n- 查看镜像\n```shell\ndocker images\n-a :列出本地所有的镜像（含中间映像层，默认情况下，过滤掉中间映像层）；\n--digests :显示镜像的摘要信息；\n-f :显示满足条件的镜像；\n--format :指定返回值的模板文件；\n--no-trunc :显示完整的镜像信息；\n-q :只显示镜像ID。\n```\n\n- 拉取镜像\n```shell\ndocker pull name:tag\n```\n\n- 推送镜像\n```shell\ndocker push myapache:v1\n```\n\n- 导出镜像\n```shell\ndocker save -o <保存路径> <镜像名称:标签>\n\ndocker save -o ./ubuntu18.tar ubuntu:18.04\n```\n\n- 导入镜像\n```shell\ndocker load -i 文件名 或者docker load --input 文件名\n\ndocker load --input ./ubuntu18.tar\n```\n\n- 删除镜像\n```shell\ndocker rmi images_id\n```\n\n- 删除所有镜像\n```shell\ndocker rmi `docker images -q`\n```\n\n- 搜索镜像\n```shell\ndocker search *\n```\n\n- 查看指定镜像的创建历史\n```shell\ndocker history [OPTIONS] IMAGE\n\nOPTIONS说明：\n-H :以可读的格式打印镜像大小和日期，默认为true；\n--no-trunc :显示完整的提交记录；\n-q :仅列出提交记录ID。\n```\n\n### 容器\n\n- 查看正在运行的容器\n```shell\ndocker ps 或者docker container ls\n```\n\n- 查看所有容器\n```shell\ndocker ps -a 或者 docker container ls -a\n```\n\n- 导出容器\n```shell\ndocker export <容器名> > <保存路径>\n\n或者docker export -o <容器名> <保存路径> -o :将输入内容写到文件。\ndocker export ubuntu18 > ./ubuntu18.tar\n将id为a404c6c174a2的容器按日期保存为tar文件\ndocker export -o mysql-`date +%Y%m%d`.tar a404c6c174a2\n```\n\n- 导入容器\n```shell\ndocker import <文件路径> <容器名>\ndocker import ./ubuntu18.tar ubuntu18\n```\n\n- 删除容器\n```shell\ndocker rm [OPTIONS] container_id\n\nOPTIONS说明：\n-f :通过 SIGKILL 信号强制删除一个运行中的容器。\n-l :移除容器间的网络连接，而非容器本身。\n-v :删除与容器关联的卷。\n```\n\n- 删除所有容器\n```shell\ndocker rm $(docker ps -a -q) \n或者\ndocker rm `docker ps -a -q`\n```\n\n- 启动容器\n```shell\ndocker container start container_id\n```\n\n- 停止所有容器\n```shell\ndocker stop $(docker ps -a -q)\n```\n\n- 杀掉运行中的容器\n```shell\ndocker kill -s(可忽略) CONTAINER\n-s :向容器发送一个信号 \n例：docker kill -s KILL mynginx\n```\n\n- 在运行的容器中执行命令\n```shell\ndocker exec [OPTIONS] CONTAINER COMMAND [ARG...]\n例如：进入容器 docker exec -itd 容器id /bin/bash\n（-d :分离模式: 在后台运行 -i :即使没有附加也保持STDIN 打开-t :分配一个伪终端）\n/bin/bash：在container中启动一个bash shell\nexit 退出bash shell\n```\n\n- 暂停容器中所有的进程\n```shell\ndocker pause container_id\n```\n\n- 恢复容器中所有的进程\n```shell\ndocker unpause container_id\n```\n\n- 创建一个新的容器不运行\n```shell\ndocker create 参数同docker run\n```\n\n- 创建一个新的容器并运行\n```shell\ndocker run\n-i: 以交互模式运行容器，通常与-t同时使用\n-t: 为容器重新分配一个伪输入终端，通常与-i同时使用\n-it 以交互模式运行\n-P: 随机端口映射，容器内部端口随机映射到主机的端口\n-p: 指定端口映射，格式为：主机(宿主)端口:容器端口\n-d 后台运行 并返回容器ID\n-v,--volume 挂载 主机目录:容器目录,绑定一个卷\n-u,--user=\"\"， 指定容器的用户\n-a,--attach=[]， 登录容器（必须是以docker run -d启动的容器）\n-w,--workdir=\"\"， 指定容器的工作目录\n-c,--cpu-shares=0， 设置容器CPU权重，在CPU共享场景使用\n-e username=\"ritchie\",--env=[] 设置环境变量容器中可以使用该环境变量\n-m,--memory=\"\"， 指定容器的内存上限\n-P,--publish-all=false，指定容器暴露的端口\n-h,--hostname=\"\"， 指定容器的主机名\n--name=”” 容器命名\n--cap-add=[]添加权限，权限清单详见：\thttp://linux.die.net/man/7/capabilities\n--cap-drop=[]删除权限，权限清单详见：\thttp://linux.die.net/man/7/capabilities\n--cidfile=\"\" 运行容器后,在指定文件中写入容器PID值，一种典型的监控系\t统用法\n--cpuset=\"\"， 设置容器可以使用哪些CPU，此参数可以用来容器独占CPU\n--device=[]， 添加主机设备给容器，相当于设备直通\n--dns=[]， 指定容器的dns服务器\n--dns-search=[]指定容器的dns搜索域名,写入到容器的/etc/resolv.conf文\t件\n--entrypoint=\"\"， 覆盖image的入口点\n--env-file=[]， 从指定文件读入环境变量\n--expose=[]， 指定容器暴露的端口，即修改镜像的暴露端口\n--link=[]， 添加链接到另一个容器，使用其他容器的IP、env等信息\n--lxc-conf=[]， 指定容器的配置文件，只有在指定--exec-driver=lxc时\t使用\n--net=\"bridge\"， 容器网络设置:\nbridge 使用docker daemon指定的网桥\nhost 容器使用主机的网络\ncontainer:NAME_or_ID > 使用其他容器的网路，共享IP和PORT等网络资源\nnone 容器使用自己的网络（类似--net=bridge），但是不进行配置\n--privileged=false指定容器是否为特权容器,特权容器拥有所有的capabilities\n--restart=\"no\"，指定容器停止后的重启策略:\nno - 容器退出时不重启\non-failure - 只在容器以非0状态码退出时重启。可选的，可以退出docker daemon尝试重启容器的次数\nalways – 不管退出状态码是什么始终重启容器。当指定always时，docker daemon将无限次数地重启容器。容器也会在daemon启动时尝试重启，不管容器当时的状态如何。\nunless-stopped – 不管退出状态码是什么始终重启容器，不过当daemon启动时，如果容器之前已经为停止状态，不要尝试启动它。\n--rm=false指定容器停止后自动删除容器(不支持以docker run -d启动的容器)\n--sig-proxy=true 设置由代理接受并处理信号，但是SIGCHLD、SIGSTOP和\tSIGKILL不能被代理\n```\n\n- 获取容器/镜像的元数据\n```shell\ndocker inspect [OPTIONS] NAME|ID [NAME|ID...]\nOPTIONS说明：\n-f :指定返回值的模板文件。\n-s :显示总的文件大小。\n--type :为指定类型返回JSON。\n```\n\n- 查看容器中运行的进程信息\n```shell\ndocker top container_id\n```\n\n- 连接到正在运行中的容器\n```shell\ndocker attach container_id\n```\n\n- 阻塞运行直到容器停止，然后打印出它的退出代码\n```shell\ndocker wait containser_id\n```\n\n### 其他\n\n- 从服务器获取实时事件\n```shell\ndocker events OPTIONS\n\nOPTIONS说明：\n-f ：根据条件过滤事件；\n--since ：从指定的时间戳后显示所有事件;\n--until ：流水时间显示到指定的时间为止；\ndocker events --since=\"1467302400\"\n```\n\n- 查看日志\n```shell\ndocker logs [OPTIONS] CONTAINER\n--details 显示更多的信息\n-f, --follow 跟踪实时日志\n--since string 显示自某个timestamp之后的日志，或相对时间，如42m（即42分钟）\n--tail string 从日志末尾显示多少行日志，默认是all\n-t, --timestamps 显示时间戳\n--until string 显示自某个timestamp之前的日志，或相对时间，如42m（即42分钟）\n\n查看指定时间后的日志，只显示最后100行：\ndocker logs -f -t --since=\"2018-02-08\" --tail=100 CONTAINER_ID\n查看最近30分钟的日志:\ndocker logs --since 30m CONTAINER_ID\n查看某时间之后的日志：\ndocker logs -t --since=\"2018-02-08T13:23:37\" CONTAINER_ID\n查看某时间段日志：\ndocker logs -t --since=\"2018-02-08T13:23:37\" --until \"2018-02-09T12:23:37\" CONTAINER_ID\n```\n\n- 列出指定的容器的端口映射\n```shell\ndocker port container_id\n```\n\n- 提交\n```shell\ndocker commit [OPTIONS] CONTAINER [REPOSITORY[:TAG]]\n\nOPTIONS说明：\n-a :提交的镜像作者；\n-c :使用Dockerfile指令来创建镜像；\n-m :提交时的说明文字；\n-p :在commit时，将容器暂停。\n\n将容器a404c6c174a2 保存为新的镜像,并添加提交人信息和说明信息。\ndocker commit -a \"runoob.com\" -m \"my apache\" a404c6c174a2 mymysql:v1\n```\n\n- 容器与主机之间的数据拷贝\n```shell\n将主机/www/runoob目录拷贝到容器96f7f14e99ab的/www目录下。\ndocker cp /www/runoob 96f7f14e99ab:/www/\n```\n\n- 查看容器文件结构更改\n```shell\ndocker diff mymysql\n```\n\n- 使用 Dockerfile 创建镜像\n```shell\ndocker build\n--build-arg=[] :设置镜像创建时的变量；\n--cpu-shares :设置 cpu 使用权重；\n--cpu-period :限制 CPU CFS周期；\n--cpu-quota :限制 CPU CFS配额；\n--cpuset-cpus :指定使用的CPU id；\n--cpuset-mems :指定使用的内存 id；\n--disable-content-trust :忽略校验，默认开启；\n-f :指定要使用的Dockerfile路径；\n--force-rm :设置镜像过程中删除中间容器；\n--isolation :使用容器隔离技术；\n--label=[] :设置镜像使用的元数据；\n-m :设置内存最大值；\n--memory-swap :设置Swap的最大值为内存+swap，\"-1\"表示不限swap；\n--no-cache :创建镜像的过程不使用缓存；\n--pull :尝试去更新镜像的新版本；\n--quiet, -q :安静模式，成功后只输出镜像 ID；\n--rm :设置镜像成功后删除中间容器；\n--shm-size :设置/dev/shm的大小，默认值是64M；\n--ulimit :Ulimit配置。\n--squash :将Dockerfile中所有的操作压缩为一层。\n--tag, -t: 镜像的名字及标签，通常name:tag或者name格式；可以在一次构建中为一个镜像设置多个标签。\n--network: 默认default。在构建期间设置RUN指令的网络模式\n```\n\n- 登陆到一个Docker镜像仓库，如果未指定镜像仓库地址，默认为官方仓库 Docker Hub\n```shell\ndocker login -u -p\n-u 登陆的用户名 -p :登陆的密码\n```\n\n- 登出一个Docker镜像仓库，如果未指定镜像仓库地址，默认为官方仓库Docker Hub\n```shell\ndocker logout\n```\n\n- 标记本地镜像，将其归入某一仓库\n```shell\ndocker tag\n```\n\n## docker加速命令\n\n```shell\ncurl -sSL https://get.daocloud.io/daotools/set_mirror.sh | sh -s http://f1361db2.m.daocloud.io\n或\nvim /etc/docker/daemon.json\n{\n  \"registry-mirrors\": [\"https://registry.docker-cn.com\"]\n}\n\n```\n> 其他站点\n> http://hub-mirror.c.163.com\n> https://3laho3y3.mirror.aliyuncs.com\n> http://f1361db2.m.daocloud.io\n> https://mirror.ccs.tencentyun.com\n> https://docker.mirrors.ustc.edu.cn\n\n## 与Spring Boot\n\n- [一键部署Spring Boot到远程Docker容器](https://mp.weixin.qq.com/s/15ZAVUg5DfcF53QpEetT7Q)\n- [Jenkins+Docker一键自动化部署SpringBoot项目](https://mp.weixin.qq.com/s/dP-c3twzR0PMUvPWZA-U0Q)\n- [搭建SpringBoot项目并将其Docker化](https://mp.weixin.qq.com/s/CXUwpTbAVoXEeB7EcrCjAw)\n- [SpringBoot使用Docker快速部署项目](https://mp.weixin.qq.com/s?__biz=Mzg2MDYzODI5Nw==&mid=2247493762&idx=1&sn=114663a4a13ba5bb27d05e0d77de37c1&source=41#wechat_redirect)\n- [Docker部署Spring Boot项目的2种方式！](https://mp.weixin.qq.com/s/du2sypGQczJh7gQz_4IX9g)\n- [SpringBoot项目构建Docker镜像深度调优](https://mp.weixin.qq.com/s?__biz=Mzg2MDYzODI5Nw==&mid=2247493962&idx=1&sn=af6c945d629003cfd30564698c017598&source=41#wechat_redirect)\n- [还在手动部署springboot项目？不妨试试它，让你部署项目飞起来！](https://mp.weixin.qq.com/s/01SZo3NNf5zuAC8wAI6C-g)\n- [Docker+Spring Boot+FastDFS搭建一套分布式文件服务器，太强了！](https://mp.weixin.qq.com/s/HSRIYQVKR9TGtwetd3LU5w)\n\n\n## 相关文章\n\n- [图解Docker架构，傻瓜都能看懂！](https://mp.weixin.qq.com/s/ELZo2z4fHonoBGXQI0M9CA)\n- [构建Java镜像的10个最佳实践](https://mp.weixin.qq.com/s/gmZDBuYDXnNdykEx66Y0Cw)\n- [10个冷门但又非常实用的Docker使用技巧！！](https://mp.weixin.qq.com/s/LOmqsoBJd7h1HPwf0i1uwQ)\n- [Docker实战总结](https://mp.weixin.qq.com/s/tTsizeLeVyvQ44GXMNqrjA)\n- [Docker从入门到干活，看这一篇足矣](https://mp.weixin.qq.com/s/t81enr-ypBxk1K4lYqWZww)\n- [如何编写最佳的Dockerfile](https://mp.weixin.qq.com/s/x-M5iKvvuseIQwUdVmxSPQ)\n- [Docker：Docker Compose详解](https://www.jianshu.com/p/658911a8cff3)\n- [CentOS/Ubuntu安装Docker和Docker Compose](https://mp.weixin.qq.com/s/fB59zXK7cPBt1asSyUpqDg)\n- [DaoCloud安装docker指南](http://guide.daocloud.io/dcs/docker-9152677.html)\n- [Docker常用命令，还有谁不会？](https://mp.weixin.qq.com/s/fzlNnJe9SMA5k3TDXOfZUA)\n- [一款吊炸天的Docker图形化工具，太强大](https://mp.weixin.qq.com/s/PpI7_fY5ACjmtmnlqr7ZMQ)\n- [5款顶级Docker容器GUI管理工具！免费又好用](https://mp.weixin.qq.com/s/w0sFaHApOSrwgva0886ijQ)\n- [Docker轻量级编排创建工具Humpback](https://mp.weixin.qq.com/s/rAOsia2LU2_Fl4vrjQ2tvA)\n- [带着问题学Kubernetes架构！](https://mp.weixin.qq.com/s/6smzsvYSbRvSPcpbfnH98A)\n- [为什么大家都在学习k8s](https://mp.weixin.qq.com/s/B2tIs6YitA93iYxEZ_8Ovw)\n- [Kuboard-Kubernetes多集群管理界面](https://kuboard.cn/)\n- [图文详解Kubernetes，傻瓜都能看懂！](https://mp.weixin.qq.com/s/WWRp-e9QPcLg8-m-V3UU1Q)\n- [Kubernetes缺少的多租户功能，你可以通过这些方式实现](https://mp.weixin.qq.com/s/8UJnsx0NJyxlKXeduhg5Yw)\n- [IDEA使用Docker插件，实现一键自动化部署](https://mp.weixin.qq.com/s/yg5ACCeeyJa0AVP1LatUhA)\n- [Docker有几种网络模式](https://mp.weixin.qq.com/s/KU3bpxiNbHGJQ_XVRqsedg)\n","tags":["安装"],"categories":["技术栈"]},{"title":"MySQL安装","slug":"MySQL安装","url":"/blog/posts/907da0ca1c57/","content":"\n## Linux安装MySQL\n\n### 压缩包安装\n\n#### 1. 下载\n\n~~[MySQL下载地址](https://downloads.mysql.com/archives/community/)~~,[MySQL下载地址](https://dev.mysql.com/downloads/)下载后将安装包上传usr/local\n\n#### 2. 解压\n\n```shell\ntar -zxvf mysql-5.6.33-linux-glibc2.5-x86_64.tar.gz\n```\n> 解压后位置usr/local/mysql\n\n#### 3. 创建用户及用户组\n\n先检查是否有mysql用户组和mysql用户,没有就添加\n```shell\n# 查看所有用户\ncat /etc/passwd\n# 查看用户组\ngroups mysql\n# 添加用户组\ngroupadd mysql\n# 删除用户组 groupdel\n```\n添加用户mysql然后将mysql用户添加到到用户组mysql\n```shell\n# -r建立系统帐号 -g<群组>指定用户所属的群组。\nuseradd -r -g mysql mysql\n# userdel删除\n```\n#### 4. 操作\n\n修改配置文件vim /etc/my.cnf（没有就新建）\n```shell\n[client]\ndefault_character_set=utf8mb4\n\n[mysql]\ndefault_character_set=utf8mb4\n\n[mysqld]\n# *\t接收所有的IPv4或IPv6连接请求\n# 0.0.0.0接受所有的IPv4地址\n# ::接受所有的IPv4或IPv6地址\n# IPv4-mapped接受所有的IPv4地址或IPv4邦定格式的地址（例 ::ffff:127.0.0.1）\n# IPv4（IPv6）只接受对应的IPv4（IPv6）地址\nbind-address=0.0.0.0\n# port=3306\n# 指定安装用户\nuser=mysql\nbasedir=/usr/local/mysql\ndatadir=/usr/local/mysql/data\nsocket=/tmp/mysql.sock\nlog-error=/usr/local/mysql/mysql.err\npid-file=/usr/local/mysql/data/mysql/mysql.pid\ncharacter_set_server=utf8mb4\n# 符号连接,如果设置为1,则mysql数据库和表里的数据支持储存在datadir目录之外的路径下,默认都是0\nsymbolic-links=0\n# 默认情况下，timestamp类型字段所在数据行被更新时，该字段会自动更新为当前时间，而参数explicit_defaults_for_timestamp控制这一种行为。\n# 数据行更新时，timestamp类型字段更新为当前时间\n# explicit_defaults_for_timestamp=off\n# 数据行更新时，timestamp类型字段不更新为当前时间。\n# explicit_defaults_for_timestamp=on\n# 允许最大连接数\nmax_connections=200\ndefault-storage-engine=INNODB\n# 连接层默认字符集\ncollation-server=utf8mb4_unicode_ci\nwait_timeout=31536000\ninteractive_timeout=31536000\n```\n\n进入安装mysql软件目录`cd mysql/`,修改目录拥有者为mysql用户`chown -R mysql:mysql ./`\n\n##### 5.6版本\n\n```shell\n# --defaults-file指定配置文件 --basedir指定Mysql安装目录 --datadir指定数据目录 --user所属用户\n./scripts/mysql_install_db --user=mysql --basedir=/usr/local/mysql --datadir=/usr/local/mysql/data\n```\n\n> 此处可能出现错误\n>\n> **错误1**：命令报错`-bash: ./scripts/mysql_install_db: /usr/bin/perl: bad interpreter: No such file or directory.`貌似提示注释器错误，没有/usr/bin/perl文件或者档案\n> **解决办法**:（安装perl跟perl-devel即可）：执行**yum -y install perl perl-devel**后在初始化数据库即可。\n>\n> **错误2**：`FATAL ERROR: please install the following Perl modules before executing * Data::Dumper`\n> **解决办法:yum install -y perl-Data-Dumper**\n>\n> 修改当前目录拥有者为root用户\n> ```shell\n> chown -R root:root ./\n> ```\n> 再次运行\n> ```shell\n> ./scripts/mysql_install_db --user=mysql --basedir=/usr/local/mysql --datadir=/usr/local/mysql/data\n> ```\n> 修改当前data目录拥有者为mysql用户\n> ```shell\n> chown -R mysql:mysql data\n> ```\n\n##### 5.7版本\n\n执行命令\n```shell\n./mysqld --defaults-file=/etc/my.cnf --basedir=/usr/local/mysql/ --datadir=/usr/local/mysql/data/ --user=mysql --initialize\n```\n查看初始密码,控制台查看或者`cat /usr/local/mysql/mysql.err`localhost:后面的一串字符就是初始密码\n\n##### 8.0版本\n\n```shell\nbin/mysqld --user=mysql --initialize --datadir=/usr/local/mysql/data\n```\nroot@localhost之后的一串字符就是初始密码\n\n> **错误1**：服务未成功启动查看data目录下的localhost.localdomain.err错误日志\n> 遇到`Can't start server : Bind on unix socket: Address already in use Do you already have another mysqld server running on socket: /tmp/mysql.sock ?`解决方法是删除 rm -rf /tmp/mysql.sock(由于以前安装过mysql的原因)\n>\n> **错误2**：./mysqld: error while loading shared libraries: libnuma.so.1: cannot open shared object file: No such file or directory就执行下`yum install -y libaio`、` yum -y install numactl`后再执行初始化\n\n#### 5. 启动mysql\n\n添加开机启动，把启动脚本放到开机初始化目录。\n\n```shell\ncp /usr/local/mysql/support-files/mysql.server /etc/init.d/mysql\n```\n赋予可执行权限\n```shell\nchmod +x /etc/init.d/mysql\n```\n启动mysql服务\n```shell\nservice mysql start\n```\n\n#### 6. 修改密码\n\n```shell\n# 测试进入mysql\nmysql -uroot -proot\n# UPDATE直接编辑user表\nupdate mysql.user set password=password('新密码') where host='localhost' and user='root';\n# 或者使用SET PASSWORD命令\nset password for root@localhost = password('新密码');\n# 或者使用mysqladmin修改密码\n./bin/mysqladmin -hlocalhost.localdomain -uroot -p旧密码 password '新密码'\n# MySQL8使用\nupdate user set authentication_string='' where user='root';# 将字段置为空\nALTER user 'root'@'localhost' IDENTIFIED BY 'root';#修改密码为root\nALTER USER 'root'@'localhost' IDENTIFIED WITH mysql_native_password BY '新密码';\n# 进入mysql更改密码\nset password='root';\n# 使密码生效\nflush privileges;\n```\n\n#### 7. 修改远程连接\n\n改表法\n```shell\nuse mysql\nupdate user set host='%' where user='root';\nflush privileges;\n```\n授权法\n```shell\nGRANT ALL PRIVILEGES ON *.* TO 'root'@'%' IDENTIFIED BY '密码' WITH GRANT OPTION;\n# 如果你想允许用户myuser从ip为192.168.1.6的主机连接到mysql服务器，并使用mypassword作为密码\nGRANT ALL PRIVILEGES ON *.* TO 'myuser'@'192.168.1.3' IDENTIFIED BY 'mypassword' WITH GRANT OPTION;\n```\n\n#### 8. 其他\n\n添加服务自启动\n```shell\nchkconfig --add mysql\n```\n显示服务列表\n```shell\nchkconfig --list\n# 如果看到mysql的服务，并且3,4,5都是on的话则成功，如果是off，则执行\nchkconfig --level 345 mysql on\n```\n\n> 假如服务未成功启动[log-error set to '/var/log/mariadb/mariadb.log', however file don't exists](http://blog.csdn.net/duyuanhai/article/details/78604894)**报错的情况下执行以下操作**:需要创建报错中缺失的文件夹`mkdir z/var/log/mariadb，touch /var/log/mariadb/mariadb.log`\n\n创建软连接\n```shell\nln -s /usr/local/mysql/bin/mysql /usr/local/bin/mysql\n```\nmysql -uroot -proot无法连接的话\n\n```shell\nln -s /var/lib/mysql/mysql.sock /tmp/mysql.sock\n```\n\n默认配置文件路径如下\n\n```\n配置文件：/etc/my.cnf\n日志文件：/var/log/var/log/mysqld.log\n服务启动脚本：/usr/lib/systemd/system/mysqld.service\nsocket文件：/var/run/mysqld/mysqld.pid\n```\n### yum安装\n\n#### 一、安装本地YUM源、MySQL在MySQL官网中下载YUM源rpm安装包\n\n(1) 把上面的rpm文件下载下来放到服务器上,或者在linux系统中通过wget命令下载\n\n```shell\nwget http://dev.mysql.com/get/mysql80-community-release-el7-1.noarch.rpm\n```\n\n(2) 下载完成后使用yum命令本地安装yum源\n\n```shell\nyum localinstall mysql80-community-release-el7-1.noarch.rpm\n```\n\n(3) 执行完毕后使用下面的命令检查是否安装成功\n\n```shell\nyum repolist enabled | grep \"mysql.*-community.*\"\n```\n\n(4) 安装服务器\n\n```shell\nyum install -y mysql-community-server\n```\n\n#### 二、配置MySQL\n\n服务命令\n```shell\n# 启动MySQL服务\nsystemctl start mysqld\n# 查看服务启动状态\nsystemctl status mysqld\n# 开机启动\nsystemctl enable mysqld\n# 重新加载开机启动配置\nsystemctl daemon-reload\n\n# 修改root默认密码\n# 查询默认密码\ncat /var/log/mysqld.log | grep 'temporary password'\n# 登录mysql,用刚才从文件中找到的密码\nmysql -uroot -p\n# 尝试修改密码,使用下面的命令修改root用户的密码\nALTER USER 'root'@'localhost' IDENTIFIED WITH mysql_native_password BY '你的密码';\n\n# 添加远程登陆\n# 创建一个能全局访问的用户root\n# CREATE USER 'root'@'%' IDENTIFIED BY 你的密码'';\n# 给用户授权任何远程主机都可以访问数据库 \nGRANT ALL PRIVILEGES ON *.* TO 'root'@'%'WITH GRANT OPTION;\n# 输入刷新命令使修改生效\nFLUSH PRIVILEGES;\n# 修改密码的加密方式\n# 找到mysql的配置文件vim /etc/my.cnf，把密码的加密方式改成之前版本的,8.0版本更换了密码的加密方式,我们就先用旧的\n# 找到default-authentication-plugin，将其注释取消\ndefault-authentication-plugin=mysql_native_password\n# 修改完my.cnf后重启服务，使其生效\nsystemctl restart mysqld\n\n# sql_mode=only_full_group_by问题\n# 查看sql_mode\nselect @@global.sql_mode;\n# 查询出来的值为：\nONLY_FULL_GROUP_BY,STRICT_TRANS_TABLES,NO_ZERO_IN_DATE,NO_ZERO_DATE,ERROR_FOR_DIVISION_BY_ZERO,NO_ENGINE_SUBSTITUTION\n\n# 修改my.cnf，在[mysqld]栏下新增sql_mode，将ONLY_FULL_GROUP_BY去掉\nvim /etc/my.cnf\nsql_mode='STRICT_TRANS_TABLES,NO_ZERO_IN_DATE,NO_ZERO_DATE,ERROR_FOR_DIVISION_BY_ZERO,NO_ENGINE_SUBSTITUTION'\n# 重启服务\nsystemctl restart mysqld\n```\n\n### rpm安装\n\n(1) 需要通过rpm相关指令，来查询当前系统中是否存在已安装的mysql软件包\n\n```shell\n# 查询当前系统中安装的所有软件\nrpm -qa\n# 查询当前系统中安装的名称带mysql的软件\nrpm -qa | grep mysql\n# 查询当前系统中安装的名称带mariadb的软件\nrpm -qa | grep mariadb\n```\n\n(2) 卸载\n\n```shell\nrpm -ev --nodeps 软件名称\n# 卸载之前请先关闭mysql服务，命令如下\nsystemctl stop mysqld\n# 依次卸载相关服务\nrpm -e --nodeps mysql-community-server\nrpm -e --nodeps mysql-community-client\nrpm -e --nodeps mysql-community-libs\nrpm -e --nodeps mysql-community-common\n#卸载mariadb\nrpm -e --nodeps mariadb-libs-5.5.64-1.el7.x86_64\n# 删除数据库配置文件\nrm -rf /etc/my.cnf\n# 删除数据库数据文件\nrm -rf /var/lib/mysql\n# 删除日志临时文件\nrm -rf /var/log/mysqld.log\n```\n\n(3) 下载rpm\n\n![下载](https://img-blog.csdnimg.cn/img_convert/7ec61ebdc9103cd70f3dd52efbd4ea19.png)\n或者使用wget\n\n```shell\n# 下载server包\nwget https://cdn.mysql.com//Downloads/MySQL-5.7/mysql-community-server-5.7.38-1.el7.x86_64.rpm\n# 下载client包\nwget https://cdn.mysql.com//Downloads/MySQL-5.7/mysql-community-client-5.7.38-1.el7.x86_64.rpm\n# 下载common包\nwget https://cdn.mysql.com//Downloads/MySQL-5.7/mysql-community-common-5.7.38-1.el7.x86_64.rpm\n# 下载libs包\nwget https://cdn.mysql.com//Downloads/MySQL-5.7/mysql-community-libs-5.7.38-1.el7.x86_64.rpm\n```\n\n(4) 安装\n\n> 包之间相互依赖，所以必须注意安装顺序\n> ```shell\n> # 相关依赖\n> yum install libaio -y\n> yum install net-tools -y\n> ```\n\n先装common,再装libs（确保mariadb已卸载，centos7默认支持mariadb，不支持mysql，不卸载会出现冲突）,再装client,最后装server\n```shell\n# i表示install安装；v表示verbose进度条；h表示hash哈希校验。\nrpm -ivh mysql-community-common-8.0.11-1.el7.x86_64.rpm\nrpm -ivh mysql-community-libs-8.0.11-1.el7.x86_64.rpm\n# rpm -ivh mysql-community-devel-5.7.25-1.el7.x86_64.rpm\n# rpm -ivh mysql-community-libs-compat-5.7.25-1.el7.x86_64.rpm\nrpm -ivh mysql-community-client-8.0.11-1.el7.x86_64.rpm\nrpm -ivh mysql-community-server-8.0.11-1.el7.x86_64.rpm\n```\n\n(5) 查看密码\n\n对于rpm安装的mysql，在mysql第一次启动时，会自动帮我们生成root用户的访问密码，并且输出在mysql的日志文件/var/log/mysqld.log中，我们可以查看这份日志文件，从而获取到访问密码\n```shell\ngrep 'temporary password' /var/log/mysqld.log\n# 或者\ncat /var/log/mysqld.log | grep password\n```\n\n(6) 配置其他内容如编码则新建`/etc/my.cnf`\n\n\n## Windows安装MySQL\n\n### 卸载\n```\n关闭mysql服务:net stop mysql\n删除mysql服务:mysqld -remove\n删除mysql相关文件夹，如想保留数据可保留data文件\n```\n### 安装\n\n(1) 解压mysql压缩包后进入mysql目录，新建my.ini文件并编辑\n\n```shell\n[mysqld]\n# 设置3306端口\nport = 3306\n# 设置mysql的安装目录\nbasedir=D:\\mysql\\mysql-5.8.11-winx64\n# 设置mysql数据库的数据的存放目录\ndatadir=D:\\mysql\\mysql-5.8.11-winx64\\data\n# 允许最大连接数\nmax_connections=200\n# 创建新表时将使用的默认存储引擎\ndefault-storage-engine=INNODB\n```\n\n(2) 将mysql加入环境变量\n\n(3) 管理员打开cmd切换到mysql bin目录下(一定要切换),执行**mysqld install**安装mysql服务，成功打印**service successfully installed**后执行**mysqld --initialize --console**会对数据库做初始化并打印相应日志：可以找到root@localhost之后的一串字符就是初始密码，然后执行net start mysql开启服务\n\n(4) 修改密码（5.7版本以前）\n\n```shell\n# mysql -uroot -p输入密码后\nset password for root@localhost = password('123456');\n# 8.0修改密码\nset password = '123456';\n```\n\n### 8.0连接navicat\n\nnavicat连接8.0需要把mysql用户登录密码加密规则还原成mysql_native_password\n\n```shell\nALTER USER 'root'@'localhost' IDENTIFIED BY 'root' PASSWORD EXPIRE NEVER; #修改加密规则\nALTER USER 'root'@'localhost' IDENTIFIED WITH mysql_native_password BY 'root'; #更新一下用户的密码\nFLUSH PRIVILEGES; #刷新权限\n# 然后use mysql;\nupdate user set host = '%' where user = 'root';\nALTER USER 'root'@'localhost' IDENTIFIED BY '新密码'\nFLUSH PRIVILEGES;\n```\n\n## 相关文章\n- [一文教你在CentOS7下安装MySQL及搭建主从复制](https://mp.weixin.qq.com/s?__biz=MzkzODE3OTI0Ng==&mid=2247491190&idx=1&sn=6e6ed61a51e2f214d19e304038bde8b4&source=41#wechat_redirect)\n- [手把手教大家搭建MySQL主从复制](https://mp.weixin.qq.com/s/R89aCCFvCvudLp6FUn2JjQ)","tags":["安装"],"categories":["数据库"]},{"title":"JVM知识点","slug":"JVM知识点","url":"/blog/posts/be5a099f7894/","content":"\n## JVM内存区域\n\n- Java虚拟机栈（栈区）\n- 本地方法栈\n- Java堆（堆区）\n- 方法区\n- 程序计数器\n\nJava虚拟机在执行Java程序的过程中会把它管理的内存划分成若干个不同的数据区域。JDK1.8和之前的版本略有不同，下面会介绍到。\n\n**JDK1.8之前**：\n\n![Java运行时数据区域（JDK1.8之前）](https://guide-blog-images.oss-cn-shenzhen.aliyuncs.com/github/javaguide/java/jvm/java-runtime-data-areas-jdk1.7.png)\n\n**JDK1.8之后**：\n\n![Java运行时数据区域（JDK1.8之后）](https://guide-blog-images.oss-cn-shenzhen.aliyuncs.com/github/javaguide/java/jvm/java-runtime-data-areas-jdk1.8.png)\n\n**线程私有的**：\n\n- 程序计数器\n- 虚拟机栈\n- 本地方法栈\n\n**线程共享的**：\n\n- 堆\n- 方法区\n- 直接内存(非运行时数据区的一部分)\n\n\nJava虚拟机规范对于运行时数据区域的规定是相当宽松的。以堆为例：堆可以是连续空间，也可以不连续。堆的大小可以固定，也可以在运行时按需扩展。虚拟机实现者可以使用任何垃圾回收算法管理堆，甚至完全不进行垃圾收集也是可以的\n\n\n### Java堆\n\nGC堆是java虚拟机所管理的内存中最大的一块内存区域，也是被各个线程共享的内存区域，在JVM启动时创建。其大小通过-Xms(最小值)和-Xmx(最大值)参数设置，-Xms为JVM启动时申请的最小内存，-Xmx为JVM可申请的最大内存。由于现在收集器都是采用分代收集算法，堆被划分为新生代和老年代。新生代可通过-Xmn参数来指定新生代的大小。对象刚创建的时候，会被创建在新生代，到一定阶段之后会移送至老年代，如果创建了一个新生代无法容纳的新对象，那么这个新对象也可以创建到老年代\n**所有对象实例以及数组都在堆上分配**。堆内存用来存放由new创建的对象实例和数组,堆中不存放基本数据类型和对象引用，只存放对象本身(包括属性,即成员变量),jvm只有一个堆区(heap)被所有线程共享\n\n- **新生代(Young Gen)** ：新生代主要存放新创建的对象，内存大小相对会比较小，垃圾回收会比较频繁。新生代分为1个Eden区和2个S区，S代表Survivor。当对象在堆创建时，将进入Eden Space。垃圾回收器进行垃圾回收时，它的策略是会把没有引用的对象直接给回收掉，还有引用的对象会被移送到Survivor区。Survivor区有S0和S1两个内存空间，每次进行YGC的时候，会将存活的对象复制到未使用的那块内存空间，然后将当前正在使用的空间完全清除掉，再交换两个空间的使用状况。如果YGC要移送的对象Survivor区无法容纳，那么就会将该对象直接移交给老年代。上面说了，到一定阶段的对象会移送到老年区，这是什么意思呢？每一个对象都有一个计数器，当每次进行YGC的时候，都会+1。通过-XX:MAXTenuringThrehold参数可以配置当计数器的值到达某个阈值时，对象就会从新生代移送至老年代。该参数的默认值为15，也就是说对象在Survivor区中的S0和S1内存空间交换的次数累加到15次之后，就会移送至老年代。如果参数配置为1，那么创建的对象就会直接移送至老年代。扫描完毕后，JVM将Eden Space和A Suvivor Space清空，然后交换A和B的角色，即下次垃圾回收时会扫描Eden Space和B Suvivor Space。这么做主要是为了减少内存碎片的产生。我们可以看到：Young Gen垃圾回收时，采用将存活对象复制到到空的Suvivor Space的方式来确保尽量不存在内存碎片，采用空间换时间的方式来加速内存中不再被持有的对象尽快能够得到回收。\n- **老年代(Tenured Gen)** ：老年代主要存放JVM认为生命周期比较长的对象（经过几次的Young Gen的垃圾回收后仍然存在），内存大小相对会比较大，垃圾回收也相对没有那么频繁（譬如可能几个小时一次）。老年代主要采用压缩的方式来避免内存碎片（将存活对象移动到内存片的一边，也就是内存整理）。当然，有些垃圾回收器（譬如CMS垃圾回收器）出于效率的原因，可能会不进行压缩。\n\n\n堆是Java虚拟机所管理的内存中最大的一块，Java堆是所有线程共享的一块内存区域，在虚拟机启动时创建。**此内存区域的唯一目的就是存放对象实例，几乎所有的对象实例以及数组都在这里分配内存**。\n\nJava世界中“几乎”所有的对象都在堆中分配，但是，随着JIT编译器的发展与逃逸分析技术逐渐成熟，栈上分配、标量替换优化技术将会导致一些微妙的变化，所有的对象都分配到堆上也渐渐变得不那么“绝对”了。从JDK1.7开始已经默认开启逃逸分析，如果某些方法中的对象引用没有被返回或者未被外面使用（也就是未逃逸出去），那么对象可以直接在栈上分配内存。\n\nJava堆是垃圾收集器管理的主要区域，因此也被称作**GC堆（Garbage Collected Heap）**。从垃圾回收的角度，由于现在收集器基本都采用分代垃圾收集算法，所以Java堆还可以细分为：新生代和老年代；再细致一点有：Eden、Survivor、Old等空间。进一步划分的目的是更好地回收内存，或者更快地分配内存。\n\n在JDK7版本及JDK7版本之前，堆内存被通常分为下面三部分：\n\n1. 新生代内存(Young Generation)\n2. 老生代(Old Generation)\n3. 永久代(Permanent Generation)\n\nEden区、两个Survivor区S0和S1都属于新生代，中间一层属于老年代，最下面一层属于永久代。\n\n**JDK8版本之后PermGen(永久)已被Metaspace(元空间)取代，元空间使用的是本地内存**。\n\n大部分情况，对象都会首先在Eden区域分配，在一次新生代垃圾回收后，如果对象还存活，则会进入S0或者S1，并且对象的年龄还会加1(Eden区->Survivor区后对象的初始年龄变为1)，当它的年龄增加到一定程度（默认为15岁），就会被晋升到老年代中。对象晋升到老年代的年龄阈值，可以通过参数-XX:MaxTenuringThreshold来设置。\n\n> **🐛修正（参见：[issue552](https://github.com/Snailclimb/JavaGuide/issues/552)）**：“Hotspot遍历所有对象时，按照年龄从小到大对其所占用的大小进行累积，当累积的某个年龄大小超过了survivor区的一半时，取这个年龄和MaxTenuringThreshold中更小的一个值，作为新的晋升年龄阈值”。\n>\n> **动态年龄计算的代码如下**\n>\n> ```c++\n>uint ageTable::compute_tenuring_threshold(size_t survivor_capacity) {\n> \t//survivor_capacity是survivor空间的大小\n> size_t desired_survivor_size = (size_t)((((double) survivor_capacity)*TargetSurvivorRatio)/100);\n> size_t total = 0;\n> uint age = 1;\n> while (age < table_size) {\n> total += sizes[age];//sizes数组是每个年龄段对象大小\n> if (total > desired_survivor_size) break;\n> age++;\n> }\n> uint result = age < MaxTenuringThreshold ? age : MaxTenuringThreshold;\n> \t...\n> }\n> ```\n\n堆这里最容易出现的就是OutOfMemoryError错误，并且出现这种错误之后的表现形式还会有几种，比如：\n\n1. **java.lang.OutOfMemoryError: GC Overhead Limit Exceeded**：当JVM花太多时间执行垃圾回收并且只能回收很少的堆空间时，就会发生此错误。\n2. **java.lang.OutOfMemoryError: Java heap space**：假如在创建新的对象时,堆内存中的空间不足以存放新创建的对象,就会引发此错误。(和配置的最大堆内存有关，且受制于物理内存大小。最大堆内存可通过`-Xmx`参数配置，若没有特别配置，将会使用默认值，详见：[Default Java 8 max heap size](https://stackoverflow.com/questions/28272923/default-xmxsize-in-java-8-max-heap-size))\n\n### Java虚拟机栈\n\n每个线程包含一个栈区，栈中只保存基础数据类型的对象和自定义对象的引用(不是对象)，对象都存放在堆区中,每个栈中的数据(原始类型和对象引用)都是私有的，其他栈不能访问。定义的局部变量也在栈内存中,线程私有,FILO(先进后出)\n\n与程序计数器一样，Java虚拟机栈也是线程私有的，它的生命周期与线程相同,随着线程的创建而创建，随着线程的死亡而死亡。栈绝对算的上是JVM运行时数据区域的一个核心，除了一些Native方法调用是通过本地方法栈实现的(后面会提到)，其他所有的Java方法调用都是通过栈来实现的（也需要和其他运行时数据区域比如程序计数器配合）。每个方法被执行的时候都会创建一个\"栈帧\",用于存储局部变量表(包括参数)、操作数栈、动态链接、方法出口等信息。每个方法被调用到执行完的过程，就对应着一个栈帧在虚拟机栈中从入栈到出栈的过程。局部变量表存放各种基本数据类型boolean、byte、char、short等。方法调用的数据需要通过栈进行传递，每一次方法调用都会有一个对应的栈帧被压入栈中，每一个方法调用结束后，都会有一个栈帧被弹出。\n\n栈由一个个栈帧组成，而每个栈帧中都拥有：局部变量表、操作数栈、动态链接、方法返回地址。和数据结构上的栈类似，两者都是先进后出的数据结构，只支持出栈和入栈两种操作。\n\n![Java虚拟机栈](https://guide-blog-images.oss-cn-shenzhen.aliyuncs.com/github/javaguide/java/jvm/stack-area.png)\n\n**局部变量表**主要存放了编译期可知的各种数据类型（boolean、byte、char、short、int、float、long、double）、对象引用（reference类型，它不同于对象本身，可能是一个指向对象起始地址的引用指针，也可能是指向一个代表对象的句柄或其他与此对象相关的位置）。\n\n![局部变量表](https://guide-blog-images.oss-cn-shenzhen.aliyuncs.com/github/javaguide/java/jvm/local-variables-table.png)\n\n**操作数栈**主要作为方法调用的中转站使用，用于存放方法执行过程中产生的中间计算结果。另外，计算过程中产生的临时变量也会放在操作数栈中。\n\n**动态链接**主要服务一个方法需要调用其他方法的场景。Class 文件的常量池里保存有大量的符号引用比如方法引用的符号引用。当一个方法要调用其他方法，需要将常量池中指向方法的符号引用转化为其在内存地址中的直接引用。动态链接的作用就是为了将符号引用转换为调用方法的直接引用，这个过程也被称为**动态连接**。\n\n![img](https://guide-blog-images.oss-cn-shenzhen.aliyuncs.com/github/javaguide/jvmimage-20220331175738692.png)\n\n栈空间虽然不是无限的，但一般正常调用的情况下是不会出现问题的。不过，如果函数调用陷入无限循环的话，就会导致栈中被压入太多栈帧而占用太多空间，导致栈空间过深。那么当线程请求栈的深度超过当前Java虚拟机栈的最大深度的时候，就抛出StackOverFlowError错误。\n\nJava方法有两种返回方式，一种是return语句正常返回，一种是抛出异常。不管哪种返回方式，都会导致栈帧被弹出。也就是说，**栈帧随着方法调用而创建，随着方法结束而销毁。无论方法正常完成还是异常完成都算作方法结束**。\n\n除了StackOverFlowError错误之外，栈还可能会出现OutOfMemoryError错误，这是因为如果栈的内存大小可以动态扩展，如果虚拟机在动态扩展栈时无法申请到足够的内存空间，则抛出OutOfMemoryError异常。\n\n简单总结一下程序运行中栈可能会出现两种错误：\n\n- **StackOverFlowError**：若栈的内存大小不允许动态扩展，那么当线程请求栈的深度超过当前Java虚拟机栈的最大深度的时候，就抛出StackOverFlowError错误。\n- **OutOfMemoryError**：如果栈的内存大小可以动态扩展，如果虚拟机在动态扩展栈时无法申请到足够的内存空间，则抛出OutOfMemoryError异常。\n\n### 本地方法栈\n\n线程私有FILO(先进后出)\n和虚拟机栈所发挥的作用非常相似，区别是：**虚拟机栈为虚拟机执行Java方法（也就是字节码）服务，而本地方法栈则为虚拟机使用到的Native方法服务**。在HotSpot虚拟机中和Java虚拟机栈合二为一。本地方法被执行的时候，在本地方法栈也会创建一个栈帧，用于存放该本地方法的局部变量表、操作数栈、动态链接、出口信息。方法执行完毕后相应的栈帧也会出栈并释放内存空间，也会出现StackOverFlowError和OutOfMemoryError两种错误。\n\n\n### 方法区\njdk1.7及以前方法区也被称为永久代，1.8之后移除，取而代之的为metaspace元空间,注意：元空间，永久代都是方法区的一种实现,永久代主要存放类定义、字节码和常量等很少会变更的信息。永久带是方法区的一种实现，可以理解为就是方法区，类似于java的接口和实现类\n\n方法区是各个线程共享的内存区域，它用于存储已被虚拟机加载的类信息、静态变量、final类型的常量、属性和方法信息，即时编译器编译后的代码等数据,也称”永久代”，它用于存储虚拟机加载的类信息、常量、静态变量、是各个线程共享的内存区域。可以通过-XX:PermSize和-XX:MaxPermSize参数限制方法区的大小。\n\n运行时常量池：是方法区的一部分，其中的主要内容来自于JVM对Class的加载。\nClass文件中除了有类的版本、字段、方法、接口等描述信息外，还有一项信息是常量池，用于存放编译器生成的各种符号引用，这部分内容将在类加载后放到方法区的运行时常量池中。\n\nJava1.7之前，常量池是存放在方法区中的，运行常量池是方法区的一部分。方法区是堆的一个逻辑部分，他有一个名字叫做非堆。\n> jdk1.7之前\n> ![](/images/jvm1.7.png)\n\n\nJava1.7，把字符串常量池放到了堆中。JVM已经将运行时常量池从方法区中移了出来，在JVM堆开辟了一块区域存放常量池。\n> jdk1.7\n> ![](images/jvm7.png)\n\nJava8之后，取消了整个永久代区域，取而代之的是元空间。方法区概念保留，方法区的实现改为了元空间，元空间也是方法区的一种实现,常量池还是在堆中,没有再对常量池进行调整。元空间占用本地内存，不再占用jvm内存\n> jdk1.8\n> ![](/images/jvm1.8.png)\n\n方法区属于是JVM运行时数据区域的一块逻辑区域，是各个线程共享的内存区域。《Java虚拟机规范》只是规定了有方法区这么个概念和它的作用，方法区到底要如何实现那就是虚拟机自己要考虑的事情了。也就是说，在不同的虚拟机实现上，方法区的实现是不同的。当虚拟机要使用一个类时，它需要读取并解析Class文件获取相关信息，再将信息存入到方法区。方法区会存储已被虚拟机加载的**类信息、字段信息、方法信息、常量、静态变量、即时编译器编译后的代码缓存等数据**。\n\n**方法区和永久代以及元空间是什么关系呢**？方法区和永久代以及元空间的关系很像Java中接口和类的关系，类实现了接口，这里的类就可以看作是永久代和元空间，接口可以看作是方法区，也就是说永久代以及元空间是HotSpot虚拟机对虚拟机规范中方法区的两种实现方式。并且，永久代是JDK1.8之前的方法区实现，JDK1.8及以后方法区的实现变成了元空间。\n\n![HotSpot虚拟机方法区的两种实现](https://guide-blog-images.oss-cn-shenzhen.aliyuncs.com/github/javaguide/java/jvm/method-area-implementation.png)\n\n**为什么要将永久代(PermGen)替换为元空间(MetaSpace)呢?**\n\n下图来自《深入理解Java虚拟机》第3版2.2.5\n\n![img](https://guide-blog-images.oss-cn-shenzhen.aliyuncs.com/github/javaguide/java/jvm/20210425134508117.png)\n\n1、整个永久代有一个JVM本身设置的固定大小上限，无法进行调整，而元空间使用的是本地内存，受本机可用内存的限制，虽然元空间仍旧可能溢出，但是比原来出现的几率会更小。\n\n> 当元空间溢出时会得到如下错误：java.lang.OutOfMemoryError:MetaSpace\n\n你可以使用`-XX：MaxMetaspaceSize`标志设置最大元空间大小，默认值为unlimited，这意味着它只受系统内存的限制。`-XX：MetaspaceSize`调整标志定义元空间的初始大小如果未指定此标志，则Metaspace将根据运行时的应用程序需求动态地重新调整大小。\n\n2、元空间里面存放的是类的元数据，这样加载多少类的元数据就不由`MaxPermSize`控制了,而由系统的实际可用空间来控制，这样能加载的类就更多了。\n\n3、在JDK8，合并HotSpot和JRockit的代码时,JRockit从来没有一个叫永久代的东西,合并之后就没有必要额外的设置这么一个永久代的地方了。\n\n> [JDK为什么废弃永久代，而引入元空间](https://mp.weixin.qq.com/s/ZxKyeZ6ZkxVR7NsTlkqtKQ)\n\n**方法区常用参数有哪些？**\n\nJDK1.8之前永久代还没被彻底移除的时候通常通过下面这些参数来调节方法区大小。\n\n\n```java\n-XX:PermSize=N //方法区(永久代)初始大小\n-XX:MaxPermSize=N //方法区(永久代)最大大小,超过这个值将会抛出OutOfMemoryError异常:java.lang.OutOfMemoryError:PermGen\n```\n\n相对而言，垃圾收集行为在这个区域是比较少出现的，但并非数据进入方法区后就“永久存在”了。\n\nJDK1.8的时候，方法区（HotSpot的永久代）被彻底移除了（JDK1.7 就已经开始了），取而代之是元空间，元空间使用的是本地内存。下面是一些常用参数：\n\n```java\n-XX:MetaspaceSize=N //设置Metaspace的初始（和最小大小）\n-XX:MaxMetaspaceSize=N //设置Metaspace的最大大小\n```\n\n与永久代很大的不同就是，如果不指定大小的话，随着更多类的创建，虚拟机会耗尽所有可用的系统内存。\n\n#### 运行时常量池\n\nClass文件中除了有类的版本、字段、方法、接口等描述信息外，还有用于存放编译期生成的各种字面量（Literal）和符号引用（Symbolic Reference）的常量池表(Constant Pool Table)。\n\n字面量是源代码中的固定值的表示法，即通过字面我们就能知道其值的含义。字面量包括整数、浮点数和字符串字面量。常见的符号引用包括类符号引用、字段符号引用、方法符号引用、接口方法符号。\n\n《深入理解Java虚拟机》7.34节第三版对符号引用和直接引用的解释如下：\n\n![符号引用和直接引用](https://guide-blog-images.oss-cn-shenzhen.aliyuncs.com/github/javaguide/java/jvm/symbol-reference-and-direct-reference.png)\n\n常量池表会在类加载后存放到方法区的运行时常量池中。运行时常量池的功能类似于传统编程语言的符号表，尽管它包含了比典型符号表更广泛的数据。既然运行时常量池是方法区的一部分，自然受到方法区内存的限制，当常量池无法再申请到内存时会抛出`OutOfMemoryError`错误。\n\n#### 字符串常量池\n\n**字符串常量池**是JVM为了提升性能和减少内存消耗针对字符串（String类）专门开辟的一块区域，主要目的是为了避免字符串的重复创建。\n\n```java\n// 在堆中创建字符串对象”ab“\n// 将字符串对象”ab“的引用保存在字符串常量池中\nString aa = \"ab\";\n// 直接返回字符串常量池中字符串对象”ab“的引用\nString bb = \"ab\";\nSystem.out.println(aa==bb);// true\n```\n\nHotSpot虚拟机中字符串常量池的实现是`src/hotspot/share/classfile/stringTable.cpp`,`StringTable`本质上就是一个`HashSet<String>`,容量为`StringTableSize`（可以通过`-XX:StringTableSize`参数来设置）。\n\n**StringTable中保存的是字符串对象的引用，字符串对象的引用指向堆中的字符串对象**。\n\nJDK1.7之前，字符串常量池存放在永久代。JDK1.7字符串常量池和静态变量从永久代移动了Java堆中。\n\n![method-area-jdk1.6](https://guide-blog-images.oss-cn-shenzhen.aliyuncs.com/github/javaguide/java/jvm/method-area-jdk1.6.png)\n\n![method-area-jdk1.7](https://guide-blog-images.oss-cn-shenzhen.aliyuncs.com/github/javaguide/java/jvm/method-area-jdk1.7.png)\n\n**JDK 1.7为什么要将字符串常量池移动到堆中？**\n\n主要是因为永久代（方法区实现）的GC回收效率太低，只有在整堆收集(Full GC)的时候才会被执行GC。Java程序中通常会有大量的被创建的字符串等待回收，将字符串常量池放到堆中，能够更高效及时地回收字符串内存。\n\n> 相关问题：[JVM常量池中存储的是对象还是引用呢？](https://www.zhihu.com/question/57109429/answer/151717241)\n> 最后再来分享一段周志明老师在[《深入理解Java虚拟机（第3版）》样例代码&勘误](https://github.com/fenixsoft/jvm_book)Github仓库的[issue#112](https://github.com/fenixsoft/jvm_book/issues/112)中说过的话：\n> **运行时常量池、方法区、字符串常量池这些都是不随虚拟机实现而改变的逻辑概念，是公共且抽象的，Metaspace、Heap是与具体某种虚拟机实现相关的物理概念，是私有且具体的。**\n\n### 程序计数器\n\n存放下一条指令所在单元地址的地方\n程序计数器是一块较小的内存空间，可以看作当前线程所执行的字节码的行号指示器。在虚拟机的模型里，字节码解释器工作时就是通过改变这个计数器的值来选取下一条需要执行的字节码指令，分支、循环、异常处理、线程恢复等基础功能都需要依赖计数器完成。另外，为了线程切换后能恢复到正确的执行位置，每条线程都需要有一个独立的程序计数器，各线程之间计数器互不影响，独立存储，我们称这类内存区域为“线程私有”的内存。\n\n从上面的介绍中我们知道了程序计数器主要有两个作用：\n\n- 字节码解释器通过改变程序计数器来依次读取指令，从而实现代码的流程控制，如：顺序执行、选择、循环、异常处理。\n- 在多线程的情况下，程序计数器用于记录当前线程执行的位置，从而当线程被切换回来的时候能够知道该线程上次运行到哪儿了。\n\n⚠️ 注意：程序计数器是唯一一个不会出现OutOfMemoryError的内存区域，它的生命周期随着线程的创建而创建，随着线程的结束而死亡\n\n### 直接内存\n\n直接内存是一种特殊的内存缓冲区，并不在Java堆或方法区中分配的，而是通过JNI的方式在本地内存上分配的。\n\n直接内存并不是虚拟机运行时数据区的一部分，也不是虚拟机规范中定义的内存区域，但是这部分内存也被频繁地使用。而且也可能导致`OutOfMemoryError`错误出现。\n\nJDK1.4中新加入的**NIO(New Input/Output)类**，引入了一种基于通道（Channel）与缓存区（Buffer）的I/O方式，它可以直接使用Native函数库直接分配堆外内存，然后通过一个存储在Java堆中的DirectByteBuffer对象作为这块内存的引用进行操作。这样就能在一些场景中显著提高性能，因为避免了在Java堆和Native堆之间来回复制数据。\n\n直接内存的分配不会受到Java堆的限制，但是，既然是内存就会受到本机总内存大小以及处理器寻址空间的限制。\n\n类似的概念还有**堆外内存**。在一些文章中将直接内存等价于堆外内，个人觉得不是特别准确。\n\n堆外内存就是把内存对象分配在堆（新生代+老年代+永久代）以外的内存，这些内存直接受操作系统管理（而不是虚拟机），这样做的结果就是能够在一定程度上减少垃圾回收对应用程序造成的影响\n\n### 示例\n\n```java\nFoo foo = new Foo();\nfoo.f();\n```\n以上代码的内存实现原理为：\n1. Foo类首先被装载到JVM的方法区，其中包括类的信息，包括方法和构造等。\n2. 在栈内存中分配引用变量foo。\n3. 在堆内存中按照Foo类型信息分配实例变量内存空间；然后，将栈中引用foo指向foo对象堆内存的首地址。\n4. 使用引用foo调用方法，根据foo引用的类型Foo调用f方法。\n\n![](/images/jvmdemo.png)\n![](/images/jvmdemo2.jpg)\n\n> [JVM底层原理最全知识总结--GitHub](https://github.com/doocs/jvm)\n\n\n### 相关文章\n\n- [2万字长文包教包会JVM内存结构](https://mp.weixin.qq.com/s/VDZNpS4Qk0jvv_MctVXhww)\n- [图文并茂，傻瓜都能看懂的JVM内存布局](https://mp.weixin.qq.com/s/oDeO8Td-SJn9g4mRygqtSw)\n- [小白都能看得懂的java虚拟机内存模型](https://mp.weixin.qq.com/s/m2dp6jv8lfmy-S2gmQDHvA)\n- [深入理解堆外内存Metaspace](https://mp.weixin.qq.com/s/xB-uiqy4eVsNovEknO9_5w)\n- [求你了，别再说Java对象都是在堆内存上分配空间的了](https://mp.weixin.qq.com/s/Owlhu5IFpDAyu0WYcK1EhQ)\n- [终于搞懂了Java 8的内存结构，再也不纠结方法区和常量池了！！](https://mp.weixin.qq.com/s/8uGOt1OJloZMHQl43vrbyQ)\n- [个人笔记，深入理解JVM，很全！](https://mp.weixin.qq.com/s/1rxmi3lg02I7nHwWbldvLA)\n- [JVM内存布局详解，图文并茂，写得太好了！](https://mp.weixin.qq.com/s/RySwVg9SYvSM-ONdB9cOVw)\n\n## JVM垃圾回收\n\n> 常见面试题：\n> - 如何判断对象是否死亡（两种方法）。\n> - 简单的介绍一下强引用、软引用、弱引用、虚引用（虚引用与软引用和弱引用的区别、使用软引用能带来的好处）。\n> - 如何判断一个常量是废弃常量\n> - 如何判断一个类是无用的类\n> - 垃圾收集有哪些算法，各自的特点？\n> - HotSpot为什么要分为新生代和老年代？\n> - 常见的垃圾回收器有哪些？\n> - 介绍一下CMS,G1收集器。\n> - MinorGc和FullGC有什么不同呢？\n\n### 堆空间的基本结构\n\nJava的自动内存管理主要是针对对象内存的回收和对象内存的分配。同时，Java自动内存管理最核心的功能是堆内存中对象的分配与回收。\n\nJava堆是垃圾收集器管理的主要区域，因此也被称作**GC堆（Garbage Collected Heap）**。\n\n从垃圾回收的角度来说，由于现在收集器基本都采用分代垃圾收集算法，所以Java堆被划分为了几个不同的区域，这样我们就可以根据各个区域的特点选择合适的垃圾收集算法。\n\n在JDK7版本及JDK7版本之前，堆内存被通常分为下面三部分：\n\n1. 新生代内存(Young Generation)\n2. 老生代(Old Generation)\n3. 永久代(Permanent Generation)\n\nEden区、两个Survivor区S0和S1都属于新生代，中间一层属于老年代，最下面一层属于永久代。\n\n**JDK8版本之后PermGen(永久)已被Metaspace(元空间)取代，元空间使用的是直接内存**。\n\n> 关于堆空间结构更详细的介绍，可以回过头看看[Java内存区域详解](https://javaguide.cn/java/jvm/memory-area.html)这篇文章。\n\n### 内存分配和回收原则\n\n#### 对象优先在Eden区分配\n\n大多数情况下，对象在新生代中Eden区分配。当Eden区没有足够空间进行分配时，虚拟机将发起一次Minor GC。下面我们来进行实际测试以下。\n\n测试代码：\n\n```java\npublic class GCTest {\n\tpublic static void main(String[] args) {\n\t\tbyte[] allocation1, allocation2;\n\t\tallocation1 = new byte[30900*1024];\n\t}\n}\n```\n\n\n运行结果(红色字体描述有误，应该是对应于JDK1.7的永久代)：\n\n![img](https://my-blog-to-use.oss-cn-beijing.aliyuncs.com/18-8-26/28954286.jpg)\n\n从上图我们可以看出Eden区内存几乎已经被分配完全（即使程序什么也不做，新生代也会使用2000多k内存）。假如我们再为allocation2分配内存会出现什么情况呢？\n\n\n```java\nallocation2 = new byte[900*1024];\n```\n\n![img](https://my-blog-to-use.oss-cn-beijing.aliyuncs.com/18-8-26/28128785.jpg)\n\n给allocation2分配内存的时候Eden区内存几乎已经被分配完了\n\n当Eden区没有足够空间进行分配时，虚拟机将发起一次Minor GC。GC期间虚拟机又发现allocation1无法存入Survivor空间，所以只好通过**分配担保机制**把新生代的对象提前转移到老年代中去，老年代上的空间足够存放allocation1，所以不会出现Full GC。执行Minor GC后，后面分配的对象如果能够存在Eden区的话，还是会在Eden区分配内存。可以执行如下代码验证：\n\n```java\npublic class GCTest {\n\tpublic static void main(String[] args) {\n\t\tbyte[] allocation1, allocation2,allocation3,allocation4,allocation5;\n\t\tallocation1 = new byte[32000*1024];\n\t\tallocation2 = new byte[1000*1024];\n\t\tallocation3 = new byte[1000*1024];\n\t\tallocation4 = new byte[1000*1024];\n\t\tallocation5 = new byte[1000*1024];\n\t}\n}\n```\n\n#### 大对象直接进入老年代\n\n大对象就是需要大量连续内存空间的对象（比如：字符串、数组）。大对象直接进入老年代主要是为了避免为大对象分配内存时由于分配担保机制带来的复制而降低效率。\n\n#### 长期存活的对象将进入老年代\n\n既然虚拟机采用了分代收集的思想来管理内存，那么内存回收时就必须能识别哪些对象应放在新生代，哪些对象应放在老年代中。为了做到这一点，虚拟机给每个对象一个对象年龄（Age）计数器。\n\n大部分情况，对象都会首先在Eden区域分配。如果对象在Eden出生并经过第一次Minor GC后仍然能够存活，并且能被Survivor容纳的话，将被移动到Survivor空间（s0或者s1）中，并将对象年龄设为1(Eden区->Survivor区后对象的初始年龄变为1)。\n\n对象在Survivor中每熬过一次MinorGC,年龄就增加1岁，当它的年龄增加到一定程度（默认为15岁），就会被晋升到老年代中。对象晋升到老年代的年龄阈值，可以通过参数`-XX:MaxTenuringThreshold`来设置。\n\n> 修正（[issue552](https://github.com/Snailclimb/JavaGuide/issues/552)）：“Hotspot遍历所有对象时，按照年龄从小到大对其所占用的大小进行累积，当累积的某个年龄大小超过了survivor区的50%时（默认值是50%，可以通过`-XX:TargetSurvivorRatio=percent`来设置，参见[issue1199](https://github.com/Snailclimb/JavaGuide/issues/1199)），取这个年龄和MaxTenuringThreshold中更小的一个值，作为新的晋升年龄阈值”。\n>\n> jdk8官方文档引用：https://docs.oracle.com/javase/8/docs/technotes/tools/unix/java.html。\n>\n> ![img](https://guide-blog-images.oss-cn-shenzhen.aliyuncs.com/java-guide-blog/image-20210523201742303.png)\n>\n> **动态年龄计算的代码如下：**\n>\n>\n> ```c++\n> uint ageTable::compute_tenuring_threshold(size_t survivor_capacity) {\n> //survivor_capacity是survivor空间的大小\n> size_t desired_survivor_size = (size_t)((((double)survivor_capacity)*TargetSurvivorRatio)/100);\n> size_t total = 0;\n> uint age = 1;\n> while (age < table_size) {\n> //sizes数组是每个年龄段对象大小\n> total += sizes[age];\n> if (total > desired_survivor_size) {\n> break;\n> }\n> age++;\n> }\n> uint result = age < MaxTenuringThreshold ? age : MaxTenuringThreshold;\n> ...\n> }\n> ```\n>\n> 额外补充说明([issue672](https://github.com/Snailclimb/JavaGuide/issues/672))：**关于默认的晋升年龄是15，这个说法的来源大部分都是《深入理解Java虚拟机》这本书**。如果你去Oracle的官网阅读[相关的虚拟机参数](https://docs.oracle.com/javase/8/docs/technotes/tools/unix/java.html)，你会发现`-XX:MaxTenuringThreshold=threshold`这里有个说明\n>\n> **Sets the maximum tenuring threshold for use in adaptive GC sizing. The largest value is 15. The default value is 15 for the parallel (throughput) collector, and 6 for the CMS collector.默认晋升年龄并不都是15，这个是要区分垃圾收集器的，CMS就是6.**\n\n#### 主要进行gc的区域\n\n周志明先生在《深入理解Java虚拟机》第二版中P92如是写道：\n\n> ~~*“老年代GC（Major GC/Full GC），指发生在老年代的GC……”*~~\n> 上面的说法已经在《深入理解Java虚拟机》第三版中被改正过来了。\n\n**总结：**\n\n针对HotSpot VM的实现，它里面的GC其实准确分类只有两大种：\n\n部分收集(Partial GC)：\n\n- 新生代收集（Minor GC/Young GC）：只对新生代进行垃圾收集；\n- 老年代收集（Major GC/Old GC）：只对老年代进行垃圾收集。需要注意的是Major GC在有的语境中也用于指代整堆收集；\n- 混合收集（Mixed GC）：对整个新生代和部分老年代进行垃圾收集。\n\n整堆收集(Full GC)：收集整个Java堆和方法区。\n\n#### 空间分配担保\n\n空间分配担保是为了确保在Minor GC之前老年代本身还有容纳新生代所有对象的剩余空间。《深入理解Java虚拟机》第三章对于空间分配担保的描述如下：\n\n> JDK6 Update 24之前，在发生Minor GC之前，虚拟机必须先检查老年代最大可用的连续空间是否大于新生代所有对象总空间，如果这个条件成立，那这一次Minor GC可以确保是安全的。如果不成立，则虚拟机会先查看`-XX:HandlePromotionFailure`参数的设置值是否允许担保失败(Handle Promotion Failure);如果允许，那会继续检查老年代最大可用的连续空间是否大于历次晋升到老年代对象的平均大小，如果大于，将尝试进行一次Minor GC，尽管这次Minor GC是有风险的;如果小于，或者`-XX:HandlePromotionFailure`设置不允许冒险，那这时就要改为进行一次Full GC。\n>\n> JDK6 Update 24之后的规则变为只要老年代的连续空间大于新生代对象总大小或者历次晋升的平均大小，就会进行Minor GC，否则将进行Full GC。\n\n### 死亡对象判断方法\n\n堆中几乎放着所有的对象实例，对堆垃圾回收前的第一步就是要判断哪些对象已经死亡（即不能再被任何途径使用的对象）。\n\n#### 引用计数法\n\n给对象中添加一个引用计数器：\n\n- 每当有一个地方引用它，计数器就加1；\n- 当引用失效，计数器就减1；\n- 任何时候计数器为0的对象就是不可能再被使用的。\n\n**这个方法实现简单，效率高，但是目前主流的虚拟机中并没有选择这个算法来管理内存，其最主要的原因是它很难解决对象之间相互循环引用的问题**。\n\n所谓对象之间的相互引用问题，如下面代码所示：除了对象`objA`和`objB`相互引用着对方之外，这两个对象之间再无任何引用。但是他们因为互相引用对方，导致它们的引用计数器都不为0，于是引用计数算法无法通知GC回收器回收他们。\n\n```java\npublic class ReferenceCountingGc {\n    Object instance = null;\n    public static void main(String[] args) {\n        ReferenceCountingGc objA = new ReferenceCountingGc();\n        ReferenceCountingGc objB = new ReferenceCountingGc();\n        objA.instance = objB;\n        objB.instance = objA;\n        objA = null;\n        objB = null;\n    }\n}\n```\n\n#### 可达性分析算法\n\n这个算法的基本思想就是通过一系列的称为GC Roots的对象作为起点，从这些节点开始向下搜索，节点所走过的路径称为引用链，当一个对象到GC Roots没有任何引用链相连的话，则证明此对象是不可用的，需要被回收。下图中的Object 6~Object 10之间虽有引用关系，但它们到GC Roots不可达，因此为需要被回收的对象。\n\n**哪些对象可以作为GC Roots呢？**\n\n- 虚拟机栈(栈帧中的本地变量表)中引用的对象\n- 本地方法栈(Native方法)中引用的对象\n- 方法区中类静态属性引用的对象\n- 方法区中常量引用的对象\n- 所有被同步锁持有的对象\n\n**对象可以被回收，就代表一定会被回收吗**？\n\n即使在可达性分析法中不可达的对象，也并非是“非死不可”的，这时候它们暂时处于“缓刑阶段”，要真正宣告一个对象死亡，至少要经历两次标记过程；可达性分析法中不可达的对象被第一次标记并且进行一次筛选，筛选的条件是此对象是否有必要执行`finalize`方法。当对象没有覆盖`finalize`方法，或`finalize`方法已经被虚拟机调用过时，虚拟机将这两种情况视为没有必要执行。\n\n被判定为需要执行的对象将会被放在一个队列中进行第二次标记，除非这个对象与引用链上的任何一个对象建立关联，否则就会被真的回收。\n\n> Object类中的finalize方法一直被认为是一个糟糕的设计，成为了Java语言的负担，影响了Java语言的安全和GC的性能。JDK9版本及后续版本中各个类中的finalize方法会被逐渐弃用移除。忘掉它的存在吧！\n>\n> 参考：\n>\n> - [JEP 421: Deprecate Finalization for Removal](https://openjdk.java.net/jeps/421)\n> - [是时候忘掉finalize方法了](https://mp.weixin.qq.com/s/LW-paZAMD08DP_3-XCUxmg)\n\n#### 引用类型总结\n\n无论是通过引用计数法判断对象引用数量，还是通过可达性分析法判断对象的引用链是否可达，判定对象的存活都与“引用”有关。JDK1.2之前，Java中引用的定义很传统：如果reference类型的数据存储的数值代表的是另一块内存的起始地址，就称这块内存代表一个引用。JDK1.2以后，Java对引用的概念进行了扩充，将引用分为强引用、软引用、弱引用、虚引用四种（引用强度逐渐减弱）\n\n**1．强引用（StrongReference）**\n\n以前我们使用的大部分引用实际上都是强引用，这是使用最普遍的引用。如果一个对象具有强引用，那就类似于**必不可少的生活用品**，垃圾回收器绝不会回收它。当内存空间不足，Java虚拟机宁愿抛出OutOfMemoryError错误，使程序异常终止，也不会靠随意回收具有强引用的对象来解决内存不足问题。\n\n**2．软引用（SoftReference）**\n\n如果一个对象只具有软引用，那就类似于**可有可无的生活用品**。如果内存空间足够，垃圾回收器就不会回收它，如果内存空间不足了，就会回收这些对象的内存。只要垃圾回收器没有回收它，该对象就可以被程序使用。软引用可用来实现内存敏感的高速缓存。\n\n软引用可以和一个引用队列（ReferenceQueue）联合使用，如果软引用所引用的对象被垃圾回收，JAVA虚拟机就会把这个软引用加入到与之关联的引用队列中。\n\n**3．弱引用（WeakReference）**\n\n如果一个对象只具有弱引用，那就类似于**可有可无的生活用品**。弱引用与软引用的区别在于：只具有弱引用的对象拥有更短暂的生命周期。在垃圾回收器线程扫描它所管辖的内存区域的过程中，一旦发现了只具有弱引用的对象，不管当前内存空间足够与否，都会回收它的内存。不过，由于垃圾回收器是一个优先级很低的线程，因此不一定会很快发现那些只具有弱引用的对象。\n\n弱引用可以和一个引用队列（ReferenceQueue）联合使用，如果弱引用所引用的对象被垃圾回收，Java虚拟机就会把这个弱引用加入到与之关联的引用队列中。\n\n**4．虚引用（PhantomReference）**\n\n\"虚引用\"顾名思义，就是形同虚设，与其他几种引用都不同，虚引用并不会决定对象的生命周期。如果一个对象仅持有虚引用，那么它就和没有任何引用一样，在任何时候都可能被垃圾回收。\n\n**虚引用主要用来跟踪对象被垃圾回收的活动**。\n\n**虚引用与软引用和弱引用的一个区别在于**：虚引用必须和引用队列（ReferenceQueue）联合使用。当垃圾回收器准备回收一个对象时，如果发现它还有虚引用，就会在回收对象的内存之前，把这个虚引用加入到与之关联的引用队列中。程序可以通过判断引用队列中是否已经加入了虚引用，来了解被引用的对象是否将要被垃圾回收。程序如果发现某个虚引用已经被加入到引用队列，那么就可以在所引用的对象的内存被回收之前采取必要的行动。\n\n特别注意，在程序设计中一般很少使用弱引用与虚引用，使用软引用的情况较多，这是因为**软引用可以加速JVM对垃圾内存的回收速度，可以维护系统的运行安全，防止内存溢出（OutOfMemory）等问题的产生**。\n\n#### 如何判断一个常量是废弃常量？\n\n运行时常量池主要回收的是废弃的常量。那么，我们如何判断一个常量是废弃常量呢？\n\n**JDK1.7及之后版本的JVM已经将运行时常量池从方法区中移了出来，在Java堆（Heap）中开辟了一块区域存放运行时常量池**。\n\n> **🐛修正（参见：[issue747](https://github.com/Snailclimb/JavaGuide/issues/747)，[reference](https://blog.csdn.net/q5706503/article/details/84640762)）**：\n>\n> 1. **JDK1.7之前运行时常量池逻辑包含字符串常量池存放在方法区,此时hotspot虚拟机对方法区的实现为永久代**\n> 2. **JDK1.7字符串常量池被从方法区拿到了堆中,这里没有提到运行时常量池,也就是说字符串常量池被单独拿到堆,运行时常量池剩下的东西还在方法区,也就是hotspot中的永久代**。\n> 3. **JDK1.8hotspot移除了永久代用元空间(Metaspace)取而代之,这时候字符串常量池还在堆,运行时常量池还在方法区,只不过方法区的实现从永久代变成了元空间(Metaspace)**\n\n假如在字符串常量池中存在字符串\"abc\"，如果当前没有任何String对象引用该字符串常量的话，就说明常量\"abc\"就是废弃常量，如果这时发生内存回收的话而且有必要的话，\"abc\"就会被系统清理出常量池了。\n\n#### 如何判断一个类是无用的类\n\n方法区主要回收的是无用的类，那么如何判断一个类是无用的类的呢？\n\n判定一个常量是否是“废弃常量”比较简单，而要判定一个类是否是“无用的类”的条件则相对苛刻许多。类需要同时满足下面3个条件才能算是无用的类：\n\n- 该类所有的实例都已经被回收，也就是Java堆中不存在该类的任何实例。\n- 加载该类的ClassLoader已经被回收。\n- 该类对应的`java.lang.Class`对象没有在任何地方被引用，无法在任何地方通过反射访问该类的方法。\n\n虚拟机可以对满足上述3个条件的无用类进行回收，这里说的仅仅是“可以”，而并不是和对象一样不使用了就会必然被回收。\n\n### 垃圾收集算法\n\n#### 标记-清除算法\n\n该算法分为“标记”和“清除”阶段：首先标记出所有不需要回收的对象，在标记完成后统一回收掉所有没有被标记的对象。它是最基础的收集算法，后续的算法都是对其不足进行改进得到。这种垃圾收集算法会带来两个明显的问题：\n\n1. **效率问题**\n2. **空间问题（标记清除后会产生大量不连续的碎片）**\n\n#### 标记-复制算法\n\n为了解决效率问题，“标记-复制”收集算法出现了。它可以将内存分为大小相同的两块，每次使用其中的一块。当这一块的内存使用完后，就将还存活的对象复制到另一块去，然后再把使用的空间一次清理掉。这样就使每次的内存回收都是对内存区间的一半进行回收。\n\n#### 标记-整理算法\n\n根据老年代的特点提出的一种标记算法，标记过程仍然与“标记-清除”算法一样，但后续步骤不是直接对可回收对象回收，而是让所有存活的对象向一端移动，然后直接清理掉端边界以外的内存。\n\n#### 分代收集算法\n\n当前虚拟机的垃圾收集都采用分代收集算法，这种算法没有什么新的思想，只是根据对象存活周期的不同将内存分为几块。一般将java堆分为新生代和老年代，这样我们就可以根据各个年代的特点选择合适的垃圾收集算法。\n\n比如在新生代中，每次收集都会有大量对象死去，所以可以选择”标记-复制“算法，只需要付出少量对象的复制成本就可以完成每次垃圾收集。而老年代的对象存活几率是比较高的，而且没有额外的空间对它进行分配担保，所以我们必须选择“标记-清除”或“标记-整理”算法进行垃圾收集。\n\n> **延伸面试问题**：HotSpot为什么要分为新生代和老年代？\n> 根据上面的对分代收集算法的介绍回答。\n\n### 垃圾收集器\n\n**如果说收集算法是内存回收的方法论，那么垃圾收集器就是内存回收的具体实现**。\n\n虽然我们对各个收集器进行比较，但并非要挑选出一个最好的收集器。因为直到现在为止还没有最好的垃圾收集器出现，更加没有万能的垃圾收集器，我们能做的就是根据具体应用场景选择适合自己的垃圾收集器。试想一下：如果有一种四海之内、任何场景下都适用的完美收集器存在，那么我们的HotSpot虚拟机就不会实现那么多不同的垃圾收集器了。\n\n#### Serial收集器\n\nSerial（串行）收集器是最基本、历史最悠久的垃圾收集器了。大家看名字就知道这个收集器是一个单线程收集器了。它的单线程的意义不仅仅意味着它只会使用一条垃圾收集线程去完成垃圾收集工作，更重要的是它在进行垃圾收集工作的时候必须暂停其他所有的工作线程（Stop The World），直到它收集结束。\n\n**新生代采用标记-复制算法，老年代采用标记-整理算法**。\n\n虚拟机的设计者们当然知道Stop The World带来的不良用户体验，所以在后续的垃圾收集器设计中停顿时间在不断缩短（仍然还有停顿，寻找最优秀的垃圾收集器的过程仍然在继续）。\n\n但是Serial收集器有没有优于其他垃圾收集器的地方呢？当然有，它简单而高效（与其他收集器的单线程相比）。Serial收集器由于没有线程交互的开销，自然可以获得很高的单线程收集效率。Serial收集器对于运行在Client模式下的虚拟机来说是个不错的选择。\n\n#### ParNew收集器\n\nParNew收集器其实就是Serial收集器的多线程版本，除了使用多线程进行垃圾收集外，其余行为（控制参数、收集算法、回收策略等等）和Serial收集器完全一样。\n\n新生代采用标记-复制算法，老年代采用标记-整理算法。\n\n它是许多运行在Server模式下的虚拟机的首要选择，除了Serial收集器外，只有它能与CMS收集器（真正意义上的并发收集器，后面会介绍到）配合工作。\n\n并行和并发概念补充：\n\n- **并行（Parallel）**：指多条垃圾收集线程并行工作，但此时用户线程仍然处于等待状态。\n- **并发（Concurrent）**：指用户线程与垃圾收集线程同时执行（但不一定是并行，可能会交替执行），用户程序在继续运行，而垃圾收集器运行在另一个CPU上。\n\n#### Parallel Scavenge收集器\n\nParallel Scavenge收集器也是使用标记-复制算法的多线程收集器，它看上去几乎和ParNew都一样。那么它有什么特别之处呢？\n\n\n```text\n-XX:+UseParallelGC 使用Parallel收集器+老年代串行\n\n-XX:+UseParallelOldGC 使用Parallel收集器+老年代并行\n```\n\nParallel Scavenge收集器关注点是吞吐量（高效率的利用CPU）。CMS等垃圾收集器的关注点更多的是用户线程的停顿时间（提高用户体验）。所谓吞吐量就是CPU中用于运行用户代码的时间与CPU总消耗时间的比值。Parallel Scavenge收集器提供了很多参数供用户找到最合适的停顿时间或最大吞吐量，如果对于收集器运作不太了解，手工优化存在困难的时候，使用Parallel Scavenge收集器配合自适应调节策略，把内存管理优化交给虚拟机去完成也是一个不错的选择。\n\n使用`java -XX:+PrintCommandLineFlags -version`命令查看\n\n\n```bash\n-XX:InitialHeapSize=262921408 -XX:MaxHeapSize=4206742528 -XX:+PrintCommandLineFlags -XX:+UseCompressedClassPointers -XX:+UseCompressedOops -XX:+UseParallelGC\njava version \"1.8.0_211\"\nJava(TM) SE Runtime Environment (build 1.8.0_211-b12)\nJava HotSpot(TM) 64-Bit Server VM (build 25.211-b12, mixed mode)\n```\n\nJDK1.8默认使用的是Parallel Scavenge+Parallel Old，如果指定了-XX:+UseParallelGC参数，则默认指定了-XX:+UseParallelOldGC，可以使用-XX:-UseParallelOldGC来禁用该功能\n\n#### SerialOld收集器\n\nSerial收集器的老年代版本，它同样是一个单线程收集器。它主要有两大用途：一种用途是在JDK1.5以及以前的版本中与Parallel Scavenge收集器搭配使用，另一种用途是作为CMS收集器的后备方案。\n\n#### Parallel Old收集器\n\nParallel Scavenge收集器的老年代版本。使用多线程和“标记-整理”算法。在注重吞吐量以及CPU资源的场合，都可以优先考虑Parallel Scavenge收集器和Parallel Old收集器。\n\n#### CMS收集器\n\nCMS（Concurrent Mark Sweep）收集器是一种以获取最短回收停顿时间为目标的收集器。它非常符合在注重用户体验的应用上使用。CMS（Concurrent Mark Sweep）收集器是HotSpot虚拟机第一款真正意义上的并发收集器，它第一次实现了让垃圾收集线程与用户线程（基本上）同时工作。从名字中的Mark Sweep这两个词可以看出，CMS收集器是一种标记-清除算法实现的，它的运作过程相比于前面几种垃圾收集器来说更加复杂一些。整个过程分为四个步骤：\n\n- **初始标记**:暂停所有的其他线程，并记录下直接与root相连的对象，速度很快；\n- **并发标记**:同时开启GC和用户线程，用一个闭包结构去记录可达对象。但在这个阶段结束，这个闭包结构并不能保证包含当前所有的可达对象。因为用户线程可能会不断的更新引用域，所以GC线程无法保证可达性分析的实时性。所以这个算法里会跟踪记录这些发生引用更新的地方。\n- **重新标记**:重新标记阶段就是为了修正并发标记期间因为用户程序继续运行而导致标记产生变动的那一部分对象的标记记录，这个阶段的停顿时间一般会比初始标记阶段的时间稍长，远远比并发标记阶段时间短\n- **并发清除**:开启用户线程，同时GC线程开始对未标记的区域做清扫。\n\n从它的名字就可以看出它是一款优秀的垃圾收集器，主要优点：**并发收集、低停顿**。但是它有下面三个明显的缺点：\n\n- **对CPU资源敏感；**\n- **无法处理浮动垃圾；**\n- **它使用的回收算法-“标记-清除”算法会导致收集结束时会有大量空间碎片产生。**\n\n#### G1收集器\n\n**G1(Garbage-First)是一款面向服务器的垃圾收集器,主要针对配备多颗处理器及大容量内存的机器.以极高概率满足GC停顿时间要求的同时,还具备高吞吐量性能特征**。\n\n被视为JDK1.7中HotSpot虚拟机的一个重要进化特征。它具备以下特点：\n\n- **并行与并发**：G1能充分利用CPU、多核环境下的硬件优势，使用多个CPU（CPU或者CPU核心）来缩短Stop-The-World停顿时间。部分其他收集器原本需要停顿Java线程执行的GC动作，G1收集器仍然可以通过并发的方式让java程序继续执行。\n- **分代收集**：虽然G1可以不需要其他收集器配合就能独立管理整个GC堆，但是还是保留了分代的概念。\n- **空间整合**：与CMS的“标记-清除”算法不同，G1从整体来看是基于“标记-整理”算法实现的收集器；从局部上来看是基于“标记-复制”算法实现的。\n- **可预测的停顿**：这是G1相对于CMS的另一个大优势，降低停顿时间是G1和CMS共同的关注点，但G1除了追求低停顿外，还能建立可预测的停顿时间模型，能让使用者明确指定在一个长度为M毫秒的时间片段内。\n\nG1收集器的运作大致分为以下几个步骤：\n\n- **初始标记**\n- **并发标记**\n- **最终标记**\n- **筛选回收**\n\nG1收集器在后台维护了一个优先列表，每次根据允许的收集时间，优先选择回收价值最大的Region(这也就是它的名字Garbage-First的由来)。这种使用Region划分内存空间以及有优先级的区域回收方式，保证了G1收集器在有限时间内可以尽可能高的收集效率（把内存化整为零）。\n\n#### ZGC收集器\n\n与CMS中的ParNew和G1类似，ZGC也采用标记-复制算法，不过ZGC对该算法做了重大改进。在ZGC中出现Stop The World的情况会更少！\n\n> 详情可以看：[《新一代垃圾回收器ZGC的探索与实践》](https://tech.meituan.com/2020/08/06/new-zgc-practice-in-meituan.html)\n> [原文链接](https://javaguide.cn/java/jvm/jvm-garbage-collection.html)\n\n\n### 相关文章\n\n- [7种JVM垃圾回收器及垃圾回收流程](https://mp.weixin.qq.com/s/fyorrpT5-hFpIS5aEDNjZA)\n- [深度解析ZGC](https://mp.weixin.qq.com/s/NYrUEYmuWQw6RrN1VVILpg)\n- [从历代GC算法角度刨析ZGC](https://mp.weixin.qq.com/s/VrjjPUm-p6h1YmU8olN6-Q)\n- [深度揭秘垃圾回收底层，这次让你彻底弄懂它](https://mp.weixin.qq.com/s/pb7h9ROzr5LOLA0gMnquqQ)\n- [看完这篇垃圾回收，和面试官扯皮没问题了](https://mp.weixin.qq.com/s/EcTwdaxP-Z4jG7PDq50nSA)\n- [详解Java性能优化和JVM GC（垃圾回收机制）](https://mp.weixin.qq.com/s/Bfz7sqaI4iJuGe_i5dUXgA)\n- [图文并茂，万字详解，带你掌握JVM垃圾回收！](https://mp.weixin.qq.com/s/EYOD4mQ7ErB11xMrGgzfCw)\n- [一篇文章搞懂GC垃圾回收](https://mp.weixin.qq.com/s?__biz=Mzg2MDYzODI5Nw==&mid=2247494090&idx=1&sn=0cb942ee592e277014b6cd16059c13c7&source=41#wechat_redirect)\n- [垃圾回收策略和算法，看这篇就够了](https://mp.weixin.qq.com/s/Anj6PRc9UPiWGDnpKahxUQ)\n- [一口气问了我18个JVM问题](https://mp.weixin.qq.com/s/U8uzm3YjdqoYCtLPPosSrg)\n- [一文了解Java 8-18，垃圾回收的十次进化](https://mp.weixin.qq.com/s/nF489_nmrFAWcw1IfFF9lg)\n- [CMS和G1改用三色标记法，可达性分析到底做错了什么](https://mp.weixin.qq.com/s/LfGrLo0fVrR85qOXAZxnvw)\n\n## JVM调优\n\n根据刚刚涉及的jvm的知识点，我们可以尝试对JVM进行调优，主要就是堆内存那块。\n\n所有线程共享数据区大小=新生代大小+年老代大小+持久代大小。持久代一般固定大小为64m。所以java堆中增大年轻代后，将会减小年老代大小（因为老年代的清理是使用fullgc，所以老年代过小的话反而是会增多fullgc的）。此值对系统性能影响较大，Sun官方推荐配置为java堆的3/8。\n\n### 调整最大堆内存和最小堆内存\n\n-Xmx –Xms：指定java堆最大值（默认值是物理内存的1/4(<1GB)）和初始java堆最小值（默认值是物理内存的1/64(<1GB)。默认(MinHeapFreeRatio参数可以调整)空余堆内存小于40%时，JVM就会增大堆直到-Xmx的最大限制.默认(MaxHeapFreeRatio参数可以调整)空余堆内存大于70%时，JVM会减少堆直到-Xms的最小限制。简单点来说，你不停地往堆内存里面丢数据，等它剩余大小小于40%了，JVM就会动态申请内存空间不过会小于-Xmx，如果剩余大小大于70%，又会动态缩小不过不会小于–Xms。就这么简单\n\n开发过程中，通常会将-Xms与-Xmx两个参数配置成相同的值，其目的是为了能够在java垃圾回收机制清理完堆区后不需要重新分隔计算堆区的大小而浪费资源。我们执行下面的代码\n\n\n```java\nSystem.out.println(\"Xmx=\" + Runtime.getRuntime().maxMemory() / 1024.0 / 1024 + \"M\");  //系统的最大空间\nSystem.out.println(\"free mem=\" + Runtime.getRuntime().freeMemory() / 1024.0 / 1024 + \"M\");  //系统的空闲空间\nSystem.out.println(\"total mem=\" + Runtime.getRuntime().totalMemory() / 1024.0 / 1024 + \"M\");  //当前可用的总空间\n```\n\n注意：此处设置的是Java堆大小，也就是新生代大小+老年代大小\n\n![img](https://my-blog-to-use.oss-cn-beijing.aliyuncs.com/2019-11/5e7b352c16d74c789c665af46d3a2509-new-imagedd645dae-307d-4572-b6e2-b5a9925a46cd.png)\n\n设置一个VMoptions的参数\n\n```\n-Xmx20m -Xms5m -XX:+PrintGCDetails\n```\n\n![img](https://my-blog-to-use.oss-cn-beijing.aliyuncs.com/2019-11/fe99e355f4754fa4be7427cb65261f3d-new-imagebb5cf485-99f8-43eb-8809-2a89e6a1768e.png)\n\n再次启动main方法\n\n![img](https://my-blog-to-use.oss-cn-beijing.aliyuncs.com/2019-11/300539f6560043dd8a3fe085d28420e6-new-image3c581a2e-196f-4b01-90f1-c27731b4610b.png)\n\n这里GC弹出了一个Allocation Failure分配失败，这个事情发生在PSYoungGen，也就是年轻代中，这时候申请到的内存为18M，空闲内存为4.214195251464844M，我们此时创建一个字节数组看看，执行下面的代码\n\n```java\nbyte[] b = new byte[1 * 1024 * 1024];\nSystem.out.println(\"分配了1M空间给数组\");\nSystem.out.println(\"Xmx=\" + Runtime.getRuntime().maxMemory() / 1024.0 / 1024 + \"M\");  //系统的最大空间\nSystem.out.println(\"free mem=\" + Runtime.getRuntime().freeMemory() / 1024.0 / 1024 + \"M\");  //系统的空闲空间\nSystem.out.println(\"total mem=\" + Runtime.getRuntime().totalMemory() / 1024.0 / 1024 + \"M\");\n```\n\n![img](https://my-blog-to-use.oss-cn-beijing.aliyuncs.com/2019-11/bdd717d0a3394be7a733760052773374-new-image371b5d59-0020-4091-9874-603c0ab0073d.png)\n\n此时free memory就又缩水了，不过total memory是没有变化的。Java会尽可能将total mem的值维持在最小堆内存大小。\n\n```java\nbyte[] b = new byte[10 * 1024 * 1024];\nSystem.out.println(\"分配了10M空间给数组\");\nSystem.out.println(\"Xmx=\" + Runtime.getRuntime().maxMemory() / 1024.0 / 1024 + \"M\");  //系统的最大空间\nSystem.out.println(\"free mem=\" + Runtime.getRuntime().freeMemory() / 1024.0 / 1024 + \"M\");  //系统的空闲空间\nSystem.out.println(\"total mem=\" + Runtime.getRuntime().totalMemory() / 1024.0 / 1024 + \"M\");  //当前可用的总空间\n```\n\n![img](https://my-blog-to-use.oss-cn-beijing.aliyuncs.com/2019-11/0fd7550ae2144adca8ed2ede12d5fb96-new-image0c31ff20-289d-4088-8c67-a846d0c5d1e0.png)\n\n这时候我们创建了一个10M的字节数据，这时候最小堆内存是顶不住的。我们会发现现在的total memory已经变成了15M，这就是已经申请了一次内存的结果。此时我们再跑一下这个代码\n\n```java\nSystem.gc();\nSystem.out.println(\"Xmx=\" + Runtime.getRuntime().maxMemory() / 1024.0 / 1024 + \"M\");    //系统的最大空间\nSystem.out.println(\"free mem=\" + Runtime.getRuntime().freeMemory() / 1024.0 / 1024 + \"M\");  //系统的空闲空间\nSystem.out.println(\"total mem=\" + Runtime.getRuntime().totalMemory() / 1024.0 / 1024 + \"M\");  //当前可用的总空间\n```\n\n![img](https://my-blog-to-use.oss-cn-beijing.aliyuncs.com/2019-11/4cc44b5d5d1c40c48640ece6a296b1ac-new-image4b57baf6-085b-4150-9c60-ac51b0f815d7.png)\n\n此时我们手动执行了一次fullgc，此时total memory的内存空间又变回5.5M了，此时又是把申请的内存释放掉的结果。\n\n### 调整新生代和老年代的比值\n\n-XX:NewRatio---新生代（eden+2*Survivor）和老年代（不包含永久区）的比值。例如：-XX:NewRatio=4，表示新生代:老年代=1:4，即新生代占整个堆的1/5。在Xms=Xmx并且设置了Xmn的情况下，该参数不需要进行设置。\n\n### 调整Survivor区和Eden区的比值\n\n-XX:SurvivorRatio（幸存代）---设置两个Survivor区和eden的比值。例如：8，表示两个Survivor:eden=2:8，即一个Survivor占年轻代的1/10\n\n### 设置年轻代和老年代的大小\n\n-XX:NewSize---设置年轻代大小\n\n-XX:MaxNewSize---设置年轻代最大值\n\n可以通过设置不同参数来测试不同的情况，反正最优解当然就是官方的Eden和Survivor的占比为8:1:1，然后在刚刚介绍这些参数的时候都已经附带了一些说明，感兴趣的也可以看看。反正最大堆内存和最小堆内存如果数值不同会导致多次的gc，需要注意。\n\n### 小总结\n\n根据实际事情调整新生代和幸存代的大小，官方推荐新生代占java堆的3/8，幸存代占新生代的1/10，在OOM时，记得Dump出堆，确保可以排查现场问题，通过下面命令你可以输出一个.dump文件，这个文件可以使用VisualVM或者Java自带的Java VisualVM工具。\n\n```\n-Xmx20m-Xms5m-XX:+HeapDumpOnOutOfMemoryError-XX:HeapDumpPath=你要输出的日志路径\n```\n\n一般我们也可以通过编写脚本的方式来让OOM出现时给我们报个信，可以通过发送邮件或者重启程序等来解决。\n\n### 永久区的设置\n\n```\n-XX:PermSize -XX:MaxPermSize\n```\n\n初始空间（默认为物理内存的1/64）和最大空间（默认为物理内存的1/4）。也就是说，jvm启动时，永久区一开始就占用了PermSize大小的空间，如果空间还不够，可以继续扩展，但是不能超过MaxPermSize，否则会OOM。\n\n> tips：如果堆空间没有用完也抛出了OOM，有可能是永久区导致的。堆空间实际占用非常少，但是永久区溢出一样抛出OOM。\n\n### JVM的栈参数调优\n\n#### 调整每个线程栈空间的大小\n\n可以通过-Xss：调整每个线程栈空间的大小，JDK5.0以后每个线程堆栈大小为1M，以前每个线程堆栈大小为256K。在相同物理内存下,减小这个值能生成更多的线程。但是操作系统对一个进程内的线程数还是有限制的，不能无限生成，经验值在3000~5000左右\n\n#### 设置线程栈的大小\n\n```\n-XXThreadStackSize： 设置线程栈的大小(0 means use default stack size)\n```\n\n这些参数都是可以通过自己编写程序去简单测试的，这里碍于篇幅问题就不再提供demo了\n\n### JVM其他参数介绍\n\n#### 设置内存页的大小\n\n```\n-XXThreadStackSize：\n    设置内存页的大小，不可设置过大，会影响Perm的大小\n```\n\n#### 设置原始类型的快速优化\n\n```\n-XX:+UseFastAccessorMethods：\n    设置原始类型的快速优化\n```\n\n#### 设置关闭手动GC\n\n```\n-XX:+DisableExplicitGC：\n    设置关闭System.gc()(这个参数需要严格的测试)\n```\n\n#### 设置垃圾最大年龄\n\n```\n-XX:MaxTenuringThreshold\n    设置垃圾最大年龄。如果设置为0的话,则年轻代对象不经过Survivor区,直接进入年老代.\n    对于年老代比较多的应用,可以提高效率。如果将此值设置为一个较大值,\n    则年轻代对象会在Survivor区进行多次复制,这样可以增加对象再年轻代的存活时间,\n    增加在年轻代即被回收的概率。该参数只有在串行GC时才有效.\n```\n\n#### 加快编译速度\n\n```\n-XX:+AggressiveOpts\n```\n\n加快编译速度\n\n#### 改善锁机制性能\n\n```\n-XX:+UseBiasedLocking\n```\n\n#### 禁用垃圾回收\n\n```\n-Xnoclassgc\n```\n\n#### 设置堆空间存活时间\n\n```\n-XX:SoftRefLRUPolicyMSPerMB\n    设置每兆堆空闲空间中SoftReference的存活时间，默认值是1s。\n```\n\n#### 设置对象直接分配在老年代\n\n```\n-XX:PretenureSizeThreshold\n    设置对象超过多大时直接在老年代分配，默认值是0。\n```\n\n#### 设置TLAB占eden区的比例\n\n```\n-XX:TLABWasteTargetPercent\n    设置TLAB占eden区的百分比，默认值是1%。\n```\n\n#### 设置是否优先YGC\n\n```\n-XX:+CollectGen0First\n    设置FullGC时是否先YGC，默认值是false。\n```\n> [原文链接](https://javaguide.cn/java/jvm/jvm-intro.html)\n\n## JVM工具\n\n- **jps**(JVM Process Status）:类似UNIX的ps命令。用于查看所有Java进程的启动类、传入参数和Java虚拟机参数等信息；\n- **jstat**（JVM Statistics Monitoring Tool）:用于收集HotSpot虚拟机各方面的运行数据;\n- **jinfo** (Configuration Info for Java):Configuration Info for Java,显示虚拟机配置信息;\n- **jmap** (Memory Map for Java):生成堆转储快照;\n- **jhat**(JVM Heap Dump Browser):用于分析heapdump文件，它会建立一个HTTP/HTML服务器，让用户可以在浏览器上查看分析结果;\n- **jstack**(Stack Trace for Java):生成虚拟机当前时刻的线程快照，线程快照就是当前虚拟机内每一条线程正在执行的方法堆栈的集合。\n\n### jps:查看所有Java进程\n\njps(JVM Process Status)命令类似UNIX的ps命令。\n\n`jps`：显示虚拟机执行主类名称以及这些进程的本地虚拟机唯一ID（Local Virtual Machine Identifier,LVMID）。\n`jps -q`：只输出进程的本地虚拟机唯一ID。\n\n```powershell\nC:\\Users\\SnailClimb>jps\n7360 NettyClient2\n17396\n7972 Launcher\n16504 Jps\n17340 NettyServer\n```\n\n`jps -l`:输出主类的全名，如果进程执行的是Jar包，输出Jar路径。\n\n```powershell\nC:\\Users\\SnailClimb>jps -l\n7360 firstNettyDemo.NettyClient2\n17396\n7972 org.jetbrains.jps.cmdline.Launcher\n16492 sun.tools.jps.Jps\n17340 firstNettyDemo.NettyServer\n```\n\n`jps -v`：输出虚拟机进程启动时JVM参数。\n\n`jps -m`：输出传递给Java进程main()函数的参数。\n\n### jstat:监视虚拟机各种运行状态信息\n\njstat（JVM Statistics Monitoring Tool）使用于监视虚拟机各种运行状态信息的命令行工具。它可以显示本地或者远程（需要远程主机提供RMI支持）虚拟机进程中的类信息、内存、垃圾收集、JIT编译等运行数据，在没有GUI，只提供了纯文本控制台环境的服务器上，它将是运行期间定位虚拟机性能问题的首选工具。\n\n**jstat命令使用格式：**\n\n```powershell\njstat -<option> [-t] [-h<lines>] <vmid> [<interval> [<count>]]\n```\n\n比如`jstat -gc -h3 31736 1000 10`表示分析进程id为31736的gc情况，每隔1000ms打印一次记录，打印10次停止，每3行后打印指标头部。\n\n**常见的option如下：**\n\n- jstat -class vmid：显示ClassLoader的相关信息；\n- jstat -compiler vmid：显示JIT编译的相关信息；\n- jstat -gc vmid：显示与GC相关的堆信息；\n- jstat -gccapacity vmid：显示各个代的容量及使用情况；\n- jstat -gcnew vmid ：显示新生代信息；\n- jstat -gcnewcapcacity vmid：显示新生代大小与使用情况；\n- jstat -gcold vmid ：显示老年代和永久代的行为统计，从jdk1.8开始,该选项仅表示老年代，因为永久代被移除了；\n- jstat -gcoldcapacity vmid：显示老年代的大小；\n- jstat -gcpermcapacity vmid：显示永久代大小，从jdk1.8开始,该选项不存在了，因为永久代被移除了；\n- jstat -gcutil vmid：显示垃圾收集信息；\n\n另外，加上`-t`参数可以在输出信息上加一个Timestamp列，显示程序的运行时间。\n\n主要是对java应用程序的资源和性能进行实时的命令行监控，包括了对heap size和垃圾回收状况的监控\n\n```shell\njstat -class pid # 输出加载类的数量及所占空间信息。\njstat -gc pid # 输出gc信息，包括gc次数和时间，内存使用状况（可带时间和显示条目参数）\n```\n\n> [你了解Java的jstat命令吗？](https://mp.weixin.qq.com/s/4vUmSsPoEI-MLVKc2NBvnw)\n\n### jinfo:实时地查看和调整虚拟机各项参数\n\njinfo vmid:输出当前jvm进程的全部参数和系统属性(第一部分是系统的属性，第二部分是JVM的参数)。\n\njinfo -flag name vmid:输出对应名称的参数的具体值。比如输出MaxHeapSize、查看当前jvm进程是否开启打印GC日志(-XX:PrintGCDetails:详细GC日志模式，这两个都是默认关闭的)。\n\n```powershell\nC:\\Users\\SnailClimb>jinfo  -flag MaxHeapSize 17340\n-XX:MaxHeapSize=2124414976\nC:\\Users\\SnailClimb>jinfo  -flag PrintGC 17340\n-XX:-PrintGC\n```\n\n使用jinfo可以在不重启虚拟机的情况下，可以动态的修改jvm的参数。尤其在线上的环境特别有用,请看下面的例子：\n\njinfo -flag [+|-]name vmid开启或者关闭对应名称的参数。\n\n```powershell\nC:\\Users\\SnailClimb>jinfo  -flag  PrintGC 17340\n-XX:-PrintGC\n\nC:\\Users\\SnailClimb>jinfo  -flag  +PrintGC 17340\n\nC:\\Users\\SnailClimb>jinfo  -flag  PrintGC 17340\n-XX:+PrintGC\n```\n\n```shell\njinfo pid # 查看指定pid的所有JVM信息\njinfo -flags pid # 查看设置过值的参数\njinfo -flag InitialHeapSize pid # 查看初始堆内存\njinfo -flag MaxHeapSize pid # 查看最大堆内存\njinfo -flag PermSize pid # 查看初始分配的非堆内存\njinfo -flag MaxPermSize pid # 查看最大允许分配的非堆内存\njinfo -flag NewSize pid # 查看年轻代初始内存\njinfo -flag MaxNewSize pid # 查看年轻代最大内存\njinfo -flag NewRatio pid # 查看年轻代与年老代的比值\njinfo -flag SurvivorRatio pid # 查看年轻代中Eden区与Survivor区的比值\njinfo -flag MaxTenuringThreshold pid # 查看对象如果在Survivor区移动了N次还没有被垃圾回收就进入年老代\njinfo -flag UseSerialGC pid # 查看串行收集器\njinfo -flag UseParallelGC pid # 查看并行收集器\njinfo -flag UseParNewGC pid # 查看并行收集器\njinfo -flag UseParallelOldGC pid # 查看并行收集器\njinfo -flag UseConcMarkSweepGC pid # 查看CMS回收器\njinfo -flag UseG1GC pid # 查看G1回收器\njinfo -flag PrintGCDetails pid # 查看是否打印GC日志\n```\n\n### jmap:生成堆转储快照\n\njmap（MemoryMapforJava）命令用于生成堆转储快照。如果不使用jmap命令，要想获取Java堆转储，可以使用“-XX:+HeapDumpOnOutOfMemoryError”参数，可以让虚拟机在OOM异常出现之后自动生成dump文件，Linux命令下可以通过kill-3发送进程退出信号也能拿到dump文件。\n\njmap的作用并不仅仅是为了获取dump文件，它还可以查询finalizer执行队列、Java堆和永久代的详细信息，如空间使用率、当前使用的是哪种收集器等。和jinfo一样，jmap有不少功能在Windows平台下也是受限制的。\n\n示例：将指定应用程序的堆快照输出到桌面。后面，可以通过jhat、Visual VM等工具分析该堆文件。\n\n```powershell\nC:\\Users\\SnailClimb>jmap -dump:format=b,file=C:\\Users\\SnailClimb\\Desktop\\heap.hprof 17340\nDumping heap to C:\\Users\\SnailClimb\\Desktop\\heap.hprof ...\nHeap dump file created\n```\n\n```shell\njmap -heap pid # 输出堆内存设置和使用情况（JDK11使用jhsdb jmap --heap --pid pid）\njmap -histo pid # 查看堆中对象数量和大小，包括类名，对象数量，对象占用大小\njmap -histo:live pid # 同上，只输出存活对象信息\njmap -clstats pid # 输出加载类信息\njmap -help # jmap命令帮助信息\n```\n\n**生成dump文件**\n\n方式一、jmap -dump:live,format=b,file=heap-dump.bin (pid)\n方式二、使用JConsole的dumpHeap按钮生成Heap Dump文件\n方式三、在JVM的配置参数中可以添加\n-XX:+HeapDumpOnOutOfMemoryError参数，当应用抛出OutOfMemoryError时自动生成dump文件；\n-XX:HeapDumpPath=/home/liuke/jvmlogs/：生成堆文件地址\n在JVM的配置参数中添加-Xrunhprof:head=site参数，会生成java.hprof.txt文件，不过这样会影响JVM的运行效率，不建议在生产环境中使用（未亲测）\n\n### jhat:分析heapdump文件\n\njhat用于分析heapdump文件，它会建立一个HTTP/HTML服务器，让用户可以在浏览器上查看分析结果。\n\n```powershell\nC:\\Users\\SnailClimb>jhat C:\\Users\\SnailClimb\\Desktop\\heap.hprof\nReading from C:\\Users\\SnailClimb\\Desktop\\heap.hprof...\nDump file created Sat May 04 12:30:31 CST 2019\nSnapshot read, resolving...\nResolving 131419 objects...\nChasing references, expect 26 dots..........................\nEliminating duplicate references..........................\nSnapshot resolved.\nStarted HTTP server on port 7000\nServer is ready.\n```\n\n访问[http://localhost:7000/](http://localhost:7000/)\n\n### jstack:生成虚拟机当前时刻的线程快照\n\njstack（Stack Trace for Java）命令用于生成虚拟机当前时刻的线程快照。线程快照就是当前虚拟机内每一条线程正在执行的方法堆栈的集合.\n\n生成线程快照的目的主要是定位线程长时间出现停顿的原因，如线程间死锁、死循环、请求外部资源导致的长时间等待等都是导致线程长时间停顿的原因。线程出现停顿的时候通过`jstack`来查看各个线程的调用堆栈，就可以知道没有响应的线程到底在后台做些什么事情，或者在等待些什么资源。\n\n**下面是一个线程死锁的代码。我们下面会通过jstack命令进行死锁检查，输出死锁信息，找到发生死锁的线程**。\n\n\n```java\npublic class DeadLockDemo {\n    private static Object resource1 = new Object();//资源1\n    private static Object resource2 = new Object();//资源2\n\n    public static void main(String[] args) {\n        new Thread(() -> {\n            synchronized (resource1) {\n                System.out.println(Thread.currentThread() + \"get resource1\");\n                try {\n                    Thread.sleep(1000);\n                } catch (InterruptedException e) {\n                    e.printStackTrace();\n                }\n                System.out.println(Thread.currentThread() + \"waiting get resource2\");\n                synchronized (resource2) {\n                    System.out.println(Thread.currentThread() + \"get resource2\");\n                }\n            }\n        }, \"线程 1\").start();\n\n        new Thread(() -> {\n            synchronized (resource2) {\n                System.out.println(Thread.currentThread() + \"get resource2\");\n                try {\n                    Thread.sleep(1000);\n                } catch (InterruptedException e) {\n                    e.printStackTrace();\n                }\n                System.out.println(Thread.currentThread() + \"waiting get resource1\");\n                synchronized (resource1) {\n                    System.out.println(Thread.currentThread() + \"get resource1\");\n                }\n            }\n        }, \"线程 2\").start();\n    }\n}\n```\n\nOutput\n\n\n```text\nThread[线程 1,5,main]get resource1\nThread[线程 2,5,main]get resource2\nThread[线程 1,5,main]waiting get resource2\nThread[线程 2,5,main]waiting get resource1\n```\n\n线程A通过synchronized(resource1)获得resource1的监视器锁，然后通过Thread.sleep(1000);让线程A休眠1s为的是让线程B得到执行然后获取到resource2的监视器锁。线程A和线程B休眠结束了都开始企图请求获取对方的资源，然后这两个线程就会陷入互相等待的状态，这也就产生了死锁。\n\n**通过jstack命令分析：**\n\n\n```powershell\nC:\\Users\\SnailClimb>jps\n13792 KotlinCompileDaemon\n7360 NettyClient2\n17396\n7972 Launcher\n8932 Launcher\n9256 DeadLockDemo\n10764 Jps\n17340 NettyServer\n\nC:\\Users\\SnailClimb>jstack 9256\n```\n\n输出的部分内容如下：\n\n```powershell\nFound one Java-level deadlock:\n=============================\n\"线程2\":\n  waiting to lock monitor 0x000000000333e668 (object 0x00000000d5efe1c0, a java.lang.Object),\n  which is held by \"线程1\"\n\"线程1\":\n  waiting to lock monitor 0x000000000333be88 (object 0x00000000d5efe1d0, a java.lang.Object),\n  which is held by \"线程2\"\n\nJava stack information for the threads listed above:\n===================================================\n\"线程2\":\n        at DeadLockDemo.lambda$main$1(DeadLockDemo.java:31)\n        - waiting to lock <0x00000000d5efe1c0> (a java.lang.Object)\n        - locked <0x00000000d5efe1d0> (a java.lang.Object)\n        at DeadLockDemo$$Lambda$2/1078694789.run(Unknown Source)\n        at java.lang.Thread.run(Thread.java:748)\n\"线程1\":\n        at DeadLockDemo.lambda$main$0(DeadLockDemo.java:16)\n        - waiting to lock <0x00000000d5efe1d0> (a java.lang.Object)\n        - locked <0x00000000d5efe1c0> (a java.lang.Object)\n        at DeadLockDemo$$Lambda$1/1324119927.run(Unknown Source)\n        at java.lang.Thread.run(Thread.java:748)\n\nFound 1 deadlock.\n```\n\n可以看到jstack命令已经帮我们找到发生死锁的线程的具体信息。\n\n### JDK可视化分析工具\n\n### JConsole:Java监视与管理控制台\n\nJConsole是基于JMX的可视化监视、管理工具。可以很方便的监视本地及远程服务器的java进程的内存使用情况。你可以在控制台输出`console`命令启动或者在JDK目录下的bin目录找到`jconsole.exe`然后双击启动。\n\n#### 连接Jconsole\n\n如果需要使用JConsole连接远程进程，可以在远程Java程序启动时加上下面这些参数:\n\n```properties\n-Djava.rmi.server.hostname=外网访问ip地址\n-Dcom.sun.management.jmxremote.port=60001 //监控的端口号\n-Dcom.sun.management.jmxremote.authenticate=false //关闭认证\n-Dcom.sun.management.jmxremote.ssl=false\n```\n\n在使用JConsole连接时，远程进程地址如下：\n\n\n```text\n外网访问ip地址:60001\n```\n\n#### 内存监控\n\nJConsole可以显示当前内存的详细信息。不仅包括堆内存/非堆内存的整体信息，还可以细化到eden区、survivor区等的使用情况，如下图所示。点击右边的“执行GC(G)”按钮可以强制应用程序执行一个FullGC。\n\n> - **新生代 GC（Minor GC）**:指发生新生代的的垃圾收集动作，Minor GC非常频繁，回收速度一般也比较快。\n> - **老年代 GC（Major GC/Full GC）**:指发生在老年代的GC，出现了Major GC经常会伴随至少一次的Minor GC（并非绝对），Major GC的速度一般会比Minor GC的慢10倍以上。\n\n#### 线程监控\n\n类似我们前面讲的`jstack`命令，不过这个是可视化的。最下面有一个\"检测死锁(D)\"按钮，点击这个按钮可以自动为你找到发生死锁的线程以及它们的详细信息。\n\n### Visual VM:多合一故障处理工具\n\nVisualVM提供在Java虚拟机(Java Virutal Machine,JVM)上运行的Java应用程序的详细信息。在VisualVM的图形用户界面中，您可以方便、快捷地查看多个Java应用程序的相关信息。[VisualVM官网](https://visualvm.github.io/)、[VisualVM中文文档](https://visualvm.github.io/documentation.html)。\n\n下面这段话摘自《深入理解Java虚拟机》。\n\n> VisualVM（All-in-One Java Troubleshooting Tool）是到目前为止随JDK发布的功能最强大的运行监视和故障处理程序，官方在VisualVM的软件说明中写上了“All-in-One”的描述字样，预示着他除了运行监视、故障处理外，还提供了很多其他方面的功能，如性能分析（Profiling）。VisualVM的性能分析功能甚至比起JProfiler、YourKit等专业且收费的Profiling工具都不会逊色多少，而且VisualVM还有一个很大的优点：不需要被监视的程序基于特殊Agent运行，因此他对应用程序的实际性能的影响很小，使得他可以直接应用在生产环境中。这个优点是JProfiler、YourKit等工具无法与之媲美的。\n\nVisualVM基于NetBeans平台开发，因此他一开始就具备了插件扩展功能的特性，通过插件扩展支持，VisualVM可以做到：\n\n- 显示虚拟机进程以及进程的配置、环境信息（jps、jinfo）。\n- 监视应用程序的CPU、GC、堆、方法区以及线程的信息（jstat、jstack）。\n- dump以及分析堆转储快照（jmap、jhat）。\n- 方法级的程序运行性能分析，找到被调用最多、运行时间最长的方法。\n- 离线程序快照：收集程序的运行时配置、线程dump、内存dump等信息建立一个快照，可以将快照发送开发者处进行Bug反馈。\n- 其他plugins的无限的可能性......\n\n> 这里就不具体介绍VisualVM的使用，如果想了解的话可以看:\n> - [https://visualvm.github.io/documentation.html](https://visualvm.github.io/documentation.html)\n> - https://www.ibm.com/developerworks/cn/java/j-lo-visualvm/index.html\n> \n> [原文链接](https://javaguide.cn/java/jvm/jdk-monitoring-and-troubleshooting-tools.html)\n\n###  jstack\n\n主要用于生成指定进程当前时刻的线程快照，线程快照是当前java虚拟机每一条线程正在执行的方法堆栈的集合，生成线程快照的主要目的是用于定位线程出现长时间停顿的原因，如线程间死锁、死循环、请求外部资源导致长时间等待。\n\n### jcmd\n\n在JDK1.7之后，新增了一个命令行工具jcmd。它是一个多功能工具，可以用来导出堆，查看java进程，导出线程信息，执行GC等。jcmd拥有jmap的大部分功能，Oracle官方建议使用jcmd代替jmap。\n\n### 相关文章\n\n- [这几款JVM故障诊断处理工具，你还不会？](https://mp.weixin.qq.com/s/b5pnoqWNMIp7JnfUWyWLoA)\n- [JVM性能调优监控工具jps、jstack、jmap、jhat、jstat、hprof使用详解](https://mp.weixin.qq.com/s/72X1qvTsCLneFffzIpKoaA)\n- [有了这款可视化工具，Java应用性能调优so easy！(JVisualVM简介)](https://mp.weixin.qq.com/s/rLhkbpe0Uo3Nsy8Xs68Z7A)\n- [死锁的4种排查工具！](https://mp.weixin.qq.com/s/5l6q1-jcfZNegpUsriaj5g)\n- [6款Java8自带工具，轻松分析定位JVM问题！](https://mp.weixin.qq.com/s/phD24wUysUDQkx5pAjp5tg)\n\n\n## JVM参数\n\n### 示例\n```\n-Xms:初始堆大小,默认值:物理内存的1/64(<1GB)\n默认(MinHeapFreeRatio参数可以调整)空余堆内存小于40%时，JVM就会增大堆直到-Xmx的最大限制.\n\n-Xmx:最大堆大小,默认值:物理内存的1/4(<1GB)\n默认(MaxHeapFreeRatio参数可以调整)空余堆内存大于70%时，JVM会减少堆直到-Xms的最小限制\n\n-Xmn:年轻代大小(1.4or lator)\n此处的大小是（eden+ 2 survivor space).与jmap -heap中显示的New gen是不同的。整个堆大小=年轻代大小+老年代大小+持久代（永久代）大小.增大年轻代后,将会减小年老代大小.此值对系统性能影响较大,Sun官方推荐配置为整个堆的3/8\n\n-XX:NewSize:年轻代大小(for 1.3/1.4)\n\n-XX:MaxNewSize:年轻代最大值(for 1.3/1.4)\n\n-XX:PermSize:设置持久代(perm gen)初始值,默认值:物理内存的1/64\n\n-XX:MaxPermSize:设置持久带最大值\n\n**注意：jdk8取消持久带用元空间代替**\n\n-XX:MetaspaceSize代替-XX:PermSize\n\n-XX:MaxMetaspaceSize代替-XX:MaxPermSize\n\n-Xss:每个线程的堆栈大小\n\nJDK5.0以后每个线程堆栈大小为1M,以前每个线程堆栈大小为256K.更具应用的线程所需内存大小进行调整,在相同物理内存下,减小这个值能生成更多的线程,但是操作系统对一个进程内的线程数还是有限制的,不能无限生成,经验值在3000~5000左右一般小的应用,如果栈不是很深,应该是128k够用的大的应用建议使用256k.这个选项对性能影响比较大，需要严格的测试\n\n-XX:NewRatio:年轻代(包括Eden和两个Survivor区)与年老代的比值(除去持久代)\n-XX:NewRatio=4表示年轻代与年老代所占比值为1:4,年轻代占整个堆栈的1/5Xms=Xmx并且设置了Xmn的情况下，该参数不需要进行设置\n\n-XX:SurvivorRatio:Eden区与Survivor区的大小比值\n-XX:SurvivorRatio=8,则两个Survivor区与一个Eden区的比值为2:8,一个Survivor区占整个年轻代的1/10\n\n-XX:MaxDirectMemorySize：设置New I/O(java.nio) direct-buffer allocations的最大大小\n当Direct ByteBuffer分配的堆外内存到达指定大小后，即触发Full GC。注意该值是有上限的，默认是64M，最大sun.misc.VM.maxDirectMemory()，在程序中中可以获得-XX:MaxDirectMemorySize的设置的值。\n\n-XX:+DisableExplicitGC:关闭System.gc()\n\n-XX:PretenureSizeThreshold:对象超过多大是直接在旧生代分配\t0\n单位字节,新生代采用Parallel ScavengeGC时无效另一种直接在旧生代分配的情况是大的数组对象,且数组中无外部引用对象.\n\n-XX:ParallelGCThreads:并行收集器的线程数\n此值最好配置与处理器数目相等,同样适用于CMS\n\n-XX:MaxGCPauseMillis:每次年轻代垃圾回收的最长时间(最大暂停时间)\n如果无法满足此时间,JVM会自动调整年轻代大小,以满足此值.\n```\n\n例：\n\n```shell\njava\n\n  -Xms64m #JVM启动时的初始堆大小\n\n  -Xmx128m #最大堆大小\n\n  -Xmn64m #年轻代的大小，其余的空间是老年代\n\n  -XX:MaxMetaspaceSize=128m #\n\n  -XX:CompressedClassSpaceSize=64m #使用 -XX：CompressedClassSpaceSize 设置为压缩类空间保留的最大内存。\n\n  -Xss256k #线程\n\n  -XX:InitialCodeCacheSize=4m # 代码缓存区域设定值\n\n  -XX:ReservedCodeCacheSize=8m # 这是由 JIT（即时）编译器编译为本地代码的本机代码（如JNI）或Java方法的空间\n\n  -XX:MaxDirectMemorySize=16m\n\n  -XX:NativeMemoryTracking=summary #开启内存追踪\n\n  -jar app.jar\n```\n### 堆内存相关\n\n> Java虚拟机所管理的内存中最大的一块，Java堆是所有线程共享的一块内存区域，在虚拟机启动时创建。**此内存区域的唯一目的就是存放对象实例，几乎所有的对象实例以及数组都在这里分配内存**。\n\n#### 显式指定堆内存–Xms和-Xmx\n\n与性能有关的最常见实践之一是根据应用程序要求初始化堆内存。如果我们需要指定最小和最大堆大小（推荐显示指定大小），以下参数可以帮助你实现：\n\n```bash\n-Xms<heap size>[unit]\n-Xmx<heap size>[unit]\n```\n\n- **heap size**表示要初始化内存的具体大小。\n- **unit**表示要初始化内存的单位。单位为**g**(GB)、**m**（MB）、**k**（KB）。\n\n举个栗子🌰，如果我们要为JVM分配最小2GB和最大5GB的堆内存大小，我们的参数应该这样来写：\n\n```bash\n-Xms2G -Xmx5G\n```\n\n#### 显式新生代内存(Young Generation)\n\n根据[Oracle官方文档](https://docs.oracle.com/javase/8/docs/technotes/guides/vm/gctuning/sizing.html)，在堆总可用内存配置完成之后，第二大影响因素是为Young Generation在堆内存所占的比例。默认情况下，YG的最小大小为1310*MB*，最大大小为无限制。\n\n一共有两种指定新生代内存(Young Ceneration)大小的方法：\n\n1.通过-XX:NewSize和-XX:MaxNewSize指定\n\n```bash\n-XX:NewSize=<young size>[unit]\n-XX:MaxNewSize=<young size>[unit]\n```\n\n举个栗子🌰，如果我们要为新生代分配最小256m的内存，最大1024m的内存我们的参数应该这样来写：\n\n\n```bash\n-XX:NewSize=256m\n-XX:MaxNewSize=1024m\n```\n\n2.通过-Xmn<young size\\>[unit]指定\n\n举个栗子🌰，如果我们要为新生代分配256m的内存（NewSize与MaxNewSize设为一致），我们的参数应该这样来写：\n\n```bash\n-Xmn256m\n```\n\nGC调优策略中很重要的一条经验总结是这样说的：\n\n> 将新对象预留在新生代，由于Full GC的成本远高于Minor GC，因此尽可能将对象分配在新生代是明智的做法，实际项目中根据GC日志分析新生代空间大小分配是否合理，适当通过“-Xmn”命令调节新生代大小，最大限度降低新对象直接进入老年代的情况。\n\n另外，你还可以通过-XX:NewRatio=<int\\>来设置老年代与新生代内存的比值。\n\n比如下面的参数就是设置老年代与新生代内存的比值为1。也就是说老年代和新生代所占比值为1：1，新生代占整个堆栈的1/2。\n\n```text\n-XX:NewRatio=1\n```\n\n#### 显式指定永久代/元空间的大小\n\n**从Java8开始，如果我们没有指定Metaspace的大小，随着更多类的创建，虚拟机会耗尽所有可用的系统内存（永久代并不会出现这种情况）**。\n\nJDK1.8之前永久代还没被彻底移除的时候通常通过下面这些参数来调节方法区大小\n\n```bash\n# 方法区(永久代)初始大小\n-XX:PermSize=N\n# 方法区(永久代)最大大小,超过这个值将会抛出OutOfMemoryError异常:java.lang.OutOfMemoryError:PermGen\n-XX:MaxPermSize=N\n```\n\n相对而言，垃圾收集行为在这个区域是比较少出现的，但并非数据进入方法区后就“永久存在”了。\n\n**JDK1.8的时候，方法区（HotSpot的永久代）被彻底移除了（JDK1.7就已经开始了），取而代之是元空间，元空间使用的是本地内存**。下面是一些常用参数：\n\n```bash\n# 设置Metaspace的初始（和最小大小）\n-XX:MetaspaceSize=N\n# 设置Metaspace的最大大小，如果不指定大小的话，随着更多类的创建，虚拟机会耗尽所有可用的系统内存。\n-XX:MaxMetaspaceSize=N\n```\n\n### 垃圾收集相关\n\n#### 垃圾回收器\n\n为了提高应用程序的稳定性，选择正确的[垃圾收集](http://www.oracle.com/webfolder/technetwork/tutorials/obe/java/gc01/index.html)算法至关重要。\n\nJVM具有四种类型的GC实现：\n\n- 串行垃圾收集器\n- 并行垃圾收集器\n- CMS垃圾收集器\n- G1垃圾收集器\n\n可以使用以下参数声明这些实现：\n\n```bash\n-XX:+UseSerialGC\n-XX:+UseParallelGC\n-XX:+UseParNewGC\n-XX:+UseG1GC\n```\n\n> 有关*垃圾回收*实施的更多详细信息，请参见[此处](https://github.com/Snailclimb/JavaGuide/blob/master/docs/java/jvm/JVM垃圾回收.md)。\n\n#### GC日志记录\n\n生产环境上，或者其他要测试GC问题的环境上，一定会配置上打印GC日志的参数，便于分析GC相关的问题。\n\n\n```bash\n# 必选\n# 打印基本GC信息\n-XX:+PrintGCDetails\n-XX:+PrintGCDateStamps\n# 打印对象分布\n-XX:+PrintTenuringDistribution\n# 打印堆数据\n-XX:+PrintHeapAtGC\n# 打印Reference处理信息\n# 强引用/弱引用/软引用/虚引用/finalize相关的方法\n-XX:+PrintReferenceGC\n# 打印STW时间\n-XX:+PrintGCApplicationStoppedTime\n\n# 可选\n# 打印safepoint信息，进入STW阶段之前，需要要找到一个合适的safepoint\n-XX:+PrintSafepointStatistics\n-XX:PrintSafepointStatisticsCount=1\n\n# GC日志输出的文件路径\n-Xloggc:/path/to/gc-%t.log\n# 开启日志文件分割\n-XX:+UseGCLogFileRotation\n# 最多分割几个文件，超过之后从头文件开始写\n-XX:NumberOfGCLogFiles=14\n# 每个文件上限大小，超过就触发分割\n-XX:GCLogFileSize=50M\n```\n\n### 处理OOM\n\n对于大型应用程序来说，面对内存不足错误是非常常见的，这反过来会导致应用程序崩溃。这是一个非常关键的场景，很难通过复制来解决这个问题。这就是为什么JVM提供了一些参数，这些参数将堆内存转储到一个物理文件中，以后可以用来查找泄漏:\n\n```bash\n-XX:+HeapDumpOnOutOfMemoryError\n-XX:HeapDumpPath=./java_pid<pid>.hprof\n-XX:OnOutOfMemoryError=\"< cmd args >;< cmd args >\"\n-XX:+UseGCOverheadLimit\n```\n\n这里有几点需要注意:\n\n- **HeapDumpOnOutOfMemoryError**指示JVM在遇到**OutOfMemoryError**错误时将heap转储到物理文件中。\n- **HeapDumpPath**表示要写入文件的路径;可以给出任何文件名;但是，如果JVM在名称中找到一个`<pid>`标记，则当前进程的进程id将附加到文件名中，并使用`.hprof`格式\n- **OnOutOfMemoryError**用于发出紧急命令，以便在内存不足的情况下执行;应该在`cmd args`空间中使用适当的命令。例如，如果我们想在内存不足时重启服务器，我们可以设置参数:`-XX:OnOutOfMemoryError=\"shutdown -r\"`。\n- **UseGCOverheadLimit**是一种策略，它限制在抛出OutOfMemory错误之前在GC中花费的VM时间的比例\n\n### 其他\n\n- `-server`:启用“Server Hotspot VM”;此参数默认用于64位JVM\n- `-XX:+UseStringDeduplication`:*Java 8u20*引入了这个JVM参数，通过创建太多相同String的实例来减少不必要的内存使用;这通过将重复String值减少为单个全局`char []`数组来优化堆内存。\n- `-XX:+UseLWPSynchronization`:设置基于LWP(轻量级进程)的同步策略，而不是基于线程的同步。\n- `-XX:LargePageSizeInBytes`:设置用于Java堆的较大页面大小;它采用GB/MB/KB的参数;页面大小越大，我们可以更好地利用虚拟内存硬件资源;然而，这可能会导致PermGen的空间大小更大，这反过来又会迫使Java堆空间的大小减小。\n- `-XX:MaxHeapFreeRatio`:设置GC后,堆空闲的最大百分比，以避免收缩。\n- `-XX:SurvivorRatio`:eden/survivor空间的比例,例如`-XX:SurvivorRatio=6`设置每个survivor和eden之间的比例为1:6。\n- `-XX:+UseLargePages`:如果系统支持，则使用大页面内存;请注意，如果使用这个JVM参数，OpenJDK 7可能会崩溃。\n- `-XX:+UseStringCache`:启用String池中可用的常用分配字符串的缓存。\n- `-XX:+UseCompressedStrings`:对String对象使用`byte []`类型，该类型可以用纯ASCII格式表示。\n- `-XX:+OptimizeStringConcat`:它尽可能优化字符串串联操作。\n\n### 文章推荐\n\n- [JVM参数配置说明-阿里云官方文档-2022](https://help.aliyun.com/document_detail/148851.html)\n- [JVM内存配置最佳实践-阿里云官方文档-2022](https://help.aliyun.com/document_detail/383255.html)\n- [求你了，GC日志打印别再瞎配置了](https://segmentfault.com/a/1190000039806436)\n- [一次大量JVM Native内存泄露的排查分析（64M问题）](https://juejin.cn/post/7078624931826794503)\n- [一次线上JVM调优实践，FullGC40次/天到10天一次的优化过程](https://heapdump.cn/article/1859160)\n- [听说JVM性能优化很难？今天我小试了一把](https://shuyi.tech/archives/have-a-try-in-jvm-combat)\n- [你们要的线上GC问题案例来啦](https://mp.weixin.qq.com/s/df1uxHWUXzhErxW1sZ6OvQ)\n- [Java中9种常见的CMS GC问题分析与解决](https://tech.meituan.com/2020/11/12/java-9-cms-gc.html)\n- [从实际案例聊聊Java应用的GC优化](https://tech.meituan.com/2017/12/29/jvm-optimize.html)\n- [常用的J?VM参数，你现在就记好！](https://mp.weixin.qq.com/s/3yvA2m66--Nxsf_xQDJZog)\n- [每天100万次登陆请求，8G内存该如何设置JVM参数？](https://mp.weixin.qq.com/s/vZtg3XolIYJuiqJIeM2j3A)\n\n\n## 相关文章\n\n### JVM知识点\n\n- [JVM知识点全面梳理！](https://mp.weixin.qq.com/s/IUw8SroGMJdvqPFQ1xEHxg)\n- [《深入理解Java虚拟机》把这个知识点讲错了？](https://mp.weixin.qq.com/s/4qZLipxi-zjrdI-_6Ae11A)\n- [JVM史上最最最完整知识总结！](https://mp.weixin.qq.com/s/GAXLr0cIcLnGaTXVstHKvA)\n- [JVM面试的30个知识点](https://mp.weixin.qq.com/s/PM_Q3718Du81ZFEx0ARrkw)\n- [满满的一整篇，全是JVM核心知识点！](https://mp.weixin.qq.com/s/GfAfffbF_uiphN0Zd3YouQ)\n- [大白话带你认识JVM、JVM的常用参数](https://mp.weixin.qq.com/s/CZtW-rDXgRl9E_3N2ou5TA)\n- [JVM夺命连环10问](https://mp.weixin.qq.com/s/_0IANOvyP_UNezDm0bxXmg)\n- [【干货】JVM完整深入解析！](https://mp.weixin.qq.com/s/vd72d6wqc_fq7YUgA7d1fA)\n- [如何在面试时搞定Java虚拟机](https://mp.weixin.qq.com/s/6MSezM6g4JPoML0kDtx64Q)\n- [JAVA虚拟机(JVM)面试题](https://mp.weixin.qq.com/s/eNpHYG9T5DIdh7QNpLg4dQ)\n- [JVM史上最最最完整深入解析，不看后悔一百次！](https://mp.weixin.qq.com/s/EYUNHXLP9jjTTA1Ig6vCYQ)\n- [【秒懂！】JVM虚拟机图文详解！一点都不难！](https://mp.weixin.qq.com/s/A4BbKDmy8oe3uBfHfrgGQA)\n- [从JMM透析volatile与synchronized原理](https://mp.weixin.qq.com/s/s2nnXDY7phqKX07nwZiHyA)\n- [JVM锁优化和逃逸分析详解](https://mp.weixin.qq.com/s/guITssdS4aYXzDJD3xl4HA)\n\n\n### 内存溢出分析\n\n- [Java内存泄漏了，怎么排查？](https://mp.weixin.qq.com/s/OtSoZWIl4XuIzqTayfORHw)\n- [常见OOM异常分析](https://mp.weixin.qq.com/s/DssuTRyN1RVWhQgMClP_ng)\n- [大厂的OOM优化和监控方案](https://mp.weixin.qq.com/s/-pZDUcW2ljBSIxZV0w0iSw)\n- [如何排查Java内存泄漏?看完我给跪了!](https://mp.weixin.qq.com/s/fH6Go6_1TcvHfxDPnyZvhg)\n- [常见的OOM异常分析（硬核干货）](https://mp.weixin.qq.com/s/oNWwPZ56yLCp1KQyir17AA)\n- [深入剖析线上内存溢出的原因](https://mp.weixin.qq.com/s?__biz=Mzg2MDYzODI5Nw==&mid=2247494348&idx=1&sn=0096ff574aace3d725bcbb3323a67d01&source=41#wechat_redirect)\n- [一套完整的Java线上故障排查技巧，建议收藏](https://mp.weixin.qq.com/s/03qoBFLpvkRVhaorwCNJ5Q)\n- [GC垃圾收集器&JVM调优汇总](https://mp.weixin.qq.com/s/UzGeDeuZORaJkzFr_V6-5A)\n- [图解Java虚拟机中GC的复制算法和“标记-整理”算法](https://mp.weixin.qq.com/s/1_Dz7vl9k01FtyYHIn107w)\n- [Java项目线上故障排查：从CPU、磁盘、内存、网络、GC一条龙完整套路！](https://mp.weixin.qq.com/s/ujqwk7pdz5a-UpR6q5ogFQ)","categories":["Java"]},{"title":"Git","slug":"Git","url":"/blog/posts/30bd6b4cbf9a/","content":"\n### Git常用命令\n\n#### 上传git仓库\n\n```shell\n# 创建.git目录\ngit init\ngit add -A\ngit commit -m \"init\"\ngit remote add origin https://github.com/xmxe/project.git\n# 断开连接\ngit remote remove origin\n# 增加一个新的远程仓库\n#git remote add name url\n# 删除指定远程仓库\n#git remote remove name\n# 获取指定远程仓库的详细信息\n#git remote show origin\n\ngit pull --rebase origin master \ngit push origin master\n# git push --set-upstream origin master\n\n# 本地生成ssh密钥\nssh-keygen -t rsa -C \"你的邮箱\"生成sshkey\ngit remote set-url origin git@github.com:xmxe/springcloud.git\n```\n#### git diff\n\n```shell\n# 可以查看当前没有add的内容修改（不在缓冲区的文件变化）\ngit diff\n# 查看已经add但没有commit的改动（在缓冲区的文件变化）\ngit diff --cached\n# 是上面两条命令的合并\ngit diff HEAD\n# 查看文件和另一个分支的区别\ngit diff branch-name file-name\n# 查看两次提交的区别\ngit diff commit-id commit-id\n```\n\n#### git reset\n\n```shell\n# 将未commit的文件移出Staging区\ngit reset HEAD\n# 重置Staging区与上次commit的一样\ngit reset --hard  \n# 重置Commit代码和远程分支代码一样\ngit reset --hard origin/master\n# 回退到上个commit 删除工作空间改动代码，撤销commit，撤销git add\ngit reset --hard HEAD^\n# 回退到前3次提交之前，以此类推，回退到n次提交之前\ngit reset --hard HEAD~3\n# 撤销git add操作（已经将文件添加到暂存区）不加filename就是上一次add里面的全部撤销了\ngit reset HEAD filename\n# 回退到指定版本 --hard强制将暂存区和工作目录都同步到你指定的提交\ngit reset --hard commitid\n# 不删除工作空间改动代码，撤销commit，不撤销git add\ngit reset --soft HEAD^\n# 不删除工作空间改动代码，撤销commit，并且撤销git add操作。这个为默认参数,git reset --mixed HEAD^和git reset HEAD^效果是一样的。\ngit reset --mixed HEAD^\n```\n\n#### git revert\n\n> 应用场景：有一天测试突然跟你说，你开发上线的功能有问题，需要马上撤回，否则会影响到系统使用。这时可能会想到用reset回退，可是你看了看分支上最新的提交还有其他同事的代码，用reset会把这部分代码也撤回了。由于情况紧急，又想不到好方法，还是任性的使用reset，然后再让同事把他的代码合一遍（同事听到想打人），于是你的技术形象在同事眼里一落千丈\n\n```shell\n# 回退到指定版本。与git reset --hard commitid相比区别是在我们确认了在需要回退的版本之后的提交都可以不需要的时候，我们可以直接使用git reset命令，但是当我们只是需要撤销某个版本的时候，后面的提交还需要保留，我们就可以使用git revert.revert会生成一条新的提交记录，这时会让你编辑提交信息，编辑完后:wq保存退出就好了。-n/--no edit：这个选项不会打开文本编辑器。它将直接恢复上次的提交。\ngit revert -n commitid\n# 撤销前一次commit ,此次操作之前和之后的commit都会被保留，并且会把这次撤销作为一次最新的提交\ngit revert HEAD\n# 撤销前前一次commit\ngit revert HEAD^\n```\n\n#### git commit\n\n```shell\n# (git add -u + git commit -m \"\"组合)\ngit commit -am ''\n# 修改commit信息\ngit commit --amend\n# 使用新的一次commit，来覆盖上一次commit\ngit commit --amend -m \"message\"\n# 提交Staging中在指定文件到本地仓库区\ngit commit file1 file2 -m \"message\n# 修改上次提交的用户名和邮箱\ngit commit --amend --author=\"name <email>\" --no-edit\n```\n#### git add\n\n```shell\ngit add -A # 保存所有的修改\ngit add . # 保存新的添加和修改，但是不包括删除\ngit add -u # 保存修改和删除，但是不包括新建文件。\n```\n\n#### git branch\n\n```shell\n# 查看本地分支(不包括远程分支)\ngit branch\n# 查看远程分支\ngit branch -a\n# 列出本地所有分支 并显示最后一次提交的哈希值\ngit branch -v\n# 在-v的基础上 并且显示上游分支的名字\ngit branch -vv\n# 列出上游所有分支\ngit branch -r\n# 创建分支(未切换)\ngit branch <name>\n# 删除分支，不能删除当前所在分支\ngit branch -d <name>\n# 设置分支上游\ngit branch --set-upstream-to origin/master\n\n# 远程分支重命名,远程分支是指：假设你当前已经将该分支推送到远程了，这种情况修改起来要稍微多几步\n# 方案1:先重命名本地分支，然后推送到远程分支\n# 1.先重命名本地分支\ngit branch -m 旧分支名称 新分支名称\n# 2.删除远程分支(如果删除的是默认分支的话会失败，需要先更改默认分支)\ngit push --delete origin 旧分支名称\n# 3.上传新修改名称的本地分支\ngit push origin 新分支名称\n# 4.修改后的本地分支关联远程分支\ngit branch --set-upstream-to origin/新分支名称\n\n# 方案2(未测试):先修改远程仓库分支，然后与本地仓库同步\ngit branch -m master 2021.x\n# 获取源分支\ngit fetch origin\n# 切换源分支为远程分支\ngit branch -u origin/2021.x 2021.x\n# 设置远程分支\ngit remote set-head origin -a\n# 方案2另一种实现(未测试):先修改远程仓库分支，然后与本地仓库同步\n# 首先，在本地仓库中切换到需要同步的分支上：这里<branch-name>是需要同步的分支名称\ngit checkout <branch-name>\n# 接下来，从远程仓库中获取最新的分支列表和分支状态信息：这会更新本地仓库中的远程分支信息。\ngit fetch\n# 然后，使用以下命令来重置本地分支到远程分支的最新状态：这将强制将本地分支指向远程分支的最新状态。\ngit reset --hard origin/<branch-name>\n\n# 下载所有分支\n# git clone下载的是默认分支,分支较少的话可以使用git branch -a查看所有远程分支然后使用'git checkout 分支名'来下载其他分支。如果分支较多的话使用--bare,裸仓库(bare repository)指的是除了git仓库不包含其他工作文件的仓库，可以通过git clone --bare来生成。\ngit clone --bare https://github.com/xx/project.git .git\n# 或者git config --unset core.bare\ngit config --bool core.bare false\n# 上面的命令执行完,再执行该命令,就可以看到仓库里面的内容了\ngit reset --hard\n# 下载指定分支\ngit clone -b 分支名称 git地址\n# 为了节省磁盘空间，您可以使用以下命令克隆仅导致单个分支的历史记录,如果--single-branch未添加到命令中，则所有分支的历史记录将被克隆。大型存储库可能会出现此问题。\ngit clone -b <branch_name> --single-branch <url>\n```\n\n#### git checkout\n\n```shell\n# 切换分支\ngit checkout <name>\n# 创建+切换分支\ngit checkout -b <name>\n# 创建本地分支并关联远程分支\ngit checkout -b local-branch origin/remote-branch\n# 丢弃某个文件工作区的修改（还原修改过的文件）\ngit checkout -- file\n# 放弃本地所有修改，没有提交的可以回到未修改前版本，不包括新增删除的文件\ngit checkout .\n# 放弃本地所有修改，包括新增删除的文件\ngit checkout . && git clean -df\n#（先切换到develop分支然后把feature分支合并到develop分支）\ngit checkout develop && git merge feature\n# 将某个文件回滚到某个版本\ngit checkout commitid [file]\n\n# 删除历史提交记录\n# 创建孤立分支,没有以前的提交记录\ngit checkout --orphan <name>\n# 切换到一个脱离主分支的另外一条全新主分支，不用太在意叫什么，因为后面还会修改分支名称\ngit checkout --orphan latest_branch\n# 暂存所有改动过的文件，内容为当前旧分支的最新版本所有文件\ngit add -A\n#提交更改\ngit commit -am \"commit message\"\n#删除原始主分支\ngit branch -D main\n#将当前分支重命名为 main\ngit branch -m main\n#最后，强制更新您的存储库\ngit push -f origin main\n```\n#### git merge&git rebase\n\n```shell\n# 合并指定分支到当前分支\ngit merge <name>\n# 将指定分支合并到当前分支\ngit rebase branch-name\n# 执行commit id将rebase停留在指定commit处\ngit rebase -i commit-id\n# 执行commit id将rebase停留在 项目首次commit处\ngit rebase -i --root\n```\n##### git rebase和git merge区别\n\n采用merge和rebase后，git log的区别，merge命令不会保留merge的分支的commit，rebase会保留所有的commit：rebase会把你当前分支的commit放到公共分支的最后面,所以叫变基。就好像你从公共分支又重新拉出来这个分支一样。举例:如果你从master拉了个feature分支出来,然后你提交了几个commit,这个时候刚好有人把他开发的东西合并到master了,这个时候master就比你拉分支的时候多了几个commit,如果这个时候你rebase master的话，就会把你当前的几个commit，放到那个人commit的后面。merge会把公共分支和你当前的commit合并在一起，形成一个新的commit提交\n\n处理冲突的方式：（一股脑）使用merge命令合并分支，解决完冲突，执行git add .和git commit -m 'fix conflict'。这个时候会产生一个commit。（交互式）使用rebase命令合并分支，解决完冲突，执行git add .和git rebase --continue，不会产生额外的commit。这样的好处是干净，分支上不会有无意义的解决分支的commit；坏处，如果合并的分支中存在多个commit，需要重复处理多次冲突。\n\ngit pull和git pull --rebase区别：git pull做了两个操作分别是‘获取’和合并。所以加了rebase就是以rebase的方式进行合并分支得到一条干净的分支流。\n\ngit merge和git merge --no-ff的区别\n自己尝试merge命令后，发现merge时并没有产生一个commit。不是说merge时会产生一个merge commit吗？注意：只有在冲突的时候，解决完冲突才会自动产生一个commit。如果想在没有冲突的情况下也自动生成一个commit，记录此次合并就可以用：git merge --no-ff命令,如果不加--no-ff则被合并的分支之前的commit都会被抹去，只会保留一个解决冲突后的merge commit。\n\n> [合并代码还在用git merge？我们都用git rebase！](https://mp.weixin.qq.com/s/T_8bkWI-JSP5ixdVIvVAGQ)\n> [新来个技术总监，禁止我们用Git的rebase](https://mp.weixin.qq.com/s/CBz0ea6m623GtuTX5UkeQQ)\n\n#### git tag\n\n```shell\n# 查看所有标签，可以知道历史版本的tag\ngit tag\n# 打标签，默认为HEAD。比如git tag v1.0\ngit tag <tagName>\n# 根据版本号打上标签\ngit tag <tagName> commit_id\n# 创建带说明的标签。-a指定标签名，-m指定说明文字\ngit tag -a <tagName> -m \"<说明>\"\n# 查看标签信息\ngit show <tagName>\n# 删除标签\ngit tag -d <tagName>\n# 推送某个标签到远程\ngit push origin <tagname>\n# 一次性推送全部尚未推送到远程的本地标签\ngit push origin --tags\n# 这会将空引用推送到远程仓库，从而删除名为<tag-name>的远程标签。\ngit push origin :refs/tags/<tag-name>\n\n```\n#### git stash\n\n> 应用场景：某一天你正在feature分支开发新需求，突然产品经理跑过来说线上有bug，必须马上修复。而此时你的功能开发到一半，于是你急忙想切到master分支，然后你就会看到以下报错：Your local changes to the following...因为当前有文件更改了，需要提交commit保持工作区干净才能切分支。由于情况紧急，你只有急忙commit上去，commit信息也随便写了个“暂存代码”，于是该分支提交记录就留了一条黑历史，如果你学会stash，就不用那么狼狈了。你只需要：git stash就这么简单，代码就被存起来了。当你修复完线上问题，切回feature分支，想恢复代码也只需要：git stash apply,但是恢复后，stash内容并不删除，需要用git stash drop来删除\n\n```shell\n# 保存当前未commit的代码\ngit stash\n# 保存当前未commit的代码并添加备注\ngit stash save \"备注的内容\"\n# 列出stash的所有记录\ngit stash list\n# 删除stash的所有记录\ngit stash clear\n# 应用最近一次的stash\ngit stash apply\n# 应用最近一次的stash，随后删除该记录\ngit stash pop\n# 删除最近的一次stash\ngit stash drop\n\n# 当有多条 stash，可以指定操作stash，首先使用stash list列出所有记录：\ngit stash list \n# stash@{0}: WIP on ...\n# stash@{1}: WIP on ...\n# stash@{2}: On ...\n\n# 应用第二条记录：\ngit stash apply stash@{1}\n# 删除stash@{1}存储的内容\ngit stash drop stash@{1}\n# pop，drop 同理。\n```\n\n#### git config\n\n```shell\ngit config --gloabl http.postBuffer 524288000\ngit config --gloabl http.sslVerify \"false\"\ngit config --global user.name \"your name\"\ngit config --global user.email \"your email\"\n# 保存密码\ngit config --global credential.helper store\n# 配置git图形界面编码为utf-8\ngit config --global gui.encoding=utf-8\n# 设置当前项目提交代码的用户名 \ngit config user.name name\n```\n\n#### git cherry-pick\n```shell\n# 把a分支的一个提交复制到b分支\n# 首先复制a分支的commitid 然后切换到b分支执行\ngit cherry-pick commitid\n# 一次转移多个提交：将commit1和commit2两个提交应用到当前分支。\ngit cherry-pick commit1 commit2\n# 多个连续的commit，也可区间复制：将commit1到commit2这个区间的commit都应用到当前分支（包含commit1、commit2），commit1是最早的提交。\ngit cherry-pick commit1^..commit2\n# 在cherry-pick多个commit时，可能会遇到代码冲突，这时cherry-pick会停下来，让用户决定如何继续操作\n# 这时需要解决代码冲突，重新提交到暂存区,然后使用cherry-pick --continue让cherry-pick继续进行下去。\ngit cherry-pick --continue\n# 但有时候可能需要在代码冲突后，放弃或者退出流程：放弃cherry-pick：\ngit cherry-pick --abort\n# 回到操作前的样子，就像什么都没发生过。\ngit cherry-pick --quit\n```\n\n#### git push\n```shell\n# 删除远程分支\ngit push origin :master\n#  删除远程标签\ngit push origin --delete tag tag-name\n# 上传本地仓库到远程分支\ngit push remote branch-name\n# 强行推送当前分支到远程分支\ngit push remote branch-name --force\n# 推送所有分支到远程仓库\ngit push remote --all\n# 推送所有标签\ngit push --tags\n# 推送指定标签\ngit push origin tag-name\n#  删除远程标签（需要先删除本地标签）\ngit push origin :refs/tags/tag-name\n# 将本地dev分支push到远程master分支\ngit push origin dev:master\n```\n\n#### git restore\n\n```shell\n# git restore命令在工作区是不会起作用的\n# 此命令将暂存区中的文件恢复到最后一次提交的状态，并将其从暂存区中移除，但保留工作目录中的修改。这意味着该文件的修改将不会包含在下一次的提交\ngit restore --staged file\n# 此命令将工作目录中的文件恢复到最后一次提交的状态，并且不会对暂存区进行任何操作。它将丢弃工作目录中对文件的修改，恢复到最后提交的版本。\ngit restore file\n```\n#### git删除github文件夹但不删除本地的 以.idea为例\n\n```shell\ngit rm -r --cached .idea # --cached不会把本地的.idea删除\ngit commit -m 'delete .idea dir'\ngit push -u origin master\n```\n#### git删除大文件\n\n```shell\n# 显示10个最大的文件id列表\ngit verify-pack -v .git/objects/pack/pack-*.idx | sort -k 3 -n | tail -10\n# 根据文件id查询文件路径\ngit rev-list --objects --all | grep 08a7475\n# 删除文件的历史记录\ngit filter-branch --force --index-filter 'git rm --cached --ignore-unmatch 文件名' --prune-empty --tag-name-filter cat -- --all\n\ngit filter-branch --index-filter # 让每个提交的文件都复制到索引(.git/index)中 然后运行过滤器命令：git rm --cached --ignore-unmatch文件名，让每个提交都删除掉“文件名”文件,然后--prune-empty 把空的提交“修剪”掉,然后--tag-name-filter cat把每个tag保持原名字，指向修改后的对应提交,最后-- --all将所有ref（包括branch、tag）都执行上面的重写\n\n# 删除缓存下来的ref和git操作记录\ngit for-each-ref --format='delete %(refname)' refs/original | git update-ref --stdin\ngit reflog expire --expire=now --all\n\n# 垃圾回收\n# 上面2步把大文件的索引都切断了，这个时候进行垃圾回收，就可以很明显看到效果了\ngit gc --prune=now\n# 把.git里面的修改推上去,这个时候普通的push是不行的，需要强推\ngit push --force\n\n```\n\n#### 设置Git短命令\n```shell\n# 方式一\ngit config --global alias.ps push\n# 方式二 打开全局配置文件\nvim ~/.gitconfig\n# 写入内容\n[alias] \n        co = checkout\n        ps = push\n        pl = pull\n        mer = merge --no-ff\n        cp = cherry-pick\n# 使用\n# 等同于 git cherry-pick <commitHash>\ngit cp <commitHash>\n```\n\n### 相关文章\n\n- [Git不要只会pull和push，试试这5条提高效率的命令](https://mp.weixin.qq.com/s/ct6GWiE_hzoXUNeriLAnng)\n- [Git各指令的本质，真是通俗易懂啊！](https://mp.weixin.qq.com/s/MM7sQiFPh2vIuGvg1-813Q)\n- [Git科普文，Git基本原理&各种骚操作](https://mp.weixin.qq.com/s/csEgAjJwH75_IvAnFBIuvw)\n- [一文快速掌握Git用法](https://mp.weixin.qq.com/s/xoyQ4TzVKLQb2VjZJLUqFQ)\n- [精心整理：Git从入门到精通、包教包会、收藏一下、随时学习](https://mp.weixin.qq.com/s/wtPizZ3RlwY3Ex1Lc3Pf4A)\n- [大牛总结的Git使用技巧，写得太好了！](https://mp.weixin.qq.com/s/OchvVMGoBzSFWhou4WhrWw)\n- [通过.git目录深入理解Git！](https://mp.weixin.qq.com/s/q6tI0qctvciJhNz_5KLx-w)\n- [Git命令全方位学习](https://mp.weixin.qq.com/s/KwTsWyFh07iYdfINqo9UTg)\n- [图解Git，一目了然！](https://mp.weixin.qq.com/s/TW_qUWRVEnberle5q0ws9Q)\n- [Git代码防丢指南，再也不怕丢失代码了！](https://mp.weixin.qq.com/s/dYiWQQ5PsSS7F7z7cDLEuw)\n- [20个最常用的Git命令，你都会用吗？](https://mp.weixin.qq.com/s/XB_G7TZqBX8r3CJlvqA0Cg)\n- [45个GIT经典操作场景，专治不会合代码](https://mp.weixin.qq.com/s/Fa8mmQpNZ1S80Kg9Oyocbw)\n- [Git的奇技淫巧](https://github.com/521xueweihan/git-tips)\n- [如何配置SSH管理多个Git仓库和以及多个Github账号](https://mp.weixin.qq.com/s/ADzad6e6uTF0vpZ903SJ-A)\n","tags":["随笔"]},{"title":"Nginx","slug":"Nginx","url":"/blog/posts/2ab37fc4e673/","content":"\n## Nginx安装\n\n### 下载\n\n```shell\n# 下载nginx:\nwget http://nginx.org/download/nginx-1.8.1.tar.gz\n# 下载openssl:\nwget https://www.openssl.org/source/openssl-fips-2.0.16.tar.gz\n# 下载zlib:\nwget http://www.zlib.net/zlib-1.2.11.tar.gz\n# 下载pcre:\nwget ftp://ftp.csx.cam.ac.uk/pub/software/programming/pcre/pcre-8.39.tar.gz\n# 如果没有安装c++编译环境，还得安装，通过```yum install gcc-c++```完成安装\n```\n\n### 编译安装\n```shell\n# openssl：\n[root@localhost] tar zxvf openssl-fips-2.0.16.tar.gz\n[root@localhost] cd openssl-fips-2.0.16\n[root@localhost] ./config && make && make install\n\n# pcre:\n[root@localhost] tar zxvf pcre-8.39.tar.gz\n[root@localhost] cd pcre-8.39\n[root@localhost]  ./configure && make && make install\n\n# zlib:\n[root@localhost]tar zxvf zlib-1.2.11.tar.gz\n[root@localhost] cd zlib-1.2.11\n[root@localhost]  ./configure && make && make install\n\n# 最后安装nginx\n[root@localhost]tar zxvf nginx-1.8.1.tar.gz\n[root@localhost] cd nginx-1.8.1\n[root@localhost]  ./configure && make && make install\n```\n\n### 启动nginx\n```shell\n/usr/local/nginx/sbin/nginx\n/usr/local/nginx/sbin/nginx -s stop # 立即停止nginx，不保存相关信息\n/usr/local/nginx/sbin/nginx -s quit  # 正常退出nginx，保存相关信息\n/usr/local/nginx/sbin/nginx -s reload # 重启\n```\n> [Linux安装Nginx详细图解教程](https://www.cnblogs.com/lovexinyi8/p/5845017.html)\n\n### 将nginx做成系统服务并且开机自启动\n\n由于是源码安装，需要手动创建nginx.service服务\n> 不止nginx，其他源码安装的想要实现开机自启动就在/lib/systemd/system目录下自定义服务即可\n```shell\nvim /lib/systemd/system/nginx.service\n# 编辑内容\n[Unit]\nDescription=nginx.service\nAfter=network.target\n\n[Service]\nType=forking\nExecStart=/usr/local/nginx/sbin/nginx\nExecReload=/usr/local/nginx/sbin/nginx -s reload\nExecStop=/usr/local/nginx/sbin/nginx -s quit\nPrivateTmp=true\n\n[Install]\nWantedBy=multi-user.target\n\n# 参数介绍：\n# [Unit]:服务的说明\n# Description:描述服务\n# After:描述服务类别\n# [Service]服务运行参数的设置\n# Type=forking是后台运行的形式\n# ExecStart为服务的具体运行命令\n# ExecReload为重启命令\n# ExecStop为停止命令\n# PrivateTmp=True表示给服务分配独立的临时空间\n# 注意：[Service]的启动、重启、停止命令全部要求使用绝对路径\n# [Install]运行级别下服务安装的相关设置，可设置为多用户，即系统运行级别为3\n```\n:wq! 保存退出。\n```shell\n# 设置开机启动\nsystemctl enable nginx.service\n# 其他命令\n# 启动nginx服务\nsystemctl start nginx.service　\n# 停止开机自启动\nsystemctl disable nginx.service\n# 查看服务当前状态\nsystemctl status nginx.service\n# 重新启动服务\nsystemctl restart nginx.service　\n# 查看所有已启动的服务\nsystemctl list-units --type=service\n```\n\n## Nginx知识点\n\n### nginx判断\n\n1、正则表达式匹配：\n\n==：等值比较;\n\\~：与指定正则表达式模式匹配时返回“真”，判断匹配与否时区分字符大小写；\n\\~\\*：与指定正则表达式模式匹配时返回“真”，判断匹配与否时不区分字符大小写；\n!\\~：与指定正则表达式模式不匹配时返回“真”，判断匹配与否时区分字符大小写；\n!\\~\\*：与指定正则表达式模式不匹配时返回“真”，判断匹配与否时不区分字符大小写；\n\n2、文件及目录匹配判断：\n\n-f, !-f：判断指定的路径是否为存在且为文件；\n-d, !-d：判断指定的路径是否为存在且为目录；\n-e, !-e：判断指定的路径是否存在，文件或目录均可；\n-x, !-x：判断指定路径的文件是否存在且可执行；\n\n### 语法规则\n\nlocation [=|~|~*|^~] /uri/ { … }\n\n- =：表示精确匹配,这个优先级也是最高的\n- ^\\~：表示uri以某个常规字符串开头，理解为匹配url路径即可。nginx不对url做编码，因此请求为/static/20%/aa，可以被规则^~ /static/ /aa匹配到（注意是空格）。\n- \\~：表示区分大小写的正则匹配\n- \\~\\*：表示不区分大小写的正则匹配(和上面的唯一区别就是大小写)\n- !\\~和!\\~\\*：分别为区分大小写不匹配及不区分大小写不匹配的正则\n- /：通用匹配，任何请求都会匹配到，默认匹配.\n\n**语法的一些规则和优先级**\n多个location配置的情况下匹配顺序为：首先匹配=，其次匹配^\\~,其次是按文件中顺序的正则匹配，最后是交给/通用匹配。当有匹配成功时候，停止匹配，按当前匹配规则处理请求。\n\n### ngx_http_core_module模块的变量\n\n- **$arg_PARAMETER**：HTTP请求中某个参数的值，如/index.php?site=www.domain.com 可以用$arg_site取得www.domain.com 这个值。\n- **$args HTTP**：请求中的完整参数。例如，在请求/index.php?width=400&height=200中，$args表示字符串width=400&height=200.\n- **$binary_remote_addr**：二进制格式的客户端地址。例如：\\x0A\\xE0B\\x0E\n- **$body_bytes_sent**：表示在向客户端发送的http响应中，包体部分的字节数\n- **$content_length**：表示客户端请求头部中的Content-Length字段\n- **$content_type**：表示客户端请求头部中的Content-Type字段\n- **$cookie_COOKIE**：表示在客户端请求头部中的cookie字段\n- **$document_root**：表示当前请求所使用的root配置项的值\n- **$uri**：表示当前请求的URI，不带任何参数\n- **$document_uri与$uri含义相同**\n- **$request_uri**：表示客户端发来的原始请求URI，带完整的参数。$uri和$document_uri未必是用户的原始请求，在内部重定向后可能是重定向后的URI，而$request_uri永远不会改变，始终是客户端的原始URI\n- **$host**：表示客户端请求头部中的Host字段。如果Host字段不存在，则以实际处理的server（虚拟主机）名称代替。如果Host字段中带有端口，如IP:PORT，那么$host是去掉端口的，它的值为IP。$host是全小写的。这些特性与http_HEADER中的http_host不同，http_host只取出Host头部对应的值。\n- **$hostname**：表示Nginx所在机器的名称，与gethostbyname调用返回的值相同\n- **$http_HEADER**：表示当前HTTP请求中相应头部的值。HEADER名称全小写。例如，示请求中Host头部对应的值用$http_host表\n- **$sent_http_HEADER**：表示返回客户端的HTTP响应中相应头部的值。HEADER名称全小写。例如，用$sent_http_content_type表示响应中Content-Type头部对应的值\n- **$is_args**：表示请求中的URI是否带参数，如果带参数，$is_args值为?，如果不带参数，则是空字符串\n- **$limit_rate**：表示当前连接的限速是多少，0表示无限速\n- **$nginx_version**：表示当前Nginx的版本号\n- **$query_string**：请求URI中的参数，与$args相同，然而$query_string是只读的不会改变\n- **$remote_addr**：表示客户端的地址\n- **$remote_port**：表示客户端连接使用的端口\n- **$remote_user**：表示使用Auth Basic Module时定义的用户名\n- **$request_filename**：表示用户请求中的URI经过root或alias转换后的文件路径\n- **$request_body**：表示HTTP请求中的包体，该参数只在proxy_pass或fastcgi_pass中有意义\n- **$request_body_file**：表示HTTP请求中的包体存储的临时文件名\n- **$request_completion**：当请求已经全部完成时，其值为“ok”。若没有完成，就要返回客户端，则其值为空字符串；或者在断点续传等情况下使用HTTP range访问的并不是文件的最后一块，那么其值也是空字符串。\n- **$request_method**：表示HTTP请求的方法名，如GET、PUT、POST等\n- **$scheme**：表示HTTP scheme，如在请求https://nginx.com/中表示https\n- **$server_addr **：表示服务器地址\n- **$server_name**：表示服务器名称\n- **$server_port**：表示服务器端口\n- **$server_protocol**：表示服务器向客户端发送响应的协议，如HTTP/1.1或HTTP/1.0\n\n### 日志配置\n\n- **$remote_addr,$http_x_forwarded_for- **：记录客户端IP地址\n- **$remote_user**：记录客户端用户名称\n- **$request**：记录请求的URL和HTTP协议\n- **$status**：记录请求状态\n- **$body_bytes_sent**：发送给客户端的字节数，不包括响应头的大小；该变量与Apache模块mod_log_config里的“%B”参数兼容。\n- **$bytes_sent**：发送给客户端的总字节数。\n- **$connection**：连接的序列号。\n- **$connection_requests**：当前通过一个连接获得的请求数量。\n- **$msec**：日志写入时间。单位为秒，精度是毫秒。\n- **$pipe**：如果请求是通过HTTP流水线(pipelined)发送，pipe值为“p”，否则为“.”。\n- **$http_referer**：记录从哪个页面链接访问过来的\n- **$http_user_agent**：记录客户端浏览器相关信息\n- **$request_length**：请求的长度（包括请求行，请求头和请求正文）。\n- **$request_time**：请求处理时间，单位为秒，精度毫秒；从读入客户端的第一个字节开始，直到把最后一个字符发送给客户端后进行日志写入为止。\n- **$time_iso8601**：ISO8601标准格式下的本地时间。\n- **$time_local**：通用日志格式下的本地时间。\n\n### if\n\n- 语法：if (condition) { … }\n- 默认值：none\n- 使用字段：server, location\n- 注意：尽量考虑使用trp_files代替。\n- 判断的条件可以有以下值：\n\n1. 一个变量的名称：空字符传”“或者一些“0”开始的字符串为false。\n2. 字符串比较：使用=或!=运算符\n3. 正则表达式匹配：使用\\~(区分大小写)和\\~\\*(不区分大小写)，取反运算!\\~和!\\~\\*。\n4. 文件是否存在：使用-f和!-f操作符\n5. 目录是否存在：使用-d和!-d操作符\n7. 文件、目录、符号链接是否存在：使用-e和!-e操作符\n8. 文件是否可执行：使用-x和!-x操作符\n\n### return\n\n- 语法：return code\n- 默认值：none\n- 使用字段：server,location,if\n- nginx隐藏版本号\n- nginx.conf中修改http zone中的变量值： server_tokens off;\n- php-fpm fastcgi.conf中的变量值： fastcgi_param SERVER_SOFTWARE nginx;\n\n### nginx正向代理\n\n```nginx\n\nserver {\n\nlisten 8090;\n\nlocation / {\n\nresolver 218.85.157.99 218.85.152.99;\n\nresolver_timeout 30s;\n\nproxy_pass http://$host$request_uri;\n\n}\n\naccess_log /data/httplogs/proxy-$host-aceess.log;\n\n}\n```\n\nresolver指令\n\n- 语法: resolver address ... [valid=time];\n- 默认值: —\n- 配置段: http, server, location\n- 配置DNS服务器IP地址。可以指定多个，以轮询方式请求。\n- nginx会缓存解析的结果。默认情况下，缓存时间是名字解析响应中的TTL字段的值，可以通过valid参数更改。\n\n## 相关文章\n\n- [这是一个Nginx极简教程，目的在于帮助新手快速入门Nginx。](https://github.com/dunwu/nginx-tutorial)\n- [就是要让你搞懂Nginx，这篇就够了！](https://mp.weixin.qq.com/s/5Q_VQoQY6kJiMwMHHDIijA)\n- [Nginx为什么快到根本停不下来？](https://mp.weixin.qq.com/s/e7r2Jt1DlF_4HpZU_IKZkQ)\n- [手把手教你在CentOS7上搭建Nginx](https://mp.weixin.qq.com/s?__biz=MzkzODE3OTI0Ng==&amp;mid=2247490879&amp;idx=1&amp;sn=bd93bc46cdfb7919b9a304c176927dd8&amp;source=41#wechat_redirect)\n- [nginx实现动态分离,解决css和js等图片加载问题](https://www.cnblogs.com/sz-jack/p/5206159.html)\n- [nginx反向代理tomcat，js，css静态资源不加载问题](https://blog.csdn.net/white1114579650/article/details/120151335)\n- [彻底搞懂Nginx的五大应用场景](https://mp.weixin.qq.com/s/v6j2HStMHBDlak6UGTF0Hw)\n- [nginx配置参数](https://blog.51cto.com/ting2junshui/2066268)\n- [Nginx轻松搞定跨域问题！](https://mp.weixin.qq.com/s/clSjaLJSht5J8woIaiH4gA)\n- [如何使用Nginx优雅地限流？](https://mp.weixin.qq.com/s/YXJ1jcr7XLKTbzf9kyjiEg)\n- [Nginx如何限流？](https://mp.weixin.qq.com/s/R6GajrvNphXfgKWDsFWzFw)\n- [nginxconfig.io](https://nginxconfig.io)\n- [为什么Nginx比Apache更牛叉？](https://mp.weixin.qq.com/s/pPV5s3uO1sjPTAhz_BDcJg)\n- [如何用Nginx代理MySQL连接，并限制可访问IP？](https://mp.weixin.qq.com/s/6lvKIQb4yk7uTmufr9pJ8w)\n","tags":["安装"]},{"title":"Linux相关","slug":"Linux相关","url":"/blog/posts/a8a3d9e9be38/","content":"\n\n## 相关命令\n\n### 关闭ssh情况下不退出进程\n\n**nohup command &**\ncommand参数表示要执行的命令行。但是这种方式启动项目会默认生成一个nohup.out的文件来记录日志，而且会越来越大，不生成日志使用>/dev/null 2>&1，这条命令的作用是将标准输出1重定向到/dev/null中。/dev/null代表linux的空设备文件，所有往这个文件里面写入的内容都会丢失，俗称“黑洞”。那么执行了>/dev/null之后，标准输出就会不再存在，没有任何地方能够找到输出的内容。\n\n> 1>/dev/null 2>&1 &详解：\n> 0：表示标准输入stdin (键盘输入)\n> 1：表示标准输出stdout，系统默认为1，可省略(即1>/dev/null等价于>/dev/null)\n> 2：表示标准错误stderr\n> \\>：表示重定向（即将输出定向到指定路径文件，>/dev/null表示Linux的空设备文件，即将标准输出重定向到空设备文件，即不输出任何信息到终端，即不显示任何信息。）\n> 2>&1：其中的&表示等同于的意思，即2(标准错误stderr)的重定向等同于1\n&表示后台运行\n\n最终命令\n```shell\nnohup command >/dev/null 2>&1 &\n# 后台执行abc.jar即便关闭终端也继续运行 不输出任何日志\nnohup java -jar abc.jar >/dev/null 2>&1 &\n\n# command >/dev/null 2>&1 & == command 1>/dev/null 2>&1 &\n# 1>/dev/null:表示标准输出重定向到空设备文件,也就是不输出任何信息到终端,不显示任何信息。2>&1:表示标准错误输出重定向等同于标准输出,因为之前标准输出已经重定向到了空设备文件,所以标准错误输出也重定向到空设备文件。这条命令的意思就是在后台执行这个程序,并将错误输出2重定向到标准输出1,然后将标准输出1全部放到/dev/null文件,也就是清空.所以可以看出\">/dev/null 2>&1 &\"常用来避免shell命令或者程序等运行中有内容输出\n# 将错误日志输出到文件\nnohup command 2>error.log &\n```\n\n### 安装ifconfig\n\n```shell\n# centos\nyum -y install net-tools\n# ubuntu\napt install net-tools\n```\n\n- apt与apt-get的区别\napt可以看作apt-get和apt-cache命令的子集,可以为包管理提供必要的命令选项。apt-get虽然没被弃用，但作为普通用户，还是应该首先使用apt\n\n> [centos7更换yum源](https://mirrors.cnnic.cn/help/centos/)\n\n\n### 配置jdk环境变量\n\n```shell\nvim /etc/profile\nexport JAVA_HOME=/usr/jdk1.8.0_121\nexport CLASSPATH=.:$JAVA_HOME/jre/lib/rt.jar:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar\nexport PATH=$PATH:$JAVA_HOME/bin\nsource /etc/profile # 使环境变量生效\necho $PATH # 查看环境变量\n```\n\nubuntu按照上面的步骤在/etc/environment再配置一遍，配置完成后\n```shell\nsource /etc/environment\n```\n\n### 安装iptables\n\n```shell\nsystemctl stop firewalld.service # 停止服务\nsystemctl mask firewalld.service # 屏蔽服务\nyum -y install iptables-services  # 安装iptables服务\nsystemctl enable iptables # 开机启动iptables\nsystemctl start iptables # 启动iptables\nservice iptables save # 保存防火墙规则\n```\n\n### centos7防火墙\n\n```shell\n# 启动\nsystemctl start firewalld.service\n# 关闭\nsystemctl stop firewalld.service\n# 查看状态\nsystemctl status firewalld.service\n# 开机禁用 \nsystemctl disable firewalld.service\n# 开机启用\nsystemctl enable firewalld.service\n# 查看系统服务列表\nsystemctl list-unit-files\n```\n\n### 查看开机启动的服务列表：\n\n```shell\n# 查看某个服务是否开机启动：\nsystemctl list-unit-files|grep enabled\n# 查看启动失败的服务列表：systemctl --failed\nsystemctl is-enabled firewalld.service\n\n# 端口开放\nfirewall-cmd --zone=public --add-port=80/tcp --permanent\n#（--permanent永久生效，没有此参数重启后失效）\n\n# 重新载入\nfirewall-cmd --reload查看\nfirewall-cmd --zone= public --query-port=80/tcp\n# 删除\nfirewall-cmd --zone= public --remove-port=80/tcp --permanent\n\n# 查看开放的端口\nfirewall-cmd --list-ports\n\n# ubuntu\n#查看防火墙当前状态\nsudo ufw status\n# 开启防火墙\nsudo ufw enable\n# 关闭防火墙\nsudo ufw disable\n# 查看防火墙版本\nsudo ufw version\n# 默认允许外部访问本机\nsudo ufw default allow\n# 默认拒绝外部访问主机\nsudo ufw default deny\n# 允许外部访问53端口\nsudo ufw allow 53\n# 拒绝外部访问53端口\nsudo ufw deny 53\n# 允许某个IP地址访问本机所有端口\nsudo ufw allow from 192.168.0.1\n# 重启防火墙\nsudo ufw reload\n```\n### 生成文件\n\n\\> 直接把内容生成到指定文件，会覆盖源文件中的内容，还有一种用途是直接生成一个空白文件，相当于touch命令\n```shell\necho 1 > a.txt 输出1\n# >>尾部追加，不会覆盖掉文件中原有的内容\necho 2 >> a.txt 输出1 2\n\ncat file1 >> file2 # 把file1的文档内容输入file2这个文档里\n```\n### 用户/用户组\n\n```shell\n# 查看linux所有用户\ncut -d : -f 1 /etc/passwd\n# 查看linux所有用户组\ncut -d : -f 1 /etc/group # -d : 以“：”为分割符进行分割 -f 1展示第一列\ngroups # 用户名：查看用户所在用户组\n```\n\n### centos7 定时运行任务脚本\n\n```shell\n# 安装crontab\nyum install vixie-cron\nyum install crontabs\n# 开启crontab服务\nservice crond start # 启动服务\nservice crond stop # 关闭服务\nservice crond restart # 重新启动服务\nservice crond reload # 又一次加载配置\n# 添加任务(两种方式)\n1. crontab -e  * * * * * /usr/local/a.sh。\ncrontab -l # 列出当前的全部调度任务\ncrontab -l -u jp #列出用户jp的全部调度任务\ncrontab -r # 删除全部任务调度工作\n2. # 直接编辑 vim /etc/crontab\n# 添加* * * * * root /usr/local/a.sh\n# 注意要使用绝对路径\n\n# 查看邮件\ncat /var/spool/mail/root\n# 查看日志\ncd /var/log\nls cron\n\n```\n> [Linux定时任务调度(crontab)，太实用了！](https://mp.weixin.qq.com/s/c91XWEQvr9Axcf0hjvuKgg)\n\n\n### 修改文件所属用户和用户组\n\n```shell\n# 修改a.txt文件所属用户为jay，所属用户组为fefjay\nchown jay:fefjay a.txt\n# 递归修改文件夹my及包含的所有子文件（夹）的所属用户和用户组\nchown -R jay:fefjay my\n```\n\n### curl命令\n\n```shell\ncurl -d 'login=emma＆password=123'-X POST URL\ncurl -d 'login=emma' -d 'password=123' -X POST URL\n-X # POST可以省略\n\ncurl -L -X POST URL -d 'id=3&pwd=jae_123'\ncurl -H \"Content-Type: application/json\" -X POST -d '{\"abc\":123,\"bcd\":\"nihao\"}' URL\n\n```\n\n> [Linux curl命令最全详解](https://blog.csdn.net/angle_chen123/article/details/120675472)\n> [curl其他参数介绍](https://www.cnblogs.com/fan-gx/p/12321351.html)\n> [堪称神器的命令行工具系列——curl](https://mp.weixin.qq.com/s/ryhphDFvx0ml9NrDUks2NA)\n\n\n### Linux系统服务\n\n```shell\n# 添加自定义系统服务的目录：\n/lib/systemd/system # lib/systemd/system真实地址是/usr/lib/system/system地址，\n/usr/lib/systemd/system/ # 软件包安装的单元\n/etc/systemd/system/ # 系统管理员安装的单元,优先级更高\n# 优先级为 /etc/systemd/system /run/systemd/system /lib/systemd/system\n# 如果同一选项三个地方都配置了，优先级高的会覆盖优先级低的。\n\n# 开机启动执行命令：编辑/etc/rc.d/rc.local\n# 添加要执行的命令或者脚本,并且赋予执行权限\nchmod +x /etc/rc.d/rc.local\n```\n\n### 复制目录\n```shell\ncp -r\n```\n\n### 查看隐藏目录\n```shell\nls -a\n```\n\n### 图形化页面卡死重启\n```shell\nkill -9 gnome-shell pid\n```\n\n### 查看centos版本\n```shell\ncat /etc/redhat-release\n```\n\n### 重命名文件\n\n```shell\n# 例:把a替换为xxx\nrename “a” “xxx” *.txt\n# 或者使用mv命令\n```\n\n### 端口\n\n```shell\nnetstat -antu # 可以查看所有tcp、udp端口开放情况\nnetstat -ntlp # 查看正在运行的端口(t代表tcp 加u查看udp)\nlsof -i: 9090 # 查看某一端口运行的程序\nnetstat -ntulp|grep # 端口号 查看指定端口被哪个进程占用的情况\nps -ef|grep abc # 查找abc进程\nps -aux # 显示所有进程\n# 发现A进程占用该端口号\nps -ef|grep A #查看pid\n\nkill 9 pid #杀掉进程\n```\n\n### 找到pid并kill的shell脚本\n\n```shell\n#!/bin/sh\n# #!/bin/bash是指此脚本使用/bin/bash来解释执行。其中，#!是一个特殊的表示符，其后，跟着解释此脚本的shell路径。命令文件所在的路径是/bin/sh或者/usr/bin/sh.bash只是shell的一种，还有很多其它shell，如：sh,csh,ksh,tcsh.除第一行外，脚本中所有以“#”开头的行都是注释。\n# 注意不要有空格，否则解释成命令\njar=abc.jar\n# ``等价于$()\npid=$(ps -ef | grep java | grep $jar |grep -v grep | awk '{print $2}')\nkill -9 $pid\necho \"$pid killed\"\nnohup command >/dev/null 2>&1 &\n```\n> [shell菜鸟教程](https://www.runoob.com/linux/linux-shell.html)\n\n### 设置静态ip后无法连接外网的问题\n因为动态ip会自动分配DNS，而静态ip需要手动配置DNS。centos7在/etc/sysconfig/network-scripts/ifcfg-ens33 写入DNS1=114.114.114.114。ubuntu在/etc/resolv.conf写入nameserver 114.114.114.114\n\n> [Linux配置IP地址](https://www.cnblogs.com/adforce/p/3363681.html)\n\n### 命令行更改MAC地址\n\n```shell\n/sbin/ifconfig eth0 down\n/sbin/ifconfig eth0 hw ether 00:50:56:94:16:a8\n/sbin/ifconfig eth0 up\nservice network restart\n```\n\n### 查看文件\n\n```shell\ncat h.txt | grep -v # \"hello\"过滤掉特定字符串,效率低，因为有管道\ngrep -v \"hello\" h.txt # 可以直接跟文件名，效率快\n\nhead -n k = head -n +k = head k\ntail -n k = tail -n -k = tail k # k为指定行数\n\nhead -n 3 = head -n +3 = head -3 # 显示文件前3行\n\ntail -n 3 = tail -n -3 = tail -3  # 显示文件最后3行\nhead -n -k # 其中-k的意义是除了最后k行的所有行\nhead -n -3 filename # 查看filename除了最后3行的所有行\ntail -n +k # 是从第k行开始，输出所有行\ntail -n +3 # 从第三行开始输出所有行\ntail -f finename # 实时跟踪文件，如果文件不存在，则终止\ntail -F filename # 如果文件不存在，会继续尝试\n```\n\n### 源码安装配置(configure)、编译(make)、安装(make install)\n\n```shell\n./configure --prefix=/usr/local/test\n# 编译出错时，清除编译生成的文件\nmake distclean\n# 编译安装到指定目录下\nmake PREFIX=/usr/local/redis install\n# 卸载\nmake uninstall\n```\n\n### 设置keepalived服务开机启动\n\n```shell\nchkconfig keepalived on\nchkconfig --add name\n          --del name\n          --list\n```\n\n### Ubuntu图形界面允许root登陆\n\n```shell\n# 设置root密码\nsudo passwd root  # 终端会先验证密码 然后在设置root密码\nvi /usr/share/lightdm/lightdm.conf.d/50-ubuntu.conf\n# 增加两行  greeter-show-manual-login=true all-guest=false#不允许guest用户登陆\ncd /etc/pam.d  # 编辑gdm-autologin和gdm-password文件 注释掉auth required pam_succeed_if.so user != root quiet_success\nvi /root/.profile # 将mesg n || true 修改为 tty -s && mesg n || true\nreboot\n\n```\n\n> [解决Ubuntu的root账号无法登录SSH问题](https://www.cnblogs.com/yixius/articles/6971054.html)\n\n\n### 查看指定目录大小\n```shell\ndu -h --max-depth=1 /usr\n```\n\n### 查看磁盘空间\n```shell\ndf -h\n```\n\n### 查看内存\n```shell\nfree -h\n\ntop -n 2 # 表示更新两次后终止更新显示并退出\ntop -d 3 # 表示更新周期为3秒\n# top命令进入展示信息后按大写M表示按照内存大小排序展示 按大写P表示按照CPU使用率进行排序\n```\n\n### 读、写、运行\n\n三项权限可以用数字表示，r=4,w=2,x=1 rw-r--r--用数字表示成644\n```shell\nchmod 754 filename\n# 将filename文件的读写运行权限赋予文件所有者，把读和运行的权限赋予群组用户，把读的权限赋予其他用户。\n# chmod +x是将文件状态改为可执行，而chmod 777是改变文件读写权限。\n```\n\n### centos7卸载openjdk\n\n```shell\nrpm -qa|grep jdk  # 查看已有的openjdk -q(query) -a(all)\nrpm -ev --nodeps (上条命令的查询结果) #卸载\n\nubuntu \napt-get remove openjdk*\n\n```\n### rpm参数\n\n```yaml\n-a　查询所有套件。\n-b<完成阶段><套件档>+或-t<完成阶段><套件档>+　设置包装套件的完成阶段，并指定套件档的文件名称。\n-c　只列出组态配置文件，本参数需配合\"-l\"参数使用。\n-d　只列出文本文件，本参数需配合\"-l\"参数使用。\n-e<套件档>或--erase<套件档>　删除指定的套件。\n-f<文件>+　查询拥有指定文件的套件。\n-h或--hash　套件安装时列出标记。\n-i　显示套件的相关信息。\n-i<套件档>或--install<套件档>　安装指定的套件档。\n-l　显示套件的文件列表。\n-p<套件档>+　查询指定的RPM套件档。\n-q　使用询问模式，当遇到任何问题时，rpm指令会先询问用户。\n-R　显示套件的关联性信息。\n-s　显示文件状态，本参数需配合\"-l\"参数使用。\n-U<套件档>或--upgrade<套件档>升级指定的套件档。\n-v　显示指令执行过程。\n-vv　详细显示指令执行过程，便于排错。\n-addsign<套件档>+　在指定的套件里加上新的签名认证。\n--allfiles　安装所有文件。\n--allmatches　删除符合指定的套件所包含的文件。\n--badreloc　发生错误时，重新配置文件。\n--buildroot<根目录>　设置产生套件时，欲当作根目录的目录。\n--changelog　显示套件的更改记录。\n--checksig<套件档>+　检验该套件的签名认证。\n--clean　完成套件的包装后，删除包装过程中所建立的目录。\n--dbpath<数据库目录>　设置欲存放RPM数据库的目录。\n--dump　显示每个文件的验证信息。本参数需配合\"-l\"参数使用。\n--excludedocs　安装套件时，不要安装文件。\n--excludepath<排除目录>　忽略在指定目录里的所有文件。\n--force　强行置换套件或文件。\n--ftpproxy<主机名称或IP地址>　指定FTP代理服务器。\n--ftpport<通信端口>　设置FTP服务器或代理服务器使用的通信端口。\n--help　在线帮助。\n--httpproxy<主机名称或IP地址>　指定HTTP代理服务器。\n--httpport<通信端口>　设置HTTP服务器或代理服务器使用的通信端口。\n--ignorearch　不验证套件档的结构正确性。\n--ignoreos　不验证套件档的结构正确性。\n--ignoresize　安装前不检查磁盘空间是否足够。\n--includedocs　安装套件时，一并安装文件。\n--initdb　确认有正确的数据库可以使用。\n--justdb　更新数据库，当不变动任何文件。\n--nobulid　不执行任何完成阶段。\n--nodeps　不验证套件档的相互关联性。\n--nofiles　不验证文件的属性。\n--nogpg　略过所有GPG的签名认证。\n--nomd5　不使用MD5编码演算确认文件的大小与正确性。\n--nopgp　略过所有PGP的签名认证。\n--noorder　不重新编排套件的安装顺序，以便满足其彼此间的关联性。\n--noscripts　不执行任何安装Script文件。\n--notriggers　不执行该套件包装内的任何Script文件。\n--oldpackage　升级成旧版本的套件。\n--percent　安装套件时显示完成度百分比。\n--pipe<执行指令>　建立管道，把输出结果转为该执行指令的输入数据。\n--prefix<目的目录>　若重新配置文件，就把文件放到指定的目录下。\n--provides　查询该套件所提供的兼容度。\n--queryformat<档头格式>　设置档头的表示方式。\n--querytags　列出可用于档头格式的标签。\n--rcfile<配置文件>　使用指定的配置文件。\n--rebulid<套件档>　安装原始代码套件，重新产生二进制文件的套件。\n--rebuliddb　以现有的数据库为主，重建一份数据库。\n--recompile<套件档>　此参数的效果和指定\"--rebulid\"参数类似，当不产生套件档。\n--relocate<原目录>=<新目录>　把本来会放到原目录下的文件改放到新目录。\n--replacefiles　强行置换文件。\n--replacepkgs　强行置换套件。\n--requires　查询该套件所需要的兼容度。\n--resing<套件档>+　删除现有认证，重新产生签名认证。\n--rmsource　完成套件的包装后，删除原始代码。\n--rmsource<文件>　删除原始代码和指定的文件。\n--root<根目录>　设置欲当作根目录的目录。\n--scripts　列出安装套件的Script的变量。\n--setperms　设置文件的权限。\n--setugids　设置文件的拥有者和所属群组。\n--short-circuit　直接略过指定完成阶段的步骤。\n--sign　产生PGP或GPG的签名认证。\n--target=<安装平台>+　设置产生的套件的安装平台。\n--test　仅作测试，并不真的安装套件。\n--timecheck<检查秒数>　设置检查时间的计时秒数。\n--triggeredby<套件档>　查询该套件的包装者。\n--triggers　展示套件档内的包装Script。\n--verify　此参数的效果和指定\"-q\"参数相同。\n--version　显示版本信息。\n--whatprovides<功能特性>　查询该套件对指定的功能特性所提供的兼容度。\n--whatrequires<功能特性>　查询该套件对指定的功能特性所需要的兼容度。\n\n```\n\n### 从服务器复制文件到本地\n\n```shell\n# 从服务器复制文件到本地\nscp root@192.168.1.100:/data/test.txt /home/myfile/\n# 从服务器复制文件夹到本地\nscp -r root@192.168.1.100:/data/ /home/myfile/\n# 从本地复制文件到服务器\nscp /home/myfile/test.txt root@192.168.1.100:/data/\n# 从本地复制文件夹到服务器\nscp -r /home/myfile/ root@192.168.1.100:/data/\n```\n\n### ssh双向免密登录服务器A、B\n\n```shell\n# 生成密钥\nssh-keygen -t rsa\n# 生成密钥的位置\ncd /root/.ssh/\n# 此命令在A机器执行，目的将A的公钥发送至B机器\nscp id_rsa.pub root@BIP地址:/root/.ssh/id_rsa_A.pub\n# 此命令在B机器执行，目的将B的公钥发送至A机器\nscp id_rsa.pub root@AIP地址:/root/.ssh/id_rsa_B.pub\n\n# 如果发送文件夹 使用scp -r /test root@B:/root/test\n\n# 查看远程复制是否成功 写入密钥\ncat id_rsa_A(或者B).pub >> authorized_keys\n# 如果出现agent admitted failure to sign using the key\nssh-add ~/.ssh/id_rsa \n# ssh A和ssh B测试ssh本机和远程是否已经免密登录（第一次免密登录需要输入密码，以后不需要）\nssh登陆 ssh root@ip\n```\n> [科普：什么是SSH？](https://mp.weixin.qq.com/s/1e4aGp_cx0E_qCHVuS3GMg)\n\n\n### centos7/ubuntu通用\n\n```shell\n# 默认命令行\nsystemctl set-default multi-user.target  （init 3）\n\n# 默认图形页面\nsystemctl set-default graphical.target  (init 5)\n```\n\n### 查看系统内核 uname -r\n\n```shell\n# 全部内核\nrpm -qa | grep kernel\nyum list installed | grep kernel\n#删除多余内核\nyum remove kernel-\n```\n\n### 测试端口是否开通\n\n```shell\nssh -v -p port username@ip\n# -v 调试模式(会打印日志) -p 指定端口 username可以随意\n\n# 使用 telnet 命令\ntelnet ip 端口\n\n# 使用nc命令\nnc -vu ip 端口\n# -v 输出交互或出错信息，新手调试时尤为有用,-u指定nc使用UDP协议，默认为TCP\n```\n\n### arthas调试常用命令\n\n```\ncls 清空当前屏幕内容\n\ntrace 类名 方法名  '#cost > 10'\n查看某个类的某个方法执行多长时间 (加后面参数只会展示耗时大于10ms的调用路径)\n\nsc 类名 查看该类是否被jvm加载\n\nsm 类名 查看该类的方法信息\n\njad 类名 反编译\n\nwatch 类名 方法名 \"{params,returnObj,throwExp}\" -e -b  -x 2\n观察方法出参，返回值，-b在调用之前观察 -e在方法异常之后观察 -x指定输出结果的遍历深度 默认为1\n\nthread 查看线程\n\nredefine class文件(带class后缀名):热部署，修改jvm的class并不会修改本地的class文件 服务重启后失效 注意class新增属性、方法且方法正在运行的时候会热部署失败\n\n退出arthas\nquit或exit 退出当前连接，完全退出使用stop(所有客户端连接都会退出)\n\nlogger --name ROOT --level debug动态修改日志级别\n```\n\n> [arthas官方文档](https://arthas.aliyun.com/doc/quick-start.html)\n> [学会arthas，让你3年经验掌握5年功力](https://mp.weixin.qq.com/s/7RMjfIYlmskMsifnhIzxgA)\n\n\n### Linux交换空间(swap space)\n\n交换空间是磁盘上的一块区域，可以是一个分区，也可以是一个文件，或者是他们的组合。简单点说，当系统物理内存吃紧时，Linux会将内存中不常访问的数据保存到swap上，这样系统就有更多的物理内存为各个进程服务，而当系统需要访问swap上存储的内容时，再将swap上的数据加载到内存中，这就是我们常说的swap out和swap in\n\n**设置交换空间**\n```shell\n# 1. 在Home目录创建一个大小为16G的swap文件，块大小为1MByte，总共1K个块，也就是总共1GB\ndd if=/dev/zero of=~/swapfile bs=1M count=1k\n# 2. 格式化新增的swap文件\nmkswap ～/swapfile\n# 3. 启动新增的swap文件\nswapon ～/swapfile\n# 4. 查看swap空间大小，发现增加了16G\nfree\n# 5. 关闭新增的swap文件\nswapoff ～/swapfile\n# 6. 设置开机自启动，开机后自动启动新增的swap文件，在/etc/fstab中新增如下命令\n/swapfile none swap sw 0 3\n```\n\n### 挂载\n```shell\n# 一、查看磁盘挂载目录\ndf -h（查看分区情况及数据盘名称）\nmkdir /usr（如果没有就创建，否则此步跳过）\n# 二、卸载磁盘\numount /data（卸载硬盘已挂载的data目录）\n# 三、挂载到新目录\nmount /dev/vdb1 /usr（挂载到usr目录）\n# 四、修改 /etc/fstab\nvi /etc/fstab （编辑fstab文件修改或添加，使重启后可以自动挂载）\n/dev/vdb1 /usr ext3 auto 0 0\n# 五、重新挂载所有分区\nmount -a\n# 六、验证\ndf -h /usr/\n# 七、重启服务器\nreboot\n```\n需要注意的是：在实际操作过程中，挂载的目录会覆盖掉原目录的文件信息。可以先进行旧目录备份，挂载完成后在恢复数据。\n\n\n### source filename与sh filename及./filename执行脚本的区别\n\n1. 当shell脚本具有可执行权限时，用`sh filename`与`./filename`执行脚本是没有区别的。`./filename`是因为当前目录没有在PATH中，所有”.”是用来表示当前目录的。\n2. `sh filename`重新建立⼀个子shell，在子shell中执行脚本里面的语句，该子shell继承父shell的环境变量，但子shell新建的、改变的变量不会被带回父shell，除非使用export。\n3. `source filename`这个命令其实只是简单地读取脚本里面的语句依次在当前shell里面执行，没有建立新的子shell。那么脚本里面所有新建、改变变量的语句都会保存在当前shell里面\n\n\n## 环境变量\n\n### 环境变量文件\n\n- /etc/profile：此文件为系统的每个用户设置环境信息，当用户第一次登录时，该文件被执行，并从/etc/profile.d目录的配置文件中收集shell的设置\n- /etc/bashrc：为每一个运行bash shell的用户执行此文件，当bash shell被打开时，该文件被读取也就是说，当用户shell执行了bash时，运行这个文件；\n- ~/.bash_profile：每个用户都可使用该文件输入专用于自己使用的shell信息，当用户登录时，该文件仅仅执行一次！默认情况下，它设置一些环境变量，执行用户的.bashrc文件\n- ~/.bashrc：该文件包含用于你的bash shell的bash信息，当登录时以及每次打开新的shell时，该文件被读取。该文件存储的是专属于个人bash shell的信息，当登录时以及每次打开一个新的shell时，执行这个文件。在这个文件里可以自定义用户专属的个人信息。\n- \\~/.bash_logout：当每次退出系统(退出bash shell)时，执行该文件；另外，/etc/profile中设定的变量(全局)的可以作用于任何用户，而\\~/.bashrc等中设定的变量(局部)只能继承/etc/profile中的变量，他们是“父子”关系；\n\n### Ubuntu下bash的几个初始化文件\n- /etc/profile 全局(公有)配置，不管是哪个用户，登录时都会读取该文件；\n- /etc/bashrc Ubuntu下没有此文件，与之对应的是/etc/bash.bashrc，它也是全局的；bash执行时，不管是何种方式，都会读取此文件；\n- \\~/.profile 若bash是以login方式执行时，读取\\~/.bash_profile，若它不存在，则读取\\~/.bash_login，若前两者不存在，读取\\~/.profile；另外，图形模式登录时，此文件将被读取，即使存在\\~/.bash_profile和\\~/.bash_login；\n- \\~/.bash_login 若bash是以login方式执行时，读取\\~/.bash_profile，若它不存在，则读取\\~/.bash_login，若前两者都不存在，则读取\\~/.profile；\n- \\~/.bash_profile Ubuntu默认没有此文件，可新建。只有bash是以login形式执行时，才会读取此文件。通常该配置文件还会配置成去读取\\~/.bashrc；\n- \\~/.bashrc当bash是以non-login形式执行时，读取此文件。若是以login形式执行，则不会读取此文件；\n- \\~/.bash_logout注销时，且是login形式，此文件才会读取。也就是说，在文本模式注销时，此文件会被读取，图形模式注销时，此文件不会被读取。\n- /etc/environment系统的环境变量，系统应用程序的执行与用户环境可以是无关的，但与系统环境是相关的\n1. 在登录时,操作系统定制用户环境时使用的第一个文件就是/etc/profile,此文件为系统的每个用户设置环境信息,当用户第一次登录时,该文件被执行。\n2. 在登录时操作系统使用的第二个文件是/etc/environment，系统在读取你自己的profile前,设置环境文件的环境变量。\n3. 在登录时用到的第三个文件是.profile文件,每个用户都可使用该文件输入专用于自己使用的shell信息,,该文件仅仅执行一次!默认情况下,他设置一些环境变量,执行用户的.bashrc文件。/etc/bashrc:为每一个运行bash shell的用户执行此文件。当bash shell被打开时,该文件被读取。\n4. /etc/environment是设置整个系统的环境，而/etc/profile是设置所有用户的环境，前者与登录用户无关，后者与登录用户有关。\n先执行/etc/enviroment，后执行/etc/profile\n\n> [Linux环境变量配置的6种方法](https://mp.weixin.qq.com/s/D32FE4CS2pT89czVUv0SOg)\n\n## 相关文章\n\n- [Linux基础知识总结](https://javaguide.cn/cs-basics/operating-system/linux-intro.html)\n- [程序员必备的150个Linux命令！](https://mp.weixin.qq.com/s/wejhjmbtS16AdlDo0JiLMA)\n- [新人必备的Linux命令！](https://mp.weixin.qq.com/s/TvUcd7mLkHxD_botoo7BQg)\n- [1000+常用的Linux命令来袭](https://mp.weixin.qq.com/s/43JCaDehv0nbzDKbe0S8Kg)\n- [作为一名Java开发人员，应该从多大程度上掌握Linux？](https://www.zhihu.com/question/319076999/answer/672319588?utm_source=wechat_timeline&utm_medium=social&utm_oi=1040923520439672832&from=timeline)\n- [30个实例详解TOP命令](https://mp.weixin.qq.com/s/2FDa8fOm6x4-1MP9l9N7vA)\n- [ping命令的7大用法,看完秒变大神！](https://mp.weixin.qq.com/s/liiz58PC1JlfPaEW81wUpg)\n- [听说你ping用的很6,给我图解一下ping的工作原理](https://mp.weixin.qq.com/s/4pPdcSak3YLyqiz4Ns8htQ)\n- [ping命令还能这么玩？](https://mp.weixin.qq.com/s/fLY3JH5_4_h4vABV7B-4uw)\n- [后端线上问题排查常用命令收藏](https://mp.weixin.qq.com/s?__biz=Mzg2MDYzODI5Nw==&mid=2247494347&idx=1&sn=2f6bfe4a5716d4b0aba6d8b5348f7033&source=41#wechat_redirect)\n- [Java开发常用的Linux命令知识积累](https://mp.weixin.qq.com/s/jlmPjqFTfP4JNmbhpGoHsA)\n- [这篇Linux总结的真棒！](https://mp.weixin.qq.com/s/F_0kiOWgQS4BINx2t2Rqtw)\n- [Linux远程桌面管理工具！功能真心强大](https://mp.weixin.qq.com/s/UT3RYrUGP2E5JQbf1Z8c8w)\n- [在Linux上保护SSH服务器连接的8种方法](https://mp.weixin.qq.com/s/kxvc95RJoSKqcXwwmyrPKg)\n- [发现谁用kill -9关闭程序就开除！](https://mp.weixin.qq.com/s/zmfCC083VBrA8P6uWHL2DQ)\n- [万字详解Linux常用指令](https://mp.weixin.qq.com/s/hXDAZD0LUJ9g85t6fkBR-Q)\n- [最强Linux命令总结（特别推荐版）](https://mp.weixin.qq.com/s/KN9OPZMhmuA6rhx5YTjwDA)\n- [Linux命令大全搜索工具，内容包含Linux命令手册、详解、学习、搜集](https://github.com/jaywcjlove/linux-command)","tags":["计算机"]},{"title":"启动命令","slug":"启动命令","url":"/blog/posts/e3cef6238a63/","content":"\n### redis\n\n```shell\n./redis-server /usr/local/redis/redis.conf\n./redis-cli -h 127.0.0.1 -p 6379 -a 123456\n./redis-cli -h 127.0.0.1 -p 6379 -a 123456 shutdown # 关闭redis\n```\n\n### nginx\n\n```shell\n/usr/local/nginx/sbin/nginx -s reload\n/usr/local/nginx/sbin/nginx -s quit # (正常退出，保存相关信息)\n/usr/local/nginx/sbin/nginx -s stop # (立即退出，不保存相关信息)\n\n# windows\nstart nginx(.exe)\n```\n\n### zookeeper\n\n```shell\n./zkServer.sh start\n./zkCli.sh -server 127.0.0.1:2181\n```\n\n### kafka\n\n```shell\n./kafka-server-start.sh ../config/server.properties 1>/dev/null 2>&1 &\n./kafka-server-stop.sh\n./zookeeper-server-start.sh ../conf/zookeeper.properties\n./zookeeper-server-stop.sh\n```\n\n### nexus\n\n```shell\n./bin/nexus start\n# 访问地址:http://192.168.236.131:8081/nexus 默认账号:admin/admin123\n```\n\n### cas_server\n\n```shell\ncd /usr/local/cas/cas-overlay-template\n./build.sh run\n# 清除target文件夹\n./build.sh clean\n# 其他命令可以查看build.sh里面的脚本内容,控制台出现READY后访问ip:port/cas/login 即可登陆,默认账号：casuser:Mellon\n```\n\n### nacos\n\n```shell\n# 单机模式启动\n./startup.sh -m standalone\n# 集群启动使用外置数据源,需要配置cluster.conf且节点数量要>=3\n./startup.sh\n# 集群启动使用内置数据源\nsh startup.sh -p embedded\n# 默认账号：nacos:nacos\n```\n\n### arthas\n\n```shell\njava -jar arthas-boot.jar [java pid]\n# web console：127.0.0.1:8563 127.0.0.1:3658\n```\n\n### consul\n\n```shell\n# -dev表示开发模式运行，只有本地可以访问，另外还有-server表示服务模式运行\nconsul agent -dev\n# 可以使用ip访问\n./consul agent -dev -client 0.0.0.0 -ui\n# 访问localhost:8500\n```\n\n### weblogic\n\n```shell\n# 执行./startWebLogic.sh\n# 访问http://ip:7001/console\n/home/test/Oracle/Middleware/Oracle_Home/user_projects/domains/base_domain/bin\n```\n\n### shell脚本\n\n> [9个实用shell脚本，建议收藏](https://mp.weixin.qq.com/s/KhCmbC5UPuRgmqGX-uOJCw)\n> [Shell编程基础知识总结](https://javaguide.cn/cs-basics/operating-system/shell-intro.html)\n>\n> $#是传给脚本的参数个数\n> $0是脚本本身的名字\n> $1是传递给该shell脚本的第一个参数\n> $2是传递给该shell脚本的第二个参数\n> $@是传给脚本的所有参数的列表\n> $\\*是以一个单字符串显示所有向脚本传递的参数，与位置变量不同，参数可超过9个\n> $$是脚本运行的当前进程ID号\n> $?是显示最后命令的退出状态，0表示没有错误，其他表示有错误\n\n#### 启动jar\n```shell\n#!/bin.sh\n# 注意不要有空格\ngatewayjar=gateway.jar\ngatewaypid=$(ps -ef | grep java | grep $gatewayjar | grep -v 'grep'| awk '{print $2}')\n# 两个命令在一行的话使用分号断开\nif [ $gatewaypid ]; then\n\tkill -9 $gatewaypid\n\techo \"进程$gatewaypid killed\"\nfi\nnohup java -jar gateway.jar >/dev/null 2>&1 &\n# 判断上一个命令是否执行成功 $?返回上一个命令执行状态 0成功1失败 -ne表示不等于\nif [ $? -ne 0 ]; then\n\techo \"start failed\"\nelse\n\techo \"start success\"\nfi\n\n# ...\n```\n\n#### kill所有java进程\n```shell\n#!/bin.sh\n# 查找进程们（注意可能返回多个）\npIdArr=$(ps -ef | grep java | grep -v \"grep\" | awk '{print $2}')\n# 遍历kill进程们\nfor pId in ${pIdArr}; do\n  kill -9 ${pId}\n  echo \"进程：${pId} 被kill\"\ndone\n```\n#### ruoyi脚本\n```shell\n#!/bin/sh\n# ./ry.sh start:启动 stop:停止 restart:重启 status:状态\nAppName=ruoyi-admin.jar\n\n# JVM参数\nJVM_OPTS=\"-Dname=$AppName  -Duser.timezone=Asia/Shanghai -Xms512m -Xmx1024m -XX:MetaspaceSize=128m -XX:MaxMetaspaceSize=512m -XX:+HeapDumpOnOutOfMemoryError -XX:+PrintGCDateStamps  -XX:+PrintGCDetails -XX:NewRatio=1 -XX:SurvivorRatio=30 -XX:+UseParallelGC -XX:+UseParallelOldGC\"\nAPP_HOME=`pwd`\nLOG_PATH=$APP_HOME/logs/$AppName.log\n# start stop restart status\nif [ \"$1\" = \"\" ];\nthen\n\t# echo -e 将转义后的内容输出到屏幕上\n    echo -e \"\\033[0;31m 未输入操作名 \\033[0m  \\033[0;34m {start|stop|restart|status} \\033[0m\"\n    exit 1\nfi\n\nif [ \"$AppName\" = \"\" ];\nthen\n    echo -e \"\\033[0;31m 未输入应用名 \\033[0m\"\n    exit 1\nfi\n\nfunction start()\n{\n    PID=`ps -ef |grep java|grep $AppName|grep -v grep|awk '{print $2}'`\n\n\tif [ x\"$PID\" != x\"\" ]; then\n\t    echo \"$AppName is running...\"\n\telse\n\t\tnohup java $JVM_OPTS -jar $AppName > /dev/null 2>&1 &\n\t\techo \"Start $AppName success...\"\n\tfi\n}\n\nfunction stop()\n{\n    echo \"Stop $AppName\"\n\n\tPID=\"\"\n\tquery(){\n\t\tPID=`ps -ef |grep java|grep $AppName|grep -v grep|awk '{print $2}'`\n\t}\n\n\tquery\n\tif [ x\"$PID\" != x\"\" ]; then\n\t\tkill -TERM $PID\n\t\techo \"$AppName (pid:$PID) exiting...\"\n\t\twhile [ x\"$PID\" != x\"\" ]\n\t\tdo\n\t\t\tsleep 1\n\t\t\tquery\n\t\tdone\n\t\techo \"$AppName exited.\"\n\telse\n\t\techo \"$AppName already stopped.\"\n\tfi\n}\n\nfunction restart()\n{\n    stop\n    sleep 2\n    start\n}\n\nfunction status()\n{\n    PID=`ps -ef |grep java|grep $AppName|grep -v grep|wc -l`\n    if [ $PID != 0 ];then\n        echo \"$AppName is running...\"\n    else\n        echo \"$AppName is not running...\"\n    fi\n}\n\ncase $1 in\n    start)\n    start;;\n    stop)\n    stop;;\n    restart)\n    restart;;\n    status)\n    status;;\n    *)\n\nesac\n\n```\n\n","tags":["随笔"]},{"title":"Oracle相关及常用函数","slug":"Oracle相关及常用函数","url":"/blog/posts/1f755d9b6744/","content":"\n## Sys和System用户区别\n\n- **Sys**: 拥有DBA、SysDBA、Sysoper（系统操作员）角色或权限，是Oracle权限最高的用户，只能以SysDBA或Sysoper登录，不能以Normal形式登录。\n- **System**: 拥有DBA、Sysdba权限或角色，可以以普通用户的身份登录。\n\n## Sysdba、Sysoper、DBA区别\n\n- Sysdba用户: 可以改变字符集、创建删除数据库、登录之后用户是SYS（shutdown、startup）\n- Sysoper: 用户不可改变字符集、不能创、删数据库、登陆之后用户是PUBLIC（shutdown、startup）\n- DBA用户: 只有在启动数据库后才能执行各种管理工作。\n- Sysdba> Sysoper>普通的DBA\n\n## Oracle中的角色\n\n1. CONNECT\n2. RESOURCE\n3. DBA\n4. EXP_FULL_DATABASE\n5. IMP_FULL_DATABASE\n6. DELETE_CATALOG_ROLE\n7. EXECUTE_CATALOG_ROLE\n8. SELECT_CATALOG_ROLE\n\n- **CONNECT角色**：--是授予最终用户的典型权利，最基本的\n\n```sql\nALTER SESSION --修改会话\nCREATE CLUSTER --建立聚簇\nCREATE DATABASE LINK --建立数据库链接\nCREATE SEQUENCE --建立序列\nCREATE SESSION --建立会话\nCREATE SYNONYM --建立同义词\nCREATE VIEW --建立视图www_bitscn_com中国.网管联盟\n```\n\n- **RESOURCE角色**： --是授予开发人员的\n\n```sql\nCREATE CLUSTER --建立聚簇\nCREATE PROCEDURE --建立过程\nCREATE SEQUENCE --建立序列\nCREATE TABLE --建表\nCREATE TRIGGER --建立触发器\nCREATE TYPE --建立类型\n```\n\n- **DBA角色**：拥有系统所有系统级权限（系统管理员）\n\n- **IMP_FULL_DATABASE角色、EXP_FULL_DATABASE角色**：\n\n```sql\nBACKUP ANY TABLE --备份任何表\nEXECUTE ANY PROCEDURE --执行任何操作\nSELECT ANY TABLE --查询任何表\n```\n\n- **SELECT_CATALOG_ROLE角色**：具有从数据字典查询的权利，(尤其是此处数据字典：就是数据库运行时的各种信息参考：Oracle 11g体系结构--数据字典)\n\n- **EXECUTE_CATALOG_ROLE角色**：具有从数据字典中执行部分过程和函数的权利。\n\n\n## 分区表\n\n当表中的数据量不断增大，查询数据的速度就会变慢，应用程序的性能就会下降，这时就应该考虑对表进行分区。表进行分区后，逻辑上表仍然是一张完整的表，只是将表中的数据在物理上存放到多个表空间(物理文件上)，这样查询数据时，不至于每次都扫描整张表。\n\n## PL/SQL /Oracle乱码问题解决方案\n\n```sql\nSelect userenv(‘language’) from dual; -- 查看服务器端编码\n\nselect * from V$NLS_PARAMETERS；-- 查看NLS_LANGUAGE的值与第一个的查询结果是否一致，假如不一致需要设置环境变量，变量名：NLS_LANG 变量值：第1个查到的值 重启PL/SQL(假如在乱码之前已经插入数据，那么配置环境变量后依然乱码，需要删除数据重新导入)\n```\n\n## Oracle的操作\n\n**cmd进入oracle** \n\n```sql\nsqlplus 账户名/密码 as 角色名 --（sys用户必须带as sysdba）例:sqlplus sys/admin as sysdba\n\nexit --退出\n```\n\n**用户账号相关操作**\n\n```sql\n-- 创建用户\ncreate user xxx identified by xxx;\n-- 授权\ngrant create session, connect, resource to xxx;\n-- 删除用户 加cascade可以一同删除用户数据\ndrop user username cascade;\n-- 解锁登陆账号\nalter user scott account unlock;\n-- 冻结登陆账号\nalter user dbaName account lock;\n-- 修改登录账号密码\nalter user dbaName identified by \"password\";\n\n-- 查看所有用户相关信息\nSELECT * FROM DBA_USERS;-- 查询DBA用户\nSELECT * FROM ALL_USERS;-- 查询所有用户\nSELECT * FROM USER_USERS;-- 查询系统用户\n\n-- 查看用户系统权限\nSELECT * FROM DBA_SYS_PRIVS;\nSELECT * FROM USER_SYS_PRIVS;\n\n-- 查看用户对象或角色权限\nSELECT * FROM DBA_TAB_PRIVS;\nSELECT * FROM ALL_TAB_PRIVS;\nSELECT * FROM USER_TAB_PRIVS;\n\n-- 查看所有角色\nSELECT * FROM DBA_ROLES;\n\n-- 查看用户或角色所拥有的角色\nSELECT * FROM DBA_ROLE_PRIVS;\nSELECT * FROM USER_ROLE_PRIVS;\nselect * from role_sys_privs\n```\n\n**表空间相关**\n\n```sql\nSelect * FROM Dba_Tablespaces; -- 查看所有的表空间\nSelect * FROM DBA_DATA_FILES; -- 查看所有的表空间以及对应的地址\n\n-- 创建临时表空间\ncreate temporary tablespace user_temp\ntempfile 'D:\\APP\\ADMINISTRATOR\\ORADATA\\ORCL\\user_temp.dbf'\nsize 50m  --初始空间50m\nautoextend on -- on为表空间自动扩展\nnext 50m maxsize 20480m -- 每次50m最大2048m\nextent management local; -- 本地管理表空间\n\n-- 创建数据表空间\ncreate tablespace user_data\nlogging\ndatafile 'D:\\APP\\ADMINISTRATOR\\ORADATA\\ORCL\\user_data.dbf'\nsize 50m\nautoextend on\nnext 50m maxsize 20480m\nextent management local;\n\n-- 创建用户指定表空间\ncreate user 用户名 identified by 密码 default tablespace 表空间名；\n```\n\n## Oracle相关函数\n\n### wm_concat()行转列\n\nwm_concat()函数是oracle中独有的,mysql中有一个group_concat()函数。这两个函数的作用是相同的，它们的功能是：实现行转列功能，即将查询出的某一列值使用逗号进行隔开拼接，成为一条数据\n\n```sql\n-- 根据年龄获取学生的分数\nselect age,to_char(wm_concat(name)) as name,to_char(wm_concat(score)) as score from student t group by age;\n-- age  name     score\n-- 18  张三,李四  81,82\n-- 19  王五,赵六  67,90\n```\n\n### decode\n\n```sql\ndecode(c1,c2,c3,c4,c5,c6,c7....,C2x,C2x+1 ,C2x+2)\n```\n从c1之后开始,每两个参数看做是一组数,拿每组数的第一个参数和c1比较,如果相同则返回第二个参数:比如,如果c2==c1则return c3如果该组数的第一个参数和c1不相同,则比较下一组:比如,如果c2<>C1继续判断C4==C1? 相同return c5\n\n```sql\ndecode(type,'a','11','b','12','c','13','d','14','e','15',type)\n```\n\n### 树形查询\n\n```sql\nselect area_code from (select * from tableA where isvalid=1) start with area_code = 5002 connect by  prior parent_area_code = area_code;\n\n-- 父子关系查询（start with connect by prior）\nselect * from tabname t where 条件 start with t.org_parent_code='10000008' connect by t.org_code = prior t.org_parent_code\n```\n\n### 数值函数\n\n- mod(x,y) 取模 求余数\n- nvl(x,y) 如果x的值为空,则返回y\n```sql\nselect ename,sal,comm,sal+nvl(comm,0) from emp;\n-- 数值列可以直接做加减乘除运算,如果数值列值为null则加减乘除后也是null需要使用nvl函数去空\n```\n\n### 精确指定位数的函数\n\n- round(c1,c2) 能够四舍五入\n- trunc(c1,c2) 直接舍去\n  \n```sql\nc2>0 : c2表示有多少位小数\nselect round(21.23512,2) from dual;   --  21.24\nselect trunc(21.23512,2) from dual;   --  21.23\n```\nc20 : c2表示小数点向左精确c2位\n```sql\nselect round(2163.512,-2) from dual;  --  2200\nselect trunc(2163.512,-2) from dual;  --  2100\n```\nc2如果不存在,表示只保留整数部分\n```sql\nselect round(2163.512,0) from dual;  --  2164\nselect round(2163.512) from dual;    -- 2164\n```\n\n### 字符函数\n\n- length() 求字符串长度\n\n```sql\n-- 查询姓名长度为5个字符的员工姓名\nselect ename,length(ename) from emp where length(ename)=5;\n```\n\n- lower() 全部转为小写\n- upper() 全部转为大写\n- initcap() 单词首字母大写其他字母小写\n```sql\nselect ename,lower(ename),upper(ename),initcap(ename) from emp\n```\n\n- 截取字符串:substr(c1,c2[,c3]) \nc1: 原字符串\nc2: 从哪个位置开始截取\nc3: 截取长度(默认截取到最后)\n\n```sql\n-- 查询员工姓名,截取员工姓名的第一个字符,再截取姓名的最后一个字符\nselect ename,sal,substr(ename,1,1),substr(ename,length(ename)) from emp;\n-- 查询员工姓名,截取员工姓名中最中间的一个字符(偶数个截取后一位)\nselect ename, substr(ename,trunc(length(ename)/2)+1,1) from emp;\n```\n\n- 索引字符串:instr(c1,c2,c3,c4)\nc1: 原字符串\nc2: 要查找的字符串\nc3: 从哪个位置开始查找,默认值1\nc4: 第几次出现 默认值1\n\n```sql\n-- 查询员工姓名中带有E字符的员工\nselect * from emp where ename like '%E%';\nselect * from emp where instr(ename,'E')<>0;\n\nselect instr('sanhaoxuesheng','aow') from dual;\n-- instr能够用来替代like实现模糊查询\nselect * from emp where instr(ename,'%')<>0;--\n```\n\n- 拼接字符串:\n1. 符号: ||\n2. 函数: concat(c1,c2) 将c1 c2拼接为一个字符串\n\n```sql\nselect concat(concat('123','abc'),'eee') from dual;\nselect '123' || 'abc' || 'eee' from dual;\n```\n\n### 日期函数\n\n- add_months(c1,c2)\nc1:日期类型\nc2:整数,在c1日期的基础上增减c2个月份\n\n```sql\n-- 查询当前系统时间之前的一个月\nselect add_months(sysdate,-1) from dual;\n-- 查询十年后的今天\nselect add_months(sysdate,120) from dual;\n```\n\n- months_between(c1,c2)：c1 c2都是日期类型，计算两个日期之间相差多少个月份\n\n```sql\nselect round(months_between(sysdate,hiredate),2) from emp;\n```\n\n- last_day(date)\n\n```sql\n-- 查询给定日期所在月份的最后一天\nselect last_day(sysdate) from dual;\n-- 查询下一个月的第一天 \nselect last_day(sysdate)+1 from dual;\n-- 查询上一个月的第一天 \nselect last_day(add_months(sysdate,-2))+1 from dual;\n```\n\n- 计算日期:\nround(c1,c2): 四舍五入 trunc(c1,c2): 直接舍去\nc1:日期类型的值 \nc2:字符类型,日期格式 在c1日期的基础上,精确到c2指定的日期格式,如果比c2低的日期格式,默认是初始值\n\n```sql\n-- 将当前系统时间精确到年份\nselect round(sysdate,'yyyy') from dual;\n-- 如果当前时间是: 2017-2-11 13:00:00 //2017-1-1 00:00:00\n```\n\n- 查询本年度的一季度的起始日期\n\n```sql\nselect trunc(sysdate,'yyyy'),add_months(trunc(sysdate,'yyyy'),3) from dual;\n```\n\n- next_day(c1,c2) \nc1: 日期类型\nc2:字符或整数: 周期值,从c1日期来时,查找c2指定的周期的日期\n\n```sql\n-- 查询当前系统时间之后的第一个星期一\nselect sysdate,next_day(sysdate,'星期四') from dual;\n```\n\n#### 转换函数\n\n- to_date(c1,c2)\n- to_timestamp(c1,c2)\nc1: 字符类型的日期\nc2:日期格式\n能够根据c2指定的格式将c1字符类型的日期转变为日期类型\n\n```sql\ninsert into emp(empno,hiredate) values(1001, to_date('2017-1-1','yyyy-dd-mm') );\ninsert into emp(empno,hiredate) values(1002, to_date('2017-1-1 12:21:20','yyyy-dd-mm hh24:mi:ss') );\n```\n\n- to_char()\n\n```sql\n-- 将一个数值转变为一个字符串\nselect * from emp where '1'=1;\nselect * from emp where '1'=to_char(1);\n\n-- 格式化字符串或数值\nselect to_char(2341212412,'$999,999,999,999.00') from dual;\n-- 第二个参数的长度必须大于第一个参数\nselect to_char(2341212412,'L999,999,999,999.00') from dual;\n\n-- 与日期的转换\n-- to_char(c1,c2) c1: 日期类型的值 c2:日期格式 按照c2指定的日期格式,从c1中取值\n\n-- 查询当前系统时间的月份\nselect to_char(sysdate,'month') from dual;\n-- 查询当前系统时间的年月日\nselect to_char(sysdate,'yyyy-mm-dd') from dual;\n-- 查询当前系统时间是本年度第几天\nselect to_char(sysdate,'yyyy-ddd-dd-d') from dual;\n```\n\n- 对比to_date 和to_char:\n  to_date('2017','yyyy') 将字符串转变成日期 第一个参数是:字符串类型的值\n  to_char(sysdate,'yyyy') 按照指定的格式将日期转变为字符串 第一个参数是:日期值\n\n\n## 关键字\n\n- minus从第一个查询结果中,减去第二个查询结果中重复出现的数据 剩余:第一个查询结果中的部分数据\n\n```sql\nselect * from emp where ename like '%E%'\nminus \nselect * from emp where ename like '%S%';\n```\n\n- intersect 交集\n\n```sql\nselect * from emp where ename like '%E%'\nintersect\nselect * from emp where ename like '%S%';\n```\n\n## 创建表同时复制数据\n\n```sql\ncreate table 新表名(列名) as select 列名 from 旧表 where 条件;\n-- 如果表已存在，可以向已存在的表中插入数据：\ninsert into 表名(列名) select 列名 from 旧表 where 条件;\n```\n\n## 有则更新 无则新增\n根据源表对目标表进行匹配查询，匹配成功时更新，不成功时插入\n\n语法\nMERGE INTO 目标表 a USING 源表 b ON (a.字段1 = b.字段2 and a.字段n = b.字段n)\nWHEN MATCHED THEN UPDATE SET a.新字段 = b.字段 WHERE 限制条件 WHEN NOT MATCHED THEN INSERT (a.字段名1，a.字段名n) VALUES(b.字段值1, b.字段值n) WHERE 限制条件123456789\n\n例子:\n```sql\nMERGE INTO T_SCHE_PYTHON_JOB T1\nUSING (SELECT '1001' AS JOB_DEF_ID FROM dual) T2\nON ( T1.JOB_DEF_ID=T2.JOB_DEF_ID)\nWHEN MATCHED THEN\n    UPDATE SET T1.param = ${param}\nWHEN NOT MATCHED THEN \n    INSERT (JOB_DEF_ID,param) VALUES(1001,${param}); \n```","categories":["数据库"]},{"title":"SQL Server函数","slug":"SQL Server函数","url":"/blog/posts/002833e02e86/","content":"\n## sql server行转列\n\n类似MySQL group_concat()使用stuff()。stuff()将字符串插入到另一个字符串中。它从第一个字符串的开始位置删除指定长度的字符；然后将第二个字符串插入到第一个字符串的开始位置。\n\n```sql\nselect stuff('ABCDEFG',2,3,''hijk) = AhijkEFG\nSELECT id, value = stuff((SELECT ',' + value FROM temp t WHERE t.id = temp.id FOR xml path('')),1,1,'') FROM temp GROUP BY id\n\n-- 实例\nSELECT t.unit_id,t.value_param,t.code_param,conf.state\nFROM\n(SELECT \nunit_id,\nSTUFF((SELECT ',' + val1 FROM gs_pss_config b WHERE b.unit_id = a.unit_id AND b.conf_type = #{conf_type} FOR xml path('')),1,1,'')AS value_param,\nSTUFF((SELECT ',' + param_code FROM\tgs_pss_config c\tWHERE c.unit_id = a.unit_id\tAND c.conf_type = #{conf_type} FOR xml path('')),1,1,'')AS code_param\nFROM\ngs_pss_config a\nWHERE a.conf_type = #{conf_type} GROUP BY a.unit_id)t\nLEFT JOIN \ngs_yctp_unit_config conf on t.unit_id = conf.unit_id \n```\n\n\n## 统计数据考核密度\n\n```sql\nselect s.real_tag,s.tag_name,s.unit_id,\nsum(case when s.density = 0 then 1 else 0 end ) as density0,\nsum(case when s.density > 0 and s.density <=20 then 1 else 0 end ) as density20,\nsum(case when s.density > 20 and s.density <=50 then 1 else 0 end ) as density50,\nsum(case when s.density > 50 and s.density <=70 then 1 else 0 end ) as density70,\nsum(case when s.density > 70 and s.density < 100 then 1 else 0 end ) as density99,\nsum(case when s.density <=100 then 1 else 0 end ) as density100\nfrom\n(select * from gs_job_density where 1 =1 and start_time >=#{start_time} and end_time <=#{end_time}) s\ngroup by s.real_tag,s.tag_name,s.unit_id having 1 =1 and charindex(#{real_tag},real_tag)>0\norder by s.unit_id desc\nOFFSET ${start} ROW FETCH NEXT ${limit} rows only\n```\n\n## 批量添加/更新\n\n```sql\ninsert into tbl(a,b) values (1,2),(3,4),(5,6);\nupdate gs_job_pfr_mx SET val = \nCASE tag_code\nWHEN 'cals.GD_SH05_UUB00W0101Y' THEN '1.0'\nWHEN 'cals.GD_SH05_UUB00W0102Y' THEN '1.1'\nWHEN 'cals.GD_SH05_UUB00W0101D' THEN '1.2'\nWHEN 'cals.GD_SH05_UUB00W0102D' THEN '1.3'\nEND,\nTIME = GETDATE()\nWHERE tag_code IN('cals.GD_SH05_UUB00W0101Y', 'cals.GD_SH05_UUB00W0102Y', 'cals.GD_SH05_UUB00W0101D', 'cals.GD_SH05_UUB00W0102D')\n```\n\n## 分组排序\n\n```sql\nselect row_number() over(partition BY 分组字段 order by 排序字段) as rowNums,* from 表名\n\n-- 例：需要筛选两条数据，一条是每个区间段所有数据中最大的，另外一条是个数*10%的数值\nselect\nc.count,c.maxfhl,c.qjd,c.type,b.rownum,b.tag_name,b.fhl\nfrom\n(select count(qjd) as count,max(fhl) as maxfhl,qjd,type from gs_job_grade2 where tag_name = #{tag_name} GROUP BY qjd,type)c\njoin\n(select row_number() over(partition BY qjd,type order by fhl desc) as rownum,* from gs_job_grade2\nwhere tag_name = #{tag_name}) b\non c.qjd = b.qjd and c.type = b.type\nwhere b.rownum = CEILING(c.count*0.1) and c.type = #{type} and c.qjd = #{qjd}\norder by c.qjd,c.type\n\n-- 排序时添加序号列\nselect ROW_NUMBER()OVER(ORDER BY 用来排序的列的列名),XXX,XXX from XXX\n```\n\n\n## 差集、交集\n\n```sql\n-- 取差集\nselect Name from Person1\nexcept\nselect Name from Person2\n\n-- 取交集\nselect * from Person1\nInterSect\nselect * from Person2\n```\n\n\n## 函数\n\n```sql\nfloor() -- 返回小于或等于所给数字表达式的最大整数\nfloor(1.1)=1 floor(2)=2\n\nceiling() -- 返回大于或等于所给数字表达式的最小整数。\nceiling(1.1)=2 ceiling(2)=2\n\nround() -- 四舍五入\n\nsubstring()\nselect substring('abdcsef',1,3) abd\n\nreverse(express) -- 将字符串从尾部到头部排序\n\ncharindex(expression1,expression2[,start_location])获取某字符第一次出现的位置\nexpression1 必需 ---要查找的子字符串\nexpression2 必需 ---父字符串\nstart_location 可选 ---指定从父字符串开始查找的位置，默认位置从1开始\ncharindex(#{tag_name},table.tagname)\n```\n\n## SQL关联更新\n\n```sql\n-- SQL关联使用聚合函数更新sql存在更新不存在添加\n\nif exists (select * from dbo.users s where s.name='张三')\nupdate users set sex='男' where name = '张三'\nELSE\ninsert into users (name,sex) values ('张三','女')\n```\n\n## 转换数据类型的两种方式\n\n```sql\nselect * from TableName order by cast(colName as int);\n\nselect * from TableName order by convert(int,colName);\n```\n\n## 树形查询\n\n```sql\nwith cte_child(id,areaName,pid,level)\nas\n(\n --起始条件\nselect id,areaName,pid,0 as level from erp_area where id = 1 -- 优先列出第一节点查询条件或子节点查询条件\n\nunion all\n--递归条件\nselect a.id,a.areaName,a.pid,b.level+1 from erp_area a\ninner join cte_child b\non a.pid=b.id\n)\nselect * from cte_child \n```\n\n## 将文件中的数据插入数据库\n\n\n```sql\nBULK INSERT  [ schema_name ] . [ table_name ]\nFROM 'data_file'\n[ WITH (Arguments)]\n\n-- 例\nbulk insert dbo.tablename\nfrom 'D:\\abc.txt'\nWITH(\n    FIELDTERMINATOR = ',',\n    ROWTERMINATOR = '\\n',\n    DATAFILETYPE ='widechar',\n    KEEPNULLS\n)\n```\n**Arguments**\n\n- data_file：指定数据文件的full path，bulk insert命令将数据从该文件导入到Target Table中\n- ROWTERMINATOR = 'row_terminator'： 指定分隔行的字符，使用该字符来分割行（Row）\n- FIELDTERMINATOR = 'field_terminator'：指定分隔字段的字符，使用该字符来分割字段（Field或Column）\n- DATAFILETYPE = { 'char' | 'native'| 'widechar' | 'widenative' }：指定data file编码（Encoding）的类型，推荐使用widechar编码\n- CODEPAGE = { 'ACP' | 'OEM' | 'RAW' | 'code_page' }：如果data file中含有单字节（char或varchar）字符数据，使用CodePage参数指定字符列的CodePage\n- BATCHSIZE = batch_size：指定一个batch包含的数据行数量，在将数据复制到Table中时，每一个Batch作为一个单独的事务，如果一个batch复制失败，那么事务回滚。默认情况下，data file中的所有数据作为一个batch\n- CHECK_CONSTRAINTS：指定在执行bulk insert操作期间，必须检查插入的数据是否满足Target Table上的所有约束。如果没有指定CHECK_CONSTRAINTS选项，则所有CHECK和FOREIGN KEY约束都将被忽略，并且，在此操作之后，表上的所有约束将标记为不可信（not-trusted）\n- FIRE_TRIGGERS：指定是否启动Insert触发器，如果指定该选项，每个batch成功插入后，会执行Insert触发器；如果不指定该选项，不会执行Insert触发器\n- KEEPIDENTITY：指定将data file中的标识值插入到标识列（Identity Column）中，如果不指定KeepIdentity选项，Target Table中的ID列会自动分配唯一的标识值\n- KEEPNULLS：指定在执行bulk insert操作期间，空列（Empty Columns）应保留NULL值，而不是插入列的默认值\n- TABLOCK：指定在执行bulk insert操作期间，获取一个表级锁，持有表级锁，能够减少锁竞争(Lock Contention)，提高导入性能","categories":["数据库"]},{"title":"软件设计师考点笔记","slug":"软件设计师考点笔记","url":"/blog/posts/b83b4d5e2fab/","content":"\n#### 计算机组成与结构\n\n- ①**立即寻址**：操作数就包含在指令中，直接指出操作数本身，速度最快。\n②**直接寻址**：操作数存放在内存单元中，指令中直接给出操作数所在存储单元的地址。\n③**寄存器寻址**：操作数存放在某一寄存器中，指令中给出存放操作数的寄存器名。\n④**寄存器间接寻址**：操作数存放在内存单元中，操作数所在存储单元的地址在某个寄存器中。\n⑤**间接寻址**：指令中给出操作数地址的地址，指令中存放的地址的地址（访问寄存器中的地址），速度最慢。\n⑥**相对寻址**：指令地址码给出的是一个偏移量（可正可负），操作数地址等于本条指令的地址加上该偏移量。\n⑦**变址寻址**：操作数地址等于变址寄存器的内容加偏移量。\n\n- CPU是计算机的控制中心，主要由运算器、控制器、寄存器组和内部总线等部件组成。\n①**运算器**由算数逻辑单元（ALU）、累加寄存器（AC）、数据缓冲寄存器（DR）、状态条件寄存器（PSW）组成\n②**寄存器组**分为专用寄存器和通用寄存器\n③**内部总线**\n④**控制器**由程序计数器、指令寄存器、指令译码器、时序产生器和操作控制器组成，它是发布命令的“决策机构”，完成协调和指挥整个计算机系统的操作。\n  - **程序计数器(PC，又称为指令计数器)**:是专用寄存器，具有寄存信息和计数两种功能，在程序开始执行前，将程序的起始地址送入PC，该地址在程序加载到内存时确定，因此PC的初始内容即是程序第一条指令的地址。执行指令时，CPU将自动修改PC的内容，以便使其保持的总是将要执行的下一条指令的地址。由于大多数指令都是按顺序执行的，因此修改的过程通常只是简单地对PC加1。当遇到转移指令时，后继指令的地址根据当前指令的地址加上一个向前或向后转移的位移量得到，或者根据转移指令给出的直接转移的地址得到。为了保证程序指令能够连续地执行下去，CPU必须具有某些手段来确定下一条指令的地址。而程序计数器正起到这种作用，所以通常又称为指令计数器。CPU首先从程序计数器获取指令地址。\n  - **指令寄存器（IR）**:用来保存当前正在执行的指令。当执行一条指令时，先把它从内存取到**数据寄存器（DR）**中，然后再传送至**指令寄存器**。为了执行任何给定的指令，必须对操作码进行测试，以便识别所要求的操作。**指令译码器**就是做这项工作的。**指令寄存器中操作码字段的输出就是指令译码器的输入**。操作码一经译码后，即可向操作控制器发出具体操作的特定信号。从内存或者告诉缓存读到的指令暂存在指令寄存器。为分析一条指令，**操作码和地址码都应该放入指令寄存器。**\n  - **地址寄存器（AR）** 用来保存当前CPU所访问的内存单元的地址。由于在内存和CPU之间存在着操作速度上的差别，所以必须使用地址寄存器来保持地址信息，直到内存的读/写操作完成为止。\n  - **指令译码器（ID）** 的功能是**对现行指令**进行分析，确定指令类型和指令所要完成的操作以及寻址方式，并将相应的控制命令发往相关部件\n\n- 计算机中**主存与外设间进行数据传输的输入输出控制方法有程序控制方式、中断方式、DMA（直接内存读取）等**。\n**在程序控制方式下**，由CPU执行程序控制数据的输入输出过程。\n**在中断方式下**，外设准备好输入数据或接收数据时向CPU发出中断请求信号，若CPU决定响应该请求，则暂停正在执行的任务，转而执行中断服务程序进行数据的输入输出处理，之后再回去执行原来被中断的任务。\n**在DMA方式下**，CPU只需向DMA控制器下达指令，让DMA控制器来处理数据的传送，数据传送完毕再把信息反馈给CPU，这样就很大程度上减轻了CPU的负担，可以大大节省系统资源。\n\n- **补码**本身是带符号位的，补码可以简化计算机运算部件的设计，补码表示的数字中0是唯一的，不像**原码有+0和-0之分**，也就意味着机器字长为n位的二进制可以表示2^n个不同的数。机器字长为n，最高位为符号位，则剩余的n-1位用来表示数值，其最大值是这n-1位都为1，也就是2^(n-1)-1。**+-0相同的是补码和移码**,在计算机中，n位补码（表示数据位），表示范围是-2^(n)-1 ~ 2^(n-1)-1，其中最小值为认为定义，以n=8为例，其中-128的补码是人为定义的10000000\n\n- **CISC （Complex Instruction Set Computer，复杂指令集计算机）**:基本思想是：进一步增强原有指令的功能，用更为复杂的新指令取代原先由软件子程序完成的功能，实现软件功能的硬件化，导致机器的指令系统越来越庞大而复杂。CISC计算机一般所含的指令数目至少300条以上，有的甚至超过500条。\n**RISC （Reduced Instruction Set Computer，精简指令集计算机）**:基本思想是：通过减少指令总数和简化指令功能，降低硬件设计的复杂度，使指令能单周期执行，并通过优化编译提高指令的执行速度，采用**硬布线控制逻辑优化编译程序**。\n\n- 计算机系统中的CPU内部对**通用寄存器**的存取操作是速度最快的，其次是Cache，内存的存取速度再次，访问速度最慢的就是作为外存的硬盘。它们共同组成分级存储体系来解决存储容量、成本和速度之间的矛盾。\n\n- ①**全相联地址映射**：主存的任意一块可以映象到Cache中的任意一块。发生块冲突概率低。\n②**直接相联映射**：主存中一块只能映象到Cache的一个特定的块中。\n③**组相联的映射**：各区中的某一块只能存入缓存的同组号的空间内，但组内各块地址之间则可以任意存放。\n\n- 两个浮点数相加，**先统一阶码**，然后对阶，对阶时，**小阶向大阶对齐，尾数右移n位**。浮点格式表示一个二进制数N的形式为N=2eXF，其中E称为阶码，F叫做尾数。在浮点表示法中，**阶码通常为含符号的纯整数，尾数为含符号的纯小数。阶码即指数（常用移码表示，也有用补码的），表示范围，尾数（补码或原码标识）表示精度**。\n\n- 16进制地址之间共有多少存储空间：3FFFH转换成10进制=3 * 16^3+15 * 16^2+15 * 16^1+15 * 16^0减去另外的16进制转10进制，然后+1\n\n- 计算机系统的存储器按所处的位置可分为内存和外存。\n按构成存储器的材料可分为磁存储器、半导体存储器和光存储器。\n按存储器的工作方式可分为读写存储器和只读存储器。\n按访问方式可分为按地址访问的存储器和按内容访问的存储器。\n按寻址方式可分为随机存储器、顺序存储器和直接存储器。\n**相联存储器是一种按内容访问的存储器**\n\n- 程序的局限性表现在**时间局部性和空间局部性**：**时间局部性**是指如果程序中的某条指令一旦被执行，则不久的将来该指令可能再次被执行；**空间局部性**是指一旦程序访问了某个存储单元，则在不久的将来，其附近的存储单元也最有可能被访问。\n\n- 程序员可以访问的是**程序计数器**，**指令寄存器**对程序员用户透明\n\n- 总线带宽 = 时钟频率 / 时钟周期 * 总线字节。例：200MHz（时钟频率）/ 5(时钟周期）*（32位/8=4字节)\n\n- 采用模二除法运算的只有循环冗余检验CRC\n\n- **可寻址范围** = 内存容量 / 机器字长。内存容量32G，字长32位，可寻址范围：内存容量2GB=2 * 1024 * 1024 * 1024 * 8位，按字编址时，存储单元的个数为2 * 1024 * 1024 * 1024 * 8 / 32=512 * 1024 * 1024，即可寻址范围为512MB\n\n- ①**软件可靠性**指的是一个系统对于给定的时间间隔内、在给定条件下**无失效**运作的概率。可以用MTTF / （1+MTTF）来度量，其中MTTF为平均无故障时间。\n②**软件可用性**是指在给定的时间点上，一个软件系统能够按照规格说明**正确运行**的概率。可以用MTBF / （1+MTBF）来度量，其中MTBF为平均失效间隔时间。\n③**软件可维护性**是在给定的使用条件下，在规定的时间间隔内，使用规定的过程和资源完成**维护活动**的概率，可以用1 / （1+MTTR）来度量，其中MTTR为平均修复时间。\n\n- **数据总线**用于传输数据信息，**地址总线**用来传送地址，**控制总线**用来传送控制信号和时序信号。\n釆用总线结构主要有以下优点：\n简化系统结构，便于系统设计制造；\n大大减少了连线数目，便于布线，减小体积，提髙系统的可靠性；\n便于接口设计，所有与总线连接的设备均釆用类似的接口；\n便于系统的扩充、更新与灵活配置，易于实现系统的模块化；\n便于设备的软件设计，所有接口的软件就是对不同的口地址进行操作；\n便于故障诊断和维修，同时也降低了成本。\n总线是一组能为多个部件分时共享的信息传送线，用来连接多个部件并为之提供信息交换通路，通过**总线复用方式可以减少总线中信号线的数量，以较少的信号线传输更多的信息**。\n- **流水线** ：取指2ns，分析3ns，执行1ns，100条指令流水线执行= 3（分析时间） * 99 + 6 。**顺序执行**=6 * 100。\n\n- 当系统中有多个中断请求时，中断系统按优先级进行排队。若在处理低级中断过程中又有高级中断申请中断，则高级中断可以打断低级中断处理，转去处理高级中断，等处理完高级中断后再返回去处理原来的低级中断，称为中断嵌套。**实现中断嵌套用后进先出的栈来保护断点和现场最有效**。\n\n- 一个磁道移到另一个磁道需要6ms，磁道非连续存放，相邻数据快的平均距离为10个磁道，每块的延时及传输时间需要100ms和20ms，读取100快文件需要（6<转移> * 10<距离> + 100 + 20） * 100\n\n- 千小时可靠度3个部件\n**串联**的可靠度为R * R * R；\n**并联**的可靠度为1-（1-R） * （1-R） * （1-R）；\n前两个部件并联后与第三个部件串联的可靠度为（1-（1-R） * （1-R）） * R；第一个部件与后两个部件并联构成的子系统串联的可靠度为R *（1-（1-R） * （1-R））。\n\n- 原码补码移码反码\n\n- **常见的摘要算法**：CRC，MD5，SHA1，SHA256，PIPEMD。\n**常见的对称加密算法有**：DES，三重DES、RC-5、IDEA、AES。\n**常见的非对称加密算法**：RSA、ECC、DSA。一般公钥用于加密和签名，而私钥用于解密和认证。\n**数字签名算法**：RSA；\n利用**CA的公钥**验证数字证书的真实性，利用报文摘要算法生成报文的主要目的是**防止报文被篡改**；保证数字证书不被篡改的方法是使用CA的私钥对数字证书签名，用户通过CA的签名验证网站的真伪。\nA和B使用**数字证书**对用户的身份进行认证，使用**数字签名**确保消息不可置否\n\n- 入侵检测技术包括专家系统、模型检测、简单匹配；漏洞扫描不是入侵检测的内容\n\n- 最先获得键盘或鼠标输入信息的是**中断程序**\n\n\n#### 程序语言\n\n- n个成员开发小组的沟通路径**n *（n-1）/ 2**\n\n- 在源程序中，可由用户（程序员）为**变量、函数和数据类型**等命名，无法给关键字（保留字）进行命名。\n\n- 程序设计语言的基本成分包括数据、运算、控制和传输等。**程序设计语言的控制成分包括顺序、选择和循环3种结构**。\n\n- **词法分析输出记号流**，**语法分析**没有错误的话**构造出语法树**，括号不匹配属于语法错误\n\n- 编译程序对源程序的翻译过程分为词法分析、语法分析、语义分析、中间代码生成、代码优化和目标代码生成，以及符号表管理和出错处理。\n①源程序可以被看成是一个字符串，**词法分析**是编译过程的第一阶段，其任务是对源程序从前到后（从左到右）逐个字符地扫描，从中识别出一个个的“单词”符号,识别其中如**关键字（或称保留字）、标识符、常数、运算符以及分隔符（标点符号和括号）等**。\n②**语法分析**的任务是在词法分析的基础上，根据语言的语法规则将单词符号序列分解成各类语法单位，如“表达式”、“语句”、“程序”等。例如**程序语句的结构是否合法，语言结构出错、if…end if不匹配，缺少分号、括号不匹配、表达式缺少操作数**等。\n③**语义分析阶段**主要检查源程序是否包含语义错误，并收集类型信息供后面的代码生成阶段使用，进行类型分析和检查，主要检测源程序是否存在静态语义错误。包括：**运算符和运算类型不符合（String a + int b），取余时用浮点数等**。只有语法和语义都正确的源程序才能被翻译成正确的目标代码。\n④**中间代码**是一种简单且含义明确的记号系统，与具体的机器无关，可以有若干种形式。可以将不同的高级程序语言翻译成同一种中间代码。由于与具体机器无关，**使用中间代码有利于进行与机器无关的优化处理，以及提高编译程序的可移植性**。中间代码是源程序的一种内部表示，或称中间语言。中间代码的作用是可使编译程序的结构在逻辑上更为简单明确，使用中间代码可提高编译程序的可移植性，常见的有**逆波兰记号、四元式、三元式和树。目标代码生成阶段的工作与目标机器的体系结构密切相关，分配寄存器的工作在目标代码生成阶段进行**\n\n- 语法分析方法分为两类：自上而下（自顶向下）分析法和自下而上（自底向上）分析法，**递归下降分析法和预测分析法属于自上而下分析法，移进-归约分析法属于自下而上（自底向上）分析法**。语法制导翻译是一种静态语义分析。\n\n- 有限自动机，进行词法分析的工具\n\n- 正则表达式\n\n- 程序运行时的用户内存空间一般划分为代码区、静态数据区、栈区和堆区，其中**栈区和堆区也称为动态数据区。全局变量的存储空间在静态数据区**\n\n- 变量是内存单元的抽象，用于在程序中表示数据。当变量存储的是内存单元地址时，称为指针变量，或者说指针变量指向了另一个变量。**指针变量**可以定义在函数或复合语句内，也可以定义在所有的函数之外，**可以是全局变量，也可以是局部变量**。需要区分指针变量与指针所指向的变量，无论指针变量指向何种变量，其存储空间大小都是一样的。\n\n\n#### 操作系统※\n\n- 前驱图中，PV的排放：箭头的起点是V（释放）操作，箭头的终点位置是P（申请）操作，在前驱图中先分析信号量的位置，根据图中某个操作应该可以得到每个进程的信号量的位置，没有具体信息的话信号量按照从左到右，从上到下进行排放，根据排放获取答案信息\n\n- 磁盘格式化容量：面数 * （磁道数/面 即每面的磁道数） * （扇区数/道 即每道的扇区数） * （字节数/扇区 即每扇区的字节数）。非格式化容量：面数 * （磁道数/面 即每面的磁道数） * 内圈周长 * 最大密度数。磁道数=外直径-内直径\n\n- 不发生死锁：（进程所需资源数-1）* 并发数 + 1\n\n- McCabe环形复杂度 = e（条数）-n（节点数） + 2\n\n- 磁盘调度管理中，先进行移臂调度寻找磁道，再进行旋转调度寻找扇区。\n\n- 因为先来先服务是谁先请求先满足谁的请求，而最短寻找时间优先是根据当前磁臂到要请求访问磁道的距离，谁短满足谁的请求，故**先来先服务和最短寻找时间优先算法可能会随时改变移动臂的运动方向**\n\n- 采用**中断方式和DMA方式**时，CPU与外设可并行工作。程序查询方式是由CPU主动查询外设的状态，在外设准备好时传输数据。\n\n- 公用信号量，实现进程间的互斥，初始值为1或资源数量；\n私用信号量，实现进程间的同步，初始值为0或某个正整数\n\n- 资源可用数为8，有3个进程竞争资源，每个进程需要i个资源，发生死锁的最小值i为4。（i=3时，进程1分配3个资源，进程2分配3个资源，进程3分配2个资源，此时不会发生死锁）\n\n- 进程控制块的组织方式有链接方式和索引方式。\n\n- 用户将磁盘块文件逐块从磁盘读入缓冲区，并送至用户区进行处理，采用**单缓冲区花费的时间**为（块读入缓冲区的时间+缓冲区送入用户区的时间） * 磁盘块 + 系统对每个磁盘块处理的时间，**双缓冲区时间**：快读入缓冲区的时间 * 磁盘块 + 缓冲区送入用户区的时间 + 系统对每个磁盘块处理的时间\n\n- 访问一个数据块的时间应为**寻道时间+（旋转延迟时间及传输时间之和）**。根据题意，每块的**旋转延迟时间及传输时间共需120ms**，磁头从一个磁道移至另一个磁道需要6ms，但逻辑上相邻数据块的平均距离为10个磁道，即读完一个数据块到下一个数据块**寻道时间需要60ms**。通过上述分析，本题访问一个数据块的时间T=120ms+60ms=180ms，而读取一个100块的文件共需要18000ms\n\n\n#### 软件工程基础知识\n\n- ①**极限编程XP**是激发开发人员创造性、使得管理负担最小的一组技术，不会使编码速度更快，不用编写测试代码，有价值观、原则、实践、行为四部分组成。价值观包括沟通、简单性、反馈、勇气\n②**水晶法**Crystal认为每一个不同的项目都需要一套不同的策略、约定和方法论；\n③**并列争球法（Scrum）使用迭代的方法**，其中把每30天一次的迭代成为一个冲刺，并按需求的优先级来实现产品。多个自组织和自治小组并行地递增实现产品，并通过简短的日常情况会议进行协调。\n\n- ①**瀑布模型**是将软件生存周期各个活动规定为依线性顺序连接的若干阶段的模型，**适合于软件需求很明确的软件项目，难以适应变化的需求。**\n②**V模型**是瀑布模型的一种演变模型，**将测试和分析与设计关联进行**，加强分析与设计的验证。\n③**演化模型**特别适用于**对软件需求缺乏准确认识**的情况。可以尽快投入使用并在使用过程中不断完善\n④**原型模型**是一种演化模型，通过快速构建可运行的原型系统，然后根据运行过程中获取的用户反馈进行改进。**原型的用途是获知用户的真正需求**，因此原型模型可以有效地引发系统需求。不可以用来指导代码优化。\n⑤**螺旋模型**将瀑布模型和演化模型结合起来，加入了两种模型均忽略的**风险分析**。\n⑥**喷泉模型**是典型的面向对象生命周期模型，是一种以用户需求为动力，以对象作为驱动的模型。**该模型克服了瀑布模型不支持软件重用和多项开发活动集成的局限性**。“喷泉”一词本身体现了迭代和无间隙特性。**迭代意味着模型中的开发活动常常需要重复多次，在迭代过程中不断地完善软件系统，适用于大型软件开发**，包含维护周期，因此维护与开发之间没有本质区别\n⑦**增量模型**是一种非整体开发的模型，该模型具有较大的灵活性，适合于**软件需求不明确**的一种模型。使用该模型开发产品，**一般是尽快构造出可运行的产品**，然后在该产品的基础上再增加需要的新的构建，使产品更趋于完善\n\n- 软件开发配置管理：软件配置标识、变更管理、版本控制、系统建立、配制审核、配置状态报告，不包括风险管理，质量控制\n\n- ①**风险避免**即放弃或不进行可能带来损失的活动或工作。例如，为了避免洪水风险，可以把工厂建在地势较高、排水方便的地方，这是一种主动的风险控制方法，**是最好的风险控制策略**。\n②**风险监控**是指在决策主体的运行过程中，对风险的发展与变化情况进行全程监督，并根据需要进行应对策略的调整。\n③**风险管理**是指在一个肯定有风险的环境里把风险减至最低的管理过程。对于风险我们可以转移，可以规避，但不能消除。风险管理是软件项目管理的一项重要任务。在进行风险管理时，**根据风险的优先级来确定风险控制策略，而优先级是根据风险暴露来确定的**。\n④**风险暴露**是一种量化风险影响的指标，等于风险影响乘以风险概率，风险影响是当风险发生时造成的损失。\n⑤**风险概率**是风险发生的可能性。\n⑥**风险控制**是风险管理的一个重要活动，**定义风险参照水准是风险评估的一类技术**，对于大多数软件项目来说成本、速度和性能是三种典型的风险参照水准\n\n- 软件风险一般包括**不确定性和损失**两个特性，其中不确定性是指风险可能发生，也可能不发生。损失是当风险确实发生时，会引起的不希望的后果和损失。救火和危机管理是对不适合但经常采用的软件风险管理策略。已知风险和未知风险是对软件风险进行分类的一种方式。员工和预算是在识别项目风险时需要识别的因素\n\n- ①CL0（未完成的）：过程域未执行或未得到CL1中定义的所有目标。\n②CL1（已执行的）：其共性目标是过程将可标识的输入工作产品转换成可标识的输出工作产品，以实现支持过程域的特定目标。\n③CL2（已管理的）：其共性目标是集中于已管理的过程的制度化。根据组织级政策规定过程的运作将使用哪个过程，项目遵循已文档化的计划和过程描述，所有正在工作的人都有权使用足够的资源，所有工作任务和工作产品都被监控、控制、和评审。建立基本的项目管理和实践来跟踪项目费用、进度和功能特性为可重复级的核心；\n④CL3（已定义级的）：其共性目标集中于已定义的过程的制度化。过程是按照组织的裁剪指南从组织的标准过程中裁剪得到的，还必须收集过程资产和过程的度量，并用于将来对过程的改进。使用标准开发过程（或方法论）构建（或集成）系统为已定义级的核心；\n⑤CL4（定量管理的）：其共性目标集中于可定量管理的过程的制度化。使用测量和质量保证来控制和改进过程域，建立和使用关于质量和过程执行的质量目标作为管理准则。管理层寻求更主动地应对系统的开发问题为已管理级的核心；\n⑥CL5（优化的）：使用量化（统计学）手段改变和优化过程域，以满足客户的改变和持续改进计划中的过程域的功效，连续地监督和改进标准化的系统开发过程为优先级的核心。\n\n- **概要设计**将需求转化为软件的模块划分，确定模块之间的调用关系；\n结构化设计方法中，概要设计阶段进行软件体系结构的设计、数据设计和接口设计；\n面向对象设计方法中，概要设计阶段进行体系结构设计、初步的类设计/数据设计、结构设计；\n**详细设计**将模块进行细化，得到详细的数据结构和算法；编码根据详细设计进行代码的编写，得到可以运行的软件，并进行单元测试\n结构化设计方法中，详细设计阶段进行数据结构和算法的设计。\n面向对象设计方法中，详细设计阶段进行构件设计。\n\n- 结构化设计和面向对象设计是两种不同的设计方法，**结构化设计**根据系统的数据流图进行设计，模块体现为函数、过程及子程序；**面向对象设计**基于面向对象的基本概念进行，模块体现为类、对象和构件等\n\n- **Gantt图**用水平条状图描述，它以日历为基准描述项目任务，可以清楚地表示任务的持续时间和任务之间的并行，但是**不能清晰地描述各个任务之间的依赖关系。**\n**PERT图**是一种网络模型，描述一个项目的各任务之间的关系。可以明确表达任务之间的依赖关系，即哪些任务完成后才能开始另一些任务，以及如期完成整个工程的关键路径，但是**不能清晰地描述各个任务之间的并行关系**。\n\n- 软件需求包括功能需求、非功能需求和设计约束三个方面的内容。\n**功能需求**是所开发的软件必须具备什么样的功能\n**非功能需求**是指产品必须具备的属性或品质，如**可靠性、性能、响应时间和扩展性**等等，“软件产品必须能够在3秒内对用户请求作出响应”主要表述软件的响应时间，属于非功能需求\n**设计约束**通常对解决方案的一些约束说明。\n\n- 里程碑（最晚多少天不影响工期）关键路径不能缩短工期\n\n- COCOMOII估算选择**不包括用例数**（包括对象点、功能点、代码行）\n\n- 冗余附加技术是指为实现结构、信息和时间冗余技术所需的资源和技术，包括程序、指令、数据、存放和调动它们的空间和通道等。在屏蔽硬件错误的容错技术中，冗余附加技术包括：关键程序和数据的冗余及调用；检测、表决、切换、重构和复算的实现。在屏蔽软件错误的容错技术中，冗余附加技术包括：冗余备份程序的存储及调用；实现错误检测和错误恢复的程序；实现容错软件所需的固化程序\n\n- 软件开发过程中，需求分析阶段的输出不包括**软件体系结构图**\n\n- 信息库不属于配置数据库。\n\n- **UP（统一过程）模型**是一种以用例和风险为驱动、以架构为中心、迭代并且增量的开发过程，由UML方法和工具支持。UP过程定义了五个阶段，起始阶段、精化阶段、构建阶段、移交阶段和产生阶段。开发过程中有多次迭代，每次迭代都包含计划、分析、设计、构造、集成和测试，以及内部和外部发布，每阶段达到某个里程碑时结束。其中**初启阶段的里程碑是生命周期目标，精化阶段的里程碑是生命周期架构，构建阶段的里程碑是初始运作功能，移交阶段的里程碑是产品发布。**\n\n- **初启阶段结束**时产生一个构想文档、一个有关用例模型的调查、一个初始的业务用例、一个早期的风险评估和一个可以显示阶段和迭代的项目计划等制品\n**精化阶段结束**时产生一个补充需求分析、一个软件架构描述和一个可执行的架构原型等制品\n**构建阶段结束**成果是一个准备交到最终用户手中的产品，包括具有最初运作能力的在适当的平台上集成的软件产品、用户手册和对当前版本的描述\n**阶段结束时**移交给用户产品发布版本\n\n- ①**功能性**是指与功能及其指定的性质有关的一组软件质量，功能性包含质量子特性安全性\n②**可靠性**是指衡量在规定的一段时间内和规定条件下维护性能水平的一组软件质量，可靠性质量子特性不包括安全性。\n③**可维护性**是指与软件维护的难易程度相关的一组软件属性\n④**易使用性**是指与使用难易程度及规定或隐含用户对使用方式所做的评价相关的属性。\n⑤**可维护性**质量特性是指与软件维护的难易程度相关的一组软件属性，它包含了易分析性、稳定性、易测试性和易改变性4个子特性。其中：**易分析性**是描述诊断缺陷或失效原因、判定待修改程度的难易程度的特性。**稳定性**是描述修改造成难以预料的后果的风险程度，风险程度越低，稳定性越好。**易测试性**是描述测试已修改软件的难易程度的特性。**易改变性**是描述修改、排错或适应环境变化的难易程度。\n\n\n#### 系统开发与运行\n\n- 外部实体一般为组织机构、人员、第三方系统，试题、图书等不是外部实体\n\n- McCabe环形复杂度=e（条数）-n（节点数）+2\n\n- **数据流图**是结构化分析方法的重要模型，用于描述系统的功能、输入、输出和数据存储等。在绘制数据流图中，每条数据流的起点或者终点必须是加工，即**至少有一端是加工**。在分层数据流图中，**必须要保持父图与子图平衡。每个加工必须既有输入数据流又有输出数据流**。必须要保持数据守恒。也就是说，**一个加工所有输出数据流中的数据必须能从该加工的输入数据流中直接获得，或者是通过该加工能产生的数据**。\n数据流图中有四个要素：\n①**外部实体**，也称为数据源或数据汇点，表示要处理的数据的输入来源或处理结果要送往何处，不属于目标系统的一部分，通常为组织、部门、人、相关的软件系统或者硬件设备\n②**数据流**表示数据沿箭头方向的流动\n③**加工**是对数据对象的处理或变换\n④**数据存储**在数据流中起到保存数据的作用，可以是数据库文件或者任何形式的数据组织。顶层数据流图描述了系统的输入与输出。对基本加工的说明有三种描述方式：结构化语言、判断表（决策表）、判断树（决策树）。基本加工逻辑描述的基本原则为：1.对数据流图的每一个基本加工，必须有一个基本加工逻辑说明。2.基本加工逻辑说明必须描述基本加工如何把输入数据流变换为输出数据流的加工规则。3.加工逻辑说明必须描述实现加工的策略而不是实现加工的细节。4.加工逻辑说明中包含的信息应是充足的，完备的，有用的，无冗余的\n**实体联系图**也是一个常用的数据模型，**用于描述数据对象及数据对象之间的关系**。实体联系图有三个要素:\n①**实体**是目标系统所需要的复合信息的表示，也称为数据对象\n②**属性**定义数据对象的特征\n③**联系**是不同数据对象之间的关系。在该系统中患者是一个数据对象，即实体，具有多种属性\n\n- **演绎推理**，就是从一般性的前提出发，通过推导即“演绎”，得出具体陈述或个别结论的过程。\n**归纳法**以一系列经验事物或知识素材为依据，寻找出其服从的基本规律或共同规律，并假设同类事物中的其他事物也服从这些规律，从而将这些规律作为预测同类事物的其他事物的基本原理的一种认知方法\n\n- ①**数据耦合**：一个模块访问另一个模块时，彼此之间是通过简单数据参数（不是控制参数、公共数据结构或外部变量）来交换输入、输出信息的。\n②**公共耦合**：若一组模块都访问同一个公共数据环境，则它们之间的耦合就称为公共耦合。公共的数据环境可以是**全局数据结构**、共享的通信区、内存的公共覆盖区等。\n③**外部耦合**：一组模块都访问同一**全局简单变量而不是同一全局数据结构**，而且不是通过参数表传递该全局变量的信息，则称之为外部耦合。\n④**标记耦合**：一组模块通过参数表传递记录信息，数据结构本身传递就是标记耦合。这个记录是某一**数据结构**的子结构，而不是简单变量\n⑤**控制耦合**：是指一个模块通过传送开关、标志、名字等控制信息，明显的控制另一个模块的功能\n⑥**内容耦合**：若一个模块直接访问另一个模块的**内部数据**、一个模块不通过正常入口转到另一个模块内部、两个模块有一部分程序代码重叠或者一个模块有多个入口。\n\n- **结构化分析的输出不包括结构图**！\n\n- ①**偶然内聚/巧合内聚**：指一个模块内的各个处理元素之间**没有任何联系。具有最低的内聚性，是最不好的一种内聚类型，不易修改，不易理解，不易维护，会影响到模块间的耦合关系**\n②**逻辑内聚**：指模块内执行几个逻辑上相似的功能，通过参数确定该模块完成哪一个功能。\n③**时间内聚**：把需要**同时执行**的动作组合在一起形成的模块。\n④**通信内聚**：指模块内所有处理元素都在**同一个数据结构**上操作，或者指各处理使用相同的输入数据或者产生相同的输出数据。\n⑤**顺序内聚/过程内景**：指一个模块中各个处理元素都密切相关于同一功能且必须顺序执行，前一个功能元素的输出就是下一个功能元素的输入。\n⑥**功能内聚**：是最强的内聚，指模块内所有元素共同完成一个功能，缺一不可\n\n- **数据字典**是指对数据的数据项、数据结构、数据流、数据存储、处理逻辑、外部实体等进行定义和描述，其目的是对数据流程图中的各个元素做出详细的说明，使用数据字典为简单的建模项目。其条目有数据流、数据项、数据存储、基本加工等，**不包括外部实体**\n\n- 系统结构图（SC）又称为模块结构图，它是软件概要设计阶段的工具，反映系统的功能实现和模块之间的联系与通信，包括各模块之间的层次结构，即反映了系统的总体结构。SC包括**模块**、**模块之间的调用关系**、**模块之间的通信**和**辅助控制符号**等4个部分，**不包括数据**\n\n- **单元测试**主要是发现程序代码中的问题，针对详细设计和软件实现阶段的工作进行的；\n**集成测试**验证系统模块是否能够根据系统和程序设计规格说明的描述进行工作，即模块以及模块之间的接口的测试\n**系统测试**则是验证系统是否确实执行需求规格说明中描述的功能和非功能要求，因此测试目标在需求分析阶段就已经定义\n\n- **软件的可维护性**是指维护人员理解、改正、改动和改进这个软件的难易程度，是软件开发阶段各个时期的关键目标。\n**软件系统的可维护性**评价指标包括可理解性、可测试性、可修改性、可靠性、可移植性、可使用性和效率，**没有可扩展性**。\n**系统的可维护性**的评价指标包括：可理解性、可测试性、可修改性。**没有可移植性**\n\n- 软件维护的类型一般有四类：\n**正确性维护**是指改正在系统开发阶段已发生而系统测试阶段尚未发现的错误；\n**适应性维护**是指使应用软件适应信息技术变化和管理需求变化而进行的修改\n**完善性维护**是为扩充功能和改善性能而进行的修改\n**预防性维护**是为了改进应用软件的可靠性和可维护性，为了适应未来变化的软硬件环境的变化，主动增加预防性的新的功能，以适应将来各类变化。\n\n- 白盒测试也称为结构测试，根据程序的内部结构和逻辑来设计测试用例，对程序的路径和过程进行测试，检查是否满足设计的需要。其常用的技术有逻辑覆盖、循环覆盖和基本路径测试。在逻辑覆盖中，\n**语句覆盖是指选择足够的测试数据使被测试程序中每条语句至少执行一次。是最弱的覆盖准则**\n**判定覆盖是指选择足够的测试数据使被测试程序中每个判定表达式至少获得一次“真”值和“假”值**。\n**条件覆盖是指构造一组测试用例，使得每一判定语句中每个逻辑条件的各种可能的值至少满足一次**。\n**路径覆盖是指覆盖被测程序中所有可能的路径。可以比语句覆盖法发现更多的错误**\n黑盒测试也称为功能测试，在完全不考虑软件的内部结构和特性的情况下来测试软件的外部特性。\n\n- 在单元测试基础上，将所有模块按照设计要求组装为系统，此时进行的测试称为集成测试。\n集成测试有多种策略：**自底向上**：从系统层次中最底层的构件开始测试，逐步向上。需要设计驱动模块来辅助测试。**自顶向下**：与自底向上相反，从最顶层的构件开始，逐步向下。需要设计桩模块来辅助测试。**三明治**：结合自底向上和自顶向下两种测试策略。该测量的优势是结合了自底向上和自顶向下的优点，如较早地验证了主要的控制构件和底层模块，并行测试程度较高等。但缺点是需要写较多的驱动模块和桩模块。**一次性**：对所有构件一次性测试，然后集成。\n\n- 软件测试的目的时发现更多的错误，**而不是证明软件的正确性**\n\n- 仓库风格缺点包括测试困难，不能保证有好的解决方案，难以建立好的控制策略，低效，昂贵的开发工作，缺少对并行机制的支持。优点包括对可更改性和可维护性的支持，可复用的知识源，支持容错性和健壮性。\n\n\n#### 网络与多媒体基础知识\n\n- **网络层-路由器**,可以识别IP地址，进行数据包的转发。\n**数据链路-层-网桥和交换机**，网桥可以识别MAC地址，进行帧转发。交换机是由硬件构成的多端口网桥。传输层和会话层主要是软件功能，都不需要专用的联网设备。\n**物理层-中继器、集线器**，中继器作用是对接收的信号进行再生放大，以延长传输的距离。集线器作用是从一个端口接收信息并向其他端口广播出去\n\n- 应用层（HTTP，DNS，SSH，FTP），表示层，会话层，传输层（TCP，UDP，TLS，OSI/RM），网络层（IP协议，ARP，ICMP（封装在IP数据报中传送，不保证可靠的提交）），数据链路层（WIFI），物理层（光纤）\n\n- ①**表现媒体**是指进行信息输入和输出的媒体，如键盘、鼠标、话筒，以及显示器、打印机等；\n②**表示媒体**指传输感觉媒体的中介媒体，即用于数据交换的编码，如图像编码、文本编码和声音编码等；\n③**传输媒体**指传输表示媒体的物理介质，如电缆、光缆、电磁波等；\n④**存储媒体**指用于存储表示媒体的物理介质，如硬盘、光盘等，\n⑤**感觉媒体**指直接作用于人的感觉器官，使人产生直接感觉的媒体，如引起听觉反应的声音，引起视觉反应的图像等\n\n- **图像数据量**=图像的总像素X图像深度（b）,需用光盘数量的计算方式如下：\n**光盘数量**=图像的总像素 X 图像深度/4GB（张）\n\n- DNS域名查询的次序是：本地的hosts文件一>本地DNS缓存一>本地DNS服务器一>根域名服务器\n\n- 如果要使得两个IPv6结点可以通过现有的IPv4网络进行通信，则应该使用**隧道技术**，如果要使得纯IPv6结点可以与纯IPv4结点进行通信，则需要使用**翻译技术**\n\n- **数字签名**技术是不对称加密算法的典型应用。数字签名的应用过程是：数据源发送方使用自己的私钥对数据校验和或其他与数据内容有关的变量进行加密处理，完成对数据的合法“签名”；数据接收方则利用对方的公钥来解读收到的“数字签名”，并将解读结果用于对数据完整性的检验，以确认签名的合法性。数字签名技术是在网络系统虚拟环境中确认身份的重要技术，完全可以代替现实过程中的“亲笔签字”，在技术和法律上有保证，可见数字签名是对签名真实性的保护\n\n- ARP攻击（ARP欺骗）是欺骗攻击的一种，通过伪造IP地址和MAC地址，能够在网络中产生大量的ARP通信量使网络阻塞，如果伪造网关的IP地址和MAC地址对，则所有发往网关的IP包将因为MAC地址错误而无法到达网关（ARP攻击一般会将MAC地址改为发起ARP攻击的主机地址），造成无法跨网段通信。处理ARP攻击的方法为首先断开ARP攻击主机的网络连接，然后用“arp-d”命令清除受攻击影响的ARP缓存\n\n- 矢量图是根据几何特性来绘制图形，矢量可以是一个点或一条线，矢量图只能靠软件生成，文件占用内在空间较小，因为这种类型的图像文件包含独立的分离图像，可以自由无限制的重新组合。它的特点是放大后图像不会失真，和分辨率无关，适用于图形设计、文字设计和一些标志设计、版式设计等。**矢量图中的图形元素称为图元**。而另一类图具有代表性的图像表示形式是**位图图像，该图采用像素来表示图像**，用矢量图形格式表示复杂图像（如人物、风景照片），并且要求很高时，将需要花费大量的时间进行变换、着色和处理光照效果等。因此，矢量图形主要用于表示线框型的图画、工程制图和美术字等。位图图像是指用像素点来描述的图。图像适合于表现比较细腻，层次较多，色彩较丰富，包含大量细节的图像，并可直接、快速地在屏幕上显示出来。但占用存储空间较大，一般需要进行数据压缩\n\n- 声音信号是模拟信号，要使声音信号数字化并传递，首先要进行A/D转换，模拟信号转数字信号\n\n- IP地址155.32.90.192/26包含了多少个主机地址？答：IPV4地址是由32个2进制位组成，/26代表前26位是网络位，剩下的6位为主机地址，32-26=6即每个网络可以有2^6-2个主机地址\n\n- A类网络主机地址数2^24个，\n\n- VLAN（虚拟局域网）允许逻辑的划分网段\n\n- 一个标准的URL格式如下：协议://主机名.域名.域名后缀或IP地址:端口号/目录/文件名目录可能是多级的wb.xyz.com.cn、\n\n- DHCP协议的功能室自动分配IP地址\n\n\n#### 数据库技术\n\n- 函数依赖 （部分依赖，完全依赖，传递依赖）A->B，B完全依赖与A。AB->C,C部分依赖于A和B，A->B,B->C,C传递依赖于A，A->BC，即满足A->B,A->C\n\n- E-R模型向关系模型转换时，两个以上实体之间**多对多的联系应该转换为一个独立的关系模式**，且该关系模式的关键字由这些实体的关键字组成，也需要重新建类，且该关系模式的关键字由这些实体的关键字组成\n\n- 数据库通常采用三级模式结构，其中，视图对应外模式、基本表对应模式、存储文件对应内模式\n\n- 数据库管理系统利用日志文件来进行事务故障恢复和系统故障恢复。在事务处理过程中，DBMS把事务开始、事务结束以及对数据库的插入、删除和修改的每一次操作写入**日志文件**。当系统正常运行时，按一定的时间间隔，把**数据库缓冲区内容写入数据文件**；一旦发生故障，DBMS的恢复子系统利用日志文件撤销事务对数据库的改变，回退到事务的初始状态\n\n- **排他锁**又称为写锁，用于对数据进行写操作时进行锁定。如果事务T对数据A加上排他锁后，就只允许事务T读取和修改数据A，其他事务对数据A不能再加任何锁，从而也不能读取和修改数据A，直到事务T释放A上的锁\n**共享锁**又称为读锁，用于对数据进行读操作时进行锁定。如果事务T对数据A加上了读锁后，事务T就只能读数据A但不可以修改，其他事务可以再对数据A加读锁来读取，只要数据A上有读锁，任何事务都只能再对其加读锁（读取）而不能加写锁（修改）\n\n- ①**分片透明**：是指用户不必关系数据是如何分片的，它们对数据的操作在全局关系上进行，即关系如何分片对用户是透明的，因此，当分片改变时应用程序可以不变。分片透明性是最高层次的透明性，如果用户能在全局关系一级操作，则数据如何分布，如何存储等细节自不必关系，其应用程序的编写与集中式数据库相同。\n②**复制透明**：用户不用关心数据库在网络中各个节点的复制情况，被复制的数据的更新都由系统自动完成。在分布式数据库系统中，可以把一个场地的数据复制到其他场地存放，应用程序可以使用复制到本地的数据在本地完成分布式操作，避免通过网络传输数据，提高了系统的运行和查询效率。但是对于复制数据的更新操作，就要涉及到对所有复制数据的更新。\n③**位置透明**：是指用户不必知道所操作的数据放在何处，即数据分配到哪个或哪些站点存储对用户是透明的\n④**局部映像透明性（逻辑透明）**：最低层次的透明性，该透明性提供数据到局部数据库的映像，即用户不必关系局部DBMS支持哪种数据模型、使用哪种数据操纵语言，数据模型和操纵语言的转换是由系统完成的。因此，局部映像透明性对异构型和同构异质的分布式数据库系统是非常重要的\n\n- DDBS的基本特点：1.物理分布性：数据不是存储在一个场地上，而是存储在计算机网络的多个场地上。2.逻辑整体性：数据物理分布在各个场地，但逻辑上是一个整体，它们被所有用户（全局用户）共享，并由一个DDBMS统一管理。3.场地自治性：各场地上的数据由本地的DBMS管理，具有自治处理能力，完成本场地的应用（局部应用）。4.场地之间协作性：各场地虽然具有高度的自治性，但是又相互协作构成一个整体\n\n- 1NF:数据库表的每一列都是不可分割的基本数据项，同一列中不能有多个数据值，不能再一列中存放多个属性，例如员工信息表，不能再信息一列中放入电话住址等信息，应单独设计成电话一列，住址一列\n2NF：每个表必须有主键，其他信息与主键一一对应，通常称这种关系为函数依赖关系，即表中其他数据元素都依赖于主键，或称该数据元素唯一的被主键所标识，例如学生表（id，姓名，成绩，合格状态）合格状态不依赖于学生信息依赖于成绩，不符合第二范式\n3NF：要求一个数据库表中不包含已在其他表中已包含的非主关键字信息，例如学生表（id，姓名，班级id，班级位置）班级表（id，名称，位置）通过学生表的班级id可以得到班级位置，无需在学生表中增加班级位置字段\nBCNF：所有非主属性对每一个候选键都是完全函数依赖，所有的主属性对每一个不包含他的候选键，也是完全函数依赖，没有任何属性完全函数依赖于非候选键的任何一组属性\n\n- ①原子性（Atomicity）：事务是原子的，要么做，要么都不做。\n②一致性（Consistency）：事务执行的结果必须保证数据库从一个一致性状态变到另一个一致性状态。\n③隔离性隔离性（lsolation）：事务相互隔离。当多个事务并发执行时，任一事务的更新操作直到其成功提交的整个过程，对其它事物都是不可见的。\n④持久性（Durability）：一旦事务成功提交，即使数据库崩溃，其对数据库的更新操作也永久有效\n\n- R1▷◁R2为自然联接，自然联接是一种特殊的等值联接，它要求两个关系中进行比较的分量必须是相同的属性，并且在结果集中将重复属性列去掉\n\n- **派生属性**：学生表中有生日和年龄字段，根据生日可以推算出年龄，所以年龄属于派生属性\n**简单属性**：与复合属性相对的，就是简单属性\n**多值属性**：一个人有多个电话号码，多个爱好，多个亲属，这些属于多值属性\n**复合属性**：可以拆分的属性就是属于复合属性，例如家庭地址可以拆分为省县乡门牌号字段\n\n- 给定关系模式R（A1，A2，A3，A4），函数依赖F（A1A3->A2，A2->A3），将R分解为{（A1，A2）（A1，A3）}则该分解为有损连接且不保持函数依赖。能够推出**所有属性且不含多余属性**的属性组称为候选码。由于A1A3→A2，根据函数依赖的性质，可知属性组A1A3决定属性A1、A2、A3，但它不能成为R的候选码，因为还有一个属性A4，A1A3不能决定它。因此，R的候选码为A1A3A4。在分解p中，我们发现少了属性A4，而且把两个函数依赖都丢了，因为关系(A1，A2)没覆盖函数依赖集F中任何一个函数依赖，关系(A1，A3)亦如此，所以分解p既是有损连接又不保持函数依赖\n\n- 首先判断候选码，先找入度为0的结点，本题中A1**没有在函数依赖右侧出现**，因此体现在图示中，即入度为0，候选码必定包含属性A1。\n\n- π1,5,7 ->（Φ2=5）即对R × S结果第1、5、7列的投影，对应属性R.A、S.B、S.E；FROM R,S后跟随的是结果元组行的WHERE筛选条件，即对R × S结果选择第2列=第5列的元组，对应属性为R.B=S.B\n\n\n#### 算法与数据结构※\n\n- ①图的存储结构**邻接表**对每个点和每条边都要遍历一遍，时间复杂度O(n（节点）+e（边)），**邻接矩阵**时间复杂度O(n^2)\n②**无向图的邻接矩阵**是一个对称矩阵，每条边会表示两次，因此矩阵中的非零元素数目为**2e**，**有向图**使用邻接矩阵存储，矩阵中的非零元素数目为**e**\n③完全图适合采用邻接矩阵存储。\n④无向连通图只保证每对结点间都有路径。\n⑤顶点数为n，变数为e，对于无向图中的两个顶点u和v，若存在边（u，v），则该边为计算U的度和V的度各贡献一个值1，因此，所有顶点的度数之和为e的两倍\n\n- ①图的遍历是指对图中所有顶点进行访问且只访问一次的过程。因为图的任一个结点都可能与其余顶点相邻接，所以在访问了某个顶点之后，可能沿着某路径又回到该结点上。因此为了避免顶点的重复访问，在图的遍历过程中，必须对已访问过的顶点进行标记。\n②深度优先遍历和广度优先遍历是两种遍历图的基本方法。**广度优先遍历**方法为：从图中某个顶点V出发，在访问了v之后依次访问v的各个未被访问过的邻接点，然后分别从这些邻接点出发依次访问它们的邻接点，并使“先被访问的顶点的邻接点”先于“后被访问的顶点的邻接点”被访问，直至图中所有己被访问的顶点的邻接点都被访问到。若此时还有未被访问的顶点，则另选图中的一个未被访问的顶点作为起点，重复上述过程，直至图中所有的顶点都被访问到为止。广度优先遍历图的特点是尽可能先进行横向搜索，即最先访问的顶点的邻接点也先被访问。为此，**引入队列来保存己访问过的顶点序列**，即每当一个顶点被访问后，就将其放入队中，当队头顶点出队时，就访问其未被访问的邻接点并令这些邻接顶点入队\n\n- ①**完全二叉树**：除最后一层外，每一层上的节点数均达到最大值；在最后一层上只缺少右边的若干结点，去掉最后一层是满二叉树，在形态上是平衡二叉树，完全二叉树的高度h与其节点数n之间存在确定的关系\n②**平衡二叉树（AVL）**：它或者是一颗空树，或者具有以下性质的二叉树：它的左子树和右子树的深度之差的绝对值不超过1，且它的左子树和右子树都是一颗平衡二叉树\n③**最优二叉树**：哈夫曼树（权值最小的两个节点互为兄弟节点）二叉树的节点总数2n-1（n个权值），哈夫曼树中叶子节点的权值越小则距离树根越远，叶子节点的权值越大则距离树根越近\n④**满二叉树**：每一层上的节点数均达到最大值\n\n- 二叉树遍历：前序遍历-根左右，中序遍历-左根右，后序遍历-左右根，前序遍历和后序遍历不能够造出二叉树的中序遍历序列\n\n- ①归并排序：在最坏情况下，时间复杂度是O（nlogn）；空间复杂度O(n)，采用**分治法**\n②选择排序：不稳定性。每一轮选出最小者交换到左侧，最大优势省去了多余的元素交换，时间复杂度O(n^2),空间复杂度O(1)\n③插入排序：维护一个有序区，把元素一个一个插入到有序区的适当位置，直到所有元素有序为止，时间空间复杂度On、O1，最坏的情况下时间复杂度为O(n^2),空间复杂度为O(1)\n④**快速排序**：冒泡排序演化而来，用了**分治法**的思想，冒泡排序在每一轮只把一个元素冒泡到数列的一端，而快速排序在每一轮挑选一个基准元素，并让其他比它大的元素移动到数列一边，比它小的元素移动到数列的另一端，从而把数列拆解成了两个部分,时间复杂度平均O(nlogn)最坏O(n^2)，空间复杂度O(1)\n⑤鸡尾酒排序：冒泡排序的优化适合大部分元素已经有序\n⑥希尔排序：直接插入排序的升级版，预处理数据使元素变成基本有序,时间复杂度O(n^1.3),空间复杂度O(1)\n⑦输入数组{1，1，2，4，7，5}基本有序（从小到大），在这种情况下，插入排序算法的时间复杂度为0（n），归并排序和堆排序的时间复杂度为0（nlgn），而快速排序的时间复杂度为0（n2）\n⑧ 插入排序在输入数据基本有序的情况下，是其计算时间的最好情况，复杂度为O（n），其他情况下时间复杂度为O（n2）。快速排序在输入数据有序或者逆序的情况下，是其计算时间的最坏情况，复杂度为O（n2），其他情况下时间复杂度为O（nlgn）。而归并排序和堆排序算法在所有情况下的时间复杂度均为O（nlgn）\n\n- ①**分治法**：对于一个规模为n的问题，若该问题可以容易地解决（比如说规模n较小）则直接解决；否则将其分解为k个规模较小的子问题，这些子问题互相独立且与原问题形式相同，递归地解这些子问题，然后将各子问题的解合并得到原问题的解。\n②**动态规划法**：对于每一步的决策，列出各种可能的局部解，在依据某种判定条件，舍弃那些肯定不能得到最优解的局部解，在每一步都经过筛选，以每一步都是最优解来保证全局是最优解。动态规划算法与分治法不同的是，适合于用动态规划求解的问题，经分解得到子问题往往不是互相独立的。若用分治法来解这类问题，则分解得到的子问题数目太多，有些子问题被重复计算了很多次。如果能够保存已解决的子问题的答案，而在需要时再找出已求得的答案，这样就可以避免大量的重复计算，节省时间。可以用一个表来记录所有已解的子问题的答案。不管该子问题以后是否被用到，只要它被计算过，就将其结果填入表中。这就是动态规划法的基本思路。\n③**贪心算法**：它是一种不追求最优解，只希望得到较为满意解的方法。贪心算法一般可以快速得到满意的解，因为它省去了为找到最优解而穷尽所有可能所必须耗费的大量时间。贪心算法常以当前情况为基础做最优选择，而不考虑各种可能的整体情况，所以贪心算法不要回溯。例：地杰斯特拉算法、背包算法，贪心选择是指所求问题的整体最优解可以通过一系列**局部最优的选择**，即贪心选择来达到。这是贪心算法可行的第一个基本要素，也是贪心算法与动态规划算法的主要区别。\n④**回溯算法（试探法）**：它是一种系统地搜索问题的解的方法。回溯算法的基本思想是：从一条路往前走，能进则进，不能进则退回来，换一条路再试。其实现一般要用到递归和堆栈。例：皇后问题，回溯算法实际上一个类似枚举的搜索尝试过程，主要是在搜索尝试过程中寻找问题的解，当发现已不满足求解条件时，就“回溯”返回，尝试别的路径。回溯法是一种选优搜索法，按选优条件向前搜索，以达到目标。但当探索到某一步时，发现原先选择并不优或达不到目标，就退回一步重新选择，这种走不通就退回再走的技术为回溯法，而满足回溯条件的某个状态的点称为“回溯点”。\n\n- 节点的**度**定义为节点的子树数目\n\n- ①栈和队列都是**操作受限**的线性表：栈仅在表尾插入和删除元素；队列仅在表头删除元素、在表尾插入元素。\n②采用单循环链表表示队列：入队时初始队列为空、出队后队列变为空要进行特殊处理。入队操作和出队操作均与队列长度无关，因此其时间复杂度都为O1。\n③队列是先入先出的线性表，栈是后进先出的线性表。**一个线性序列经过队列结构后只能得到与原序列相同的元素序列，而经过一个栈结构后则可以得到多种元素序列。用两个栈可以模拟一个队列的入队和出队操作。若入栈和入队的序列相同，则出栈序列和出队序列可能相同，出栈序列和出队序列可能互为逆序，入队序列与出队序列关系为1:1，而入栈序列与出栈序列关系是1：n（n>=1）**\n④使用单链表作为栈的存储结构，因为栈的操作是先进后出，因此**无论是入栈还是出栈，都只对栈顶元素操作，而在单链表中用头指针作为栈顶指针，此时无论是出栈还是入栈，都只需要对头指针指向的栈顶指针操作即可，不需要遍历链表**\n⑤二分查找在链表结构上无法实现\n⑥优先队列是一种常用的数据结构，通常用堆实现。对应于大顶堆和小顶堆，存在最大优先队列和最小优先队列。以最大优先队列为例，优先队列除了具有堆上的一些操作，如调整堆、构建堆之外，还有获得优先队列的最大元素，抽取出优先队列的最大元素，向优先队列插入一个元素和增大优先队列中某个元素的值。其中除了获得优先队列的最大元素的时间复杂度为O1之外，其他几个操作的时间复杂度均为二叉树的高度，即O（lgn）\n\n\n#### 面向对象技术\n\n- 里程碑关键路是开始到结束的最长路径，代表最短周期\n\n- 多对多联系需要单独转换为一个关系模式，也需要重新建类\n\n- **面向对象分析**主要强调理解问题是什么，不考虑问题的解决方案\n**面向对象设计**侧重问题的解决方案，并且需要考虑实现细节问题。\n\n- ①开-闭原则是指一个软件实体应当对**扩展**开放，对**修改**关闭\n②里氏代换原则（LSP）是指一个软件实体如果使用的是—个**基类**的话，那么一定适用于其**子类**，而且软件系统觉察不出基类对象和子类对象的区别,原则：子类可以替换父类；\n③依赖倒转原则（DIP）就是要依赖于**抽象**，而不依赖于**实现**，或者说要针对接口编程，不要针对实现编程。\n④单一职责原则：设计目的单一的类\n⑤接口隔离原则：使用多个专门的接口比使用单一的总接口要好\n\n- 不同的对象在接收统一消息时可以产生完全不同的结果，这一现象称为多态，由继承机制实现。多态有参数多态、包含多态、过载多态和强制多态四类。\n①参数多态：是应用比较广泛的多态，被称为最纯的多态List list;list.add(HashMap);list.add(ConcurrentHashMap)\n②包含多态：在许多语言中都存在，最常见的例子就是子类型化，即一个类型是另一个类型的子类型。List list = new ArrayList;\n③过载多态：是同一个名字在不同的上下文中所代表的含义不同(重载)，同一个名字在不同上下文中可代表不同的含义\n④强制多态：int+double\n\n- ①**泛化**是一个类与它的一个或多个细化类之间的关系，表达一般与特殊的关系。C++使用':'，java使用extends来表示泛化关系\n②**关联**是类与类之间的一种结构关系。它描述了一组链，链是对象之间的连接,**比依赖关系更强，不存在依赖关系的偶然性，关系也不是临时性一般是长期性**,两个类之间可以**有多个由不同角色标识**的关联\n③**依赖**是两个事物间的语义关系，其中一个事物（独立事物）发生变化会影响另一个事物（依赖事物）的语义，特定事物的改变有可能影响到其他事物，假设A类变化引起了B类变化，说明B类依赖于A类.若类A的方法中仅仅使用了类B的对象，那么类A依赖于类B。\n④**聚合**关系表示整体和部分的关系，整体和部分可分开，若类A对象消失时，其他类的实例仍然存在并工作，那么就是聚合。\n⑤**组合**是一种聚合关系，但是整体与部分不可分开，其中整体负责其部分的创建和销毁，如果整体不存在了，部分也将不存在。如果类A的部分是由类B的对象组成，并且类A控制类B的生命周期，那么类A与类B是组合关系，类A对象消失时，类B也消失，就是组合关系\n⑥**实现**关系是用来规定接口和实现类或者构建结构的关系\n\n做题时查看是否出现变化或者消失的字眼，出现变化的话，A变化不会影响B变化就是关联关系，A变化会影响B变化就是依赖关系，出现消失的话，A消失B也消失就是组合关系，A消失B不会消失就是聚合关系\n\n\n- **实体类**:主要负责数据和业务逻辑；\n**边界类（接口类）**:负责和用户进行交互，即用户界面\n**控制类**:则负责实体类和界面类的交互\n\n- 1.**类图**展现了一组对象、接口、协作和它们之间的关系。在开发软件系统时，类图用于对系统的静态设计视图建模\n2.**对象图**展现了一组对象以及其之间的关系，描述了在类图中所建立的事物的实例的静态快照。\n3.**序列图**是场景的图形化表示，描述了以时间顺序组织的对象之间的交互活动。对象图的对象名会有标识，关联关系一般不会出现多重度。同步消息用实心三角箭头表示\n4.**通信图**和序列图同构，强调收发消息的对象的结构组织。\n5.**状态图**展现了一个状态机，由状态、转换、事件和活动组成，它关注系统的动态视图，强调对象行为的事件顺序。事件触发一个没有特定监护条件的迁移，对象不一定离开当前状态\n6.**活动图**是一种特殊的状态图，展现了在系统内从一个活动到另一个活动的流程，它专注于系统的动态视图。适合对业务流程进行进一步建模。\n序列图、通信图、交互图和定时图均被称为交互图，它们用于对系统的动态方面进行建模\n7.**类图**（Class Diadram）展现了一组对象、接口、协作和它们之间的关系。在面向对象系统的建模中，最常见的就是类图，它给出系统的静态设计视图。\n8.**组件图**（Component Diagram）展现了一组组件之间的组织和依赖。专注于系统的静态实现视图，与类图相关，通常把组件映射为一个或多个类、接口或协作。\n9.**通信图**（communication diagram）。通信图也是一种交互图，它强调收发消息的对象或参与者的结构组织。\n10.**部署图**（Deploy Diagram）是用来对面向对象系统的物理方面建模的方法，展现了运行时处理结点以及其中构件（制品）的配置，部署图给出了体系结构的静态实施视图。它与构件图相关，通常一个节点包含一个或多个构件，依赖关系类似于包依赖，软件系统中软件组件和硬件组件之间的物理关系通常采用UML中的部署图。\n11.**对象图**（Object Diagram）展现了某一时刻一组对象以及它们之间的关系。对象图描述了在类图中所建立的事物的实例的静态快照，给出系统的静态设计视图或静态进程视图。\n12.**用例图**（Use Case Diagram）展现了一组用例、参与者（Actor）以及它们之间的关系。这个视图主要支持系统的行为，即该系统在它的周边环境的语境中所提供的外部可见服务。用例图用于对一个系统的需求进行建模，包括说明这个系统应该做什么（从系统外部的一个视点出发），而不考虑系统应该怎样做。\n13.**状态图**可以了解到一个对象所能到达的所有状态以及对象收到的事件（消息、超时、错误、条件满足等）对对象状态的影响等\n14.**序列图**是强调消息时间顺序的交互图；\n15.**时序图**（Timing Diagram）关注沿着线性时间轴、生命线内部和生命线之间的条件改变。\n16.**交互概览图**强调控制流的交互图。交互图用于对系统的动态方面进行建模。一张交互图表现的是一个交互，由一组对象和它们之间的关系组成，包含它们之间可能传递的消息。交互图表现为序列图、通信图、交互概览图和时序图，每种针对不同的目的，能适用于不同的情况。\n\n- UML2.0中提供了多种图形，从静态和动态两个方面表现系统视图。**静态方面有类图，对象图**，**序列图、通信图、交互图和定时图均被称为交互图，它们用于对系统的动态方面进行建模**\n\n- UML中引入同步的概念，用同步棒--黑色粗线条表示+并发分支与会和\n\n- 面向对象分析包含5个活动：认定对象、组织对象、描述对象间的相互作用、定义对象的操作、定义对象的内部信息\n\n- 1.**命令（Command）模式**通过**将请求封装为一个对象**，可将不同的请求对客户进行参数化从而使使用者可以釆用不同的请求对客户进行参数化；对请求排队或记录请求日志，以及支持可撤销的操作。\n2.**策略（Strategy）设计模式**定义一系列算法，把它们一个个封装起来，并且使它们可相互替换。这一模式使得算法可独立于它的客户而变化。\n3.**抽象工厂（Abstract Factory）模式**提供一个创建一系列相关或相互依赖对象的接口，而无需指定他们具体的类。\n4.**状态（State）模式**是使得一个对象在其**内部状态改变时通过调用另一个类中的方法改变其行为**，使这个对象看起来如同修改了它的类。\n5.**责任链（Chain of Responsibility）模式**将多个对象的请求连成一条链，并沿着这条链传递该请求，直到有一个对象处理它为止，避免请求的发送者和接收者之间的耦合关系。是行为型对象模式\n6.**观察者（Observer）模式**定义对象之间的一种一对多的依赖关系，当一个对象的状态发生改变时，所有依赖于它的对象都得到通知并被自动更新。Subject（目标）知道他的观察者，Observer（观察者）为那些在目标发生改变时需获得通知的对象定义一个更新接口。ConcreteSubject（具体目标）将有关状态存入各ConcreteObserver（具体观察者）对象，当它的状态发生改变时，向它的各个观察者发出通知，ConcreteObserver（具体观察者）维护一个指向ConcreteSubject（具体目标）对象的引用，存储有关状态，实现Observer（观察者）的更新接口以使自身状态与目标的状态保持一致。\n7.**享元设计模式**是运用共享技术来有效的支持大量细粒度的对象。\n8.**外观（Facade）模式**为子系统中的**一组接口提供一个一致的界面**，Facade模式定义了一个高层接口，这个接口使得这一子系统更加容易使用。适用于需要为一个复杂子系统提供一个简单接口的情况\n9.**适配器（Adapter）模式**是将类的接口**转换成客户希望的另外一个接口**，使得原本由于接口不兼容而不能一起工作的那些类可以一起工作。是一种结构性模式。将已有类的接口转换成和目标接口兼容\n10.**桥接模式**将对象的**抽象和其实现分离**，从而可以独立地改变它们。\n11.**组合（Composite）模式**描述了如何**构造一个类层次式结构**，来表示部分-整体的层次结构。\n12.**装饰器（Decorator）模式**的意图是**动态地给一个对象添加一些额外职责**。在需要给某个对象而不是整个类添加一些功能时使用。这种模式对增加功能比生成子类更加灵活\n13.**访问者模式**系统要求这些对象实施一些依赖于某具体类（Checkout）的操作时，可以使用此模式\n14.**迭代器模式（Iterator）**：提供一种方法来**顺序访问**一个聚合对象中的各个元素，而不需要暴露该对象的内部表示。是行为型对象模式\n15.**解释器模式（Interpreter）**：给定一种语言，定义它的文法表示，并定义一个解释器，该解释器用来根据文法表示来解释语言中的句子。\n16.**生成器（Builder）模式**将一个复杂对象的构建与它的表示分离，使得同样的构建过程可以创建不同的表示。生成器模式适用于以下几种情况：①当创建复杂对象的算法应该独立于该对象的组成部分以及它们的装配方式时；②当构造过程必须允许被构造的对象有不同的表示时。工厂方法（Factory Method）定义一个用于创建对象的接口，让子类决定将哪一个类实例化，使一个类的实例化延迟到其子类。工厂方法适用于以下几种情况：①当一个类不知道它所必须创建的对象的类的时候；②当一个类希望由它的子类来指定它所创建的对象的时候；③当类将创建对象的职责委托给多个帮助子类中的某一个，并且你希望将哪一个帮助子类是代理者这一信息局部化的时候。\n17.**原型（Prototype）模式**用原型实例指定创建对象的种类，并且通过拷贝这个原型来创建新的对象。原型模式适用于以下几种情况：①当一个系统应该独立于它的产品创建、构成和表示时；②当要实例化的类是在运行时刻指定时，例如，通过动态装载；③为了避免创建一个与产品类层次平行的工厂类层次时；④当一个类的实例只能有几个不同状态组合中的一种时，建立相应数目的原型并克隆它们可能比每次用合适的状态手工实例化该类更方便一些。\n18.**单例（Singleton）设计模式**是一种创建型模式，其意图是保证一个类仅有一个实例，并提供一个访问这个唯一实例的全局访问点。单例模式适用于以下情况：①当类只能有一个实例而且客户可以从一个众所周知的访问点访问它时；②当这个唯一实例应该是通过子类化可扩展的，并且客户应该无须更改代码就能使用一个扩展的实例时\n\n- 事物：模型中的基本成员。UML中包括结构事物、行为事物、分组事物和注释事物。\n①结构事物：模型中静态部分。[类Class]+[接口Interface]+[协作Collaboration]+[用例UseCase]+[活动类]+[组件Component]+[结点Node]\n②行为事物：模型中的动态部分。[交互]+[状态机]\n③分组事物：可以把分组事物看成是一个“盒子”，模型可以在其中被分解。目前只有一种分组事物，即包（Package）。结构事物、动作事物、甚至分组事物都有可能放在一个包中。包纯粹是概念上的，只存在于开发阶段，而组件在运行时存在。\n④注释事物：注释事物是UML模型的解释部分\n\n- 绑定是一个把过程调用和响应调用所需要执行的代码加以结合的过程。在一般的程序设计语言中，**绑定是在编译时进行的，叫做静态绑定**。动态绑定则是在运行时进行的，因此，一个给定的过程调用和代码的结合直到调用发生时才进行。动态绑定是和类的继承以及多态相联系的。**在运行过程中，当一个对象发送消息请求服务时，要根据接收对象的具体情况将请求的操作与实现的方法进行连接，即动态绑定**\n\n\n#### 标准化和知识产权\n\n- 著作权中的署名权、修改权、保护作品完整权的保护期不受限制。发表权保护期受时间限制。\n- 商标权有可能无限期拥有的知识产权，保护期限可以延长\n- 软件著作权中翻译权是指以不同于原软件作品的一种程序语言转换该作品原使用的程序语言，而重现软件作品内容的创作的产品权利。简单地说，也就是指将原软件从一种程序语言转换成另一种程序语言的权利\n- 客户提供工具软件的复制品，这里侵犯了工具软件的软件著作权\n\n\n---\n\n\n\n#### 下午题\n\n数据库设计\n\n数据流图-实体联系图\n\n1.使用说明中的语句对图1-1中的实体E1-E4的名称\n\n2.使用说明中的语句对图1-2中的数据存储D1-D3的名称\n\n3.很据注明和图中术语，补齐图1-2中缺失的数据及起点和终点\n\n4.根据说明，采用结构化语言对逻辑进行描述\n\n结构化分析将数据和处理作为分析对象，数据的分析结果表示了现实世界中实体的属性及其之间的相互关系，而处理的结果则展现了系统对数据的加工和转换。面向数据流建模是目前仍然被广泛使用的方法之一，而DFD则是面向数据流建模中的重要工具，DFD将系统建模成输入一处理一输出的模型，即流入软件的数据对象，经由处理的转换，最后以结果数据对象的形式流出软件。在实际使用DFD进行数据流建模时，需要注意以下原则：①加工处理和数据流的正确使用，如一个加工必须既有输入又有输出；数据流只能和加工相关，即从加工流向加工、数据源流向加工或加工流向数据源。②每个数据流和数据存储都要在数据字典中有定义，数据字典将包括各层数据流图中数据元素的定义。③数据流图中最底层的加工处理必须有加工处理说明。④父图和子图必须平衡，即父图中某加工的输入输出（数据流）和分解这个加工的子图的输入输出数据流必须完全一致，这种一致性不一定要求数据流的名称和个数一一对应，但它们在数据字典中的定义必须一致，数据流或数据项既不能多也不能少。⑤加工处理说明和数据流图中加工处理涉及的元素保持一致。例如，在加丄处理说明中，输入数据流必须说明其如何使用，输出数据流说明如何产生或选取，数据存储说明如何选取、使用或修改。⑥一幅图中的图元个数控制在7+2以内。在题目所示的DFD图中，数据流DF2、DF6和DF7的输入、输出均不是加工，这与“数据流只能和加工相关，即从加工流向加工、数据源流向加工或加工流向数据源”相违背。加工P1只有输出，没有输入；加工P3只有输入没有输出，这与“一个加工必须既有输入又有输出”相违背。数据流DF4经过加工P4之后没有发生任何改变，说明该数据对加工P4是没有作用的，根据数据守恒原理，这条数据流不应与P4有关联。","tags":["计算机"]},{"title":"Java各类技术栈架构图汇总","slug":"Java各类技术栈架构图汇总","url":"/blog/posts/d1707b62ed37/","content":"\n## Java各类技术栈架构图汇总\n\n### 1. java类加载器架构\n\n![](images/java类加载器架构.png)\n\n### 2. JVM架构\n\n![](images/jvm架构.png)\n\n![](images/运行时数据区.png)\n\n![](images/类文件.png)\n\n### 3. Java技术体系\n\n![](images/java技术体系.png)\n\n### 4. 线程运行架构\n\n![](images/线程运行架构.png)\n\n### 5. Java体系(编译与运行)结构\n\n![](images/java体系结构.png)\n\n### 6. JMS技术架构\n\n![](images/jms技术架构1.png)\n\n![](images/jms技术架构2.png)\n\n### 7. JMX技术架构\n\n![](images/JMX技术架构1.png)\n\n![](images/JMX技术架构2.png)\n\n### 8. J2EE\n\n![](images/j2ee.png)\n\n## 框架\n\n### 1. Spring架构\n\n![](images/spring架构1.png)\n\n![](images/spring架构2.png)\n\n![](images/spring架构3.png)\n\n### 2. Hibernate架构\n\n![](images/hibernate架构1.png)\n\n![](images/hibernate架构2.png)\n\n![](images/hibernate架构3.png)\n\n### 3. ibatis架构\n\n![](images/ibatis架构.png)\n\n### 4. Struts2架构\n\n![](images/Struts2架构.png)\n\n### 5. Struts1架构\n\n![](images/Struts1架构.png)\n\n### 6. JBPM\n\n![](images/jbpm.png)\n\n### 7. EJB技术架构\n\n![](images/ejb技术架构1.png)\n\n![](images/ejb技术架构2.png)\n\n### 8. Portal\n\n![](images/portal.png)\n\n### 9. SmartClient Ajax框架架构\n\n![](images/SmartClientAjax框架架构.png)\n\n## 数据库\n\n### 1. Oracle\n\n![](images/oracle.png)\n\n### 2. MYSQL架构\n\n![](images/mysql.png)\n\n## 其他\n\n### 1. Android架构\n\n![](images/android架构.png)\n\n### 2. 云计算架构\n\n![](images/云计算架构.png)\n\n### 3. LINUX内核\n\n![](images/LINUX内核.png)\n\n### 4. vmware架构\n\n![](images/vmware架构1.png)\n\n![](images/vmware架构2.png)\n\n### 5. 门户网站应用架构蓝图\n\n![](images/门户网站应用架构蓝图.png)\n\n### 6. SOA技术架构\n\n![](images/soa技术架构.png)\n\n### 7. MIS技术架构\n\n![](images/mis技术架构.png)\n\n### 8. APUSIC ESB技术架构\n\n![](images/APUSIC-ESB技术架构1.png)\n\n![](images/APUSIC-ESB技术架构2.png)\n\n### 9. GIS esb技术架构\n\n![](images/GIS-esb技术架构.png)\n\n### 10. CRM方案架构\n\n![](images/crm方案架构.png)\n\n### 11. OA架构\n\n![](images/oa架构1.png)\n\n![](images/oa架构2.png)\n\n---\n\n- [来源](https://blog.csdn.net/qq_37651267/article/details/95244623)","categories":["Java"]}]